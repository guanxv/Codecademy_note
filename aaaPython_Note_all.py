

# 10 tip and tricks ---------- from corey --------------

#1.-----------

condition = True

x = 1 if condition else 0

print (x)

#2.--------------

num1 = 10_000_000_000
num2 = 100_000_000

total = num1 + num2

print (f'{total:,}')


#3.--------resource manage--------

with open('test.txt', 'r') as f:
	file_content = f.read()

#instead of doing

f = open('test.txt', 'r')
file_content = f.read()
f.close()

#4.-------------------------

names = ['corey', 'chris', 'dave', 'travis']

for index, name in enumerate(names, start = 1):
	print (index, name)

#5.-------------------

names = ['Peter Parker', 'Clark Kent', 'Wade Wilson', 'Bruce Wayne']
heroes = ['Spiderman', 'Superman', 'Deadpool', 'Batman']

for names, heroes in zip(names, heroes):
	print(f'{name} is actually {hero}')
	
# zip can also used for more than 2 lists
# zip will stop at shortest list

#6.----unpack values----------------

a, b = (1, 2)

print(a)
print(b)


a, _ = (1, 2)

print(a)
#print(b)

# avoid IDE to give waring of not using Varialbe b

a, b , c = (1, 2)

a, b , c = (1, 2 ,3 ,4, 5)

# both will raise error

a, b , *c = (1, 2 ,3 ,4, 5)

#a = 1
#b = 2
#c = [3,4,5]

a, b , *_ = (1, 2 ,3 ,4, 5)

#a = 1
#b = 2


a, b , *c, d = (1, 2 ,3 ,4, 5)

#a = 1
#b = 2
#c = [3, 4]
#d = 5

#7--------------------------------
class Person():
	pass

person = Person()

#you can set the attribute for class by doing this

person.first = 'Corey'
person.last = 'Schafer'

#you can also do this

first_key = 'first'
first_val = 'Corey'

setattr(person, 'first', 'Corey') #or to use the variable
setattr(person, first_key, first_val)

#to get the value

first = getattr(person, first_key)

#for the code could look like this
class Person():
	pass

person = Person()

person_info = {'first': 'Corey', 'last' : 'Schafer'}

for key, value in person_info.items():
	setattr(person, key, value)

for key in person_info.keys():
	print(getattr(person, key))


#8.---------------------------------------

username = input('Username:')
password = input('Password:')

print('Logging In...')

#to hide the password inputing on screen

from getpass import getpass

username = input('Username:')
password = getpass('Password:')

print('Logging In...')

#9---------------------------
#in command line
python password.py
python -m password # system will search to find the moudel

python -m password -c Debuge # -c will be for the password 

#10--------------------------

#study more on moudle

# in command line type python

#>>> import datetime
#>>> help(datetime)

#>>> dir(datetime)
# to list all the method and attribute


#>>> datetime.fold
#>>> datetime.max
#>>> datetime.today
# to understand the method and attribur

#>>> datetime.today()
# to use the method


#---------------------------------------------------------------------------------
#Error Handling-------------------------------------------------------------------
#---------------------------------------------------------------------------------

try:
	pass

except Exception:
	pass

else:
	pass

finally:
	pass
	
#sample 

try:
	f = open('text.txt') #this file did not exist.
	var = bad_var #bad_var is not defined

except BadVarError:
	print('sorry. This file does not exist')
	
except FileNotFoundError:
	print('sorry. This file does not exist')

except Exception: #try to put more specific error on top , more general error at bottom
	print('sorry. something went wrong') 	
	
#Or this code can be

try:
	f = open('text.txt') #this file did not exist.
	var = bad_var #bad_var is not defined

except BadVarError as e:
	print(e)
	
except FileNotFoundError as e:
	print(e)

except Exception as e:
	print(e) 
	
# explain Else and Finally

try:
	f = open('text.txt') #this file did not exist.
	var = bad_var #bad_var is not defined

except BadVarError as e:
	print(e)
	
except FileNotFoundError as e:
	print(e)

except Exception as e:
	print(e) 

else: # if no error happend
	print(f.read())
	f.close()

finally: # else only run when no error happend, finally will run no mater what every happend
	print('Executing Finally...')
	
#raise your own exception

if f.name == 'currupt_file.txt':
	raise Exception
    

#raise your own exception2    

def sum_to_one(n):
  if n < 0:
    SumToOneException("0 or Positive Numbers Only!")
  if n <= 1:
    return n
  return n + sum_to_one(????)

    
    
	
#----Function sample---------------------------------------------

#Write your function here
def double_index(lst, index):
  if index >= len(lst):
    return lst
  else:
    new_lst = lst[0:index]
    new_lst.append(lst[index]*2)
    new_lst = new_lst + lst[index+1:]
    return new_lst
  
def double_index_gux(lst, index):
  if index -1 <= len(lst):
    new_lst = lst
    new_lst[index] = new_lst[index] * 2
    return new_lst
  else:
    return lst

print(double_index_gux([3, 8, -10, 12], 2))

#Uncomment the line below when your function is done
print(double_index([3, 8, -10, 12], 2))
	
#------------ simple loop -------------------------
#Write your function here
def append_sum(lst):
  for i in range(3):
    new = lst[-1] + lst [-2]
    lst.append(new)
  return lst

#Uncomment the line below when your function is done
print(append_sum([1, 1, 2]))

#-------------LAMBDA LAMBDA LAMBDA ---------------------------------

#Write your function here

#def larger_list(lst1, lst2):
#  if len(lst1) >= len(lst2):
#    return lst1[-1]
#  else:
#    return lst2[-1]

larger_list = lambda lst1, lst2 : lst1[-1] if len(lst1) >= len(lst2) else lst2[-1]


#Uncomment the line below when your function is done
print(larger_list([4, 10, 2, 5], [-10, 2, 5, 10]))
	
#-----------------------------------------

lst.sort()

new_lst = sorted(lst)

#--------------------------------------

#Write your function here
every_three_nums = lambda start: list(range(start, 101, 3))

#def every_three_nums(start):
#  x = list(range(start, 101, 3))
#  return x


#Uncomment the line below when your function is done
print(every_three_nums(91))

#--------------------------------------

items_on_sale = ["blue_shirt", "striped_socks", "knit_dress", "red_headband", "dinosaur_onesie"]

print("Checking the sale list!")
for item in items_on_sale:
  print(item)
  if item == "knit_dress":
    AQ3WCTF,V"
	SZEX/JX. H,GHFCBDV SA
	
	
print("End of search!")

#------------------------
dog_breeds = ['french_bulldog', 'dalmation', 'shihtzu', 'poodle', 'collie']
for breed in dog_breeds:
    print(breed)
	
#--------------------------

for i in range(3):
  print("WARNING!")

#------------------------------
big_number_list = [1, 2, -1, 4, -5, 5, 2, -9]

for i in big_number_list:
  if i < 0:
    continue
  print(i)
  
# resutl  1 2 4 5 2

#-----------------------
dog_breeds = ['bulldog', 'dalmation', 'shihtzu', 'poodle', 'collie']

index = 0
while index < len(dog_breeds):
  print(dog_breeds[index])
  index += 1
  
#-------------------------

project_teams = [["Ava", "Samantha", "James"], ["Lucille", "Zed"], ["Edgar", "Gabriel"]]
for team in project_teams:


  for student in team:
    print(student)

#------------------------

sales_data = [[12, 17, 22], [2, 10, 3], [5, 12, 13]]
scoops_sold = 0

for location in sales_data:
  print(location)
  for data in location:
      scoops_sold += data

print(scoops_sold)


#--------------------LIST COMPREHENSION LIST COMPREHENSION LIST COMPREHENSION------------

words = ["@coolguy35", "#nofilter", "@kewldawg54", "reply", "timestamp", "@matchamom", "follow", "#updog"]
usernames = []

for word in words:
  if word[0] == '@':
    usernames.append(word)

#is equal with following code

usernames = [word for word in words if word[0] == '@']

#-------------------------------

>>> print(usernames)
["@coolguy35", "@kewldawg54", "@matchamom"]

messages = [user + " please follow me!" for user in usernames]
#-----------------------------

single_digits = range(10)

squares = []

for x in single_digits:
  print(x)
  squares.append(x**2)

print(squares)

cubes = [x**3 for x in single_digits]

print(cubes)

#-----------------------------

hairstyles = ["bouffant", "pixie", "dreadlocks", "crew", "bowl", "bob", "mohawk", "flattop"]

prices = [30, 25, 40, 20, 20, 35, 50, 35]

last_week = [2, 3, 5, 8, 4, 4, 6, 2]

total_price = 0
total_revenue  = 0
average_daily_revenue = 0

for price in prices:
  total_price += price

average_price = total_price / len(prices)


print("Average Haircut Price: " + str(average_price))

new_prices = [x -5 for x in prices]

#print(new_prices)

for i in range(len(hairstyles)):
  total_revenue += prices[i] * last_week[i]

print('Total Revenue: ' + str(total_revenue))

average_daily_revenue = total_revenue / 7

print('Daily Average Revenue: ' + str(average_daily_revenue))

cuts_under_30 = [hairstyles[i] for i in range(len(hairstyles)) if new_prices[i] < 30]

print(cuts_under_30)

#---------------------------

points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]


#------------- tuple --------

# to create an 1 element tuple

one_element_tuple = (1,)

#-----------------------------

def letter_check(word, letter):
  contain = False
  for l in word:
    if l == letter:
      contain = True
  return contain

print(letter_check("game", "s"))

#--------------------------------

password = "theycallme\"crazy\"91"

#------------------------------

def common_letters(string_one, string_two):
  common = []
  for letter in string_one:
    if (letter in string_two) and not (letter in common):
      common.append(letter)
  return common

#------------------------------------
def password_generator(username):
  password = username[-1]
  for i in range(len(username)-1):
    password += username[i]
  return password

print (password_generator("abc"))
#resule cab

#-------------------------------------

#Here’s an example of .lower() in action:

#>>> favorite_song = 'SmOoTH'
#>>> favorite_song_lowercase = favorite_song.lower()
#>>> favorite_song_lowercase
#'smooth'
#Every character was changed to lowercase! It’s important to remember that string methods can only create new strings, they do not change the original string.


#---------------------------------

#.split() is performed on a string, takes one argument, and returns a list of substrings found between the given argument (which in the case of .split() is known as the delimiter). The following syntax should be used:

string_name.split(delimiter)
#If you do not provide an argument for .split() it will default to splitting at spaces.

line_one = "The sky has given over"

line_one_words = line_one.split()

print (line_one_words)

['The', 'sky', 'has', 'given', 'over']

#-------------------------------------


authors = "Audre Lorde, William Carlos Williams, Gabriela Mistral, Jean Toomer, An Qi, Walt Whitman, Shel Silverstein, Carmen Boullosa, Kamala Suraiyya, Langston Hughes, Adrienne Rich, Nikki Giovanni"

author_names = authors.split(',')

print(author_names)

author_last_names = [x.split()[-1] for x in author_names]

print (author_last_names)

#-----------------------------------------

#We can also split strings using escape sequences. Escape sequences are used to indicate that we want to split by something in a string that is not necessarily a character. The two escape sequences we will cover here are

\n Newline
\t Horizontal Tab



spring_storm_text = \
"""The sky has given over 
its bitterness. 
Out of the dark change 
all day long 
rain falls and falls 
as if it would never end. 
Still the snow keeps 
its hold on the ground. 
But water, water 
from a thousand runnels! 
It collects swiftly, 
dappled with black 
cuts a way for itself 
through green ice in the gutters. 
Drop after drop it falls 
from the withered grass-stems 
of the overhanging embankment."""

spring_storm_lines = spring_storm_text.split("\n")

print (spring_storm_lines)

['The sky has given over ', 'its bitterness. ', 'Out of the dark change ', 'all day long ', 'rain falls and falls ', 'as if it would never end. ', 'Still the snow keeps ', 'its hold on the ground. ', 'But water, water ', 'from a thousand runnels! ', 'It collects swiftly, ', 'dappled with black ', 'cuts a way for itself ', 'through green ice in the gutters. ', 'Drop after drop it falls ', 'from the withered grass-stems ', 'of the overhanging embankment.']

#--------------------------------------

'delimiter'.join(list_you_want_to_join)

>>> my_munequita = ['My', 'Spanish', 'Harlem', 'Mona', 'Lisa']
>>> ' '.join(my_munequita)
'My Spanish Harlem Mona Lisa'



>>> santana_songs_csv = ','.join(santana_songs)
>>> santana_songs_csv
'Oye Como Va,Smooth,Black Magic Woman,Samba Pa Ti,Maria Maria'



smooth_fifth_verse_lines = ['Well I\'m from the barrio', 'You hear my rhythm on your radio', 'You feel the turning of the world so soft and slow', 'Turning you \'round and \'round']

smooth_fifth_verse = '\n'.join(smooth_fifth_verse_lines)

print(smooth_fifth_verse)

["Well I'm from the barrio", 'You hear my rhythm on your radio', 'You feel the turning of the world so soft and slow', "Turning you 'round and 'round"]


#--------------------------------------------
>>> featuring = "           rob thomas                 "
>>> featuring.strip()
'rob thomas'

>>> featuring = "!!!rob thomas       !!!!!"
>>> featuring.strip('!')
'rob thomas     



love_maybe_lines = ['Always    ', '     in the middle of our bloodiest battles  ', 'you lay down your arms', '           like flowering mines    ','\n' ,'   to conquer me home.    ']

love_maybe_lines_stripped = [x.strip() for x in love_maybe_lines]


love_maybe_lines = ['Always    ', '     in the middle of our bloodiest battles  ', 'you lay down your arms', '           like flowering mines    ','\n' ,'   to conquer me home.    ']

#--------

love_maybe_lines_stripped = []

for line in love_maybe_lines:
  love_maybe_lines_stripped.append(line.strip())
  
love_maybe_full = '\n'.join(love_maybe_lines_stripped)

print(love_maybe_full)


#-----------------------------------

string_name.replace(character_being_replaced, new_character)

>>> with_spaces = "You got the kind of loving that can be so smooth"
>>> with_underscores = with_spaces.replace(' ', '_')
>>> with_underscores
'You_got_the_kind_of_loving_that_can_be_so_smooth'

#------------------------------------------

>>> 'smooth'.find('t')
'4'

>>>"smooth".find('oo')
'2'

#-------------------------------------

def favorite_song_statement(song, artist):
  return "My favorite song is {} by {}.".format(song, artist)
  

def favorite_song_statement(song, artist):
    return "My favorite song is {song} by {artist}.".format(song=song, artist=artist)

	
def favorite_song_statement(song, artist):
    return "My favorite song is {song_in_str} by {artist_in_str}.".format(song_in_str=song, artist_in_str=artist)
	
#---------------------------------------

highlighted_poems = "Afterimages:Audre Lorde:1997,  The Shadow:William Carlos Williams:1915, Ecstasy:Gabriela Mistral:1925,   Georgia Dusk:Jean Toomer:1923,   Parting Before Daybreak:An Qi:2014, The Untold Want:Walt Whitman:1871, Mr. Grumpledump's Song:Shel Silverstein:2004, Angel Sound Mexico City:Carmen Boullosa:2013, In Love:Kamala Suraiyya:1965, Dream Variations:Langston Hughes:1994, Dreamwood:Adrienne Rich:1987"

print(highlighted_poems)
print("\n")

highlighted_poems_list = highlighted_poems.split(',')

print(highlighted_poems_list)
print("\n")

highlighted_poems_stripped =[]

for a in highlighted_poems_list:
  highlighted_poems_stripped.append(a.strip())

print(highlighted_poems_stripped)
print("\n")

highlighted_poems_details = []
for a in highlighted_poems_stripped:
  highlighted_poems_details.append(a.split(':'))

print(highlighted_poems_details)
print("\n")

titles = []
poets = []
dates = []

for a in highlighted_poems_details:
  titles.append(a[0])
  poets.append(a[1])
  dates.append(a[2])

print(titles)
print("\n")

print(poets)
print("\n")

print(dates)
print("\n")

for i in range(len(titles)):
  print("The poem {TITLE} was published by {POET} in {DATE}.".format(TITLE = titles[i], POET = poets[i], DATE = dates[i] ))
  
#---------------------------------------------------------

daily_sales = \
"""Edith Mcbride   ;,;$1.21   ;,;   white ;,; 
09/15/17   ,Herbert Tran   ;,;   $7.29;,; 
white&blue;,;   09/15/17 ,Paul Clarke ;,;$12.52 
;,;   white&blue ;,; 09/15/17 ,Lucille Caldwell   
;,;   $5.13   ;,; white   ;,; 09/15/17,
Eduardo George   ;,;$20.39;,; white&yellow 
;,;09/15/17   ,   Danny Mclaughlin;,;$30.82;,;   
purple ;,;09/15/17 ,Stacy Vargas;,; $1.85   ;,; 
purple&yellow ;,;09/15/17,   Shaun Brock;,; 
$17.98;,;purple&yellow ;,; 09/15/17 , 
Erick Harper ;,;$17.41;,; blue ;,; 09/15/17, 
Michelle Howell ;,;$28.59;,; blue;,;   09/15/17   , 
Carroll Boyd;,; $14.51;,;   purple&blue   ;,;   
09/15/17   , Teresa Carter   ;,; $19.64 ;,; 
white;,;09/15/17   ,   Jacob Kennedy ;,; $11.40   
;,; white&red   ;,; 09/15/17, Craig Chambers;,; 
$8.79 ;,; white&blue&red   ;,;09/15/17   , Peggy Bell;,; $8.65 ;,;blue   ;,; 09/15/17,   Kenneth Cunningham ;,;   $10.53;,;   green&blue   ;,; 
09/15/17   ,   Marvin Morgan;,;   $16.49;,; 
green&blue&red   ;,;   09/15/17 ,Marjorie Russell 
;,; $6.55 ;,;   green&blue&red;,;   09/15/17 ,
Israel Cummings;,;   $11.86   ;,;black;,;  
09/15/17,   June Doyle   ;,;   $22.29 ;,;  
black&yellow ;,;09/15/17 , Jaime Buchanan   ;,;   
$8.35;,;   white&black&yellow   ;,;   09/15/17,   
Rhonda Farmer;,;$2.91 ;,;   white&black&yellow   
;,;09/15/17, Darren Mckenzie ;,;$22.94;,;green 
;,;09/15/17,Rufus Malone;,;$4.70   ;,; green&yellow 
;,; 09/15/17   ,Hubert Miles;,;   $3.59   
;,;green&yellow&blue;,;   09/15/17   , Joseph Bridges  ;,;$5.66   ;,; green&yellow&purple&blue 
;,;   09/15/17 , Sergio Murphy   ;,;$17.51   ;,;   
black   ;,;   09/15/17 , Audrey Ferguson ;,; 
$5.54;,;black&blue   ;,;09/15/17 ,Edna Williams ;,; 
$17.13;,; black&blue;,;   09/15/17,   Randy Fleming;,;   $21.13 ;,;black ;,;09/15/17 ,Elisa Hart;,; $0.35   ;,; black&purple;,;   09/15/17   ,
Ernesto Hunt ;,; $13.91   ;,;   black&purple ;,;   
09/15/17,   Shannon Chavez   ;,;$19.26   ;,; 
yellow;,; 09/15/17   , Sammy Cain;,; $5.45;,;   
yellow&red ;,;09/15/17 ,   Steven Reeves ;,;$5.50   
;,;   yellow;,;   09/15/17, Ruben Jones   ;,; 
$14.56 ;,;   yellow&blue;,;09/15/17 , Essie Hansen;,;   $7.33   ;,;   yellow&blue&red
;,; 09/15/17   ,   Rene Hardy   ;,; $20.22   ;,; 
black ;,;   09/15/17 ,   Lucy Snyder   ;,; $8.67   
;,;black&red  ;,; 09/15/17 ,Dallas Obrien ;,;   
$8.31;,;   black&red ;,;   09/15/17,   Stacey Payne 
;,;   $15.70   ;,;   white&black&red ;,;09/15/17   
,   Tanya Cox   ;,;   $6.74   ;,;yellow   ;,; 
09/15/17 , Melody Moran ;,;   $30.84   
;,;yellow&black;,;   09/15/17 , Louise Becker   ;,; 
$12.31 ;,; green&yellow&black;,;   09/15/17 ,
Ryan Webster;,;$2.94 ;,; yellow ;,; 09/15/17 
,Justin Blake ;,; $22.46   ;,;white&yellow ;,;   
09/15/17,   Beverly Baldwin ;,;   $6.60;,;   
white&yellow&black ;,;09/15/17   ,   Dale Brady   
;,;   $6.27 ;,; yellow   ;,;09/15/17 ,Guadalupe Potter ;,;$21.12   ;,; yellow;,; 09/15/17   , 
Desiree Butler ;,;$2.10   ;,;white;,; 09/15/17  
,Sonja Barnett ;,; $14.22 ;,;white&black;,;   
09/15/17, Angelica Garza;,;$11.60;,;white&black   
;,;   09/15/17   ,   Jamie Welch   ;,; $25.27   ;,; 
white&black&red ;,;09/15/17   ,   Rex Hudson   
;,;$8.26;,;   purple;,; 09/15/17 ,   Nadine Gibbs 
;,;   $30.80 ;,;   purple&yellow   ;,; 09/15/17   , 
Hannah Pratt;,;   $22.61   ;,;   purple&yellow   
;,;09/15/17,Gayle Richards;,;$22.19 ;,; 
green&purple&yellow ;,;09/15/17   ,Stanley Holland 
;,; $7.47   ;,; red ;,; 09/15/17 , Anna Dean;,;$5.49 ;,; yellow&red ;,;   09/15/17   ,
Terrance Saunders ;,;   $23.70  ;,;green&yellow&red 
;,; 09/15/17 ,   Brandi Zimmerman ;,; $26.66 ;,; 
red   ;,;09/15/17 ,Guadalupe Freeman ;,; $25.95;,; 
green&red ;,;   09/15/17   ,Irving Patterson 
;,;$19.55 ;,; green&white&red ;,;   09/15/17 ,Karl Ross;,;   $15.68;,;   white ;,;   09/15/17 , Brandy Cortez ;,;$23.57;,;   white&red   ;,;09/15/17, 
Mamie Riley   ;,;$29.32;,; purple;,;09/15/17 ,Mike Thornton   ;,; $26.44 ;,;   purple   ;,; 09/15/17, 
Jamie Vaughn   ;,; $17.24;,;green ;,; 09/15/17   , 
Noah Day ;,;   $8.49   ;,;green   ;,;09/15/17   
,Josephine Keller ;,;$13.10 ;,;green;,;   09/15/17 ,   Tracey Wolfe;,;$20.39 ;,; red   ;,; 09/15/17 ,
Ignacio Parks;,;$14.70   ;,; white&red ;,;09/15/17 
, Beatrice Newman ;,;$22.45   ;,;white&purple&red 
;,;   09/15/17, Andre Norris   ;,;   $28.46   ;,;   
red;,;   09/15/17 ,   Albert Lewis ;,; $23.89;,;   
black&red;,; 09/15/17,   Javier Bailey   ;,;   
$24.49   ;,; black&red ;,; 09/15/17   , Everett Lyons ;,;$1.81;,;   black&red ;,; 09/15/17 ,   
Abraham Maxwell;,; $6.81   ;,;green;,;   09/15/17   
,   Traci Craig ;,;$0.65;,; green&yellow;,; 
09/15/17 , Jeffrey Jenkins   ;,;$26.45;,; 
green&yellow&blue   ;,;   09/15/17,   Merle Wilson 
;,;   $7.69 ;,; purple;,; 09/15/17,Janis Franklin   
;,;$8.74   ;,; purple&black   ;,;09/15/17 ,  
Leonard Guerrero ;,;   $1.86   ;,;yellow  
;,;09/15/17,Lana Sanchez;,;$14.75   ;,; yellow;,;   
09/15/17   ,Donna Ball ;,; $28.10  ;,; 
yellow&blue;,;   09/15/17   , Terrell Barber   ;,; 
$9.91   ;,; green ;,;09/15/17   ,Jody Flores;,; 
$16.34 ;,; green ;,;   09/15/17,   Daryl Herrera 
;,;$27.57;,; white;,;   09/15/17   , Miguel Mcguire;,;$5.25;,; white&blue   ;,;   09/15/17 ,   
Rogelio Gonzalez;,; $9.51;,;   white&black&blue   
;,;   09/15/17   ,   Lora Hammond ;,;$20.56 ;,; 
green;,;   09/15/17,Owen Ward;,; $21.64   ;,;   
green&yellow;,;09/15/17,Malcolm Morales ;,;   
$24.99   ;,;   green&yellow&black;,; 09/15/17 ,   
Eric Mcdaniel ;,;$29.70;,; green ;,; 09/15/17 
,Madeline Estrada;,;   $15.52;,;green;,;   09/15/17 
, Leticia Manning;,;$15.70 ;,; green&purple;,; 
09/15/17 ,   Mario Wallace ;,; $12.36 ;,;green ;,; 
09/15/17,Lewis Glover;,;   $13.66   ;,;   
green&white;,;09/15/17,   Gail Phelps   ;,;$30.52   
;,; green&white&blue   ;,; 09/15/17 , Myrtle Morris 
;,;   $22.66   ;,; green&white&blue;,;09/15/17"""

#------------------------------------------------
# Start coding below!

daily_sales_replaced = daily_sales.replace(';,;','|')

daily_transactions = daily_sales_replaced.split(",")

daily_transactions_split = []

for transaction in daily_transactions:
  daily_transactions_split.append(transaction.split('|'))
  
transactions_clean = []

for transaction in daily_transactions_split:
  for item in transaction:
    transactions_clean.append(item.strip().strip("\n"))
    
customers = []
sales = []
thread_sold = []

for i in range(int((len(transactions_clean)+1)/4)):
  k = i*4
  j = k + 1
  l = j + 1
  customers.append(transactions_clean[k])
  sales.append(transactions_clean[j])
  thread_sold.append(transactions_clean[l])

#print(customers)
#print(sales)
#print(thread_sold)

total_sales = 0

for sale in sales:
  total_sales += float(sale.strip('$'))

#print(total_sales)

thread_sold_split = []

for color in thread_sold:
  if "&" not in color:
    thread_sold_split.append(color)
  else:
    thread_sold_split.append(color.split('&')[0])
    thread_sold_split.append(color.split('&')[1])

print(thread_sold_split)

def color_count(color):
  count = 0
  for col in thread_sold_split:
    if color == col:
      count += 1
  return count

print(color_count('white'))

color = ['red','yellow','green','white','black','blue','purple']

for c in color:
  print('{c} thread has been sold {times}'.format(c = c, times = color_count(c)))

  
#----------------

letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
# Write your unique_english_letters function here:
def unique_english_letters(word):
  unique_letter = ''
  for letter in word:
    print(letter)
    if letter not in unique_letter:
      unique_letter += letter
    
  return len(unique_letter)

  


# Uncomment these function calls to test your function:
print(unique_english_letters("mississippi"))
# should print 4
#print(unique_english_letters("Apple"))
# should print 4

#-----------------------------------

letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
# Write your unique_english_letters function here:
def unique_english_letters(word):
  uniques = 0
  for letter in letters:
    if letter in word:
      uniques += 1
  return uniques

# Uncomment these function calls to test your tip function:
print(unique_english_letters("mississippi"))
# should print 4
print(unique_english_letters("Apple"))
# should print 4

#----------------------------------------------

# Write your count_char_x function here:
def count_char_x(word, x):
  count = 0
  for letter in word:
    if letter == x:
      count += 1
  return count

# Uncomment these function calls to test your tip function:
print(count_char_x("mississippi", "s"))
# should print 4
print(count_char_x("mississippi", "m"))
# should print 1

#--------------------------------

# Write your count_multi_char_x function here:
def count_multi_char_x(word, x):
  count = 0
  length_letter = len(x)
  length_word = len(word)
  for i in range(0,length_word-length_letter):
    if word[i:i+length_letter] == x:
      count += 1
  return count  
  
# Uncomment these function calls to test your function:
print(count_multi_char_x("mississippi", "iss"))
# should print 2
#print(count_multi_char_x("apple", "pp"))
# should print 1

#----------------------------------

# Write your count_multi_char_x function here:
def count_multi_char_x(word, x):
  splits = word.split(x)
  print(splits)
  return(len(splits)-1)

# Uncomment these function calls to test your  function:
print(count_multi_char_x("mississippi", "iss"))
# should print 2
print(count_multi_char_x("apple", "pp"))
# should print 1

#--------------------------------------

# Write your substring_between_letters function here:
def substring_between_letters(word, start, end):
  s = 0
  e = 0
  s = word.find(start)
  print(word.find(start))
  e = word.find(end)
  print(word.find(end))
  if s >= 0 and e >= 0 :
    return word[s + 1 : e]
  else:
    return word

# Uncomment these function calls to test your function:
#print(substring_between_letters("apple", "p", "e"))
# should print "pl"
print(substring_between_letters("apple", "p", "c"))
# should print "apple"

#-----------------------------------

# Write your x_length_words function here:
def x_length_words(sentence , x):
  word_list = []
  word_list = sentence.split()
  for word in word_list:
    if len(word) >= x:
      continue
    else:
      return False
  return True


# Uncomment these function calls to test your tip function:
print(x_length_words("i like apples", 2))
# should print False
print(x_length_words("he likes apples", 2))
# should print True

#---------------------------------

# Write your every_other_letter function here:
def every_other_letter(word):
  every_other = ""
  for i in range(0, len(word), 2):
    every_other += word[i]
  return every_other

# Uncomment these function calls to test your tip function:
print(every_other_letter("Codecademy"))
# should print Cdcdm
print(every_other_letter("Hello world!"))
# should print Hlowrd
print(every_other_letter(""))
# should print 

#----------------------------------------------

# Write your reverse_string function here:


def reverse_string(word):
  rev_word = ""
  for i in range(len(word)):
    k = (i + 1) * -1
    rev_word += word[k]
    
  return rev_word
  
  
  # Uncomment these function calls to test your  function:
#print(reverse_string("Codecademy"))
# should print ymedacedoC
#print(reverse_string("Hello world!"))
# should print !dlrow olleH
#print(reverse_string(""))
# should print

#---------------------------------------------------
# Write your make_spoonerism function here:
def make_spoonerism(word1, word2):
  return word2[0]+word1[1:]+" "+word1[0]+word2[1:]

# Uncomment these function calls to test your tip function:
print(make_spoonerism("Codecademy", "Learn"))
# should print Lodecademy Cearn
print(make_spoonerism("Hello", "world!"))
# should print wello Horld!
print(make_spoonerism("a", "b"))
# should print b a

#--------------------------------------------

# Write your add_exclamation function here:
def add_exclamation(word):
  if len(word) >= 20:
    return word
  else:
    word += (20 - len(word)) * "!"
  return word
  
# Uncomment these function calls to test your function:
print(add_exclamation("Codecademy"))
# should print Codecademy!!!!!!!!!!
print(add_exclamation("Codecademy is the best place to learn"))
# should print Codecademy is the best place to learn
  
#-----------


def add_exclamation(word):
  while(len(word) < 20):
    word += "!"
  return word

#--------------------------------------
  
# Import random below:
import random

# Create random_list below:
random_list = [random.randint(1,101) for i in range(101)]

# Create randomer_number below:
randomer_number = random.choice(random_list)

# Print randomer_number below:
print(randomer_number)
  
#-------------------------------

import codecademylib3_seaborn

# Add your code below:
from matplotlib import pyplot as plt

import random

numbers_a = range(1, 13)

numbers_b = random.sample(range(1000), 12)

plt.plot(numbers_a, numbers_b)

plt.show()
#----------------------

# Import Decimal below:

from decimal import Decimal


# Fix the floating point math below:
two_decimal_points = Decimal('0.2') + Decimal('0.69')

print(two_decimal_points)

four_decimal_points = Decimal('0.53') * Decimal('0.65')

print(four_decimal_points)

#-------------------------------------
#--------script.py---------------

# Import library below:
from library import always_three


# Call your function below:
print(always_three())  

#---------library.py---------------
# Add your always_three() function below:
def always_three():
  return 3

#=---------------------------------------------
def unit_price(weight):
  unit_price_check = 0
  if weight <= 2:
    unit_price_check = 1.5
  elif weight <= 6:
    unit_price_check = 3
  elif weight <= 10:
    unit_price_check = 4
  elif weight > 10:
    unit_price_check = 4.75
  return unit_price_check

def ground_shipping(weight):
  cost = 0
  cost = unit_price(weight) * weight
  cost += 20
  return cost

def drone_shipping(weight):
  cost = 0
  cost = unit_price(weight) * 3 * weight
  cost += 0
  return cost

premium_ground_shipping = 125

#print(ground_shipping(10))
#print(drone_shipping(1.5))

def cheaper_shipping(weight):
  ground = ground_shipping(weight)
  drone = drone_shipping(weight)
  premium = premium_ground_shipping
  method = ""
  price = 0
  
  if (ground <= drone) and (ground <= premium):
    method = "Ground"
    price = ground
  elif (drone <= ground) and (drone <= premium):
    method = "Drone"
    price = drone
  elif (premium <= ground) and (premium <= drone):
    method = "Premium"
    price = premium
  print("{method} is the cheapest way to ship and the price is ${price}".format(method = method, price = price))

cheaper_shipping(10)
  
#--------------------------------
import random

money = 100

#Write your game of chance functions here
def flip_coin(guess, bet):
  i = 0
  i = random.randint(0,1)
  if i == 0:
    coin = "Heads"
  else:
    coin = "Tails"
  print("The Result is \"{coin}\"!".format(coin = coin))
  print("Your Guess is \"{guess}\"!".format(guess = guess))
    
  if guess.lower() !="Heads".lower() and guess.lower() !="Tails".lower():
    print("You have to bet on \"Heads\" or \"Tails\" !!!")
    return 0
 
  if guess.lower() == coin.lower():
    print("You Win!!!")
    return bet
  
  else:
    print("You Lose!!!")
    return bet*-1
#------------------------------
def cho_han(guess, bet):
  i = random.randint(1,6)
  j = random.randint(1,6)
  total = i + j
  
  print ("Dice 1 is {i}\
          Dice 2 is {j}\
          Total is {Total}\
          Your Bet is {guess}".format(i = i, j = j, Total = total, guess = guess))
  
  if guess.lower() != "odd" and guess.lower() != "even":
    print("You have to bet \"Odd\" or \"Even\"")
    return 0
  
  if (total % 2 == 0) and (guess.lower() == "even"):
    print("You Win!!!")
    return bet
  elif (total % 2 == 1) and (guess.lower() == "odd"):
    print("You Win!!!")
    return bet
  else:
    print("You Lost!!!")
    return bet*-1

#-------------------------
def card_bet(bet):
  i = random.randint(1,13)
  j = random.randint(1,13)
  
  card_list = [0, "A", 2, 3, 4, 5, 6, 7, 8, 9, 10, "J", "Q" , "K"]
  
  print("You get a \"{card}\"!".format(card = card_list[i]))
  print("Computer get a \"{card}\"!".format(card = card_list[j]))
  if i > j:
    print ("You Win !!!")
    return bet
  elif i < j:
    print ("You Lost !!!")
    return bet*-1
  else:
    print ("Tie !!! Try Again.")
    return 0
#-----------------------
def roulette(bet):
  

#Call your game of chance functions here


#----------------------------------------

with open('welcome.txt') as text_file:
  text_data = text_file.read()
  
print(text_data)

with open('how_many_lines.txt') as lines_doc:
  for line in lines_doc.readlines():
    print(line)

with open('millay_sonnet.txt') as sonnet_doc:
  first_line = sonnet_doc.readline()
  second_line = sonnet_doc.readline()
  print(second_line)
 
 with open('bad_bands.txt', 'w') as bad_bands_doc:
  bad_bands_doc.write("abc")
  
 with open('cool_dogs.txt', 'a') as cool_dogs_file:
  cool_dogs_file.write("Air Buddy")
  


#--------------  
fun_cities_file = open('fun_cities.txt', 'a')

# We can now append a line to "fun_cities".
fun_cities_file.write("Montréal")

# But we need to remember to close the file
fun_cities_file.close()
#---------------

with open('logger.csv') as log_csv_file:
  a = log_csv_file.read()
print(a)

#-----------------
##########
users.csv

Name,Username,Email
Roger Smith,rsmith,wigginsryan@yahoo.com
Michelle Beck,mlbeck,hcosta@hotmail.com
Ashley Barker,a_bark_x,a_bark_x@turner.com
Lynn Gonzales,goodmanjames,lynniegonz@hotmail.com
###########

import csv

list_of_email_addresses = []
with open('users.csv', newline='') as users_csv:
  user_reader = csv.DictReader(users_csv)
  for row in user_reader:
    list_of_email_addresses.append(row['Email'])
	
	
import csv
isbn_list =[]
with open('books.csv') as books_csv:
  books_reader = csv.DictReader(books_csv, delimiter='@')
  for row in books_reader:
    isbn_list.append(row['ISBN'])
  
print(isbn_list)
    
#--------------------------------------

big_list = [{'name': 'Fredrick Stein', 'userid': 6712359021, 'is_admin': False}, {'name': 'Wiltmore Denis, 'userid': 2525942, 'is_admin': False}, {'name': 'Greely Plonk', 'userid': 15890235, 'is_admin': False}, {'name': 'Dendris Stulo', 'userid': 572189563, 'is_admin': True}] 

import csv

with open('output.csv', 'w') as output_csv:
  fields = ['name', 'userid', 'is_admin']
  output_writer = csv.DictWriter(output_csv, fieldnames=fields)

  output_writer.writeheader()
  for item in big_list:
    output_writer.writerow(item)

#---------------------------------------	
access_log = [{'time': '08:39:37', 'limit': 844404, 'address': '1.227.124.181'}, {'time': '13:13:35', 'limit': 543871, 'address': '198.51.139.193'}, {'time': '19:40:45', 'limit': 3021, 'address': '172.1.254.208'}, {'time': '18:57:16', 'limit': 67031769, 'address': '172.58.247.219'}, {'time': '21:17:13', 'limit': 9083, 'address': '124.144.20.113'}, {'time': '23:34:17', 'limit': 65913, 'address': '203.236.149.220'}, {'time': '13:58:05', 'limit': 1541474, 'address': '192.52.206.76'}, {'time': '10:52:00', 'limit': 11465607, 'address': '104.47.149.93'}, {'time': '14:56:12', 'limit': 109, 'address': '192.31.185.7'}, {'time': '18:56:35', 'limit': 6207, 'address': '2.228.164.197'}]
fields = ['time', 'address', 'limit']

import csv

with open("logger.csv", "w") as logger_csv:
  log_writer = csv.DictWriter(logger_csv, fieldnames = fields )
  log_writer.writeheader()
  for item in access_log:
    log_writer.writerow(item)

with open("logger.csv") as textfile:
  log_reader = textfile.read()
  print(log_reader)
  
#----------------------------------------

import json

with open('purchase_14781239.json') as purchase_json:
  purchase_data = json.load(purchase_json)

print(purchase_data['user'])
# Prints 'ellen_greg'

#-------------------------
turn_to_json = {
  'eventId': 674189,
  'dateTime': '2015-02-12T09:23:17.511Z',
  'chocolate': 'Semi-sweet Dark',
  'isTomatoAFruit': True
}


import json

with open('output.json', 'w') as json_file:
  json.dump(turn_to_json, json_file)

  
#---------------------

my_dict = {}

my_dict["new_key"] = "new_value"

print(my_dict)

#-------------------

user_ids = {"teraCoder": 9018293, "proProgrammer": 119238}

user_ids.update({"theLooper" : 138475, "stringQueen" : 85739})

print(user_ids)

#--------------------

names = ['Jenny', 'Alexus', 'Sam', 'Grace']
heights = [61, 70, 67, 64]

students = {key:value for key, value in zip(names, heights)}
#students is now {'Jenny': 61, 'Alexus': 70, 'Sam': 67, 'Grace': 64}

#---------------------------

drinks = ["espresso", "chai", "decaf", "drip"]
caffeine = [64, 40, 0, 120]

zipped_drinks = zip(drinks, caffeine)

drinks_to_caffeine = {drinks: x for drinks, x in zipped_drinks}

#-------------------------------------

songs = ["Like a Rolling Stone", "Satisfaction", "Imagine", "What's Going On", "Respect", "Good Vibrations"]
playcounts = [78, 29, 44, 21, 89, 5]

plays = {songs: x for songs, x in zip(songs, playcounts)}

#print(plays)

plays["Purple Haze"] = 1

plays["Respect"] = 94

library = {"The Best Songs" : plays, "Sunday Feelings" : {} }

print(library)

#----------------------------------


zodiac_elements = {"water": ["Cancer", "Scorpio", "Pisces"], "fire": ["Aries", "Leo", "Sagittarius"], "earth": ["Taurus", "Virgo", "Capricorn"], "air":["Gemini", "Libra", "Aquarius"]}


if "energy" in zodiac_elements:
  print(zodiac_elements["energy"])
else:
  print("Not a Zodiac element")
  
#-----------------------------------

key_to_check = "Landmark 81"
try:
  print(building_heights[key_to_check])
except KeyError:
  print("That key doesn't exist!")

#--------------------------

building_heights = {"Burj Khalifa": 828, "Shanghai Tower": 632, "Abraj Al Bait": 601, "Ping An": 599, "Lotte World Tower": 554.5, "One World Trade": 541.3}

#this line will return 632:
building_heights.get("Shanghai Tower")

#this line will return None:
building_heights.get("My House")

building_heights.get('Shanghai Tower', 0)
# return 632

building_heights.get('Mt Olympus', 0)
# return 0

building_heights.get('Kilimanjaro', 'No Value')
# return 'No Value'

#--------------------

raffle = {223842: "Teddy Bear", 872921: "Concert Tickets", 320291: "Gift Basket", 412123: "Necklace", 298787: "Pasta Maker"}

>>> raffle.pop(320291, "No Prize")
"Gift Basket"
>>> raffle
{223842: "Teddy Bear", 872921: "Concert Tickets", 412123: "Necklace", 298787: "Pasta Maker"}
>>> raffle.pop(100000, "No Prize")
"No Prize"
>>> raffle
{223842: "Teddy Bear", 872921: "Concert Tickets", 412123: "Necklace", 298787: "Pasta Maker"}
>>> raffle.pop(872921, "No Prize")
"Concert Tickets"
>>> raffle
{223842: "Teddy Bear", 412123: "Necklace", 298787: "Pasta Maker"}

#----------------------------


available_items = {"health potion": 10, "cake of the cure": 5, "green elixir": 20, "strength sandwich": 25, "stamina grains": 15, "power stew": 30}
health_points = 20

print(health_points)
print(available_items)

health_points += available_items.pop("stamina grains", 0)

print(health_points)
print(available_items)

health_points += available_items.pop("power stew", 0)

print(health_points)
print(available_items)

health_points += available_items.pop("mystic bread", 0)


print(health_points)
print(available_items)

#----------------------------------
test_scores = {"Grace":[80, 72, 90], "Jeffrey":[88, 68, 81], "Sylvia":[80, 82, 84], "Pedro":[98, 96, 95], "Martin":[78, 80, 78], "Dina":[64, 60, 75]}

>>> list(test_scores)
["Grace", "Jeffrey", "Sylvia", "Pedro", "Martin", "Dina"]

#---------------------------------

#Dictionaries also have a .keys() method that returns a dict_keys object. A dict_keys object is a view object, which provides a look at the current state of the dicitonary, without the user being able to modify anything. The dict_keys object returned by .keys() is a set of the keys in the dictionary. You cannot add or remove elements from a dict_keys object, but it can be used in the place of a list for iteration:

for student in test_scores.keys():
  print(student)
will yield:

"Grace"
"Jeffrey"
"Sylvia"
"Pedro"
"Martin"
"Dina"

#---------------------------------------
test_scores = {"Grace":[80, 72, 90], "Jeffrey":[88, 68, 81], "Sylvia":[80, 82, 84], "Pedro":[98, 96, 95], "Martin":[78, 80, 78], "Dina":[64, 60, 75]}

for score_list in test_scores.values():
  print(score_list)
  
  will yield:
F
[80, 72, 90]
[88, 68, 81]
[80, 82, 84]
[98, 96, 95]
[78, 80, 78]
[64, 60, 75]


#There is no built-in function to get all of the values as a list, but if you really want to, you can use:

list(test_scores.values())

#-------------------------------

biggest_brands = {"Apple": 184, "Google": 141.7, "Microsoft": 80, "Coca-Cola": 69.7, "Amazon": 64.8}

for company, value in biggest_brands.items():
  print(company + " has a value of " + str(value) + " billion dollars. ")
  
#which would yield this output:

Apple has a value of 184 billion dollars.
Google has a value of 141.7 billion dollars.
Microsoft has a value of 80 billion dollars.
Coca-Cola has a value of 69.7 billion dollars.
Amazon has a value of 64.8 billion dollars.

#-----------------------------------------------

pct_women_in_occupation = {"CEO": 28, "Engineering Manager": 9, "Pharmacist": 58, "Physician": 40, "Lawyer": 37, "Aerospace Engineer": 9}

for position , percentage in pct_women_in_occupation. items():
  print("Women make up {position} percent of {percentage}s.".format(position = percentage, percentage = position))

  
#------------------------

letters = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
points = [1, 3, 3, 2, 1, 4, 2, 4, 1, 8, 5, 1, 3, 4, 1, 3, 10, 1, 1, 1, 1, 4, 4, 8, 4, 10]

letters_low = []

for letter in letters:
  letters_low.append(letter.lower())

letters = letters + letters_low
points = points + points

#print(letters)
#print(points)

letter_to_points = {letters : x for letters, x in zip(letters, points )}

letter_to_points[' '] = 0

print(letter_to_points)

def score_word(word):
  point_total = 0
  for letter in word:
    point_total += letter_to_points.get(letter, 0)
  return point_total

brownie_points = score_word("BROWNIE")

print(brownie_points)
#should return 15

player_to_words = {
  "player1" : ["BLUE", "TENNIS", "EXIT"],
  "wordNerd" : ["EARTH", "EYES", "MACHINE"],
  "Lexi Con" : ["ERASER", "BELLY", "HUSKY"],
  "Prof Reader" : ["ZAP", "COMA", "PERIOD"]
}

player_to_points = {}

for player in player_to_words:
  player_points = 0
  for word in player_to_words[player]:
    word_points = score_word(word)
    player_points += word_points
  player_to_points[player] = player_points

print(player_to_points)

def play_word(player, word):
  #check player name
  try:
    player_to_words[player]
  except KeyError:
    print("Player Does Not Exist!")
    return
  #attatch the word
  if word in player_to_words[player]:
    print("The Word is Already Exist!")
    return 
  else:
    player_to_words[player].append(word)

#play_word("pyer1", "BLe")
#print(player_to_words)

letters_low = []

for letter in letters:
  letters_low.append(letter.lower())

print(letters_low)


#------------------------

# Write your max_key function here:
def max_key(my_dictionary):
  max_value = 0
  max_value = max(list(my_dictionary.values()))
  print(max_value)
  for key in my_dictionary:
    if my_dictionary[key] == max_value:
      return key
    
    
  
  
  # Uncomment these function calls to test your  function:
print(max_key({1:100, 2:1, 3:4, 4:10}))
# should print 1
print(max_key({"a":100, "b":10, "c":1000}))
# should print "c"

#---------------------------------------

# Write your max_key function here:
def max_key(my_dictionary):
  largest_key = float("-inf")
  largest_value = float("-inf")
  for key, value in my_dictionary.items():
    if value > largest_value:
      largest_value = value
      largest_key = key
  return largest_key

# Uncomment these function calls to test your  function:
print(max_key({1:100, 2:1, 3:4, 4:10}))
# should print 1
print(max_key({"a":100, "b":10, "c":1000}))
# should print "c"
  
  
#-----------------------------------------

# Write your count_first_letter function here:
def count_first_letter(names):
  first_letter = []
  name_dict = {}
  for key in names:
    if key[0] not in first_letter:
      first_letter.append(key[0])
      name_dict[key[0]] = 0
    
  for key, value in names.items():
    name_dict[key[0]] += len(value)
  
  return name_dict
    
    
    
# Uncomment these function calls to test your  function:
print(count_first_letter({"Stark": ["Ned", "Robb", "Sansa"], "Snow" : ["Jon"], "Lannister": ["Jaime", "Cersei", "Tywin"]}))
# should print {"S": 4, "L": 3}
print(count_first_letter({"Stark": ["Ned", "Robb", "Sansa"], "Snow" : ["Jon"], "Sannister": ["Jaime", "Cersei", "Tywin"]}))
# should print {"S": 7}

#----------------------------------
  
 # Write your count_first_letter function here:
def count_first_letter(names):
  letters = {}
  for key in names:
    first_letter = key[0]
    if first_letter not in letters:
      letters[first_letter] = 0
    letters[first_letter] += len(names[key])
  return letters

# Uncomment these function calls to test your  function:
print(count_first_letter({"Stark": ["Ned", "Robb", "Sansa"], "Snow" : ["Jon"], "Lannister": ["Jaime", "Cersei", "Tywin"]}))
# should print {"S": 4, "L": 3}
print(count_first_letter({"Stark": ["Ned", "Robb", "Sansa"], "Snow" : ["Jon"], "Sannister": ["Jaime", "Cersei", "Tywin"]}))
# should print {"S": 7}

#----------------------------
class Dog():
  dog_time_dilation = 7

  def time_explanation(self):
    print("Dogs experience {} years for every 1 human year.".format(self.dog_time_dilation))

pipi_pitbull = Dog()
pipi_pitbull.time_explanation()
# Prints "Dogs experience 7 years for every 1 human year."

#-------------------------------

class DistanceConverter:
  kms_in_a_mile = 1.609
  def how_many_kms(self, miles):
    return miles * self.kms_in_a_mile

converter = DistanceConverter()
kms_in_5_miles = converter.how_many_kms(5)
print(kms_in_5_miles)
# prints "8.045"

#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#PYTHON CHEATSHEET PYTHON CHEATSHEET PYTHON CHEATSHEET   #-#-#--#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Cheatsheets / Learn Python 3

Strings Strings Strings Strings Strings Strings Strings Strings Strings
Print PDF icon
Print Cheatsheet


In computer science, sequences of characters are referred to as strings. Strings can be any length and can include any character such as letters, numbers, symbols, and whitespace (spaces, tabs, new lines).

Escaping Characters
Backslashes (\) are used to escape characters in a Python string.

For instance, to print a string with quotation marks, the given code snippet can be used.
'''

txt = "She said \"Never let go\"."
print(txt) # She said "Never let go".
'''

The in Syntax
The in syntax is used to determine if a letter or a substring exists in a string. It returns True if a match is found, otherwise False is returned.
'''

game = "Popular Nintendo Game: Mario Kart"

print("l" in game) # Prints: True
print("x" in game) # Prints: False

'''
Indexing and Slicing Strings
Python strings can be indexed using the same notation as lists, since strings are lists of characters. A single character can be accessed with bracket notation ([index]), or a substring can be accessed using slicing ([start:end]).

Indexing with negative numbers counts from the end of the string.
'''

str = 'yellow'
str[1]     # => 'e'
str[-1]    # => 'w'
str[4:6]   # => 'ow'
str[:4]    # => 'yell'
str[-3:]   # => 'low'

'''
Iterate String
To iterate through a string in Python, “for…in” notation is used.
'''

str = "hello"
for c in str:
  print(c)
  
# h
# e
# l
# l
# o

'''
Built-in Function len()
In Python, the built-in len() function can be used to determine the length of an object. It can be used to compute the length of strings, lists, sets, and other countable objects.
'''

length = len("Hello")
print(length)
# Output: 5

colors = ['red', 'yellow', 'green']
print(len(colors))
# Output: 3

'''
String Concatenation
To combine the content of two strings into a single string, Python provides the + operator. This process of joining strings is called concatenation.
'''

x = 'One fish, '
y = 'two fish.'

z = x + y

print(z)
# Output: One fish, two fish.

'''
Immutable strings
Strings are immutable in Python. This means that once a string has been defined, it can’t be changed.

There are no mutating methods for strings. This is unlike data types like lists, which can be modified once they are created.


IndexError
When indexing into a string in Python, if you try to access an index that doesn’t exist, an IndexError is generated. For example, the following code would create an IndexError:
'''

fruit = "Berry"
indx = fruit[6]

'''
Python String .format()
The Python string method .format() replaces empty brace ({}) placeholders in the string with its arguments.

If keywords are specified within the placeholders, they are replaced with the corresponding named arguments to the method.
'''

msg1 = 'Fred scored {} out of {} points.'
msg1.format(3, 10)
# => 'Fred scored 3 out of 10 points.'

msg2 = 'Fred {verb} a {adjective} {noun}.'
msg2.format(adjective='fluffy', verb='tickled', noun='hamster')
# => 'Fred tickled a fluffy hamster.'

'''
String Method .lower()
The string method .lower() returns a string with all uppercase characters converted into lowercase.
'''

greeting = "Welcome To Chili's"

print(greeting.lower())
# Prints: welcome to chili's

'''
String Method .strip()
The string method .strip() can be used to remove characters from the beginning and end of a string.

A string argument can be passed to the method, specifying the set of characters to be stripped. With no arguments to the method, whitespace is removed.
'''

text1 = '   apples and oranges   '
text1.strip()       # => 'apples and oranges'

text2 = '...+...lemons and limes...-...'

# Here we strip just the "." characters
text2.strip('.')    # => '+...lemons and limes...-'

# Here we strip both "." and "+" characters
text2.strip('.+')   # => 'lemons and limes...-'

# Here we strip ".", "+", and "-" characters
text2.strip('.+-')  # => 'lemons and limes'

'''
String Method .title()
The string method .title() returns the string in title case. With title case, the first character of each word is capitalized while the rest of the characters are lowercase.
'''

my_var = "dark knight"
print(my_var.title()) 

# Prints: Dark Knight

'''
String Method .split()
The string method .split() splits a string into a list of items:

If no argument is passed, the default behavior is to split on whitespace.
If an argument is passed to the method, that value is used as the delimiter on which to split the string.
'''

text = "Silicon Valley"

print(text.split())     
# Prints: ['Silicon', 'Valley']

print(text.split('i'))  
# Prints: ['S', 'l', 'con Valley']

'''
Python string method .find()
The Python string method .find() returns the index of the first occurrence of the string passed as the argument. It returns -1 if no occurrence is found.
'''

mountain_name = "Mount Kilimanjaro"
print(mountain_name.find("o")) # Prints 1 in the console.

'''
String replace
The .replace() method is used to replace the occurence of the first argument with the second argument within the string.

The first argument is the old substring to be replaced, and the second argument is the new substring that will replace every occurence of the first one within the string.
'''

fruit = "Strawberry"
print(fruit.replace('r', 'R'))

# StRawbeRRy

'''
String Method .upper()
The string method .upper() returns the string with all lowercase characters converted to uppercase.
'''

dinosaur = "T-Rex"

print(dinosaur.upper()) 
# Prints: T-REX

'''
String Method .join()
The string method .join() concatenates a list of strings together to create a new string joined with the desired delimiter.

The .join() method is run on the delimiter and the array of strings to be concatenated together is passed in as an argument.
'''

x = "-".join(["Codecademy", "is", "awesome"])

print(x) 
# Prints: Codecademy-is-awesome


#
#
#
#
#
#
#
#
#
#
'''
Cheatsheets / Learn Python 3

Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists Lists
Print PDF icon
Print Cheatsheet


In Python, lists are ordered collections of items that allow for easy use of a set of data.

List values are placed in between square brackets [ ], separated by commas. It is good practice to put a space between the comma and the next value. The values in a list do not need to be unique (the same value can be repeated).

Empty lists do not contain any values within the square brackets.
'''

primes = [2, 3, 5, 7, 11]
print(primes)

empty_list = []

'''
Adding Lists Together
In Python, lists can be added to each other using the plus symbol +. As shown in the code block, this will result in a new list containing the same items in the same order with the first list’s items coming first.

Note: This will not work for adding one item at a time (use .append() method). In order to add one item, create a new list with a single value and then use the plus symbol to add the list.
'''

items = ['cake', 'cookie', 'bread']
total_items = items + ['biscuit', 'tart']
print(total_items)
# Result: ['cake', 'cookie', 'bread', 'biscuit', 'tart']

'''
Python Lists: Data Types
In Python, lists are a versatile data type that can contain multiple different data types within the same square brackets. The possible data types within a list include numbers, strings, other objects, and even other lists.
'''

numbers = [1, 2, 3, 4, 10]
names = ['Jenny', 'Sam', 'Alexis']
mixed = ['Jenny', 1, 2]
list_of_lists = [['a', 1], ['b', 2]]

'''
List Method .append()
In Python, you can add values to the end of a list using the .append() method. This will place the object passed in as a new element at the very end of the list. Printing the list afterwards will visually show the appended value. This .append() method is not to be confused with returning an entirely new list with the passed object.
'''

orders = ['daisies', 'periwinkle']
orders.append('tulips')
print(orders)
# Result: ['daisies', 'periwinkle', 'tulips']

'''
Aggregating Iterables Using zip()
In Python, data types that can be iterated (called iterables) can be used with the zip() function to aggregate data based on the iterables passed in.

As shown in the example, zip() is aggregating the data between the owners’ names and the dogs’ names to match the owner to their dogs. zip() returns an iterator containing the data based on what the user passes through and can be printed to visually represent the aggregated data. Empty iterables passed in will result in an empty iterator.
'''

owners_names = ['Jenny', 'Sam', 'Alexis']
dogs_names = ['Elphonse', 'Dr. Doggy DDS', 'Carter']
owners_dogs = zip(owners_names, dogs_names)
print(owners_dogs)
# Result: [('Jenny', 'Elphonse'), ('Sam', 'Dr.Doggy DDS'), ('Alexis', 'Carter')]

'''
List Item Ranges Including First or Last Item
In Python, when selecting a range of list items, if the first item to be selected is at index 0, no index needs to be specified before the :. Similarly, if the last item selected is the last item in the list, no index needs to be specified after the :.
'''

items = [1, 2, 3, 4, 5, 6]

# All items from index `0` to `3`
print(items[:4])

# All items from index `2` to the last item, inclusive
print(items[2:])

'''
List Method .count()
The .count() Python list method searches a list for whatever search term it receives as an argument, then returns the number of matching entries found.
'''

backpack = ['pencil', 'pen', 'notebook', 'textbook', 'pen', 'highlighter', 'pen']
numPen = backpack.count('pen')
print(numPen)
# Output: 3
'''

Determining List Length with len()
The Python len() function can be used to determine the number of items found in the list it accepts as an argument.
'''

knapsack = [2, 4, 3, 7, 10]
size = len(knapsack)
print(size) 
# Output: 5

'''
Zero-Indexing
In Python, list index begins at zero and ends at the length of the list minus one. For example, in this list, 'Andy' is found at index 2.
'''

names = ['Roger', 'Rafael', 'Andy', 'Novak']

'''
List Method .sort()
The .sort() Python list method will sort the contents of whatever list it is called on. Numerical lists will be sorted in ascending order, and lists of Strings will be sorted into alphabetical order. It modifies the original list, and has no return value.
'''

exampleList = [4, 2, 1, 3]
exampleList.sort()
print(exampleList)
# Output: [1, 2, 3, 4]

'''
List Indices
Python list elements are ordered by index, a number referring to their placement in the list. List indices start at 0 and increment by one.

To access a list element by index, square bracket notation is used: list[index].
'''

berries = ["blueberry", "cranberry", "raspberry"]

berries[0]   # "blueberry"
berries[2]   # "raspberry"

'''
Negative List Indices
Negative indices for lists in Python can be used to reference elements in relation to the end of a list. This can be used to access single list elements or as part of defining a list range. For instance:

To select the last element, my_list[-1].
To select the last three elements, my_list[-3:].
To select everything except the last two elements, my_list[:-2].
'''

soups = ['minestrone', 'lentil', 'pho', 'laksa']
soups[-1]   # 'laksa'
soups[-3:]  # 'lentil', 'pho', 'laksa'
soups[:-2]  # 'minestrone', 'lentil'

'''
List Slicing
A slice, or sub-list of Python list elements can be selected from a list using a colon-separated starting and ending point.

The syntax pattern is myList[START_NUMBER:END_NUMBER]. The slice will include the START_NUMBER index, and everything until but excluding the END_NUMBER item.

When slicing a list, a new list is returned, so if the slice is saved and then altered, the original list remains the same.
'''

tools = ['pen', 'hammer', 'lever']
tools_slice = tools[1:3] # ['hammer', 'lever']
tools_slice[0] = 'nail'

# Original list is unaltered:
print(tools) # ['pen', 'hammer', 'lever']
sorted() Function

'''
The Python sorted() function accepts a list as an argument, and will return a new, sorted list containing the same elements as the original. Numerical lists will be sorted in ascending order, and lists of Strings will be sorted into alphabetical order. It does not modify the original, unsorted list.
'''

unsortedList = [4, 2, 1, 3]
sortedList = sorted(unsortedList)
print(sortedList)
# Output: [1, 2, 3, 4]

#
#
#
#
#
#
#
#
#
#
'''
Cheatsheets / Learn Python 3

Dictionaries Dictionaries Dictionaries Dictionaries Dictionaries Dictionaries Dictionaries Dictionaries Dictionaries
Print PDF icon
Print Cheatsheet


Accessing and writing data in a Python dictionary
Values in a Python dictionary can be accessed by placing the key within square brackets next to the dictionary. Values can be written by placing key within square brackets next to the dictionary and using the assignment operator (=). If the key already exists, the old value will be overwritten. Attempting to access a value with a key that does not exist will cause a KeyError.

To illustrate this review card, the second line of the example code block shows the way to access the value using the key "song". The third line of the code block overwrites the value that corresponds to the key "song".
'''

my_dictionary = {"song": "Estranged", "artist": "Guns N' Roses"}
print(my_dictionary["song"])
my_dictionary["song"] = "Paradise City"

'''
Syntax of the Python dictionary
The syntax for a Python dictionary begins with the left curly brace ({), ends with the right curly brace (}), and contains zero or more key : value items separated by commas (,). The key is separated from the value by a colon (:).
'''

roaster = {"q1": "Ashley", "q2": "Dolly"}

'''
Merging Dictionaries with the .update() Method in Python
Given two dictionaries that need to be combined, Python makes this easy with the .update() function.

For dict1.update(dict2), the key-value pairs of dict2 will be written into the dict1 dictionary.

For keys in both dict1 and dict2, the value in dict1 will be overwritten by the corresponding value in dict2.
'''

dict1 = {'color': 'blue', 'shape': 'circle'}
dict2 = {'color': 'red', 'number': 42}

dict1.update(dict2)

# dict1 is now {'color': 'red', 'shape': 'circle', 'number': 42}

''' 
Dictionary value types
Python allows the values in a dictionary to be any type – string, integer, a list, another dictionary, boolean, etc. However, keys must always be an immutable data type, such as strings, numbers, or tuples.

In the example code block, you can see that the keys are strings or numbers (int or float). The values, on the other hand, are many varied data types.
'''

dictionary = {
  1: 'hello', 
  'two': True, 
  '3': [1, 2, 3], 
  'Four': {'fun': 'addition'}, 
  5.0: 5.5
}

'''
Python dictionaries
A python dictionary is an unordered collection of items. It contains data as a set of key: value pairs.
'''

my_dictionary = {1: "L.A. Lakers", 2: "Houston Rockets"}

'''
Dictionary accession methods
When trying to look at the information in a Python dictionary, there are multiple methods that access the dictionary and return lists of its contents.

.keys() returns the keys (the first object in the key-value pair), .values() returns the values (the second object in the key-value pair), and .items() returns both the keys and the values as a tuple.
'''

ex_dict = {"a": "anteater", "b": "bumblebee", "c": "cheetah"}

ex_dict.keys()
# ["a","b","c"]

ex_dict.values()
# ["anteater", "bumblebee", "cheetah"]

ex_dict.items()
# [("a","anteater"),("b","bumblebee"),("c","cheetah")]

'''
get() Method for Dictionary
Python provides a .get() method to access a dictionary value if it exists. This method takes the key as the first argument and an optional default value as the second argument, and it returns the value for the specified key if key is in the dictionary. If the second argument is not specified and key is not found then None is returned.
'''

# without default
{"name": "Victor"}.get("name")
# returns "Victor"

{"name": "Victor"}.get("nickname")
# returns None

# with default
{"name": "Victor"}.get("nickname", "nickname is not a key")
# returns "nickname is not a key"

'''
The .pop() Method for Dictionaries in Python
Python dictionaries can remove key-value pairs with the .pop() method. The method takes a key as an argument and removes it from the dictionary. At the same time, it also returns the value that it removes from the dictionary.
'''
famous_museums = {'Washington': 'Smithsonian Institution', 'Paris': 'Le Louvre', 'Athens': 'The Acropolis Museum'}
famous_museums.pop('Athens')
print(famous_museums) # {'Washington': 'Smithsonian Institution', 'Paris': 'Le Louvre'}

#
#
#
#
#
#
#
#
#
#
#
#
#
'''
Cheatsheets / Learn Python 3

Control Flow Control Flow Control Flow Control Flow Control Flow Control Flow Control Flow Control Flow Control Flow 
Print PDF icon
Print Cheatsheet


elif Statement
The Python elif statement allows for continued checks to be performed after an initial if statement. An elif statement differs from the else statement because another expression is provided to be checked, just as with the initial if statement.

If the expression is True, the indented code following the elif is executed. If the expression evaluates to False, the code can continue to an optional else statement. Multiple elif statements can be used following an initial if to perform a series of checks. Once an elif expression evaluates to True, no further elif statements are executed.
'''

# elif Statement

pet_type = "fish"

if pet_type == "dog":
  print("You have a dog.")
elif pet_type == "cat":
  print("You have a cat.")
elif pet_type == "fish":
  # this is performed
  print("You have a fish")
else:
  print("Not sure!")
  
'''  
Handling Exceptions in Python
A try and except block can be used to handle error in code block. Code which may raise an error can be written in the try block. During execution, if that code block raises an error, the rest of the try block will cease executing and the except code block will execute.
'''

def check_leap_year(year): 
  is_leap_year = False
  if year % 4 == 0:
    is_leap_year = True

try:
  check_leap_year(2018)
  print(is_leap_year) 
  # The variable is_leap_year is declared inside the function
except:
  print('Your code raised an error!')

'''  
or Operator
The Python or operator combines two Boolean expressions and evaluates to True if at least one of the expressions returns True. Otherwise, if both expressions are False, then the entire expression evaluates to False.
'''

True or True      # Evaluates to True
True or False     # Evaluates to True
False or False    # Evaluates to False
1 < 2 or 3 < 1    # Evaluates to True
3 < 1 or 1 > 6    # Evaluates to False
1 == 1 or 1 < 2   # Evaluates to True

'''
Equal Operator ==
The equal operator, ==, is used to compare two values, variables or expressions to determine if they are the same.

If the values being compared are the same, the operator returns True, otherwise it returns False.

The operator takes the data type into account when making the comparison, so a string value of "2" is not considered the same as a numeric value of 2.
'''

# Equal operator

if 'Yes' == 'Yes':
  # evaluates to True
  print('They are equal')

if (2 > 1) == (5 < 10):
  # evaluates to True
  print('Both expressions give the same result')

c = '2'
d = 2

if c == d:
  print('They are equal')
else:
  print('They are not equal')
  
'''  
Not Equals Operator !=
The Python not equals operator, !=, is used to compare two values, variables or expressions to determine if they are NOT the same. If they are NOT the same, the operator returns True. If they are the same, then it returns False.

The operator takes the data type into account when making the comparison so a value of 10 would NOT be equal to the string value "10" and the operator would return True. If expressions are used, then they are evaluated to a value of True or False before the comparison is made by the operator.
'''

# Not Equals Operator

if "Yes" != "No":
  # evaluates to True
  print("They are NOT equal")

val1 = 10
val2 = 20

if val1 != val2:
  print("They are NOT equal")

if (10 > 1) != (10 > 1000):
  # True != False
  print("They are NOT equal")
  
'''  
Comparison Operators
In Python, relational operators compare two values or expressions. The most common ones are:

< less than
> greater than
<= less than or equal to
>= greater than or equal too
If the relation is sound, then the entire expression will evaluate to True. If not, the expression evaluates to False.
'''

a = 2
b = 3
a < b  # evaluates to True
a > b  # evaluates to False
a >= b # evaluates to False
a <= b # evaluates to True
a <= a # evaluates to True

'''
if Statement
The Python if statement is used to determine the execution of code based on the evaluation of a Boolean expression.

If the if statement expression evaluates to True, then the indented code following the statement is executed.
If the expression evaluates to False then the indented code following the if statement is skipped and the program executes the next line of code which is indented at the same level as the if statement.
'''

# if Statement

test_value = 100

if test_value > 1:
  # Expression evaluates to True
  print("This code is executed!")

if test_value > 1000:
  # Expression evaluates to False
  print("This code is NOT executed!")

print("Program continues at this point.")

'''
else Statement
The Python else statement provides alternate code to execute if the expression in an if statement evaluates to False.

The indented code for the if statement is executed if the expression evaluates to True. The indented code immediately following the else is executed only if the expression evaluates to False. To mark the end of the else block, the code must be unindented to the same level as the starting if line.
'''

# else Statement

test_value = 50

if test_value < 1:
  print("Value is < 1")
else:
  print("Value is >= 1")

test_string = "VALID"

if test_string == "NOT_VALID":
  print("String equals NOT_VALID")
else:
  print("String equals something else!")

'''
and Operator
The Python and operator performs a Boolean comparison between two Boolean values, variables, or expressions. If both sides of the operator evaluate to True then the and operator returns True. If either side (or both sides) evaluates to False, then the and operator returns False. A non-Boolean value (or variable that stores a value) will always evaluate to True when used with the and operator.
'''

True and True     # Evaluates to True
True and False    # Evaluates to False
False and False   # Evaluates to False
1 == 1 and 1 < 2  # Evaluates to True
1 < 2 and 3 < 1   # Evaluates to False
"Yes" and 100     # Evaluates to True

'''
Boolean Values
Booleans are a data type in Python, much like integers, floats, and strings. However, booleans only have two values:

True
False
Specifically, these two values are of the bool type. Since booleans are a data type, creating a variable that holds a boolean value is the same as with other data types.
'''

is_true = True
is_false = False

print(type(is_true)) 
# will output: <class 'bool'>

'''
not Operator
The Python Boolean not operator is used in a Boolean expression in order to evaluate the expression to its inverse value. If the original expression was True, including the not operator would make the expression False, and vice versa.
'''

not True     # Evaluates to False
not False    # Evaluates to True
1 > 2        # Evaluates to False
not 1 > 2    # Evaluates to True
1 == 1       # Evaluates to True
not 1 == 1   # Evaluates to False
#
#
#
#
#
#
#
#
#
#
#
#
'''
Cheatsheets / Learn Python 3

Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops Loops 
Print PDF icon
Print Cheatsheet


break Keyword
In a loop, the break keyword escapes the loop, regardless of the iteration number. Once break executes, the program will continue to execute after the loop.

In this example, the output would be:

0
254
2
Negative number detected!
'''

numbers = [0, 254, 2, -1, 3]

for num in numbers:
  if (num < 0):
    print("Negative number detected!")
    break
  print(num)
  
# 0
# 254
# 2
# Negative number detected!

'''
Python List Comprehension
Python list comprehensions provide a concise way for creating lists. It consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses: [EXPRESSION for ITEM in LIST <if CONDITIONAL>].

The expressions can be anything - any kind of object can go into a list.

A list comprehension always returns a list.
'''

# List comprehension for the squares of all even numbers between 0 and 9
result = [x**2 for x in range(10) if x % 2 == 0]

print(result)
# [0, 4, 16, 36, 64]

'''
Python For Loop
A Python for loop can be used to iterate over a list of items and perform a set of actions on each item. The syntax of a for loop consists of assigning a temporary value to a variable on each successive iteration.

When writing a for loop, remember to properly indent each action, otherwise an IndentationError will result.
'''

for <temporary variable> in <list variable>:
  <action statement>
  <action statement>
 
#each num in nums will be printed below
nums = [1,2,3,4,5]
for num in nums: 
  print(num)

'''
The Python continue Keyword
In Python, the continue keyword is used inside a loop to skip the remaining code inside the loop code block and begin the next loop iteration.
'''

big_number_list = [1, 2, -1, 4, -5, 5, 2, -9]

# Print only positive numbers:
for i in big_number_list:
  if i < 0:
    continue
  print(i)

'''
Python for Loops
Python for loops can be used to iterate over and perform an action one time for each element in a list.

Proper for loop syntax assigns a temporary value, the current item of the list, to a variable on each successive iteration: for <temporary value> in <a list>:

for loop bodies must be indented to avoid an IndentationError.
'''

dog_breeds = ["boxer", "bulldog", "shiba inu"]

# Print each breed:
for breed in dog_breeds:
  print(breed)
  
'''  
Python Loops with range().
In Python, a for loop can be used to perform an action a specific number of times in a row.

The range() function can be used to create a list that can be used to specify the number of iterations in a for loop.
'''

# Print the numbers 0, 1, 2:
for i in range(3):
  print(i)

# Print "WARNING" 3 times:
for i in range(3):
  print("WARNING")

'''
Infinite Loop
An infinite loop is a loop that never terminates. Infinite loops result when the conditions of the loop prevent it from terminating. This could be due to a typo in the conditional statement within the loop or incorrect logic. To interrupt a Python program that is running forever, press the Ctrl and C keys together on your keyboard.

Python while Loops
In Python, a while loop will repeatedly execute a code block as long as a condition evaluates to True.

The condition of a while loop is always checked first before the block of code runs. If the condition is not met initially, then the code block will never run.
'''

# This loop will only run 1 time
hungry = True
while hungry:
  print("Time to eat!")
  hungry = False

# This loop will run 5 times
i = 1
while i < 6:
  print(i)
  i = i + 1
  
'''  
Python Nested Loops
In Python, loops can be nested inside other loops. Nested loops can be used to access items of lists which are inside other lists. The item selected from the outer loop can be used as the list for the inner loop to iterate over.
'''

groups = [["Jobs", "Gates"], ["Newton", "Euclid"], ["Einstein", "Feynman"]]

# This outer loop will iterate over each list in the groups list
for group in groups:
  # This inner loop will go through each name in each list
  for name in group:
    print(name)

#
#
#
#
#
#
#
#
#
#
#
#
#
'''
Cheatsheets / Learn Python 3

Modules
Print PDF icon
Print Cheatsheet

Date and Time in Python
Python provides a module named datetime to deal with dates and times.

It allows you to set date ,time or both date and time using the date(),time()and datetime() functions respectively, after importing the datetime module .
'''

import datetime
feb_16_2019 = datetime.date(year=2019, month=2, day=16)
feb_16_2019 = datetime.date(2019, 2, 16)
print(feb_16_2019) #2019-02-16

time_13_48min_5sec = datetime.time(hour=13, minute=48, second=5)
time_13_48min_5sec = datetime.time(13, 48, 5)
print(time_13_48min_5sec) #13:48:05

timestamp= datetime.datetime(year=2019, month=2, day=16, hour=13, minute=48, second=5)
timestamp = datetime.datetime(2019, 2, 16, 13, 48, 5)
print (timestamp) #2019-01-02 13:48:05

'''
Aliasing with ‘as’ keyword
In Python, the as keyword can be used to give an alternative name as an alias for a Python module or function.

'''
# Aliasing matplotlib.pyplot as plt
from matplotlib import pyplot as plt
plt.plot(x, y)

# Aliasing calendar as c
import calendar as c
print(c.month_name[1])
'''

Import Python Modules
The Python import statement can be used to import Python modules from other files.

Modules can be imported in three different ways: import module, from module import functions, or from module import *. from module import * is discouraged, as it can lead to a cluttered local namespace and can make the namespace unclear.

'''
# Three different ways to import modules:
# First way
import module
module.function()

# Second way
from module import function
function()

# Third way
from module import *
function()

'''
random.randint() and random.choice()
In Python, the random module offers methods to simulate non-deterministic behavior in selecting a random number from a range and choosing a random item from a list.

The randint() method provides a uniform random selection from a range of integers. The choice() method provides a uniform selection of a random element from a sequence.
'''

# Returns a random integer N in a given range, such that start <= N <= end
# random.randint(start, end)
r1 = random.randint(0, 10)  
print(r1) # Random integer where 0 <= r1 <= 10

# Prints a random element from a sequence
seq = ["a", "b", "c", "d", "e"]
r2 = random.choice(seq)
print(r2) # Random element in the sequence

'''
Module importing
In Python, you can import and use the content of another file using import filename, provided that it is in the same folder as the current file you are writing.
'''

# file1 content
# def f1_function():
#	  return "Hello World"

# file2
import file1

# Now we can use f1_function, because we imported file1
f1_function()


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#PYTHON CLASSES PYTHON CLASS PYTHON CLASSES PYTHON CLASS #-#-#--#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

class Shouter:
  def __init__(self):
    print("HELLO?!")

shout1 = Shouter()
# prints "HELLO?!"

shout2 = Shouter()
# prints "HELLO?!"

#---------------------

class Shouter:
  def __init__(self, phrase):
    # make sure phrase is a string
    if type(phrase) == str:

      # then shout it out
      print(phrase.upper())

shout1 = Shouter("shout")
# prints "SHOUT"

shout2 = Shouter("shout")
# prints "SHOUT"

shout3 = Shouter("let it all out")
# prints "LET IT ALL OUT"

#------------------------------

class Store:
  pass

alternative_rocks = Store()
isabelles_ices = Store()

alternative_rocks.store_name = 'Alternative Rocks'
isabelles_ices.store_name = 'Isabelle\'s Ices'

#-------------------------------

class NoCustomAttributes:
  pass

attributeless = NoCustomAttributes()

try:
  attributeless.fake_attribute
except AttributeError:
  print("This text gets printed!")

# prints "This text gets printed!"

#--------------------------------

class NoCustomAttributes:
  pass

attributeless = NoCustomAttributes()

try:
  attributeless.fake_attribute
except AttributeError:
  print("This text gets printed!")

# prints "This text gets printed!"

hasattr(attributeless, "fake_attribute")
# returns False

getattr(attributeless, "other_fake_attribute", 800)
# returns 800, the default value

#---------------------------

how_many_s = [{'s': False}, "sassafrass", 18, ["a", "c", "s", "d", "s"]]

no_s = 2

for item in how_many_s:
  if hasattr(item, "count"):
    no_s += item.count("s")

print(no_s)

#-----------------------------------


#Create a function named more_than_n that has three parameters named lst, item, and n.

#The function should return True if item appears in the list more than n times. The function should return False otherwise.

#Write your function here
def more_than_n(lst, item, n):
  if lst.count(item) > n:
    return True
  else:
    return False
  

#Uncomment the line below when your function is done
print(more_than_n([2, 4, 6, 2, 3, 2, 1, 2], 2, 3))

#---------------------------------

class SearchEngineEntry:
  def __init__(self, url):
    self.url = url

codecademy = SearchEngineEntry("www.codecademy.com")
wikipedia = SearchEngineEntry("www.wikipedia.org")

print(codecademy.url)
# prints "www.codecademy.com"

print(wikipedia.url)
# prints "www.wikipedia.org"
    
#----------------------------------

class SearchEngineEntry:
  secure_prefix = "https://"
  def __init__(self, url):
    self.url = url

  def secure(self):
    return "{prefix}{site}".format(prefix=self.secure_prefix, site=self.url)

codecademy = SearchEngineEntry("www.codecademy.com")

print(codecademy.secure())
# prints "https://www.codecademy.com"

print(wikipedia.secure())
# prints "https://www.wikipedia.org"

#------------------------------------

class Circle:
  pi = 3.14
  def __init__(self, diameter):
    print("Creating circle with diameter {d}".format(d=diameter))
    # Add assignment for self.radius here:
    self.radius = 0.5 * diameter
  def circumference(self):
    return (self.radius * 2 * self.pi)  #when use pi , should refer self.pi
	
#------------------------------

class FakeDict:
  pass

fake_dict = FakeDict()
fake_dict.attribute = "Cool"

dir(fake_dict)
# Prints ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'attribute']
  
fun_list = [10, "string", {'abc': True}]

type(fun_list)
# Prints <class 'list'>

dir(fun_list)  #print(dir(fun_list)
# Prints ['__add__', '__class__', [...], 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']

#--------------------------------
class Employee():
  def __init__(self, name):
    self.name = name

argus = Employee("Argus Filch")
print(argus)
# prints "<__main__.Employee object at 0x104e88390>"

#------------------------------

class Employee():
  def __init__(self, name):
    self.name = name

  def __repr__(self):
    return self.name

argus = Employee("Argus Filch")
print(argus)
# prints "Argus Filch"

#-----------------------------

class Student():
  def __init__(self, name, year):
    self.name = name
    self.year = year
    self.grades = []
    
  def add_grade(self, grade):
    if type(grade) == Grade:
      self.grades.append(grade)
    else:
      print("This is Not a Grade")
    
  def get_average(self):
    sum = 0
    for score in self.grades:
      sum += score.score
    print( sum/len(self.grades))


class Grade():
  minimum_passing = 65
  def __init__(self, score):
    self.score = score
  
  def is_passing(self):
    if self.score >= 65:
      print("Contratulation! You Passed!")
    else:
      print("Sorry, You did not Pass.")

        
roger = Student("Roger van der Weyden", 10)
sandro = Student('Sandro Botticelli', 12)
pieter = Student('Pieter Bruegel the Elder', 8)

pieter_grade1 = Grade(100)
pieter_grade2 = Grade(50)

pieter.add_grade(pieter_grade1)
pieter.add_grade(pieter_grade2)

pieter.get_average()


#pieter_grade.is_passing()

#-------------------------------------------

class User:
  is_admin = False
  def __init__(self, username)
    self.username = username

class Admin(User):
  is_admin = True
  
issubclass(Admin, User)
#returns True

#-----------------------------
  
issubclass(ZeroDivisionError, Exception)
# Returns True

#---------------------------------

class KitchenException(Exception):
  """
  Exception that gets thrown when a kitchen appliance isn't working
  """

class MicrowaveException(KitchenException):
  """
  Exception for when the microwave stops working
  """

class RefrigeratorException(KitchenException):
  """
  Exception for when the refrigerator stops working
  """
  
#In this code, we define three exceptions. First, we define a KitchenException that acts as the parent to our other, specific kitchen appliance exceptions. KitchenException subclasses Exception, so it behaves in the same way that regular Exceptions do. Afterward we define MicrowaveException and RefrigeratorException as subclasses.

#Since our exceptions are subclassed in this way, we can catch any of KitchenException‘s subclasses by catching KitchenException. For example:
  
def get_food_from_fridge():
  if refrigerator.cooling == False:
    raise RefrigeratorException
  else:
    return food

def heat_food(food):
  if microwave.working == False:
    raise MicrowaveException
  else:
    microwave.cook(food)
    return food

try:
  food = get_food_from_fridge()
  food = heat_food(food)
except KitchenException:
  food = order_takeout()
  
#------------------------------------------

# Define your exception up here:
class OutOfStock(Exception):
  pass

# Update the class below to raise OutOfStock
class CandleShop:
  name = "Here's a Hot Tip: Buy Drip Candles"
  def __init__(self, stock):
    self.stock = stock
    
  def buy(self, color):
    if self.stock[color] == 0:
      raise OutOfStock
    else:
      self.stock[color] = self.stock[color] - 1

candle_shop = CandleShop({'blue': 6, 'red': 2, 'green': 0})
candle_shop.buy('blue')

# This should raise OutOfStock:
# candle_shop.buy('green')

#--------------------Redefind method in sub class------------------------

class User:
  def __init__(self, username, permissions):
    self.username = username
    self.permissions = permissions

  def has_permission_for(self, key):
    if self.permissions.get(key):
      return True
    else:
      return False
	  
class Admin(User):
  def has_permission_for(self, key): #this method is redefined 
    return True

#--------------------------------------
class Sink:
  def __init__(self, basin, nozzle):
    self.basin = basin
    self.nozzle = nozzle

class KitchenSink(Sink):
  def __init__(self, basin, nozzle, trash_compactor=None):
    super().__init__(basin, nozzle)
    if trash_compactor:
      self.trash_compactor = trash_compactor
	  
#----------------------------------------
#When two classes have the same method names and attributes, we say they implement the same interface. An interface in Python usually refers to the names of the methods and the arguments they take.

class InsurancePolicy:
  def __init__(self, price_of_item):
    self.price_of_insured_item = price_of_item
  
class VehicleInsurance(InsurancePolicy):
  def get_rate(self):
    return 0.001 * self.price_of_insured_item

class HomeInsurance(InsurancePolicy):
  def get_rate(self):
    return 0.00005 * self.price_of_insured_item

#-------------------------------------------------
#Look at all the different things that + does! The hope is that all of these things are, for the arguments given to them, the intuitive result of adding them together. Polymorphism is the term used to describe the same syntax (like the + operator here, but it could be a method name) doing different actions depending on the type of data.


# For an int and an int, + returns an int
2 + 4 == 6

# For a float and a float, + returns a float
3.1 + 2.1 == 5.2

# For a string and a string, + returns a string
"Is this " + "addition?" == "Is this addition?"

# For a list and a list, + returns a list
[1, 2] + [3, 4] == [1, 2, 3, 4]

#----
#Is the same operation happening for each? How is it different? How is it similar? Does using len() to refer to these different operations make sense?

a_list = [1, 18, 32, 12]
a_dict = {'value': True}
a_string = "Polymorphism is cool!"

print(len(a_list))

print(len(a_dict))

print(len(a_string))

#----------------------------------------------------------


class Color:
  def __init__(self, red, blue, green):
    self.red = red
    self.blue = blue
    self.green = green

  def __repr__(self):
    return "Color with RGB = ({red}, {blue}, {green})".format(red=self.red, blue=self.blue, green=self.green)

  def add(self, other):
    """
    Adds two RGB colors together
    Maximum value is 255
    """
    new_red = min(self.red + other.red, 255)
    new_blue = min(self.blue + other.blue, 255)
    new_green = min(self.green + other.green, 255)

    return Color(new_red, new_blue, new_green)

red = Color(255, 0, 0)
blue = Color(0, 255, 0)

magenta = red.add(blue)
print(magenta)
# Prints "Color with RGB = (255, 255, 0)"


#
#In this code we defined a Color class that implements an addition function. Unfortunately, red.add(blue) is a little verbose for something that we have an intuitive symbol for (i.e., the + symbol). Well, Python offers the dunder method __add__ for this very reason! If we rename the add() method above to something that looks like this:
#

class Color: 
  def __add__(self, other):
    """
    Adds two RGB colors together
    Maximum value is 255
    """
    new_red = min(self.red + other.red, 255)
    new_blue = min(self.blue + other.blue, 255)
    new_green = min(self.green + other.green, 255)

    return Color(new_red, new_blue, new_green)
	
#
red = Color(255, 0, 0)
blue = Color(0, 255, 0)
green = Color(0, 0, 255)
#We can add them together using the + operator!
#

# Color with RGB: (255, 255, 0)
magenta = red + blue

# Color with RGB: (0, 255, 255)
cyan = blue + green

# Color with RGB: (255, 0, 255)
yellow = red + green

# Color with RGB: (255, 255, 255)
white = red + blue + green

#---------------------------------------------------------------------

class UserGroup:
  def __init__(self, users, permissions):
    self.user_list = users
    self.permissions = permissions

  def __iter__(self):
    return iter(self.user_list)

  def __len__(self):
    return len(self.user_list)

  def __contains__(self, user):
    return user in self.user_list

#__init__, our constructor, which sets a list of users to the instance variable self.user_list and sets the group’s permissions when we create a new UserGroup.

#__iter__, the iterator, we use the iter() function to turn the list self.user_list into an iterator so we can use for user in user_group syntax. For more information on iterators, review Python’s documentation of Iterator Types.

#__len__, the length method, so when we call len(user_group) it will return the length of the underlying self.user_list list.

#__contains__, the check for containment, allows us to use user in user_group syntax to check if a User exists in the user_list we have.

class User:
  def __init__(self, username):
    self.username = username

diana = User('diana')
frank = User('frank')
jenn = User('jenn')

can_edit = UserGroup([diana, frank], {'can_edit_page': True})
can_delete = UserGroup([diana, jenn], {'can_delete_posts': True})

print(len(can_edit))
# Prints 2

for user in can_edit:
  print(user.username)
# Prints "diana" and "frank"

if frank in can_delete:
  print("Since when do we allow Frank to delete things? Does no one remember when he accidentally deleted the site?")

#--------------------------------------------
#___________________________________Class project_______________________________________________________


class Menu():
  def __init__(self, name, items, start_time, end_time):
    self.name = name
    self.items = items
    self.start_time = start_time
    self.end_time = end_time
  def __repr__(self):
    return "{menu} menu available from {start_time} to {end_time}".format(menu = self.name, start_time = self.start_time, end_time = self.end_time)
  def calculate_bill(self, purchased_items):
    tot_price = 0
    for item in purchased_items:
      tot_price += self.items.get(item, 0)
    return tot_price

class Franchise():
  def __init__(self, name, address, menus):
    self.name = name
    self.address = address
    self.menus = menus
  
  def __repr__(self):
    return "{restaurant} Located at {address}".format(restaurant = self.name, address = self.address)
  
  def available_menus(self, time):
    avaliable_menu = []
    for menu in self.menus:
      if (menu.start_time <= time) and (time <= menu.end_time):
        avaliable_menu.append(menu)
    return avaliable_menu
  
class Business():
  def __init__(self, name, franchises):
    self.name = name
    self.franchises = franchises
       
brunch = Menu('Brunch', {'pancakes': 7.50, 'waffles': 9.00, 'burger': 11.00, 'home fries': 4.50, 'coffee': 1.50, 'espresso': 3.00, 'tea': 1.00, 'mimosa': 10.50, 'orange juice': 3.50}, 11, 16 )
# we can save the menus into a bruch_menu variable, and then use the variable for the input of menu class


early_bird = Menu('Early Bird', {'salumeria plate': 8.00, 'salad and breadsticks (serves 2, no refills)': 14.00, 'pizza with quattro formaggi': 9.00, 'duck ragu': 17.50, 'mushroom ravioli (vegan)': 13.50, 'coffee': 1.50, 'espresso': 3.00,
},15 , 18)

dinner = Menu("Dinner", {'crostini with eggplant caponata': 13.00, 'ceaser salad': 16.00, 'pizza with quattro formaggi': 11.00, 'duck ragu': 19.50, 'mushroom ravioli (vegan)': 13.50, 'coffee': 2.00, 'espresso': 3.00,}, 17, 23)

kid = Menu('Kids', {'chicken nuggets': 6.50, 'fusilli with wild mushrooms': 12.00, 'apple juice': 3.00}, 11, 21)

arepas = Menu('Arepas', {'arepa pabellon': 7.00, 'pernil arepa': 8.50, 'guayanes arepa': 8.00, 'jamon arepa': 7.50
}, 10, 20 )

flagship_store = Franchise("Flagship Store", "1232 West End Road", [brunch, early_bird, dinner, kid])

new_installment = Franchise("New Installment", "12 East Mulberry Street", [brunch, early_bird, dinner, kid])

arepas_place = Franchise("Arepas Place", "189 Fitzgerald Avenue", [arepas])


basta_fazoolin = Business("Basta Fazoolin' with my Heart", [flagship_store , new_installment])

take_a_arepa = Business("Take a' Arepa", [arepas_place])

#print(kid)
#print(brunch.calculate_bill(["pancakes", "mimosa"]))
#print(early_bird.calculate_bill(["salumeria plate", "mushroom ravioli (vegan)"]))

#print(flagship_store)

#print(flagship_store.available_menus(12))
#print(flagship_store.available_menus(17))    

#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#PYTHON CLASSES CHEATSHEET PYTHON CLASS CHEATSHEET CLASS #-#-#--#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Python repr method
The Python __repr__() method is used to tell Python what the string representation of the class should be. It can only have one parameter, self, and it should return a string.
'''

class Employee:
  def __init__(self, name):
    self.name = name

  def __repr__(self):
    return self.name

john = Employee('John')
print(john) # John

'''
Python class methods
In Python, methods are functions that are defined as part of a class. It is common practice that the first argument of any method that is part of a class is the actual object calling the method. This argument is usually called self.
'''

# Dog class
class Dog:
  # Method of the class
  def bark(self):
    print("Ham-Ham")

# Create a new instance
charlie = Dog()

# Call the method
charlie.bark()
# This will output "Ham-Ham"

'''
Instantiate Python Class
In Python, a class needs to be instantiated before use.

As an analogy, a class can be thought of as a blueprint (Car), and an instance is an actual implementation of the blueprint (Ferrari).
'''

class Car:
  "This is an empty class"
  pass

# Class Instantiation
ferrari = Car()

'''
Python Class Variables
In Python, class variables are defined outside of all methods and have the same value for every instance of the class.

Class variables are accessed with the instance.variable or class_name.variable syntaxes.
'''

class my_class:
  class_variable = "I am a Class Variable!"
  
x = my_class()
y = my_class()

print(x.class_variable) #I am a Class Variable!
print(y.class_variable) #I am a Class Variable!

'''
Python init method
In Python, the .__init__() method is used to initialize a newly created object. It is called every time the class is instantiated.
'''

class Animal:
  def __init__(self, voice):
    self.voice = voice

# When a class instance is created, the instance variable
# 'voice' is created and set to the input value.
cat = Animal('Meow')
print(cat.voice) # Output: Meow

dog = Animal('Woof') 
print(dog.voice) # Output: Woof

'''
Python type() function
The Python type() function returns the data type of the argument passed to it.
'''

a = 1
print type(a) # <type 'int'>

a = 1.1
print type(a) # <type 'float'>

a = 'b'
print type(a) # <type 'str'>

a = None
print type(a) # <type 'NoneType'>

'''
Python class
In Python, a class is a template for a data type. A class can be defined using the class keyword.
'''

# Defining a class
class Animal:
  def __init__(self, name, number_of_legs):
    self.name = name
    self.number_of_legs = number_of_legs

'''
Python dir() function
In Python, the built-in dir() function, without any argument, returns a list of all the attributes in the current scope.

With an object as argument, dir() tries to return all valid object attributes.
'''

class Employee:
  def __init__(self, name):
    self.name = name

  def print_name(self):
    print("Hi, I'm " + self.name)


print(dir())
# ['Employee', '__builtins__', '__doc__', '__file__', '__name__', '__package__', 'new_employee']

print(dir(Employee))
# ['__doc__', '__init__', '__module__', 'print_name']

'''
__main__ in Python
In Python, __main__ is an identifier used to reference the current file context. When a module is read from standard input, a script, or from an interactive prompt, its __name__ is set equal to __main__.

Suppose we create an instance of a class called CoolClass. Printing the type() of the instance will result in:

'''
<class '__main__.CoolClass'>
'''

This means that the class CoolClass was defined in the current script file.

Super() Function in Python Inheritance
Python’s super() function allows a subclass to invoke its parent’s version of an overridden method.
'''

class ParentClass:
  def print_test(self):
    print("Parent Method")

class ChildClass(ParentClass):
  def print_test(self):
    print("Child Method")
    # Calls the parent's version of print_test()
    super().print_test() 
          
child_instance = ChildClass()
child_instance.print_test()
# Output:
# Child Method
# Parent Method
'''

User-defined exceptions in Python
In Python, new exceptions can be defined by creating a new class which has to be derived, either directly or indirectly, from Python’s Exception class.
'''

class CustomError(Exception):
  pass

'''
Polymorphism in Python
When two Python classes offer the same set of methods with different implementations, the classes are polymorphic and are said to have the same interface. An interface in this sense might involve a common inherited class and a set of overridden methods. This allows using the two objects in the same way regardless of their individual types.

When a child class overrides a method of a parent class, then the type of the object determines the version of the method to be called. If the object is an instance of the child class, then the child class version of the overridden method will be called. On the other hand, if the object is an instance of the parent class, then the parent class version of the method is called.
'''

class ParentClass:
  def print_self(self):
    print('A')

class ChildClass(ParentClass):
  def print_self(self):
    print('B')

obj_A = ParentClass()
obj_B = ChildClass()

obj_A.print_self() # A
obj_B.print_self() # B
'''

Dunder methods in Python
Dunder methods, which stands for “Double Under” (Underscore) methods, are special methods which have double underscores at the beginning and end of their names.

We use them to create functionality that can’t be represented as a normal method, and resemble native Python data type interactions. A few examples for dunder methods are: __init__, __add__, __len__, and __iter__.

The example code block shows a class with a definition for the __init__ dunder method.
'''

class String:
  # Dunder method to initialize object
  def __init__(self, string): 
    self.string = string
          
string1 = String("Hello World!") 
print(string1.string) # Hello World!
'''

Method Overriding in Python
In Python, inheritance allows for method overriding, which lets a child class change and redefine the implementation of methods already defined in its parent class.

The following example code block creates a ParentClass and a ChildClass which both define a print_test() method.

As the ChildClass inherits from the ParentClass, the method print_test() will be overridden by ChildClasssuch that it prints the word “Child” instead of “Parent”.
'''

class ParentClass:
  def print_self(self):
    print("Parent")

class ChildClass(ParentClass):
  def print_self(self):
    print("Child")

child_instance = ChildClass()
child_instance.print_self() # Child
'''

Python issubclass() Function
The Python issubclass() built-in function checks if the first argument is a subclass of the second argument.

In the example code block, we check that Member is a subclass of the Family class.
'''

class Family:
  def type(self):
    print("Parent class")
    
class Member(Family):
  def type(self):
    print("Child class")
     
print(issubclass(Member, Family)) # True
'''

Python Inheritance
Subclassing in Python, also known as “inheritance”, allows classes to share the same attributes and methods from a parent or superclass. Inheritance in Python can be accomplished by putting the superclass name between parentheses after the subclass or child class name.

In the example code block, the Dog class subclasses the Animal class, inheriting all of its attributes.
'''

class Animal: 
  def __init__(self, name, legs):
    self.name = name
    self.legs = legs
        
class Dog(Animal):
  def sound(self):
    print("Woof!")

Yoki = Dog("Yoki", 4)
print(Yoki.name) # YOKI
print(Yoki.legs) # 4
Yoki.sound() # Woof!
'''

+ Operator
In Python, the + operation can be defined for a user-defined class by giving that class an .__add()__ method.
'''

class A:
  def __init__(self, a):
    self.a = a 
  def __add__(self, other):
    return self.a + other.a 
    
obj1 = A(5)
obj2 = A(10)
print(obj1 + obj2) # 15



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-PYTHON FUNCTION PYTHON FUNCTION PYTHON FUNCTION PYTHON FUNCTION PYTHON FUNCTION FUNCTION#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#


# The "def" keyword is the start of a function definition
def function_name(parameter1, parameter2):
  # The placeholder variables used inside a function definition are called parameters
  print(parameter1)
  return parameter2
# The outdent signals the end of the function definition

# "Arguments" are the values passed into a function call
argument1 = "argument 1"
argument2 = "argument 2"

# A function call uses the functions name with a pair of parentheses
# and can return a value
return_val = function_name(argument1, argument2)


#---------------------

none_var = None
if none_var:
  print("Hello!")
else:
  print("Goodbye")

# Prints "Goodbye"

#None is falsy, meaning that it evaluates to False in an if statement, which is why the above code prints “Goodbye”.

#------------

# first we define session_id as None
session_id = None

if session_id is None:
  print("session ID is None!")
  # this prints out "session ID is None!"

# we can assign something to session_id
if active_session:
  session_id = active_session.id

# but if there's no active_session, we don't send sensitive data
if session_id is not None:
  send_sensitive_data(session_id)

#-----------------------------

def no_return():
  print("You've hit the point of no return")
  # notice no return statement

here_it_is = no_return()

print(here_it_is)
# Prints out "None"

#Above we defined a function called no_return() and saved the result to a variable here_it_is. When we print() the value of here_it_is we get None, referring to Python’s None.

def fifty_percent_off(item):
  # Check if item.cost exists
  if hasattr(item, 'cost'):
    return item.cost / 2

  # If not, return None 
  return

sale_price = fifty_percent_off(product)

if sale_price is None:
  print("This product is not for sale!")
  
#Here we have implemented a function that returns the cost of a product with “50% Off” (or “half price”). We check if the item passed to our function has a cost attribute. If it exists, we return half the cost. If not, we simply return, which returns None.
#----------------------

# store the result of this print() statement in the variable prints_return
prints_return = print("What does this print function return?")

# print out the value of prints_return
print(prints_return)

# call sort_this_list.sort() and save that in list_sort_return
sort_this_list = [14, 631, 4, 51358, 50000000]

list_sort_return = sort_this_list.sort()

# print out the value of list_sort_return
print(list_sort_return)


#----------------------

# Function definition with two required parameters
def create_user(username, is_admin):
  create_email(username)
  set_permissions(is_admin)

# Function call with all two required arguments
user1 = create_user('johnny_thunder', True)

# Raises a "TypeError: Missing 1 required positional argument"
user2 = create_user('djohansen')

#----

# Function defined with one required and one optional parameter
def create_user(username, is_admin=False):
  create_email(username)
  set_permissions(is_admin)

# Calling with two arguments uses the default value for is_admin
user2 = create_user('djohansen')

#-----

# We can make all three parameters optional
def create_user(username=None, is_admin=False):
  if username is None:
    username = "Guest"
  else:
    create_email(username)
  set_permissions(is_admin)

# So we can call with just one value
user3 = create_user('ssylvain')
# Or no value at all, which would create a Guest user
user4 = create_user()

#--------------------------------

import os

def make_folders(folders_list, nest = False):
  if nest:
    """
    Nest all the folders, like
    ./Music/fun/parliament
    """
    path_to_new_folder = ""
    for folder in folders_list:
      path_to_new_folder += "/{}".format(folder)
      try:
        os.makedirs(path_to_new_folder)
      except FileExistsError:
        continue
  else:
    """
    Makes all different folders, like
    ./Music/ ./fun/ and ./parliament/
    """
    for folder in folders_list:
      try:
        os.makedirs(folder)

      except FileExistsError:
        continue

make_folders(['./Music', './fun', './parliament'])

#---------------------------------------------------

#Python will only accept functions defined with their parameters in a specific order. The required parameters need to occur before any parameters with a default argument.

# Raises a TypeError
def create_user(is_admin=False, username, password):
  create_email(username, password)
  set_permissions(is_admin)
  
# Works perfectly
def create_user(username, password, is_admin=False):
  create_email(username, password)
  set_permissions(is_admin)

#-------------------------------------------

# Define a function with a bunch of default arguments
def log_message(logging_style="shout", message="", font="Times", date=None):
  if logging_style == 'shout':
    # capitalize the message
    message = message.upper()
  print(message, date)

# Now call the function with keyword arguments
log_message(message="Hello from the past", date="November 20, 1693")

#Above we defined log_message(), which can take from 0 to 4 arguments. Since it’s not clear which order the four arguments might be defined in, we can use the parameter names to call the function. Notice that in our function call we use this syntax: message="Hello from the past". The key word message here needs to be the name of the parameter we are trying to pass the argument to.

#-----------------------------------------
def populate_list(list_to_populate=[], length=1):
  for num in range(length):
    list_to_populate.append(num)
  return list_to_populate
  
returned_list = populate_list(length=4)
print(returned_list)
# Prints [0, 1, 2, 3] -- this is expected

returned_list = populate_list(length=6)
print(returned_list)
# Prints [0, 1, 2, 3, 0, 1, 2, 3, 4, 5] -- this is a surprise!

#When we call populate_list a second time we’d expect the list [0, 1, 2, 3, 4, 5]. But the same list is used both times the function is called, causing some side-effects from the first function call to creep into the second. This is because a list is a mutable object.

#A mutable object refers to various data structures in Python that are intended to be mutated, or changed. A list has append and remove operations that change the nature of a list. Sets and dictionaries are two other mutable objects in Python.

#It might be helpful to note some of the objects in Python that are not mutable (and therefore OK to use as default arguments). int, float, and other numbers can’t be mutated (arithmetic operations will return a new number). tuples are a kind of immutable list. Strings are also immutable — operations that update a string will all return a completely new string.

#------------------------------

def add_author(authors_books, current_books=None):
  if current_books is None:
    current_books = []

  current_books.extend(authors_books)
  return current_books
  
#---------------------------------

def update_order(new_item, current_order=[]):
  current_order.append(new_item)
  return current_order

# First order, burger and a soda
order1 = update_order({'item': 'burger', 'cost': '3.50'})
order1 = update_order({'item': 'soda', 'cost': '1.50'}, order1)

# Second order, just a soda
order2 = update_order({'item': 'soda', 'cost': '1.50'})

# What's in that second order again?
print(order2)

#---------------- fix of above code --------------------

def update_order(new_item, current_order=None):
  if current_order is None:
    current_order = []
  current_order.append(new_item)
  return current_order

# First order, burger and a soda
order1 = update_order({'item': 'burger', 'cost': '3.50'})
order1 = update_order({'item': 'soda', 'cost': '1.50'}, order1)

# Second order, just a soda
order2 = update_order({'item': 'soda', 'cost': '1.50'})

# What's in that second order again?
print(order2)

#------------------------
def multiple_returns(cool_num1, cool_num2):
  sum_nums = cool_num1 + cool_num2
  div_nums = cool_num1 / cool_num2
  return sum_nums, div_nums

  
sum_and_div = multiple_returns(20, 10)

print(sum_and_div)
# Prints "(30, 2)"

print(sum_and_div[0])
# Prints "30"

#So we get those two values back in what’s called a tuple, an immutable list-like object indicated by parentheses. 

#What if we wanted to save these two results in separate variables? 
#Well we can by unpacking the function return. We can list our new variables, comma-separated, that correspond to the number of values returned:

sum, div = sum_and_div(18, 9)

print(sum)
# Prints "27"

print(div)
# Prints "2"

#----------------------

#The first method is called positional argument unpacking, because it unpacks arguments given by position.

def shout_strings(*args):
  for argument in args:
    print(argument.upper())

shout_strings("hi", "what do we have here", "cool, thanks!")
# Prints out:
# "HI"
# "WHAT DO WE HAVE HERE"
# "COOL, THANKS!"

#In shout_strings() we use a single asterisk (*) to indicate we’ll accept any number of positional arguments passed to the function. Our parameter args is a tuple of all of the arguments passed.

#Note that args is just a parameter name, and we aren’t limited to that name (although it is rather standard practice). We can also have other positional parameters before our *args parameter. 

def truncate_sentences(length, *sentences):
  for sentence in sentences:
    print(sentence[:length])

truncate_sentences(8, "What's going on here", "Looks like we've been cut off")
# Prints out:
# "What's g"
# "Looks li"

#------

#The Python library os.path has a function called join(). join() takes an arbitrary number of paths as arguments.

from os.path import join

path_segment_1 = "/Home/User"
path_segment_2 = "Codecademy/videos"
path_segment_3 = "cat_videos/surprised_cat.mp4"

# join all three of the paths here!

print(join(path_segment_1, path_segment_2, path_segment_3))

def myjoin2(*args):
  joined_string = args[0]
  for arg in args[1:]:
    joined_string += arg
  return joined_string

print(myjoin2(path_segment_1, path_segment_2, path_segment_3))


#----------------------------------

#Python doesn’t stop at allowing us to accept unlimited positional parameters, it gives us the power to define functions with unlimited keyword parameters too. The syntax is very similar, but uses two asterisks (**) instead of one. Instead of args, we call this kwargs — as a shorthand for keyword arguments.


def arbitrary_keyword_args(**kwargs):
  print(type(kwargs))
  print(kwargs)
  # See if there's an "anything_goes" keyword arg
  # and print it
  print(kwargs.get('anything_goes'))

arbitrary_keyword_args(this_arg="wowzers", anything_goes=101)
# Prints "<class 'dict'>
# Prints "{'this_arg': 'wowzers', 'anything_goes': 101}"
# Prints "101"

#As you can see, **kwargs gives us a dictionary with all the keyword arguments that were passed to arbitrary_keyword_args. We can access these arguments using standard dictionary methods.

#------------------------------------------------

def create_products(**products_dict):
  for name, price in products_dict.items():
    create_product(name, price)

create_products(Baseball = 3 , Shirt = 14, Guitar  = 70)

#-------------- is qual to  -------

def create_products(products_dict):
  for name, price in products_dict.items():
    create_product(name, price)

create_products({
  'Baseball': '3',
  'Shirt': '14',
  'Guitar': '70',


#---------------------------------

#This keyword argument unpacking syntax can be used even if the function takes other parameters. However, the parameters must be listed in this order:

#All named positional parameters
#An unpacked positional parameter (*args)
#All named keyword parameters
#An unpacked keyword parameter (**kwargs)
#Here’s a function with all possible types of parameter:

def main(filename, *args, user_list=None, **kwargs):
  if user_list is None:
    user_list = []

  if '-a' in args:
    user_list = all_users()

  if kwargs.get('active'):
    user_list = [user for user_list if user.active]

  with open(filename) as user_file:
    user_file.write(user_list)

#Looking at the signature of main() we define four different kinds of parameters. The first, filename is a normal required positional parameter. The second, *args, is all positional arguments given to a function after that organized as a tuple in the parameter args. The third, user_list, is a keyword parameter with a default value. Lastly, **kwargs is all other keyword arguments assembled as a dictionary in the parameter kwargs.

#A possible call to the function could look like this:

main("files/users/userslist.txt", 
     "-d", 
     "-a", 
     save_all_records=True, 
     user_list=current_users)

#In the body of main() these arguments would look like this:

#filename == "files/users/userslist.txt"
#args == ('-d', '-a)
#user_list == current_users
#kwargs == {'save_all_records': True}
	
#We can use all four of these kinds of parameters to create functions that handle a lot of possible arguments being passed to them.

#---------------------------------------------------------

#Not only can we accept arbitrarily many parameters to a function in our definition, but Python also supports a syntax that makes deconstructing any data that you have on hand to feed into a function that accepts these kinds of unpacked arguments. The syntax is very similar, but is used when a function is called, not when it’s defined.

def takes_many_args(*args):
  print(','.join(args))

long_list_of_args = [145, "Mexico City", 10.9, "85C"]

# We can use the asterisk "*" to deconstruct the container.
# This makes it so that instead of a list, a series of four different
# positional arguments are passed to the function
takes_many_args(*long_list_of_args)
# Prints "145,Mexico City,10.9,85C"

#We can use the * when calling the function that takes a series of positional parameters to unwrap a list or a tuple. This makes it easy to provide a sequence of arguments to a function even if that function doesn’t take a list as an argument. Similarly we can use ** to destructure a dictionary.

def pour_from_sink(temperature="Warm", flow="Medium")
  set_temperature(temperature)
  set_flow(flow)
  open_sink()

# Our function takes two keyword arguments
# If we make a dictionary with their parameter names...
sink_open_kwargs = {
  'temperature': 'Hot',
  'flow': "Slight",
}

# We can destructure them an pass to the function
pour_from_sink(**sink_open_kwargs)
# Equivalent to the following:
# pour_from_sink(temperature="Hot", flow="Slight")

#So we can also use dictionaries and pass them into functions as keyword arguments with that syntax. Notice that our pour_from_sink() function doesn’t even accept arbitrary **kwargs. We can use this destructuring syntax even when the function has a specific number of keyword or positional arguments it accepts. We just need to be careful that the object we’re destructuring matches the length (and names, if a dictionary) of the signature of the function we’re passing it into.

#---------------------- project -------------------------------------------

#----------------------- script.py-----------------------------------------------

from nile import get_distance, format_price, SHIPPING_PRICES
from test import test_function

# Define calculate_shipping_cost() here:
def calculate_shipping_cost(from_coords, to_coords, shipping_type = 'Overnight'):
  #from_lat, from_long = from_coords
  #to_lat, to_long = to_coords
  from_lat = from_coords[0]
  from_long = from_coords[1]
  to_lat = to_coords[0]
  to_long = to_coords[1]
  #print ("from_lat = " + str(from_lat))
  #print ("from_long = " + str(from_long))
  #print ("to_lat = " + str(to_lat))
  #print ("to_long = " + str(to_long))
  
  distance = get_distance(from_lat, from_long, to_lat, to_long)
  #distance = get_distance(*from_coords, *to_coords) #also works (unpacking)
  
  shipping_rate = SHIPPING_PRICES.get(shipping_type)
  
  price = distance * shipping_rate
  
  return format_price(price)
  

# Test the function by calling 
test_function(calculate_shipping_cost)

# Define calculate_driver_cost() here
def calculate_driver_cost(distance, *drivers):
  
  cheapest_driver = None
  cheapest_driver_price = None
  
  for driver in drivers:
    driver_time = driver.speed * distance
    price_for_driver = driver.salary * driver_time
    
    if cheapest_driver == None:
      cheapest_driver = driver
      cheapest_driver_price =price_for_driver
    elif price_for_driver < cheapest_driver_price:
      cheapest_driver = driver
      cheapest_driver_price =price_for_driver
  
  return cheapest_driver_price , cheapest_driver 

# Test the function by calling 
test_function(calculate_driver_cost)

# Define calculate_money_made() here
def calculate_money_made(**trips):
  total_money_made = 0
  for key, trip in trips.items():
    trip_revenue = trip.cost - trip.driver.cost
    total_money_made += trip_revenue
  
  return total_money_made   


# Test the function by calling 
test_function(calculate_money_made)

#------------------------------------------------- end of script.py-----------------------------------------------

#-------------------------------------------------- nile.py------------------------------------------------------

from math import sin, cos, atan2, sqrt

def get_distance(from_lat, from_long, to_lat, to_long):
  dlon = to_long - from_long
  dlat = from_lat - to_lat
  a = (sin(dlat/2)) ** 2 + cos(from_lat) * cos(to_lat) * (sin(dlon/2)) ** 2
  c = 2 * atan2(sqrt(a), sqrt(1-a))
  distance = a * c
  return distance

SHIPPING_PRICES = {
  'Ground': 1,
  'Priority': 1.6,
  'Overnight': 2.3,
}

def format_price(price):
  return "${0:.2f}".format(price)

#------------------------------------------------- nile.py----------------------------------------------------------

#-------------------------------------------------- test.py ----------------------------------------------------------

def test_function(fn):
  if fn.__name__ == "calculate_shipping_cost":
    test_shipping(fn)
  if fn.__name__ == "calculate_driver_cost":
    test_driver(fn)
  if fn.__name__ == "calculate_money_made":
    test_money(fn)

def test_shipping(f):
  try:
    costs = f((0, 0), (1, 1))
  except TypeError:
    print("calculate_shipping_cost() did not provide default argument for shipping_type")
    return
  if not type(costs) is str:
    print("calculate_shipping_cost() did not format the result in a string")
    return
  if costs != "$1.04":
    print("calculate_shipping_cost((0, 0), (1, 1)) returned {}. Expected result is {}".format(costs, "$1.04"))
    return
  print("OK! calculate_shipping_cost() passes tests")

class Driver:
  def __init__(self, speed, salary):
    self.speed = speed
    self.salary = salary

  def __repr__(self):
    return "Nile Driver speed {} salary {}".format(self.speed, self.salary)

driver1 = Driver(2, 10)
driver2 = Driver(7, 20)

def test_driver(f):
  try:
    price, driver = f(80, driver1, driver2)
  except TypeError:
    print("calculate_driver_cost() doesn't expect multiple driver arguments")
    return
  if type(driver) is not Driver:
    print("calculate_driver_cost() did not return driver")
    return
  if price != 1600:
    print("calculate_driver_cost() did not provide correct final price (expected {}, received {})".format(price,1600))
    return
  if driver is not driver1:
    print("calculate_driver_cost() did not provide least expensive driver")
    return
  print("OK! calculate_driver_cost() passes tests")

class Trip:
  def __init__(self, cost, driver, driver_cost):
    self.cost = cost
    driver.cost = driver_cost
    self.driver = driver

trip1 = Trip(200, driver1, 15)
trip2 = Trip(300, driver2, 40)

def test_money(f):
  try:
    money = f(UEXODI=trip1, DEFZXIE=trip2)
  except TypeError:
    print("calculate_money_made() doesn't expect multiple trip keyword arguments")
    return
  if type(money) not in (int, float):
    print("calculate_driver_cost() did not return a number")
    return
  if money != 445:
    print("calculate_driver_cost() did not provide correct final price (expected {}, received {})".format(money, 445))
    return
  print("OK! calculate_money_made() passes tests")

  
#------------------------------------------------ test.py----------------------------------------------------------

#------------------------------------ end of project ----------------------------------------------------------------


# Fuction is an object

def add_five(num):
	print num + 5

print (add_five)

#this means add_five is a object , just as a number or a list

# funciton inside a funciton 

def add_five(num):
	def add_two(num):
		return num + 2
	
	num_plus_two = add_two(num)
	print(num_plus_two + 3)

add_two(7)

add_five(10)

# Returning funciton from functions

def get_math_function(operation): # + or -
	def add(n1, n2):
		return n1 + n2
	def sub(n1, n2):
		return n1 - n2
	
	if operation == "+":
		return add
	elif operation == "-":
		return sub

add_function = get_math_function("+")
print (add_function(4,6))
#print out 10

#decorate a function 

def title_decorator(print_name_function):
	def wrapper():
		print("Professor:")
		print_name_function()
	return wrapper

def print_my_name():
	print("mike")

print_my_name()

decorated_function = title_decorator(print_my_name)

decorated_function()

#should print out  Professor:
#					mike

#Decorator 
def title_decorator(print_name_function):
	def wrapper():
		print("Professor:")
		print_name_function()
	return wrapper

@title_decorator
def print_my_name():
	print("mike")

print_my_name()

#this should do the same thing like above code
#Professor:
#mike

# decorator with parameters
def title_decorator(print_name_function):
	def wrapper(*args, **kwargs):
		print("Professor:")
		print_name_function(*args, **kwargs)
	return wrapper

@title_decorator
def print_my_name(nane):
	print(name)
	
print_my_name("shelby")

#----------------------------------------------------------------------------
from datetime import datetime

birthday = datetime(1982, 08, 06, 16, 30,0)

birthday.year
#return 1982

birthday.month
#return 08

birthday.weekday()
#return 0 for monday

datetime.now()
#return current date time 

datetime(2019, 01, 01) - datetime(2018, 01, 01)
#return datetime.timedelta(days = 365)

parsed_date = datetime.strptime('Jan 15, 2018', '%b %d, %Y)
#convert the string in to datetime

date_string = datetime.strftime(datetime.now(),%b %d, %Y) 
#convert datetime into string

#-----------------------------------------------------
#-----------------------------------------------------
#-#-Cheatsheets / Learn Python 3

Function Arguments
Print PDF icon
Print Cheatsheet

TOPICS
Syntax
Functions
Control Flow
Lists
Loops
Strings
Modules
Dictionaries
Files
Classes
Function Arguments
Default argument is fallback value
In Python, a default parameter is defined with a fallback value as a default argument. Such parameters are optional during a function call. If no argument is provided, the default value is used, and if an argument is provided, it will overwrite the default value.

def greet(name, msg="How do you do?"):
  print("Hello ", name + ', ' + msg)

greet("Ankit")
greet("Ankit", "How do you do?")

"""
this code will print the following for both the calls -
`Hello  Ankit, How do you do?`
"""
Mutable Default Arguments
Python’s default arguments are evaluated only once when the function is defined, not each time the function is called. This means that if a mutable default argument is used and is mutated, it is mutated for all future calls to the function as well. This leads to buggy behaviour as the programmer expects the default value of that argument in each function call.

# Here, an empty list is used as a default argument of the function.
def append(number, number_list=[]):
  number_list.append(number)
  print(number_list)
  return number_list

# Below are 3 calls to the `append` function and their expected and actual outputs:
append(5) # expecting: [5], actual: [5]
append(7) # expecting: [7], actual: [5, 7]
append(2) # expecting: [2], actual: [5, 7, 2]
Python Default Arguments
A Python function cannot define a default argument in its signature before any required parameters that do not have a default argument. Default arguments are ones set using the form parameter=value. If no input value is provided for such arguments, it will take on the default value.

# Correct order of declaring default argments in a function
def greet(name, msg = "Good morning!"):
  print("Hello ", name + ', ' + msg)
  
# The function can be called in the following ways
greet("Ankit")
greet("Kyla","How are you?")

# The following function definition would be incorrect
def greet(msg = "Good morning!", name):
  print("Hello ", name + ', ' + msg) 
# It would cause a "SyntaxError: non-default argument follows default argument"
Python function default return value
If we do not not specify a return value for a Python function, it returns None. This is the default behaviour.

# Function returning None
def my_function(): pass

print(my_function())

#Output 
None
Python variable None check
To check if a Python variable is None we can make use of the statement variable is None.

If the above statement evaluates to True, the variable value is None.

# Variable check for None
if variable_name is None:
    print "variable is None"
else:
    print "variable is NOT None"
Python function arguments
A function can be called using the argument name as a keyword instead of relying on its positional value. Functions define the argument names in its composition then those names can be used when calling the function.

# The function will take arg1 and arg2
def func_with_args(arg1, arg2):
  print(arg1 + ' ' + arg2)
  
func_with_args('First', 'Second')
# Prints:
# First Second

func_with_args(arg2='Second', arg1='First')
# Prints
# First Second

#-------------------------------------------------------------------
#-------------------------------------------------------------------



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#----------FUNCTION CHEATSHEET FUNCTION CHEATSHEET FUNCTION CHEATSHEET FUNCTION CHEATSHEET     ---------------------#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Cheatsheets / Learn Python 3

Functions
Print PDF icon
Print Cheatsheet

TOPICS
Syntax
Functions
Control Flow
Lists
Loops
Strings
Modules
Dictionaries
Files
Classes
Function Arguments
Function Parameters
Sometimes functions require input to provide data for their code. This input is defined using parameters.

Parameters are variables that are defined in the function definition. They are assigned the values which were passed as arguments when the function was called, elsewhere in the code.

For example, the function definition defines parameters for a character, a setting, and a skill, which are used as inputs to write the first sentence of a book.'''

def write_a_book(character, setting, special_skill):
  print(character + " is in " + 
        setting + " practicing her " + 
        special_skill)

'''
Multiple Parameters
Python functions can have multiple parameters. Just as you wouldn’t go to school without both a backpack and a pencil case, functions may also need more than one input to carry out their operations.

To define a function with multiple parameters, parameter names are placed one after another, separated by commas, within the parentheses of the function definition.'''

def ready_for_school(backpack, pencil_case):
  if (backpack == 'full' and pencil_case == 'full'):
    print ("I'm ready for school!")
    
'''    
Functions
Some tasks need to be performed multiple times within a program. Rather than rewrite the same code in multiple places, a function may be defined using the def keyword. Function definitions may include parameters, providing data input to the function.

Functions may return a value using the return keyword followed by the value to return.
'''

# Define a function my_function() with parameter x

def my_function(x):
  return x + 1

# Invoke the function

print(my_function(2))      # Output: 3
print(my_function(3 + 5))  # Output: 9

'''
Function Indentation
Python uses indentation to identify blocks of code. Code within the same block should be indented at the same level. A Python function is one type of code block. All code under a function declaration should be indented to identify it as part of the function. There can be additional indentation within a function to handle other statements such as for and if so long as the lines are not indented less than the first line of the function code.
'''

# Indentation is used to identify code blocks

def testfunction(number):
  # This code is part of testfunction
  print("Inside the testfunction")
  sum = 0
  for x in range(number):
    # More indentation because 'for' has a code block
    # but still part of he function
    sum += x
  return sum
print("This is not part of testfunction")
'''

Calling Functions
Python uses simple syntax to use, invoke, or call a preexisting function. A function can be called by writing the name of it, followed by parentheses.

For example, the code provided would call the doHomework() method.
'''

doHomework()

'''
Function Arguments
Parameters in python are variables — placeholders for the actual values the function needs. When the function is called, these values are passed in as arguments.

For example, the arguments passed into the function .sales() are the “The Farmer’s Market”, “toothpaste”, and “$1” which correspond to the parameters grocery_store, item_on_sale, and cost.
'''

def sales(grocery_store, item_on_sale, cost):
  print(grocery_store + " is selling " + item_on_sale + " for " + cost) 

sales("The Farmer’s Market", "toothpaste", "$1")

'''
Function Keyword Arguments
Python functions can be defined with named arguments which may have default values provided. When function arguments are passed using their names, they are referred to as keyword arguments. The use of keyword arguments when calling a function allows the arguments to be passed in any order — not just the order that they were defined in the function. If the function is invoked without a value for a specific argument, the default value will be used.
'''

def findvolume(length=1, width=1, depth=1):
  print("Length = " + str(length))
  print("Width = " + str(width))
  print("Depth = " + str(depth))
  return length * width * depth;

findvolume(1, 2, 3)
findvolume(length=5, depth=2, width=4)
findvolume(2, depth=3, width=4)
'''

Returning Multiple Values
Python functions are able to return multiple values using one return statement. All values that should be returned are listed after the return keyword and are separated by commas.

In the example, the function square_point() returns x_squared, y_squared, and z_squared.
'''

def square_point(x, y, z):
  x_squared = x * x
  y_squared = y * y
  z_squared = z * z
  # Return all three values:
  return x_squared, y_squared, z_squared

three_squared, four_squared, five_squared = square_point(3, 4, 5)

'''
The Scope of Variables
In Python, a variable defined inside a function is called a local variable. It cannot be used outside of the scope of the function, and attempting to do so without defining the variable outside of the function will cause an error.

In the example, the variable a is defined both inside and outside of the function. When the function f1() is implemented, a is printed as 2 because it is locally defined to be so. However, when printing a outside of the function, a is printed as 5 because it is implemented outside of the scope of the function.
'''

a = 5

def f1():
  a = 2
  print(a)
  
print(a)   # Will print 5
f1()       # Will print 2

'''
Returning Value from Function
A return keyword is used to return a value from a Python function. The value returned from a function can be assigned to a variable which can then be used in the program.

In the example, the function check_leap_year returns a string which indicates if the passed parameter is a leap year or not.
'''

def check_leap_year(year): 
  if year % 4 == 0:
    return str(year) + " is a leap year."
  else:
    return str(year) + " is not a leap year."

year_to_check = 2018
returned_value = check_leap_year(year_to_check)
print(returned_value) # 2018 is not a leap year.

'''
Global Variables
A variable that is defined outside of a function is called a global variable. It can be accessed inside the body of a function.

In the example, the variable a is a global variable because it is defined outside of the function prints_a. It is therefore accessible to prints_a, which will print the value of a.
'''

a = "Hello"

def prints_a():
  print(a)
  
# will print "Hello"
prints_a()

'''
Parameters as Local Variables
Function parameters behave identically to a function’s local variables. They are initialized with the values passed into the function when it was called.

Like local variables, parameters cannot be referenced from outside the scope of the function.

In the example, the parameter value is defined as part of the definition of my_function, and therefore can only be accessed within my_function. Attempting to print the contents of value from outside the function causes an error.
'''

def my_function(value):
  print(value)   
  
# Pass the value 7 into the function
my_function(7) 

# Causes an error as `value` no longer exists
print(value) 

'''

Cheatsheets / Learn Python 3

Function Arguments
Print PDF icon
Print Cheatsheet


Default argument is fallback value
In Python, a default parameter is defined with a fallback value as a default argument. Such parameters are optional during a function call. If no argument is provided, the default value is used, and if an argument is provided, it will overwrite the default value.
'''

def greet(name, msg="How do you do?"):
  print("Hello ", name + ', ' + msg)

greet("Ankit")
greet("Ankit", "How do you do?")

"""
this code will print the following for both the calls -
`Hello  Ankit, How do you do?`
"""
'''

Mutable Default Arguments
Python’s default arguments are evaluated only once when the function is defined, not each time the function is called. This means that if a mutable default argument is used and is mutated, it is mutated for all future calls to the function as well. This leads to buggy behaviour as the programmer expects the default value of that argument in each function call.
'''

# Here, an empty list is used as a default argument of the function.
def append(number, number_list=[]):
  number_list.append(number)
  print(number_list)
  return number_list

# Below are 3 calls to the `append` function and their expected and actual outputs:
append(5) # expecting: [5], actual: [5]
append(7) # expecting: [7], actual: [5, 7]
append(2) # expecting: [2], actual: [5, 7, 2]

'''
Python Default Arguments
A Python function cannot define a default argument in its signature before any required parameters that do not have a default argument. Default arguments are ones set using the form parameter=value. If no input value is provided for such arguments, it will take on the default value.
'''

# Correct order of declaring default argments in a function
def greet(name, msg = "Good morning!"):
  print("Hello ", name + ', ' + msg)
  
# The function can be called in the following ways
greet("Ankit")
greet("Kyla","How are you?")

# The following function definition would be incorrect
def greet(msg = "Good morning!", name):
  print("Hello ", name + ', ' + msg) 
# It would cause a "SyntaxError: non-default argument follows default argument"

'''
Python function default return value
If we do not not specify a return value for a Python function, it returns None. This is the default behaviour.
'''

# Function returning None
def my_function(): pass

print(my_function())

#Output 
None

'''
Python variable None check
To check if a Python variable is None we can make use of the statement variable is None.

If the above statement evaluates to True, the variable value is None.
'''

# Variable check for None
if variable_name is None:
    print "variable is None"
else:
    print "variable is NOT None"

'''
Python function arguments
A function can be called using the argument name as a keyword instead of relying on its positional value. Functions define the argument names in its composition then those names can be used when calling the function.
'''

# The function will take arg1 and arg2
def func_with_args(arg1, arg2):
  print(arg1 + ' ' + arg2)
  
func_with_args('First', 'Second')
# Prints:
# First Second

func_with_args(arg2='Second', arg1='First')
# Prints
# First Second




#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#----------PYTHON RECURSION PYTHON RECURSION PYTHON RECURSION PYTHON RECURSION PYTHON RECURSION---------------------#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Call Stacks and Execution Frames
A recursive approach requires the function invoking itself with different arguments. How does the computer keep track of the various arguments and different function invocations if it’s the same function definition?

Repeatedly invoking functions may be familiar when it occurs sequentially, but it can be jarring to see this invocation occur within a function definition.

Languages make this possible with call stacks and execution contexts.

Stacks, a data structure, follow a strict protocol for the order data enters and exits the structure: the last thing to enter is the first thing to leave.

Your programming language often manages the call stack, which exists outside of any specific function. This call stack tracks the ordering of the different function invocations, so the last function to enter the call stack is the first function to exit the call stack

we can think of execution contexts as the specific values we plug into a function call.

A function which adds two numbers:

Invoking the function with 3 and 4 as arguments...
execution context:
X = 3
Y = 4

Invoking the function with 6 and 2 as arguments...
execution context:
X = 6
Y = 2
Consider a pseudo-code function which sums the integers in an array:

 function, sum_list 
   if list has a single element
     return that single element
   otherwise...
     add first element to value of sum_list called with every element minus the first
This function will be invoked as many times as there are elements within the list! Let’s step through:

psudo-code

Sum_list(arr):
    if arr empty ?:
        return 0
    return first + sum_list(arr-1)


CALL STACK EMPTY
___________________

Our first function call...
sum_list([5, 6, 7])

CALL STACK CONTAINS
___________________
sum_list([5, 6, 7])
with the execution context of a list being [5, 6, 7]
___________________

Base case, a list of one element not met.
We invoke sum_list with the list of [6, 7]...

CALL STACK CONTAINS
___________________
sum_list([6, 7])
with the execution context of a list being [6, 7]
___________________
sum_list([5, 6, 7])
with the execution context of a list being [5, 6, 7]
___________________

Base case, a list of one element not met.
We invoke sum_list with the list of [7]...

CALL STACK CONTAINS
___________________
sum_list([7])
with the execution context of a list being [7]
___________________
sum_list([6, 7])
with the execution context of a list being [6, 7]
___________________
sum_list([5, 6, 7])
with the execution context of a list being [5, 6, 7]
___________________

We've reached our base case! List is one element. 
We return that one element.
This return value does two things:

1) "pops" sum_list([7]) from CALL STACK.
2) provides a return value for sum_list([6, 7])

----------------
CALL STACK CONTAINS
___________________
sum_list([6, 7])
with the execution context of a list being [6, 7]
RETURN VALUE = 7
___________________
sum_list([5, 6, 7])
with the execution context of a list being [5, 6, 7]
___________________

sum_list([6, 7]) waits for the return value of sum_list([7]), which it just received. 

sum_list([6, 7]) has resolved and "popped" from the call stack...


----------------
CALL STACK contains
___________________
sum_list([5, 6, 7])
with the execution context of a list being [5, 6, 7]
RETURN VALUE = 6 + 7
___________________

sum_list([5, 6, 7]) waits for the return value of sum_list([6, 7]), which it just received. 
sum_list([5, 6, 7]) has resolved and "popped" from the call stack.


----------------
CALL STACK is empty
___________________
RETURN VALUE = (5 + 6 + 7) = 18





RECURSION: CONCEPTUAL
Base Case and Recursive Step
Recursion has two fundamental aspects: the base case and the recursive step.

When using iteration, we rely on a counting variable and a boolean condition. For example, when iterating through the values in a list, we would increment the counting variable until it exceeded the length of the dataset.

Recursive functions have a similar concept, which we call the base case. The base case dictates whether the function will recurse, or call itself. Without a base case, it’s the iterative equivalent to writing an infinite loop.

Because we’re using a call stack to track the function calls, your computer will throw an error due to a stack overflow if the base case is not sufficient.

The other fundamental aspect of a recursive function is the recursive step. This portion of the function is the step that moves us closer to the base case.

In an iterative function, this is handled by a loop construct that decrements or increments the counting variable which moves the counter closer to a boolean condition, terminating the loop.

In a recursive function, the “counting variable” equivalent is the argument to the recursive call. If we’re counting down to 0, for example, our base case would be the function call that receives 0 as an argument. We might design a recursive step that takes the argument passed in, decrements it by one, and calls the function again with the decremented argument. In this way, we would be moving towards 0 as our base case.

Analyzing the Big O runtime of a recursive function is very similar to analyzing an iterative function. Substitute iterations of a loop with recursive calls.

For example, if we loop through once for each element printing the value, we have a O(N) or linear runtime. Similarly, if we have a single recursive call for each element in the original function call, we have a O(N) or linear runtime.'''


'''RECURSION: PYTHON
Building Our Own Call Stack
The best way to understand recursion is with lots of practice! At first, this method of solving a problem can seem unfamiliar but by the end of this lesson, we’ll have implemented a variety of algorithms using a recursive approach.

Before we dive into recursion, let’s replicate what’s happening in the call stack with an iterative function.

The call stack is abstracted away from us in Python, but we can recreate it to understand how recursive calls build up and resolve.

Let’s write a function that sums every number from 1 to the given input.

sum_to_one(4)
# 10
sum_to_one(11)
# 66
To depict the steps of a recursive function, we’ll use a call stack and execution contexts for each function call.

The call stack stores each function (with its internal variables) until those functions resolve in a last in, first out order.

call_stack = []
recursive_func()
call_stack = [recursive_func_1]

# within the body of recursive_func, another call to recursive_func()
call_stack = [recursive_func_1, recursive_func_2]
# the body of the second call to recursive_func resolves...
call_stack = [recursive_func_1]
# the body of the original call to recursive_func resolves...
call_stack = [] 
Execution contexts are a mapping between variable names and their values within the function during execution. We can use a list for our call stack and a dictionary for each execution context.

Let’s get started!

Instructions
1.
Define a sum_to_one() function that has n as the sole parameter.

Inside the function body:

declare the variable result and set it to 1.
declare the variable call_stack and set it to an empty list.
Use multiple return to return both of these values: result, call_stack

You can return multiple values like so:

def two_things(a, b):
  return a, b

first, second = two_things("apple", "pie")
first # "apple"
second # "pie"
2.
Fill in the sum_to_one() function body by writing a while loop after the variable call_stack.

This loop represents the recursive calls which lead to a base case.

We’ll want to loop until the input n reaches 1.

Inside the loop, create a variable execution_context and assign it to a dictionary with the key of "n_value" pointing to n.

Use a list method to add execution_context to the end of call_stack.

This is our way of simulating the recursive function calls being “pushed” onto the system’s call stack.

Decrement n after its value has been stored.

End the loop by printing call_stack.

We’re using a Python dictionary to represent the execution context of each recursive call.

Here’s how it would look if the execution context captured a value foo:

call_stack = []
while foo != 1:
  execution_context = {"foo_value": foo}
  call_stack.append(execution_context)
  foo -= 1
3.
After the while loop concludes, we’ve reached our “base case”, where n == 1.

At this point we haven’t summed any values, but we have all the information we’ll need stored in our call_stack.

In the next exercise, we’ll handle the summation of values from the execution contexts captured in call_stack.

For now, print out “BASE CASE REACHED” outside of the loop block before our multiple return statement.
'''
# define your sum_to_one() function above the function call
'''
def sum_to_one(num):

  if num == 0:
    return 0
  return num + sum_to_one(num-1)
'''

def sum_to_one(n):
  result = 1
  call_stack = []
  while n > 1:
    execution_context = {'n_value':n}
    call_stack.append(execution_context)
    n -= 1
    print(call_stack)  
  print('BASE CASE REACHED')
  return result, call_stack



print(sum_to_one(4))

'''

RECURSION: PYTHON
Building Our Own Call Stack, Part II
In the previous exercise, we used an iterative function to implement how a call stack accumulates execution contexts during recursive function calls.

We’ll now address the conclusion of this function, where the separate values stored in the call stack are accumulated into a single return value.

Instructions
1.
This is the point in a recursive function when we would begin returning values as the function calls are “popped” off the call stack.

We’ll use another while loop to simulate this process. Write the while loop below the “Base Case Reached” print statement.

This loop will run as long as there are execution contexts stored in call_stack.

Inside this second loop:

declare the variable return_value
assign the last element in call_stack to return_value.
Remove that value from call_stack otherwise you’ll have an infinite loop!
Print call_stack to see how the execution contexts are removed from call_stack.

This loop runs as long as there are elements within call_stack.

We’ll also want to remove values from call_stack in each iteration.

You can set this as a condition on a loop like so:

cats = ['buffy', 'wampus', 'felix']

while len(cats) != 0:
  cat = cats.pop()
  print(cat)

# 'felix'
# 'wampus'
# 'buffy'

2.
Print that you’re adding return_value["n_value"] to result and their respective values.

Finish the loop by retrieving "n_value" from return_value and add it to result.

Each element we .pop() from call_stack is a dictionary which represents a single recursive function call. We can access the value stored in the dictionary with the key like so:

execution_context = {"my_value": 42}
execution_context["my_value"]
# 42'''

def sum_to_one(n):
  result = 1
  call_stack = []
  
  while n != 1:
    execution_context = {"n_value": n}
    call_stack.append(execution_context)
    n -= 1
    print(call_stack)
  print("BASE CASE REACHED")

  return_value = 0
  while call_stack != []:
    
    return_value += call_stack[-1]["n_value"]
    call_stack.pop(-1)
    print(return_value)

  result += return_value

  return result, call_stack

sum_to_one(4)

'''
RECURSION: PYTHON
Sum to One with Recursion
Now that we’ve built a mental model for how recursion is handled by Python, let’s implement the same function and make it truly recursive.

To recap: We want a function that takes an integer as an input and returns the sum of all numbers from the input down to 1.

sum_to_one(4)
# 4 + 3 + 2 + 1
# 10
Here’s how this function would look if we were to write it iteratively:

def sum_to_one(n):
  result = 0
  for num in range(n, 0, -1):
    result += num
  return result

sum_to_one(4)
# num is set to 4, 3, 2, and 1
# 10
We can think of each recursive call as an iteration of the loop above. In other words, we want a recursive function that will produce the following function calls:

recursive_sum_to_one(4)
recursive_sum_to_one(3)
recursive_sum_to_one(2)
recursive_sum_to_one(1)
Every recursive function needs a base case when the function does not recurse, and a recursive step, when the recursing function moves towards the base case.

Base case:

The integer given as input is 1.
Recursive step:

The recursive function call is passed an argument 1 less than the last function call.
Instructions
1.
Define the sum_to_one() function.

It takes n as the sole parameter.

We’ll start by setting up our base case.

This function should NOT recurse if the given input, n is 1.

In the base case, we return n.

2.
Now, we’ll consider the recursive step.

We want our return value to be the current input added to the return value of sum_to_one().

We also need to invoke sum_to_one() with an argument that will get us closer to the base case.

# return {recursive call} + {current input}
This should be a single line solution:

return argument + recursive_call(argument - 1)
3.
Each recursive call is responsible for adding one of those integers to the ultimate total.

To help us visualize the different function calls, add a print statement before the recursive call that tells us the current value of n.

Use the following string for the print statement: print("Recursing with input: {0}".format(n))

Let’s test out our function. Call sum_to_one() with 7 as input and print out the result. Nice work!

If we try to print the value before the return statement, we’ll never see it!


def sum_to_one(n):
  if n == 1:
    return n
  else:
    return n + sum_to_one(n - 1)
    print("You will never see meeeeeee!")'''
    
# Define sum_to_one() below...
def sum_to_one(n):
  print("Recursing with input: {0}".format(n))
  if n == 1:
    return n
  return sum_to_one(n-1) + n

# uncomment when you're ready to test
print(sum_to_one(7))

'''RECURSION: PYTHON
Recursion and Big O
Excellent job writing your first recursive function. Our next task may seem familiar so there won’t be as much guidance.

We’d like a function factorial that, given a positive integer as input, returns the product of every integer from 1 up to the input. If the input is less than 2, return 1.

For example:

factorial(4)
# 4 * 3 * 2 * 1
# 24
Since this function is similar to the previous problem, we’ll add an additional wrinkle. You’ll need to evaluate the big O runtime of the function.

With an iterative function, we would consider how the number of iterations grows in relation to the size of the input.

For example you may ask yourself, are we looping once more for each new element in the list?

That’s linear or O(N).

Are we looping an additional number of elements in the list for each new element in the list?

That’s quadratic or O(N^2).

With recursive functions, the thought process is similar but we’re replacing loop iterations with recursive function calls.

In other words, are we recursing once more for each new element in the list?

That’s linear or O(N).

Let’s analyze our previous function, sum_to_one().

sum_to_one(4)
# recursive call to sum_to_one(3)
# recursive call to sum_to_one(2)
# recursive call to sum_to_one(1)

# Let's increase the input...

sum_to_one(5)
# recursive call to sum_to_one(4)
# recursive call to sum_to_one(3)
# recursive call to sum_to_one(2)
# recursive call to sum_to_one(1)
What do you think? We added one to the input, how many more recursive calls were necessary?

Talk through a few more inputs and then start coding when you’re ready to move on.

Instructions
1.
Define the factorial function with one parameter: n.

Set up a base case.

Think about the input(s) that wouldn’t need a recursive call for your function.

Return the appropriate value.

Factorial numbers are the total product of every number from 1 to a given input.

With 0 or 1, we don’t need any other number to compute the factorial.

if n <= 1:
  return 1
2.
Now let’s consider the recursive step for factorial().

If we’re in the recursive step that means factorial() has been invoked with an integer of at least 2.

We need to return the current input value multiplied by the return value of the recursive call.

Structure the recursive call so it invokes factorial() with an argument one less than the current input.

To compute factorial(3), we’d need factorial(2) * factorial(1). Each recursive call decrements the argument by one.

Your recursive step should look like the following:

return # {current input} * factorial(current input minus one)
3.
Nice work, test out your function by printing the result of calling factorial() with 12 as an input.

Now, change the input to a really large number, think big, and run the code.

If you chose an input large enough, you should see a RecursionError.'''

# Define factorial() below:
def factorial(n):
  print("factorial Called Once.")
  if n == 1:
    return n
  
  return n * factorial(n-1)

print(factorial(6))

'''RECURSION: PYTHON
Stack Over-Whoa!
The previous exercise ended with a stack overflow, which is a reminder that recursion has costs that iteration doesn’t. We saw in the first exercise that every recursive call spends time on the call stack.

Put enough function calls on the call stack, and eventually there’s no room left.

Even when there is room for any reasonable input, recursive functions tend to be at least a little less efficient than comparable iterative solutions because of the call stack.

The beauty of recursion is how it can reduce complex problems into an elegant solution of only a few lines of code. Recursion forces us to distill a task into its smallest piece, the base case, and the smallest step to get there, the recursive step.

Let’s compare two solutions to a single problem: producing a power set. A power set is a list of all subsets of the values in a list.

This is a really tough algorithm. Don’t be discouraged!

power_set(['a', 'b', 'c'])
# [
#   ['a', 'b', 'c'], 
#   ['a', 'b'], 
#   ['a', 'c'], 
#   ['a'], 
#   ['b', 'c'], 
#   ['b'], 
#   ['c'], 
#   []
# ]
Phew! That’s a lot of lists! Our input length was 3, and the list returned had a length of 8.

Producing subsets requires a runtime of at least O(2^N), we’ll never do better than that because a set of N elements creates a power set of 2^N elements.

Binary, a number system of base 2, can represent 2^N numbers for N binary digits. For example:

# 1 binary digit, 2 numbers
# 0 in binary
0
# 1 in binary
1

# 2 binary digits, 4 numbers
# 00 => 0
# 01 => 1
# 10 => 2
# 11 => 3
The iterative approach uses this insight for a very clever solution by including an element in the subset if its “binary digit” is 1.

set = ['a', 'b', 'c']
binary_number = "101"
# produces the subset ['a', 'c']
# 'b' is left out because its binary digit is 0
That process is repeated for all O(2^N) numbers!

Here is the complete solution. You’re not expected to understand every line, just take in the level of complexity.

def power_set(set):
  power_set_size = 2**len(set)
  result = []

  for bit in range(0, power_set_size):
    sub_set = []
    for binary_digit in range(0, len(set)):
      if((bit & (1 << binary_digit)) > 0):
        sub_set.append(set[binary_digit])
    result.append(sub_set)
  return result
Very clever but not very intuitive! Let’s try recursion.

Consider the base case, where the problem has become so simple we can solve it without doing any work.

What’s the simplest power set possible? An empty list!

power_set([])
# [[]]
Now the recursive step. We need to progress towards our base case, an empty list, so we’ll be removing an element from the input.

Examine the simplest powerset that isn’t the base case:

power_set(['a'])
# [[], ['a']]
A power set in the recursive step requires:

all subsets which contain the element
in this case "a"
all subsets which don’t contain the element
in this case [].
With the recursive approach, we’re able to articulate the problem in terms of itself. No need to bring in a whole number system to find the solution!

Here’s the recursive solution in its entirety:

def power_set(my_list):
  if len(my_list) == 0:
    return [[]]
  power_set_without_first = power_set(my_list[1:])
  with_first = [ [my_list[0]] + rest for rest in power_set_without_first ]
  return with_first + power_set_without_first
Neither of these solutions is simple, this is a complicated algorithm, but the recursive solution is almost half the code and more directly conveys what this algorithm does.

Give yourself a pat on the back for making it through a tough exercise!

Instructions
Run the code to see subsets of universities.

Try adding your own school.

See how large you can make the input list before these computations become impossibly slow…

O(2^N) runtime is no joke!'''

def power_set(my_list):
    # base case: an empty list
    if len(my_list) == 0:
        return [[]]
    # recursive step: subsets without first element
    power_set_without_first = power_set(my_list[1:])
    # subsets with first element
    with_first = [ [my_list[0]] + rest for rest in power_set_without_first ]
    # return combination of the two
    return with_first + power_set_without_first

#Power_set Function made with irterate method.
def ipower_set(set):
  power_set_size = 2**len(set)
  result = []

  for bit in range(0, power_set_size):
    sub_set = []
    for binary_digit in range(0, len(set)):
      if((bit & (1 << binary_digit)) > 0):
        sub_set.append(set[binary_digit])
    result.append(sub_set)
  return result
  
universities = ['MIT', 'UCLA', 'Stanford', 'NYU', 'BIM','DU']
power_set_of_universities = power_set(universities)
#print(ipower_set(universities))

for set in power_set_of_universities:
  print(set)

#note for shifts operation

a = 5
print( a << 10) # = 5 * pow(2, 10) = 5 * 2**10
'''These operators accept integers as arguments. They shift the first argument to the left or right by the number of bits given by the second argument.

A right shift by n bits is defined as floor division by pow(2,n). A left shift by n bits is defined as multiplication with pow(2,n).
Shift Operation'''


'''
RECURSION: PYTHON
No Nested Lists Anymore, I Want Them to Turn Flat
Let’s use recursion to solve another problem involving lists: flatten().

We want to write a function that removes nested lists within a list but keeps the values contained.

nested_planets = ['mercury', 'venus', ['earth'], 'mars', [['jupiter', 'saturn']], 'uranus', ['neptune', 'pluto']]

flatten(nested_planets)
# ['mercury', 
#  'venus', 
#  'earth', 
#  'mars', 
#  'jupiter', 
#  'saturn', 
#  'uranus', 
#  'neptune', 
#  'pluto']
Remember our tools for recursive functions. We want to identify a base case, and we need to think about a recursive step that takes us closer to achieving the base case.

For this problem, we have two scenarios as we move through the list.

The element in the list is a list itself.
We have more work to do!
The element in the list is not a list.
All set!
Which is the base case and which is the recursive step?

Instructions
1.
Define flatten() which has a single parameter named my_list.

We’ll start by declaring a variable, result and setting it to an empty list.

result is our intermediary variable that houses elements from my_list.

Return result.

2.
Returning an empty list isn’t much good to us, it should be filled with the values contained in my_list.

Use a for loop to iterate through my_list.

Inside the loop, we need a conditional for our recursive step. Check if the element in the current iteration is a list.

We can use Python’s isinstance() like so:

a_list = ['listing it up!']
not_a_list = 'string here'

isinstance(a_list, list)
# True
isinstance(not_a_list, list)
# False
For now, print "List found!" in the conditional.

Outside of the method definition, call flatten() and pass planets as an argument.

Use isinstance(iteration_element, list).

Here’s an example:

my_list = ['apples', ['cherries'], 'bananas']

for element in my_list:
  if isinstance(element, list):
    print("this element is a list!")
  else:
    print(element)

# apples
# this element is a list!
# bananas
3.
We need to make the recursive step draw us closer to the base case, where every element is not a list.

After your print statement, declare the variable flat_list, and assign it to a recursive call to flatten() passing in your iterating variable as the argument.

flatten() will return a list, update result so it now includes every element contained in flat_list.

Test flatten() by calling it on the planets and printing the result.

We can combine two lists like so:

first_list = ['a', 'b', 'c']
second_list = ['d', 'e', 'f']
first_list + second_list
# ['a', 'b', 'c', 'd', 'e', 'f']
We can use this to update the result list like so:

result = ['a']
flat_list = ['b', 'c']
result += flat_list

result # ['a', 'b', 'c']
4.
Nice work! Now the base case.

If the iterating variable is not a list, we can update result, so it includes this element at the end of the list.

flatten() should now return the complete result.

Print the result!

Why is it important that the element is added at the end?

Let’s think through how these recursive calls will work in a simple case:

nested = ['green', 'red', ['blue', 'yellow'], 'purple']

flatten(nested)
# inside flatten()...
# result = ['green']
# result = ['green', 'red']
# recursive call! 
'''
    
# define flatten() below...
def xu_flatten(list):
  result = []
  if list == []:
    return
  
  if type(list[0]) == type(list):
    flatten(list)
  else:
    result.append(list[0])
    list.pop(0)
    flatten(list)
    
  return result

def flatten(my_list):
  result = []
  
  for item in my_list:
    if isinstance(item, list):
      print("List found!")
      result +=  flatten(item)
    else:
      result.append(item)
  return result


### reserve for testing...
planets = ['mercury', 'venus', ['earth'], 'mars', [['jupiter', 'saturn']], 'uranus', ['neptune', 'pluto']]

print(flatten(planets))

'''RECURSION: PYTHON
Fibonacci? Fibonaccu!
So far our recursive functions have all included a single recursive call within the function definition.

Let’s explore a problem which pushes us to use multiple recursive calls within the function definition.

Fibonacci numbers are integers which follow a specific sequence: the next Fibonacci number is the sum of the previous two Fibonacci numbers.

We have a self-referential definition which means this problem is a great candidate for a recursive solution!

We’ll start by considering the base case. The Fibonacci Sequence starts with 0 and 1 respectively. If our function receives an input in that range, we don’t need to do any work.

If we receive an input greater than 1, things get a bit trickier. This recursive step requires two previous Fibonacci numbers to calculate the current Fibonacci number.

That means we need two recursive calls in our recursive step. Expressed in code:

fibonacci(3) == fibonacci(1) + fibonacci(2) 
Let’s walk through how the recursive calls will accumulate in the call stack:

call_stack = []
fibonacci(3)
call_stack = [fibonacci(3)]
To calculate the 3rd Fibonacci number we need the previous two Fibonacci numbers. We start with the previous Fibonacci number.

fibbonacci(2)
call_stack = [fibbonacci(3), fibbonacci(2)]
fibonacci(2) is a base case, the value of 1 is returned…

call_stack = [fibbonacci(3)]
The return value of fibonacci(2) is stored within the execution context of fibonacci(3) while ANOTHER recursive call is made to retrieve the second most previous Fibonacci number…

fibonacci(1)
call_stack = [fibonacci(3), fibonacci(1)]
Finally, fibonacci(1) resolves because it meets the base case and the value of 1 is returned.

call_stack = [fibonacci(3)]
The return values of fibonacci(2) and fibonacci(1) are contained within the execution context of fibonacci(3), which can now return the sum of the previous two Fibonacci numbers.

As you can see, those recursive calls add up fast when we have multiple recursive invocations within a function definition!

Can you reason out the big O runtime of this Fibonacci function?

Instructions
1.
Define our fibonacci() function that takes n as an argument.

Let’s address our base cases:

if the input is 1, we return 1
if the input is 0, we return 0
While we should guard against faulty values, such as numbers below 0, we’ll keep our function simple.

The base case can be expressed a number of different ways, so get creative!

Here’s an example of a base case that checks for two conditions:

def is_odd_or_negative(num):
  if num % 2 != 0:
    return "It's odd!"
  if num < 0:
    return "It's negative!"
2.
Now take care of the recursive step.

This step involves summing two recursive calls to fibonacci().

We need to retrieve the second to last and last Fibonacci values and return their sum.

We can get the second to last Fibonacci by decrementing the input by 2 and the last by decrementing the input by 1.

Again, there are a few different ways we can do this, but a single line solution is what we’re looking for.

Something similar to this:

return recursive_call(second_to_last_num) + recursive_call(last_num)
3.
Add print statements within fibonacci() to explore the different recursive calls.

Set fibonacci_runtime to the appropriate big O runtime.

You’ll notice there are quite a bit of repeated function calls with the same input. This contributes to the expensive runtime…

Can you think of a way to make this function more efficient?

Here’s a hint. https://en.wikipedia.org/wiki/Memoization'''


# define the fibonacci() function below...
def fibonacci(n):
  print('run once')
  if n == 1:
    return 1
  if n == 0:
    return 0
  
  num = fibonacci(n-1) + fibonacci(n-2)

  return num



print(fibonacci(100))
# set the appropriate runtime:
# 1, logN, N, N^2, 2^N, N!
fibonacci_runtime = "2^N"

'''
RECURSION: PYTHON
Recursive Data Structures
Data structures can also be recursive.

Trees are a recursive data structure because their definition is self-referential. A tree is a data structure which contains a piece of data and references to other trees!

Trees which are referenced by other trees are known as children. Trees which hold references to other trees are known as the parents.

A tree can be both parent and child. We’re going to write a recursive function that builds a special type of tree: a binary search tree.

Binary search trees:

Reference two children at most per tree node.
The “left” child of the tree must contain a value lesser than its parent
The “right” child of the tree must contain a value greater than its parent.
Trees are an abstract data type, meaning we can implement our version in a number of ways as long as we follow the rules above.

For the purposes of this exercise, we’ll use the humble Python dictionary:

bst_tree_node = {"data": 42}
bst_tree_node["left_child"] = {"data": 36}
bst_tree_node["right_child"] = {"data": 73}

bst_tree_node["data"] > bst_tree_node["left_child"]["data"]
# True
bst_tree_node["data"] < bst_tree_node["right_child"["data"]
# True
We can also assume our function will receive a sorted list of values as input.

This is necessary to construct the binary search tree because we’ll be relying on the ordering of the list input.

Our high-level strategy before moving through the checkpoints.

base case: the input list is empty
Return "No Child" to represent the lack of node
recursive step: the input list must be divided into two halves
Find the middle index of the list
Store the value located at the middle index
Make a tree node with a "data" key set to the value
Assign tree node’s "left child" to a recursive call using the left half of the list
Assign tree node’s "right child" to a recursive call using the right half of the list
Return the tree node
Instructions
1.
Define the build_bst() function with my_list as the sole parameter.

If my_list has no elements, return “No Child” to represent the lack of a child tree node.

This is the base case of our function.

The recursive step will need to remove an element from the input to eventually reach an empty list.

2.
We’ll be building this tree by dividing the list in half and feeding those halves to the left and right sides of the tree.

This dividing step will eventually produce empty lists to satisfy the base case of the function.

Outside of the conditional you just wrote, declare middle_idx and set it to the middle index of my_list.

Then, declare middle_value and set it to the value in my_list located at middle_idx.

Print “Middle index: “ + middle_idx.

Then, print “Middle value: “ + middle_value

You can use .format() or addition for the print the statement. Addition will require you to use str() on the variables since they are integers!

You can reach the mid-point of a list like so:

colors = ['brown', 'red', 'olive']

mid_idx = len(colors) // 2
# 1
mid_color_value = colors[mid_idx]
# 'red'
and format a string like so:

color = "blue"
print("My favorite color is: {0}".format(color))
# "My favorite color is: blue"
3.
After the print statements, declare the variable tree_node that points to a Python dictionary with a key of "data" pointing to middle_value.

tree_node represents the tree being created in this function call. We want a tree_node created for each element in the list, so we’ll repeat this process on the left and right sub-trees using the appropriate half of the input list.

Now for the recursive calls!

Set the key of "left_child" in tree_node to be a recursive call to build_bst() with the left half of the list not including the middle value as an argument.

Set the key of "right_child" in tree_node to be a recursive call to build_bst() with the right half of the list not including the middle value as an argument.

It’s very important we don’t include the middle_value in the lists we’re passing as arguments, or else we’ll create duplicate nodes!

Finally, return tree_node. As the recursive calls resolve and pop off the call stack, the final return value will be the root or “top” tree_node which contains a reference to every other tree_node.

Our recursive calls will look like the following:

tree_node["left_child"] = build_bst(left_half_of_list)
tree_node["right_child"] = build_bst(right_half_of_list)
We can copy half of a list like so:

pets = ["dogs", "cats", "lizards", "parrots", "giraffes"]
middle_idx = len(pets) // 2
# 2
first_half_pets = pets[:middle_idx + 1]
# ["dogs", "cats", "lizards"]
last_half_pets = pets[middle_idx + 1:]
# ["parrots", "giraffes"]
4.
Congratulations! You’ve built up a recursive data structure with a recursive function!

This data structure can be used to find values in an efficient O(logN) time.

Fill in the variable runtime with the runtime of your build_bst() function.

This runtime is a tricky one so don’t be afraid to use that hint!

N is the length of our input list.

Our tree will be logN levels deep, meaning there will logN times where a new parent-child relationship is created.

If we have an 8 element list, the tree is 3 levels deep: 2**3 == 8.

Each recursive call is going to copy approximately N elements when the left and right halves of the list are passed to the recursive calls. We’re reducing by 1 each time (the middle_value), but that’s a constant factor.

Putting that together, we have N elements being copied logN levels for a big O of N*logN.

'''
# Define build_bst() below...
def build_bst(my_list):
  if my_list == []:
    return "No Child"
  middle_idx = len(my_list) // 2
  middle_value = my_list[middle_idx]

  print('Middle Index: {}'.format(middle_idx))
  print('Middle Value: {}'.format(middle_value))

  tree_node = {}

  tree_node['data'] = middle_value

  tree_node['left_child'] = build_bst(my_list[:middle_idx])
  tree_node['right_child'] = build_bst(my_list[middle_idx+1:])

  return tree_node

# For testing
sorted_list = [12, 13, 14, 15, 16]
binary_search_tree = build_bst(sorted_list)
print(binary_search_tree)

# fill in the runtime as a string
# 1, logN, N, N*logN, N^2, 2^N, N!
runtime = 'N*logN'


'''RECURSION VS. ITERATION - CODING THROWDOWN
Rules of the Throwdown
This lesson will provide a series of algorithms and an iterative or recursive implementation.

Anything we write iteratively, we can also write recursively, and vice versa. Often, the difference is substituting a loop for recursive calls.

Your mission is to recreate the algorithm using the alternative strategy. If the example is recursive, write the algorithm using iteration. If the algorithm uses iteration, solve the problem using recursion.

By the end of this lesson, you’ll have gained a better understanding of the different strategies to implement an algorithm, and along the way, we’ll discuss the big O runtimes of each algorithm.

Each exercise will have a single checkpoint. You can implement the algorithm however you like as long as you’re following the prescribed approach (iterative or recursive).

If you’re feeling stuck, the hint will give a detailed breakdown of how to implement the algorithm.

We’ll start with a classic recursive example, factorial(). This function returns the product of every number from 1 to the given input.

'''
# runtime: Linear - O(N)
def factorial(n):  
  if n < 0:    
    ValueError("Inputs 0 or greater only") 
  if n <= 1:    
    return 1  
  return n * factorial(n - 1)

factorial(3)
# 6
factorial(4)
# 24
factorial(0)
# 1
factorial(-1)
# ValueError "Input must be 0 or greater"

'''
This is a linear implementation, or O(N), where N is the number given as input.

Instructions
1.
Implement your version of factorial() which has the same functionality without using any recursive calls!


Here’s the step by step strategy:

'''
# initialize a result variable set to 1
# loop while the input does not equal 0
  # reassign result to be result * input
  # decrement input by 1
# return result
'''

'''
# recursive function for factoria
# runtime: Linear - O(N)
def recu_factorial(n):  
  if n < 0:    
    ValueError("Inputs 0 or greater only") 
  if n <= 1:    
    return 1  
  return n * factorial(n - 1)

def factorial(n):
  if n < 0:
    ValueError("Inputs 0 or greater only")
  if n == 0：
    return 1

  result = 1
  for i in range(1,n):
    result *= i
  return i  


# test cases
print(factorial(3) == 6)
print(factorial(0) == 1)
print(factorial(5) == 120)

'''
RECURSION VS. ITERATION - CODING THROWDOWN
When Fibs Are Good
Nice work! We’ll demonstrate another classic recursive function: fibonacci().

fibonacci() should return the Nth Fibonacci number, where N is the number given as input. The first two numbers of a Fibonacci Sequence start with 0 and 1. Every subsequent number is the sum of the previous two.

Our recursive implementation:
'''

# runtime: Exponential - O(2^N)

def fibonacci(n):
  if n < 0:
    ValueError("Input 0 or greater only!")
  if n <= 1:
    return n
  return fibonacci(n - 1) + fibonacci(n - 2)

fibonacci(3)
# 2
fibonacci(7)
# 13
fibonacci(0)
# 0

'''
Instructions
1.
Implement your version of fibonacci() which has the same functionality without using any recursive calls!


Here’s our step-by-step strategy:
'''

# define a fibs list of [0, 1]
# if the input is <= to len() of fibs - 1
  # return value at index of input
# else:
  # while input is > len() of fibs - 1
    # next_fib will be fibs[-1] + fibs[-2]
    # append next_fib to fibs
# return value at index of input
'''

'''

# runtime: Exponential - O(2^N)
# fibonacci in Recusion method

def recu_fibonacci(n):
  if n < 0:
    ValueError("Input 0 or greater only!")
  if n <= 1:
    return n
  return fibonacci(n - 1) + fibonacci(n - 2)


def fibonacci(n):
  if n < 0:
    FibonacciValueError('Input 0 or greater Only!')
  if n <= 1:
    return n
  
  a = 0 
  b = 1

  for i in range(n-1):
    c = a + b
    a = b
    b = c    
  return c

# test cases
print(fibonacci(3) == 2)
print(fibonacci(7) == 13)
print(fibonacci(0) == 0)

print(recu_fibonacci(100))


'''
RECURSION VS. ITERATION - CODING THROWDOWN
Let's Give'em Sum Digits To Talk About
Fantastic! Now we’ll switch gears and show you an iterative algorithm to sum the digits of a number.

This function, sum_digits(), produces the sum of all the digits in a positive number as if they were each a single number:
'''

# Linear - O(N), where "N" is the number of digits in the number
def sum_digits(n):
  if n < 0:
    ValueError("Inputs 0 or greater only!")
  result = 0
  while n is not 0:
    result += n % 10
    n = n // 10
  return result + n

sum_digits(12)
# 1 + 2
# 3
sum_digits(552)
# 5 + 5 + 2
# 12
sum_digits(123456789)
# 1 + 2 + 3 + 4...
# 45

'''
Instructions
1.
Implement your version of sum_digits() which has the same functionality using recursive calls!


Here’s the outline of our strategy:

'''

# base case: if input <= 9
  # return input
# recursive step
# last_digit set to input % 10
# return recursive call with (input // 10) added to last_digit
'''

'''
def sum_digits(n):
  if n < 0:
    SumDigitsError('Input number Greater than 0!')
  if n < 10:
    return n
  
  result = 0

  result += n % 10 + sum_digits(n // 10)

  return result

# test cases
print(sum_digits(12) == 3)
print(sum_digits(552) == 12)
print(sum_digits(123456789) == 45)

'''
RECURSION VS. ITERATION - CODING THROWDOWN
It Was Min To Be
We’ll use an iterative solution to the following problem: find the minimum value in a list.
'''

def find_min(my_list):
  min = None
  for element in my_list:
    if not min or (element < min):
      min = element
  return min

find_min([42, 17, 2, -1, 67])
# -1
find_mind([])
# None
find_min([13, 72, 19, 5, 86])
# 5

'''
This solution has a linear runtime, or O(N), where N is the number of elements in the list.

Instructions
1.
Implement your version of find_min() which has the same functionality using recursive calls!


Here’s our strategy:
'''

# function definition with two inputs: 
# a list and a min that defaults to None
  # BASE CASE
  # if input is an empty list
    # return min
  # else
    # RECURSIVE STEP
    # if min is None
    # OR
    # first element of list is < min
      # set min to be first element
  # return recursive call with list[1:] and the min
'''

'''

''' this code not working
def find_min(my_list):
  min = None
  
  if len(my_list) == 0:
    return min
  
  if len(my_list) == 1:
    return my_list[0]

  min = my_list[0]

  if min < find_min(my_list[1:]):
    
    return min
'''

def find_min(my_list): # this code doesn't either
  min = None
  
  if len(my_list) == 0:
    return None
  
  if min == None or my_list[0] < min:
    min = my_list[0]
  
  return find_min(my_list[1:])
  
  
def find_min(my_list, min = None):
  if not my_list:
    return min

  if not min or my_list[0] < min:
    min = my_list[0]
  return find_min(my_list[1:], min)


# test cases

print(find_min([42, 17, 2, -1, 67]))
print(find_min([]) == None)
print(find_min([13, 72, 19, 5, 86]))

#print(find_min([42, 17, 2, -1, 67]) == -1)
#print(find_min([]) == None)
#print(find_min([13, 72, 19, 5, 86]) == 5)

'''
RECURSION VS. ITERATION - CODING THROWDOWN
Taco Cat
Palindromes are words which read the same forward and backward. Here’s an iterative function that checks whether a given string is a palindrome:
'''

def is_palindrome(my_string):
  while len(my_string) > 1:
    if my_string[0] != my_string[-1]:
      return False
    my_string = my_string[1:-1]
  return True 

palindrome("abba")
# True
palindrome("abcba")
# True
palindrome("")
# True
palindrome("abcd")
# False

'''
Take a moment to think about the runtime of this solution.

In each iteration of the loop that doesn’t return False, we make a copy of the string with two fewer characters.

Copying a list of N elements requires N amount of work in big O.

This implementation is a quadratic solution: we’re looping based on N and making a linear operation for each loop!

Here’s a more efficient version:
'''

# Linear - O(N)
def is_palindrome(my_string):
  string_length = len(my_string)
  middle_index = string_length // 2
  for index in range(0, middle_index):
    opposite_character_index = string_length - index - 1
    if my_string[index] != my_string[opposite_character_index]:
      return False  
  return True
  
'''  
Note these solutions do not account for spaces or capitalization in the input!

Instructions
1.
Implement your version of is_palindrome() which has the same functionality using recursive calls!


Here’s the outline of our strategy:
'''

# BASE CASE
# the input string is less than 2 characters
  # return True
# RECURSIVE STEP
# str[0] does not match str[-1]
  # return False
# return recursive call with str[1:-1]

'''

'''
def is_palindrome(my_string):
  if len(my_string) < 2:
    return True
  
  if my_string[0] != my_string[-1]:
    return False
  
  return is_palindrome(my_string[1:-1])


# test cases
print(is_palindrome("abba") == True)
print(is_palindrome("abcba") == True)
print(is_palindrome("") == True)
print(is_palindrome("abcd") == False)


'''
RECURSION VS. ITERATION - CODING THROWDOWN
Multiplication? Schmultiplication!
All programming languages you’re likely to use will include arithmetic operators like +, -, and *.

Let’s pretend Python left out the multiplication, *, operator.

How could we implement it ourselves? Well, multiplication is repeated addition. We can use a loop!

Here’s an iterative approach:
'''

def multiplication(num_1, num_2):
  result = 0
  for count in range(0, num_2):
    result += num_1
  return result

multiplication(3, 7)
# 21
multiplication(5, 5)
# 25
multiplication(0, 4)
# 0

'''
This implementation isn’t quite as robust as the built-in operator. It won’t work with negative numbers, for example. We don’t expect your implementation to handle negative numbers either!

What is the big O runtime of our implementation?

Instructions
1.
Implement your version of multiplication() which has the same functionality using recursive calls!


Here’s the outline of our strategy:
'''

# BASE CASE
# if either input is 0, return 0
# RECURSIVE STEP
# return one input added to a recursive call with the OTHER input decremented by 1
'''

'''
def multiplication(num1, num2):
  result = 0
  if num2 == 0:
    return result

  result += num1 + multiplication(num1, num2 - 1)
  return result



# test cases
print(multiplication(3, 7) == 21)
print(multiplication(5, 5) == 25)
print(multiplication(0, 4) == 0)

'''
RECURSION VS. ITERATION - CODING THROWDOWN
How Deep Is Your Tree?
Binary trees, trees which have at most two children per node, are a useful data structure for organizing hierarchical data.

It’s helpful to know the depth of a tree, or how many levels make up the tree.
'''

# first level
root_of_tree = {"data": 42}
# adding a child - second level
root_of_tree["left_child"] = {"data": 34}
root_of_tree["right_child"] = {"data": 56}
# adding a child to a child - third level
first_child = root_of_tree["left_child"]
first_child["left_child"] = {"data": 27}

'''
Here’s an iterative algorithm for counting the depth of a given tree.

We’re using Python dictionaries to represent each tree node, with the key of "left_child" or "right_child" referencing another tree node, or None if no child exists.
'''

def depth(tree):
  result = 0
  # our "queue" will store nodes at each level
  queue = [tree]
  # loop as long as there are nodes to explore
  while queue:
    # count the number of child nodes
    level_count = len(queue)
    for child_count in range(0, level_count):
      # loop through each child
      child = queue.pop(0)
     # add its children if they exist
      if child["left_child"]:
        queue.append(child["left_child"])
      if child["right_child"]:
        queue.append(child["right_child"])
    # count the level
    result += 1
  return result

two_level_tree = {
"data": 6, 
"left_child":
  {"data": 2}
}

four_level_tree = {
"data": 54,
"right_child":
  {"data": 93,
   "left_child":
     {"data": 63,
      "left_child":
        {"data": 59}
      }
   }
}


depth(two_level_tree)
# 2
depth(four_level_tree)
# 4
'''

This algorithm will visit each node in the tree once, which makes it a linear runtime, O(N), where N is the number of nodes in the tree.

Implement your version of depth() which has the same functionality using recursive calls!


Here’s our strategy:
'''

# function takes "tree_node" as input
  # BASE CASE
  # if tree_node is None
    # return 0
  # RECURSIVE STEP
  # set left_depth to recursive call passing tree_node's left child
  # set right_depth to recursive call passing tree_node's right child

  # if left_depth is greater than right depth:
    # return left_depth + 1
  # else
    # return right_depth + 1
'''



'''


def iterate_depth(tree):
  result = 0
  # our "queue" will store nodes at each level
  queue = [tree]
  #print(queue)
  #print('\n')
  #print(len(queue))
  # loop as long as there are nodes to explore
  while queue:
    # count the number of child nodes
    level_count = len(queue)
    for child_count in range(0, level_count):
      # loop through each child
      child = queue.pop(0)
      #print(child)
     # add its children if they exist
      if child["left_child"]:
        queue.append(child["left_child"])
      if child["right_child"]:
        queue.append(child["right_child"])
    # count the level
    result += 1
  return result

  
def depth(tree):  #Gux version
  result = 0

  if tree['left_child'] == None:
    return 1
   
  else:
    result += 1 + depth(tree['left_child'])
    return result


# HELPER FUNCTION TO BUILD TREES
def build_bst(my_list):
  if len(my_list) == 0:
    return None

  mid_idx = len(my_list) // 2
  mid_val = my_list[mid_idx]

  tree_node = {"data": mid_val}
  tree_node["left_child"] = build_bst(my_list[ : mid_idx])
  tree_node["right_child"] = build_bst(my_list[mid_idx + 1 : ])

  return tree_node

# HELPER VARIABLES
tree_level_1 = build_bst([1])
tree_level_2 = build_bst([1, 2, 3])
tree_level_4 = build_bst([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]) 


# test cases
print(depth(tree_level_1) == 1)
print(depth(tree_level_2) == 2)
print(depth(tree_level_4) == 4)

#print(depth(tree_level_1))
#print(depth(tree_level_2))
#print(depth(tree_level_4))


# first level
#root_of_tree = {"data": 42}
# adding a child - second level
#root_of_tree["left_child"] = {"data": 34}
#root_of_tree["right_child"] = {"data": 56}
# adding a child to a child - third level
#first_child = root_of_tree["left_child"]
#first_child["left_child"] = None
#first_child["right_child"] = None
#second_child = root_of_tree["right_child"]
#second_child["left_child"] = None
#second_child["right_child"] = None
#first_child["right_child"] = {"data": 67}

#print(root_of_tree)
#print(depth(root_of_tree))



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-RECURSION CHEATSHEETS #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Cheatsheets / Learn Recursion: Python

Recursion: Python
Print PDF icon
Print Cheatsheet

TOPICS
Recursion: Conceptual
Recursion: Python
Stack Overflow Error in Recursive Function
A recursive function that is called with an input that requires too many iterations will cause the call stack to get too large, resulting in a stack overflow error. In these cases, it is more appropriate to use an iterative solution. A recursive solution is only suited for a problem that does not exceed a certain number of recursive calls.

For example, myfunction() below throws a stack overflow error when an input of 1000 is used.'''

def myfunction(n):
  if n == 0:
    return n
  else:
    return myfunction(n-1)
    
'''
myfunction(1000)  #results in stack overflow error
Fibonacci Sequence
A Fibonacci sequence is a mathematical series of numbers such that each number is the sum of the two preceding numbers, starting from 0 and 1.

Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, ...
Call Stack Construction in While Loop
A call stack with execution contexts can be constructed using a while loop, a list to represent the call stack and a dictionary to represent the execution contexts. This is useful to mimic the role of a call stack inside a recursive function.

Binary Search Tree
In Python, a binary search tree is a recursive data structure that makes sorted lists easier to search. Binary search trees:

Reference two children at most per tree node.
The “left” child of the tree must contain a value lesser than its parent.
The “right” child of the tree must contain a value greater than it’s parent.
     5
    / \
   /   \
  3     8
 / \   / \
2   4 7   9
Recursion and Nested Lists
A nested list can be traversed and flattened using a recursive function. The base case evaluates an element in the list. If it is not another list, the single element is appended to a flat list. The recursive step calls the recursive function with the nested list element as input.'''

def flatten(mylist):
  flatlist = []
  for element in mylist:
    if type(element) == list:
      flatlist += flatten(element)
    else:
      flatlist += element
  return flatlist

print(flatten(['a', ['b', ['c', ['d']], 'e'], 'f']))
# returns ['a', 'b', 'c', 'd', 'e', 'f']

'''
Fibonacci Recursion
Computing the value of a Fibonacci number can be implemented using recursion. Given an input of index N, the recursive function has two base cases – when the index is zero or 1. The recursive function returns the sum of the index minus 1 and the index minus 2.

The Big-O runtime of the Fibonacci function is O(2^N).
'''
def fibonacci(n):
  if n <= 1:
    return n
  else:
    return fibonacci(n-1) + fibonacci(n-2)
'''
Modeling Recursion as Call Stack
One can model recursion as a call stack with execution contexts using a while loop and a Python list. When the base case is reached, print out the call stack list in a LIFO (last in first out) manner until the call stack is empty.

Using another while loop, iterate through the call stack list. Pop the last item off the list and add it to a variable to store the accumulative result.

Print the result.
'''
def countdown(value):
  call_stack = []
  while value > 0 : 
    call_stack.append({"input":value})
    print("Call Stack:",call_stack)
    value -= 1
  print("Base Case Reached")
  while len(call_stack) != 0:
    print("Popping {} from call stack".format(call_stack.pop()))
    print("Call Stack:",call_stack)
countdown(4)
'''
Call Stack: [{'input': 4}]             
Call Stack: [{'input': 4}, {'input': 3}]         
Call Stack: [{'input': 4}, {'input': 3}, {'input': 2}]     
Call Stack: [{'input': 4}, {'input': 3}, {'input': 2}, {'input': 1}]                                
Base Case Reached                                  
Popping {'input': 1} from call stack                       
Call Stack: [{'input': 4}, {'input': 3}, {'input': 2}]  
Popping {'input': 2} from call stack                   
Call Stack: [{'input': 4}, {'input': 3}]       
Popping {'input': 3} from call stack            
Call Stack: [{'input': 4}]                                 
Popping {'input': 4} from call stack              
Call Stack: []
'''
'''


Recursion in Python
In Python, a recursive function accepts an argument and includes a condition to check whether it matches the base case. A recursive function has:

Base Case - a condition that evaluates the current input to stop the recursion from continuing.
Recursive Step - one or more calls to the recursive function to bring the input closer to the base case.'''
def countdown(value):
  if value <= 0:   #base case  
    print("done")
  else:
    print(value)
    countdown(value-1)  #recursive case 

'''    
Build a Binary Search Tree
To build a binary search tree as a recursive algorithm do the following:

BASE CASE: 
If the list is empty, return "No Child" to show that there is no node. 

RECURSIVE STEP:
1. Find the middle index of the list.
2. Create a tree node with the value of the middle index.
3. Assign the tree node's left child to a recursive call with the left half of list as input.
4. Assign the tree node's right child to a recursive call with the right half of list as input.
5. Return the tree node.'''

def build_bst(my_list):
  if len(my_list) == 0:
    return "No Child"

  middle_index = len(my_list) // 2
  middle_value = my_list[middle_index]
  
  print("Middle index: {0}".format(middle_index))
  print("Middle value: {0}".format(middle_value))
  
  tree_node = {"data": middle_value}
  tree_node["left_child"] = build_bst(my_list[ : middle_index])
  tree_node["right_child"] = build_bst(my_list[middle_index + 1 : ])

  return tree_node
  
sorted_list = [12, 13, 14, 15, 16]
binary_search_tree = build_bst(sorted_list)
print(binary_search_tree)
'''
Recursive Depth of Binary Search Tree
A binary search tree is a data structure that builds a sorted input list into two subtrees. The left child of the subtree contains a value that is less than the root of the tree. The right child of the subtree contains a value that is greater than the root of the tree.

A recursive function can be written to determine the depth of this tree.'''

def depth(tree):
  if not tree:
    return 0
  left_depth = depth(tree["left_child"])
  right_depth = depth(tree["right_child"])
  return max(left_depth, right_depth) + 1
  
'''
Sum Digits with Recursion
Summing the digits of a number can be done recursively. For example:

552 = 5 + 5 + 2 = 12
'''
def sum_digits(n): 
  if n <= 9: 
    return n 
  last_digit = n % 10 
  return sum_digits(n // 10) + last_digit

sum_digits(552) #returns 12
'''

Palindrome in Recursion
A palindrome is a word that can be read the same both ways - forward and backward. For example, abba is a palindrome and abc is not.

The solution to determine if a word is a palindrome can be implemented as a recursive function.'''

def is_palindrome(str):
  if len(str) < 2:
    return True
  if str[0] != str[-1]:
    return False
  return is_palindrome(str[1:-1])
'''  
Fibonacci Iterative Function
A Fibonacci sequence is made up adding two previous numbers beginning with 0 and 1. For example:

0, 1, 1, 2, 3, 5, 8, 13, ...
A function to compute the value of an index in the Fibonacci sequence, fibonacci(index) can be written as an iterative function.
'''
def fibonacci(n):
  if n < 0:
    raise ValueError("Input 0 or greater only!")
  fiblist = [0, 1]
  for i in range(2,n+1):
    fiblist.append(fiblist[i-1] + fiblist[i-2])
  return fiblist[n]
'''
Recursive Multiplication
The multiplication of two numbers can be solved recursively as follows:

Base case: Check for any number that is equal to zero.
Recursive step: Return the first number plus a recursive call of the first number and the second number minus one.

'''
def multiplication(num1, num2):
  if num1 == 0 or num2 == 0:
    return 0
  return num1 + multiplication(num1, num2 - 1)
  
'''
Iterative Function for Factorials
To compute the factorial of a number, multiply all the numbers sequentially from 1 to the number.

An example of an iterative function to compute a factorial is given below.
'''
def factorial(n): 
  answer = 1
  while n != 0:
    answer *= n
    n -= 1
  return answer

'''
Recursively Find Minimum in List
We can use recursion to find the element with the minimum value in a list, as shown in the code below.
'''
def find_min(my_list):
  if len(my_list) == 0:
    return None
  if len(my_list) == 1:
    return my_list[0]
  #compare the first 2 elements
  temp = my_list[0] if my_list[0] < my_list[1] else my_list[1]
  my_list[1] = temp
  return find_min(my_list[1:])

print(find_min([]) == None)
print(find_min([42, 17, 2, -1, 67]) == -1)






#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON SORTING -- PYTHON SORTING -- PYTHON SORTING -- PYTHON SORTING -- PYTHON SORTING#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

'''BUBBLE SORT: CONCEPTUAL
Bubble Sort Introduction
Bubble sort is an introductory sorting algorithm that iterates through a list and compares pairings of adjacent elements.

According to the sorting criteria, the algorithm swaps elements to shift elements towards the beginning or end of the list.

By default, a list is sorted if for any element e and position 1 through N:

e1 <= e2 <= e3 … eN, where N is the number of elements in the list.

For example, bubble sort transforms a list:

[5, 2, 9, 1, 5]
to an ascending order, from lowest to highest:

[1, 2, 5, 5, 9]
We implement the algorithm with two loops.

The first loop iterates as long as the list is unsorted and we assume it’s unsorted to start.

Within this loop, another iteration moves through the list. For each pairing, the algorithm asks:

In comparison, is the first element larger than the second element?

If it is, we swap the position of the elements. The larger element is now at a greater index than the smaller element.

When a swap is made, we know the list is still unsorted. The outer loop will run again when the inner loop concludes.

The process repeats until the largest element makes its way to the last index of the list. The outer loop runs until no swaps are made within the inner loop.'''

'''BUBBLE SORT: CONCEPTUAL
Bubble Sort
As mentioned, the bubble sort algorithm swaps elements if the element on the left is larger than the one on the right.

How does this algorithm swap these elements in practice?

Let’s say we have the two values stored at the following indices index_1 and index_2. How would we swap these two elements within the list?

It is tempting to write code like:

list[index_1] = list[index_2]
list[index_2] = list[index_1]
However, if we do this, we lose the original value at index_1. The element gets replaced by the value at index_2. Both indices end up with the value at index_2.

Programming languages have different ways of avoiding this issue. In some languages, we create a temporary variable which holds one element during the swap:

temp = list[index_1]
list[index_1] = list[index_2]
list[index_2] = temp 
The GIF illustrates this code snippet.

Other languages provide multiple assignment which removes the need for a temporary variable.

list[index_1], list[index_2] = list[index_2], list[index_1]'''

'''BUBBLE SORT: CONCEPTUAL
Algorithm Analysis
Given a moderately unsorted data-set, bubble sort requires multiple passes through the input before producing a sorted list. Each pass through the list will place the next largest value in its proper place.

We are performing n-1 comparisons for our inner loop. Then, we must go through the list n times in order to ensure that each item in our list has been placed in its proper order.

The n signifies the number of elements in the list. In a worst case scenario, the inner loop does n-1 comparisons for each n element in the list.

Therefore we calculate the algorithm’s efficiency as:

\mathcal{O}(n(n-1)) = \mathcal{O}(n(n)) = \mathcal{O}(n^2)O(n(n−1))=O(n(n))=O(n 
2
 )
The diagram analyzes the pseudocode implementation of bubble sort to show how we draw this conclusion.

When calculating the run-time efficiency of an algorithm, we drop the constant (-1), which simplifies our inner loop comparisons to n.

This is how we arrive at the algorithm’s runtime: O(n^2).'''

'''
BUBBLE SORT: CONCEPTUAL
Bubble Sort Review
Bubble sort is an algorithm to sort a list through repeated swaps of adjacent elements. It has a runtime of O(n^2).

For nearly sorted lists, bubble sort performs relatively few operations since it only performs a swap when elements are out of order.

Bubble sort is a good introductory algorithm which opens the door to learning more complex algorithms. It answers the question, “How can we algorithmically sort a list?” and encourages us to ask, “How can we improve this sorting algorithm?”'''


'''BuBBLE SORT: PYTHON
Bubble Sort: Swap
The Bubble Sort algorithm works by comparing a pair of neighbor elements and shifting the larger of the two to the right. Bubble Sort completes this by swapping the two elements’ positions if the first element being compared is larger than the second element being compared.

Below is a quick pseudocode example of what we will create:

for each pair(elem1, elem2):
  if elem1 > elem2:
    swap(elem1, elem2)
  else:
    # analyze next set of pairs
This swap() sub-routine is an essential part of the algorithm. Bubble sort swaps elements repeatedly until the largest element in the list is placed at the greatest index. This looping continues until the list is sorted.

This GIF illustrates how swap() method works.'''

nums = [5, 2, 9, 1, 5, 6]

# Define swap() below:

def swap(arr, index_1, index_2):
  temp = arr[index_1]
  arr[index_1] = arr[index_2]
  arr[index_2] = temp

swap(nums, 3, 5)
print(nums)

'''BUBBLE SORT: PYTHON
Bubble Sort: Compare
Now that we know how to swap items in an array, we need to set up the loops which check whether a swap is necessary.

Recall that Bubble Sort compares neighboring items and if they are out of order, they are swapped.

What does it mean to be “out of order”? Since bubble sort is a comparison sort, we’ll use a comparison operator: <.

We’ll have two loops:

One loop will iterate through each element in the list.

Within the first loop, we’ll have another loop for each element in the list.

Inside the second loop, we’ll take the index of the loop and compare the element at that index with the element at the next index. If they’re out of order, we’ll make a swap!'''

'''Instructions
1.
Below the body of swap(), define a new function: bubble_sort() which has the parameter arr.

Write pass in the body of bubble_sort to start.

2.
Inside bubble_sort(), replace pass with a for loop that iterates up until the last element of the list.

Inside the for loop, check if the value in arr at index is > the value in arr at index + 1.

If it is, use swap() and pass arr, index, and index + 1 as arguments.

We can loop through every element except the last using:

pets = ['donkey', 'rabbit', 'snake', 'cat']

for index in range(len(pets) - 1):
  print(pets[index])

# donkey
# rabbit
# snake
We can make a comparison check like so:

nums = [4, 2, 7]

for index in range(len(nums) - 1):
  if nums[index] > nums[index + 1]:
    print("Elements are out of order!")
  else:
    print("Elements are in order!")

# Elements are out of order!
# Elements are in order!
3.
As you can see by the output, our list is not sorted!

One loop through the list is only sufficient to move the largest value to its correct placement.

Create another loop which iterates for each element in arr.

Move the entire contents of the function within this loop:

def bubble_sort(arr):
  for el in arr:
    # previous code goes here!
Run the code again, your list should be sorted!

Your code should now look like the following

def bubble_sort(arr):
  for el in arr:
    for index in range(len(arr)-1):
      if arr[index] > arr[index + 1]:
        swap(arr, index, index + 1)
'''

nums = [5, 2, 9, 1, 5, 6]

def swap(arr, index_1, index_2):
  temp = arr[index_1]
  arr[index_1] = arr[index_2]
  arr[index_2] = temp
  
# define bubble_sort():
def bubble_sort(arr):
  for el in arr:
    for i in range(len(arr)-1):
      if arr[i] > arr[i+1]:
        swap(arr, i, i+1)


##### test statements

print("Pre-Sort: {0}".format(nums))      
bubble_sort(nums)
print("Post-Sort: {0}".format(nums))

'''BUBBLE SORT: PYTHON
Bubble Sort: Optimized
As you were writing Bubble Sort, you may have realized that we were doing some unnecessary iterations.

Consider the first pass through the outer loop. We’re making n-1 comparisons.

nums = [5, 4, 3, 2, 1]
# 5 element list: N is 5
bubble_sort(nums)
# 5 > 4
# [4, 5, 3, 2, 1]
# 5 > 3
# [4, 3, 5, 2, 1]
# 5 > 2
# [4, 3, 2, 5, 1]
# 5 > 1
# [4, 3, 2, 1, 5]
# four comparisons total
We know the last value in the list is in its correct position, so we never need to consider it again. The second time through the loop, we only need n-2 comparisons.

As we correctly place more values, fewer elements need to be compared. An optimized version doesn’t make n^2-n comparisons, it does (n-1) + (n-2) + ... + 2 + 1 comparisons, which can be simplified to (n^2-n) / 2 comparisons.

This is fewer than n^2-n comparisons but the algorithm still has a big O runtime of O(N^2).

As the input, N, grows larger, the division by two has less significance. Big O considers inputs as they reach infinity so the higher order term N^2 completely dominates.

We can’t make Bubble Sort better than O(N^2), but let’s take a look at the optimized code and compare iterations between implementations!

We’re also taking advantage of parallel assignment in Python and abstracting away the swap() function!'''

nums = [9, 8, 7, 6, 5, 4, 3, 2, 1]
print("PRE SORT: {0}".format(nums))

def swap(arr, index_1, index_2):
  temp = arr[index_1]
  arr[index_1] = arr[index_2]
  arr[index_2] = temp

def bubble_sort_unoptimized(arr):
  iteration_count = 0
  for el in arr:
    for index in range(len(arr) - 1):
      iteration_count += 1
      if arr[index] > arr[index + 1]:
        swap(arr, index, index + 1)

  print("PRE-OPTIMIZED ITERATION COUNT: {0}".format(iteration_count))

def bubble_sort(arr):
  iteration_count = 0
  for i in range(len(arr)):
    # iterate through unplaced elements
    for idx in range(len(arr) - i - 1):
      iteration_count += 1
      if arr[idx] > arr[idx + 1]:
        # replacement for swap function
        arr[idx], arr[idx + 1] = arr[idx + 1], arr[idx]
        
  print("POST-OPTIMIZED ITERATION COUNT: {0}".format(iteration_count))

bubble_sort_unoptimized(nums.copy())
bubble_sort(nums)
print("POST SORT: {0}".format(nums))


'''MERGE SORT: CONCEPTUAL
What Is A Merge Sort?
Merge sort is a sorting algorithm created by John von Neumann in 1945. Merge sort’s “killer app” was the strategy that breaks the list-to-be-sorted into smaller parts, sometimes called a divide-and-conquer algorithm.

In a divide-and-conquer algorithm, the data is continually broken down into smaller elements until sorting them becomes really simple.

Merge sort was the first of many sorts that use this strategy, and is still in use today in many different applications.

How To Merge Sort:
Merge sorting takes two steps: splitting the data into “runs” or smaller components, and the re-combining those runs into sorted lists (the “merge”).

When splitting the data, we divide the input to our sort in half. We then recursively call the sort on each of those halves, which cuts the halves into quarters. This process continues until all of the lists contain only a single element. Then we begin merging.

When merging two single-element lists, we check if the first element is smaller or larger than the other. Then we return the two-element list with the smaller element followed by the larger element.

MERGE SORT: CONCEPTUAL
Merging
When merging larger pre-sorted lists, we build the list similarly to how we did with single-element lists.

Let’s call the two lists left and right. Bothleft and right are already sorted. We want to combine them (to merge them) into a larger sorted list, let’s call it both. To accomplish this we’ll need to iterate through both with two indices, left_index and right_index.

At first left_index and right_index both point to the start of their respective lists. left_index points to the smallest element of left (its first element) and right_index points to the smallest element of right.

Compare the elements at left_index and right_index. The smaller of these two elements should be the first element of both because it’s the smallest of both! It’s the smallest of the two smallest values.

Let’s say that smallest value was in left. We continue by incrementing left_index to point to the next-smallest value in left. Then we compare the 2nd smallest value in left against the smallest value of right. Whichever is smaller of these two is now the 2nd smallest value of both.

This process of “look at the two next-smallest elements of each list and add the smaller one to our resulting list” continues on for as long as both lists have elements to compare. Once one list is exhausted, say every element from left has been added to the result, then we know that all the elements of the other list, right, should go at the end of the resulting list (they’re larger than every element we’ve added so far).

Merge Sort Performance
Merge sort was unique for its time in that the best, worst, and average time complexity are all the same: Θ(N*log(N)). This means an almost-sorted list will take the same amount of time as a completely out-of-order list. This is acceptable because the worst-case scenario, where a sort could stand to take the most time, is as fast as a sorting algorithm can be.

Some sorts attempt to improve upon the merge sort by first inspecting the input and looking for “runs” that are already pre-sorted. Timsort is one such algorithm that attempts to use pre-sorted data in a list to the sorting algorithm’s advantage. If the data is already sorted, Timsort runs in Θ(N) time.

Merge sort also requires space. Each separation requires a temporary array, and so a merge sort would require enough space to save the whole of the input a second time. This means the worst-case space complexity of merge sort is O(N).'''

###################################################################################################

'''MERGE SORT: PYTHON
Separation
What is sorted by a sort? A sort takes in a list of some data. The data can be words that we want to sort in dictionary order, or people we want to sort by birth date, or really anything else that has an order. For the simplicity of this lesson, we’re going to imagine the data as just numbers.

The first step in a merge sort is to separate the data into smaller lists. Then we break those lists into even smaller lists. Then, when those lists are all single-element lists, something amazing happens! Well, kind of amazing. Well, you might have expected it, we do call it a “merge sort”. We merge the lists.'''

'''Instructions
1.
Define a function called merge_sort(). Give merge_sort() one parameter: items.

2.
We’re going to use merge_sort() to break down items into smaller and smaller lists, and then write a merge() function that will combine them back together.

For now, check the length of items. If items has length one or less, return items.'''

def merge_sort(items):
  if len(items) <=1:
    return items

'''MERGE SORT: PYTHON
Partitions
How do we break up the data in a merge sort? We split it in half until there’s no more data to split. Our first step is to break down all of the items of the list into their own list.

Instructions
1.
After returning all inputs that have less than 2 elements, we split everything up that’s longer.

Create the variable middle_index which is the index to the middle element in the list.

You can find the middle_index of a list by calculating its length (using the len() function) and then dividing it by 2 using integer division.

Altogether this should look like this:

middle_index = len(items) // 2
2.
Create another variable called left_split. This should be a list of all elements in the input list starting at the first up to but not including the middle_index element.

You can use the list slice operator:

a_list = ['peas', 'carrots', 'potatoes']
slice_til_here = 2
a_list[:slice_til_here]
# ['peas', 'carrots']
3.
Create one more variable called right_split which includes all elements in items from the middle_index to the end of the list.

Using a starting index to slice a list includes the element at the starting index, but an ending index does not include that element. This makes halving a list (like we’re doing) easier!

cool_list = [1, 2, 3, 4, 5, 6]
start = cool_list[:3]
end = cool_list[3:]

print(start)
# [1, 2, 3]
print(end)
# [4, 5, 6]
4.
For now, return all three of these at the bottom of the function in a single return statement. Like this:

return middle_index, left_split, right_split'''

def merge_sort(items):
  if len(items) < 2:
    return items
  
  middle_index = len(items)//2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  return middle_index, left_split, right_split

'''MERGE SORT: PYTHON
Creating the Merge Function
Our merge_sort() function so far only separates the input list into many different parts — pretty much the opposite of what you’d expect a merge sort to do. To actually perform the merging, we’re going to define a helper function that joins the data together.'''

def merge_sort(items):
  if len(items) <= 1:
    return items

  middle_index = len(items) // 2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  return middle_index, left_split, right_split

def merge(left, right):
  result = []
  return result
  
'''
Merging
Now we need to build out our result list. When we’re merging our lists together, we’re creating ordered lists that combine the elements of two lists.

Instructions
1.
Since we’re going to be removing the contents of each list until they’re both depleted, let’s start with a while loop!

Create a loop that will continue iterating while both left and right have elements. When one of those two are empty we’ll want to move on.

Remember a list is truthy if it has elements and falsy if it’s empty, so writing:

while(left and right):
Will continue until one of those two is depleted.

2.
Now we do our comparison! Check if the first element (index 0, remember) of left is smaller than the first element of right.

3.
If left[0] is smaller than right[0], we want to add it to our result! Append left[0] to our result list.

Since we’ve added it to our results we’ll want to remove it from left. Use left.pop() to remove the first element from the left list.

Add the element to result and pop it off from the front of the list:

result.append(left[0])
left.pop(0)
4.
If left[0] is larger than right[0], we want to add right[0] to our result! Append right[0] to result and then pop it out of right.'''

def merge_sort(items):
    if len(items) <= 1:
        return items

    middle_index = len(items) // 2
    left_split = items[:middle_index]
    right_split = items[middle_index:]

    return middle_index, left_split, right_split

def merge(left, right):
    result = []
    
    while (left and right):
      if left[0] < right[0]:
        result.append(left[0])
        left.pop(0)
      else:
        result.append(right[0])
        right.pop(0)
        
    return result

'''Finishing the Merge
Since we’ve only technically depleted one of our two inputs to merge(), we want to add in the rest of the values to finish off our merge() function and return the sorted list.

Instructions
1.
After our while loop, check if there are any elements still in left.

If there are, add those elements to the end of result.

You can add two lists using +:

[1, 2, 3] + [4, 5, 6] == [1, 2, 3, 4, 5, 6]
This means we can update our result list using +=

a = [1, 2, 3]
a += [4, 5, 6]

print(a)
# Prints "[1, 2, 3, 4, 5, 6]'''

def merge_sort(items):
  if len(items) <= 1:
    return items

  middle_index = len(items) // 2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  return middle_index, left_split, right_split

def merge(left, right):
  result = []

  while (left and right):
    if left[0] < right[0]:
      result.append(left[0])
      left.pop(0)
    else:
      result.append(right[0])
      right.pop(0)
    
    if left:
      result += left
    if right:
      result += right  

  return result

'''MERGE SORT: PYTHON
Finishing the Sort
Let’s update our merge_sort() function so that it returns a sorted list finally!

Instructions
1.
In merge_sort() create two new variables: left_sorted and right_sorted.

left_sorted should be the result of calling merge_sort() recursively on left_split.

right_sorted should be the result of calling merge_sort() recursively on right_split.

2.
Erase the “return” line and change it to return the result of calling merge() on left_sorted and right_sorted.'''


def merge_sort(items):
  if len(items) <= 1:
    return items

  middle_index = len(items) // 2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  #return middle_index, left_split, right_split

  left_sorted = merge_sort(left_split)
  right_sorted = merge_sort(right_split)

  return merge(left_sorted, right_sorted)

def merge(left, right):
  result = []

  while (left and right):
    if left[0] < right[0]:
      result.append(left[0])
      left.pop(0)
    else:
      result.append(right[0])
      right.pop(0)

  if left:
    result += left
  if right:
    result += right

  return result

'''MERGE SORT: PYTHON
Finishing the Sort
Let’s update our merge_sort() function so that it returns a sorted list finally!

Instructions
1.
In merge_sort() create two new variables: left_sorted and right_sorted.

left_sorted should be the result of calling merge_sort() recursively on left_split.

right_sorted should be the result of calling merge_sort() recursively on right_split.

2.
Erase the “return” line and change it to return the result of calling merge() on left_sorted and right_sorted.'''

def merge_sort(items):
  if len(items) <= 1:
    return items

  middle_index = len(items) // 2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  #return middle_index, left_split, right_split

  left_sorted = merge_sort(left_split)
  right_sorted = merge_sort(right_split)

  return merge(left_sorted, right_sorted)

def merge(left, right):
  result = []

  while (left and right):
    if left[0] < right[0]:
      result.append(left[0])
      left.pop(0)
    else:
      result.append(right[0])
      right.pop(0)

  if left:
    result += left
  if right:
    result += right

  return result

'''MERGE SORT: PYTHON
Testing the Sort
We’ve written our merge sort! The whole sort takes up two functions:

merge_sort() which is called recursively breaks down an input list to smaller, more manageable pieces. merge() which is a helper function built to help combine those broken-down lists into ordered combination lists.

merge_sort() continues to break down an input list until it only has one element and then it joins that with other single element lists to create sorted 2-element lists. Then it combines 2-element sorted lists into 4-element sorted lists. It continues that way until all the items of the lists are sorted!

Only one thing left to do, test it out!'''

def merge_sort(items):
  if len(items) <= 1:
    return items

  middle_index = len(items) // 2
  left_split = items[:middle_index]
  right_split = items[middle_index:]

  left_sorted = merge_sort(left_split)
  right_sorted = merge_sort(right_split)

  return merge(left_sorted, right_sorted)

def merge(left, right):
  result = []

  while (left and right):
    if left[0] < right[0]:
      result.append(left[0])
      left.pop(0)
    else:
      result.append(right[0])
      right.pop(0)

  if left:
    result += left
  if right:
    result += right

  return result

unordered_list1 = [356, 746, 264, 569, 949, 895, 125, 455]
unordered_list2 = [787, 677, 391, 318, 543, 717, 180, 113, 795, 19, 202, 534, 201, 370, 276, 975, 403, 624, 770, 595, 571, 268, 373]
unordered_list3 = [860, 380, 151, 585, 743, 542, 147, 820, 439, 865, 924, 387]

ordered_list1 = merge_sort(unordered_list1)
ordered_list2 = merge_sort(unordered_list2)
ordered_list3 = merge_sort(unordered_list3)

print(ordered_list1)
print(ordered_list2)
print(ordered_list3)


'''QUICKSORT: CONCEPTUAL
Introduction to Quicksort
Quicksort is an efficient recursive algorithm for sorting arrays or lists of values. The algorithm is a comparison sort, where values are ordered by a comparison operation such as > or <.

Quicksort uses a divide and conquer strategy, breaking the problem into smaller sub-problems until the solution is so clear there’s nothing to solve.

The problem: many values in the array which are out of order.

The solution: break the array into sub-arrays containing at most one element. One element is sorted by default!

We choose a single pivot element from the list. Every other element is compared with the pivot, which partitions the array into three groups.

A sub-array of elements smaller than the pivot.
The pivot itself.
A sub-array of elements greater than the pivot.
The process is repeated on the sub-arrays until they contain zero or one element. Elements in the “smaller than” group are never compared with elements in the “greater than” group. If the smaller and greater groupings are roughly equal, this cuts the problem in half with each partition step!

[6,5,2,1,9,3,8,7]
6 # The pivot
[5, 2, 1, 3] # lesser than 6
[9, 8, 7] # greater than 6


[5,2,1,3]  # these values
# will never be compared with 
[9,8,7] # these values
Depending on the implementation, the sub-arrays of one element each are recombined into a new array with sorted ordering, or values within the original array are swapped in-place, producing a sorted mutation of the original array.'''

'''QUICKSORT: CONCEPTUAL
Quicksort Runtime
The key to Quicksort’s runtime efficiency is the division of the array. The array is partitioned according to comparisons with the pivot element, so which pivot is the optimal choice to produce sub-arrays of roughly equal length?

The graphic displays two data sets which always use the first element as the pivot. Notice how many more steps are required when the quicksort algorithm is run on an already sorted input. The partition step of the algorithm hardly divides the array at all!

The worst case occurs when we have an imbalanced partition like when the first element is continually chosen in a sorted data-set.

One popular strategy is to select a random element as the pivot for each step. The benefit is that no particular data set can be chosen ahead of time to make the algorithm perform poorly.

Another popular strategy is to take the first, middle, and last elements of the array and choose the median element as the pivot. The benefit is that the division of the array tends to be more uniform.

Quicksort is an unusual algorithm in that the worst case runtime is O(N^2), but the average case is O(N * logN).

We typically only discuss the worst case when talking about an algorithm’s runtime, but for Quicksort it’s so uncommon that we generally refer to it as O(N * logN).'''

'''QUICKSORT: CONCEPTUAL
Quicksort Review
Quicksort is an efficient algorithm for sorting values in a list. A single element, the pivot, is chosen from the list. All the remaining values are partitioned into two sub-lists containing the values smaller than and greater than the pivot element.

Ideally, this process of dividing the array will produce sub-lists of nearly equal length, otherwise, the runtime of the algorithm suffers.

When the dividing step returns sub-lists that have one or less elements, each sub-list is sorted. The sub-lists are recombined, or swaps are made in the original array, to produce a sorted list of values.'''

'''QUICKSORT: PYTHON
Quicksort Introduction
We’ll be implementing a version of the quicksort algorithm in Python. Quicksort is an efficient way of sorting a list of values by partitioning the list into smaller sub-lists based on comparisons with a single “pivot” element.

Our algorithm will be recursive, so we’ll have a base case and an inductive step that takes us closer to the base case. We’ll also sort our list in-place to keep it as efficient as possible.

Sorting in place means we’ll need to keep track of the sub-lists in our algorithm using pointers and swap values inside the list rather than create new lists.

We’ll use pointers a lot in this algorithm so it’s worth spending a little time practicing. Pointers are indices that keep track of a portion of a list. Here’s an example of using pointers to represent the entire list:

my_list = ['pizza', 'burrito', 'sandwich', 'salad', 'noodles']
start_of_list = 0
end_of_list = len(my_list) - 1

my_list[start_of_list : end_of_list + 1]
# ['pizza', 'burrito', 'sandwich', 'salad', 'noodles']
Now, what if we wanted to keep my_list the same, but make a sub-list of only the first half?

end_of_half_sub_list = len(my_list) // 2
# 2

my_list[start_of_list : end_of_half_sub_list + 1]
# ['pizza', 'burrito', 'sandwich']
Finally, let’s make a sub-list that excludes the first and last elements…

start_of_sub_list = 1
end_of_sub_list = len(my_list) - 2

my_list[start_of_sub_list : end_of_sub_list]
# ['burrito', 'sandwich', 'salad']
Nice work! We’ll use two pointers, start and end to keep track of sub-lists in our algorithm. Let’s get started!'''

# Define your quicksort function
  
def quicksort(list, start, end):
  if start >= end:
    return

  print(list[start])
  start += 1
  quicksort(list, start, end)

colors = ["blue", "red", "green", "purple", "orange"]
quicksort(colors, 0, len(colors) - 1)

'''QUICKSORT: PYTHON
Pickin' Pivots
Quicksort works by selecting a pivot element and dividing the list into two sub-lists of values greater than or less than the pivot element’s value. This process of “partitioning” the list breaks the problem down into two smaller sub-lists.

For the algorithm to remain efficient, those sub-lists should be close to equal in length. Here’s an example:

[9, 3, 21, 4, 50, 8, 11]
# pick the first element, 9, as the pivot
# "lesser_than_list" becomes [3, 4, 8]
# "greater_than_list" becomes [21, 50, 11]
In this example the two sub-lists are equal in length, but what happens if we pick the first element as a pivot in a sorted list?

[1, 2, 3, 4, 5, 6]
# pick the first element, 1, as the pivot
# "lesser_than_list" becomes []
# "greater_than_list" becomes [2,3,4,5,6]
Our partition step has produced severely unbalanced sub-lists! While it may seem silly to sort an already sorted list, this is a common enough occurrence that we’ll need to make an adjustment.

We can avoid this problem by randomly selecting a pivot element from the list each time we partition. The benefit of random selection is that no particular data set will consistently cause trouble for the algorithm! We’ll then swap this random element with the last element of the list so our code consistently knows where to find the pivot.'''

'''
Instructions
1.
We’ve imported the randrange() function to assist with the random pivot. Check the documentation for how it works.

Use this function to create the variable pivot_idx, a random index between start and end.

randrange() can take two arguments which we’ll use to give us the bounds of a random number.

The second argument is not inclusive.

randrange(1, 10) # anything from 1 to 9
randrange(1, 11) # anything from 1 to 10

randrange(start, end) 
# anything from start to end - 1
2.
Make another variable pivot_element and use pivot_idx to retrieve the value located in the list which was passed in as an argument.

3.
Random is great because it protects our algorithm against inefficient runtimes, but our code will be simpler for the remainder of the algorithm if we know the pivot will always be in the same place.

Swap the end element of the list with the pivot_idx so we know the pivot element will always be located at the end of the list.'''

# use randrange to produce a random index
from random import randrange

def quicksort(list, start, end):
  if start >= end:
    return list
	# Define your pivot variables below
  pivot_idx = randrange(start, end)
  # Swap the elements in list below
  pivot_element = list[pivot_idx]
  list[pivot_idx] = list[-1]
  list[-1] = pivot_element

  # Leave these lines for testing
  print(list[start])
  start += 1
  return quicksort(list, start, end)


my_list = [32, 22]
print("BEFORE: ", my_list)
sorted_list = quicksort(my_list, 0, len(my_list) - 1)
print("AFTER: ", sorted_list)

'''QUICKSORT: PYTHON
Partitioning Party
We need to partition our list into two sub-lists of greater than or smaller than elements, and we’re going to do this “in-place” without creating new lists. Strap in, this is the most complex portion of the algorithm!

Because we’re doing this in-place, we’ll need two pointers. One pointer will keep track of the “lesser than” elements. We can think of it as the border where all values at a lower index are lower in value to the pivot. The other pointer will track our progress through the list.

Let’s explore how this will work in an example:

[5, 6, 2, 3, 1, 4]
# we randomly select "3" and swap with the last element
[5, 6, 2, 4, 1, 3]

# We'll use () to mark our "lesser than" pointer
# We'll use {} to mark our progress through the list

[{(5)}, 6, 2, 4, 1, 3]
# {5} is not less than 3, so the "lesser than" pointer doesn't move

[(5), {6}, 2, 4, 1, 3]
# {6} is not less than 3, so the "lesser than" pointer doesn't move

[(5), 6, {2}, 4, 1, 3]
# {2} is less than 3, so we SWAP the values...
[(2), 6, {5}, 4, 1, 3]
# Then we increment the "lesser than" pointer
[2, (6), {5}, 4, 1, 3]

[2, (6), 5, {4}, 1, 3]
# {4} is not less than 3, so the "lesser than" pointer doesn't move

[2, (6), 5, 4, {1}, 3]
# {1} is less than 3, so we SWAP the values...
[2, (1), 5, 4, {6}, 3]
# Then we increment the "lesser than" pointer
[2, 1, (5), 4, {6}, 3]

# We've reached the end of the non-pivot values
[2, 1, (5), 4, 6, {3}]
# Swap the "lesser than" pointer with the pivot...
[2, 1, (3), 4, 6, {5}]
Tada! We have successfully partitioned this list. Note that the “sub-lists” are not necessarily sorted, we’ll need to recursively run the algorithm on each sub-list, but the pivot has arrived at the correct location within the list.'''

'''
Create the variable lesser_than_pointer and assign it to the start of the list.

2.
Create a for loop that iterates from start to end, and set the iterating variable to idx. This will track our progress through the list (or sub-list) we’re partitioning.

To start, write continue in the for loop.

3.
Within the loop, remove continue and replace it with a conditional.

We need to do something if the element at idx is less than pivot_element.

If so:

Use parallel assignment to swap the values at lesser_than_pointer and idx.
Increment the lesser_than_pointer
We can check if list[idx] < pivot_element: to see if any alterations should be made to the list.

Here’s an example of swapping values with parallel assignment:

colors = ["blue", "red", "green"]
colors[0], colors[1] = colors[1], colors[0]
# ["red", "blue", "green"]
4.
Once the loop concludes, use parallel assignment to swap the pivot element with the value located at lesser_than_pointer.

Here’s an example:

colors = ["blue", "red", "green"]
colors[0], colors[1] = colors[1], colors[0]
# ["red", "blue", "green"]'''

from random import randrange

def quicksort(list, start, end):
  if start >= end:
    return list

  pivot_idx = randrange(start, end)
  pivot_element = list[pivot_idx]
  list[end], list[pivot_idx] = list[pivot_idx], list[end]

  # Create the lesser_than_pointer
  lesser_than_pointer = start

  # Start a for loop, use 'idx' as the variable
  for idx in range(start, end):
    # Check if the value at idx is less than the pivot
      if list[idx] < pivot_element:
      # If so: 
        # 1) swap lesser_than_pointer and idx values
        # 2) increment lesser_than_pointer
        list[idx], list[lesser_than_pointer] = list[lesser_than_pointer], list[idx]
        lesser_than_pointer += 1
      
  # After the loop is finished...
  # swap pivot with value at lesser_than_pointer
  list[end], list[lesser_than_pointer] = list[lesser_than_pointer], list[end]
  


  print(list[start])
  start += 1
  return quicksort(list, start, end)

my_list = [42, 103, 22]
print("BEFORE: ", my_list)
sorted_list = quicksort(my_list, 0, len(my_list) - 1)
print("AFTER: ", sorted_list)

'''QUICKSORT: PYTHON
Recurse, Rinse, Repeat
We’ve made it through the trickiest portion of the algorithm, but we’re not quite finished. We’ve partitioned the list once, but we need to continue partitioning until the base case is met.

Let’s revisit our example from the previous exercise where we had finished a single partition step:

# the pivot, 3, is correctly placed
whole_list = [2, 1, (3), 4, 6, 5]

less_than_pointer = 2
start = 0
end = len(whole_list) - 1
# start and end are pointers encompassing the entire list
# pointers for the "lesser than" sub-list
left_sub_list_start = start
left_sub_list_end = less_than_pointer - 1

lesser_than_sub_list = whole_list[left_sub_list_start : left_sub_list_end]
# [2, 1]

# pointers for the "greater than" sub-list
right_sub_list_start = less_than_pointer + 1
right_sub_list_end = end
greater_than_sub_list = whole_list[right_sub_list_start : right_sub_list_end]
# [4, 6, 5]
The key insight is that we’ll recursively call quicksort and pass along these updated pointers to mark the various sub-lists. Make sure you’re excluding the index that stores the newly placed pivot value or we’ll never hit the base case!'''

from random import randrange, shuffle 
def quicksort(list, start, end):
  # this portion of listay has been sorted
  if start >= end:
    return

  # select random element to be pivot
  pivot_idx = randrange(start, end + 1)
  pivot_element = list[pivot_idx]

  # swap random element with last element in sub-listay
  list[end], list[pivot_idx] = list[pivot_idx], list[end]

  # tracks all elements which should be to left (lesser than) pivot
  less_than_pointer = start
  
  for i in range(start, end):
    # we found an element out of place
    if list[i] < pivot_element:
      # swap element to the right-most portion of lesser elements
      list[i], list[less_than_pointer] = list[less_than_pointer], list[i]
      # tally that we have one more lesser element
      less_than_pointer += 1
  # move pivot element to the right-most portion of lesser elements
  list[end], list[less_than_pointer] = list[less_than_pointer], list[end]
  
  # Call quicksort on the "left" and "right" sub-lists

  quicksort(list, start, less_than_pointer - 1)
  quicksort(list, less_than_pointer + 1, end)
  
unsorted_list = [3,7,12,24,36,42]
shuffle(unsorted_list)
print(unsorted_list)
# use quicksort to sort the list, then print it out!
quicksort(unsorted_list,0,-1)
print(unsorted_list)

'''QUICKSORT: PYTHON
Quicksort Review
Congratulations on implementing the quicksort algorithm in Python. To review:

We established a base case where the algorithm will complete when the start and end pointers indicate a list with one or zero elements
If we haven’t hit the base case, we randomly selected an element as the pivot and swapped it to the end of the list
We then iterate through that list and track all the “lesser than” elements by swapping them with the iteration index and incrementing a lesser_than_pointer.
Once we’ve iterated through the list, we swap the pivot element with the element located at lesser_than_pointer.
With the list partitioned into two sub-lists, we repeat the process on both halves until base cases are met.'''

from random import randrange, shuffle

def quicksort(list, start, end):
  # this portion of list has been sorted
  if start >= end:
    return
  print("Running quicksort on {0}".format(list[start: end + 1]))
  # select random element to be pivot
  pivot_idx = randrange(start, end + 1)
  pivot_element = list[pivot_idx]
  print("Selected pivot {0}".format(pivot_element))
  # swap random element with last element in sub-lists
  list[end], list[pivot_idx] = list[pivot_idx], list[end]

  # tracks all elements which should be to left (lesser than) pivot
  less_than_pointer = start
  
  for i in range(start, end):
    # we found an element out of place
    if list[i] < pivot_element:
      # swap element to the right-most portion of lesser elements
      print("Swapping {0} with {1}".format(list[i], pivot_element))
      list[i], list[less_than_pointer] = list[less_than_pointer], list[i]
      # tally that we have one more lesser element
      less_than_pointer += 1
  # move pivot element to the right-most portion of lesser elements
  list[end], list[less_than_pointer] = list[less_than_pointer], list[end]
  print("{0} successfully partitioned".format(list[start: end + 1]))
  # recursively sort left and right sub-lists
  quicksort(list, start, less_than_pointer - 1)
  quicksort(list, less_than_pointer + 1, end)


    
  
list = [5,3,1,7,4,6,2,8]
shuffle(list)
print("PRE SORT: ", list)
print(quicksort(list, 0, len(list) -1))
print("POST SORT: ", list)

'''
RADIX SORT: CONCEPTUAL
What Is A Radix
Quick, which number is bigger: 1489012 or 54? It’s 1489012, but how can you tell? It has more digits so it has to be larger, but why exactly is that the case?

Our number system was developed by 8th century Arabic mathematicians and was successful because it made arithmetic operations more sensible and larger numbers easier to write and comprehend.

The breakthrough those mathematicians made required defining a set of rules for how to depict every number. First we decide on an alphabet: different glyphs, or digits, that we’ll use to write our numbers with. The alphabet that we use to depict numbers in this system are the ten digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. We call the length of this alphabet our radix (or base). So for our decimal system, we have a radix of 10.

Next we need to understand what those digits mean in different positions. In our system we have a ones place, a tens place, a hundreds place and so on. So what do digits mean in each of those places?

This is where explaining gets a little complicated because the actual knowledge might feel very fundamental. There’s a difference, for instance, between the digit ‘6’ and the actual number six that we represent with the digit ‘6’. This difference is similar to the difference between the letter ‘a’ (which we can use in lots of words) and the word ‘a’.

But the core of the idea is that we use these digits to represent different values when they’re used in different positions. The digit 6 in the number 26 represents the value 6, but the digit 6 used in the number 86452 represents the value 6000.

RADIX SORT: CONCEPTUAL
Base Numbering Systems
The value of different positions in a number increases by a multiplier of 10 in increasing positions. This means that a digit ‘8’ in the rightmost place of a number is equal to the value 8, but that same digit when shifted left one position (i.e., in 80) is equal to 10 * 8. If you shift it again one position you get 800, which is 10 * 10 * 8.

This is where it’s useful to incorporate the shorthand of exponential notation. It’s important to note that 100 is equal to 1. Each position corresponds to a different exponent of 10.

So why 10? It’s a consequence of how many digits are in our alphabet for numbering. Since we have 10 digits (0-9) we can count all the way up to 9 before we need to use a different position. This system that we used is called base-10 because of that.

Sorting By Radix
So how does a radix sort use this base numbering system to sort integers? First, there are two different kinds of radix sort: most significant digit, or MSD, and least significant digit, or LSD.

Both radix sorts organize the input list into ten “buckets”, one for each digit. The numbers are placed into the buckets based on the MSD (left-most digit) or LSD (right-most digit). For example, the number 2367 would be placed into the bucket “2” for MSD and into “7” for LSD.

This bucketing process is repeated over and over again until all digits in the longest number have been considered. The order within buckets for each iteration is preserved. For example, the numbers 23, 25 and 126 are placed in the “3”, “5”, and “6” buckets for an initial LSD bucketing. On the second iteration of the algorithm, they are all placed into the “2” bucket, but the order is preserved as 23, 25, 126.

Radix Sort Performance
The most amazing feature of radix sort is that it manages to sort a list of integers without performing any comparisons whatsoever. We call this a non-comparison sort.

This makes its performance a little difficult to compare to most other comparison-based sorts. Consider a list of length n. For each iteration of the algorithm, we are deciding which bucket to place each of the n entries into.

How many iterations do we have? Remember that we continue iterating until we examine each digit. This means we need to iterate for how ever many digits we have. We’ll call this average number of digits the word-size or w.

This means the complexity of radix sort is O(wn). Assuming the length of the list is much larger than the number of digits, we can consider w a constant factor and this can be reduced to O(n).

Radix Review
Take a moment to review radix sort:

A radix is the base of a number system. For the decimal number system, the radix is 10.
Radix sort has two variants - MSD and LSD
Numbers are bucketed based on the value of digits moving left to right (for MSD) or right to left (for LSD)
Radix sort is considered a non-comparison sort
The performance of radix sort is O(n)

RADIX SORT: PYTHON
Finding the Max Exponent
In our version of least significant digit radix sort, we’re going to utilize the string representation of each integer. This, combined with negative indexing, will allow us to count each digit in a number from right-to-left.

Some other implementations utilize integer division and modular arithmetic to find each digit in a radix sort, but our goal here is to build an intuition for how the sort works.

Our first step is going to be finding the max_exponent, which is the number of digits long the largest number is. We’re going to find the largest number, cast it to a string, and take the length of that string.

Instructions
1.
Define your function radix_sort() that takes a list as input and call that input to_be_sorted.

2.
In order to determine how many digits are in the longest number in the list, we’ll need to find the longest number.

Declare a new variable maximum_value and assign the max() of to_be_sorted to it.

3.
Now we want to define our max_exponent.

First, cast maximum_value to a string.
Then take the len() of that string.
Then assign that len() to a variable called max_exponent.
Then return max_exponent.
Use str(number) to convert number into a string.'''

def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))
  return max_exponent

'''
RADIX SORT: PYTHON
Setting Up For Sorting
In this implementation, we’re going to build the radix sort naturally, from the inside out. The first step we’re going to take is going to be our inner-most loop, so that we know we’ll be solid when we’re iterating through each of the exponents.

Instructions
1.
By the nature of a radix sort we need to erase and rewrite our output list a number of times. It would be bad practice to mutate the input list — in case something goes wrong with our code, or someone using our sort decides they don’t want to wait anymore. We wouldn’t want anyone out there to have to deal with the surprise of having their precious list of integers mangled.

Create a copy of to_be_sorted and save the copy into a new list called being_sorted.

Remember you can make a copy by using the slice (:) syntax without any arguments:

cool_list = [1, 19, 22]

# create a copy of cool_list
imitation_is_flattery = cool_list[:]
2.
A radix sort goes through each position of each number and puts all of the inputs in different buckets depending on the value . Since there are 10 different values (that is, 0 through 9) that a position can have, we need to create ten different buckets to put each number in.

Create a list of ten empty lists and assign the result to a variable called digits. Then return digits.

There are plenty of ways to create a list of ten empty lists, but a list comprehension would be an efficient way to do this:

six_empty_lists = [[] for digit in range(6)]'''

def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))

  # create copy of to_be_sorted here
  being_sorted = to_be_sorted[:]
  
  digits = [[] for i in range(10)]
  return digits
  
'''
RADIX SORT: PYTHON
Bucketing Numbers
The least significant digit radix sort algorithm takes each number in the input list, looks at the digits of that number in order from right to left, and incrementally stuffs each number into the bucket corresponding to the value of that digit.

First we’re going to write this logic for the least significant digit, then we’re going to loop over the code we write to do that for every digit.

Instructions
1.
We’ll need to iterate over being_sorted. Grab each value of being_sorted and save it as the temporary variable number.

Use the invocation

for temporary_variable in list_being_iterated:
To create a temporary variable called temporary_variable that stores each value of list_being_iterated.

2.
Now convert number to a string and save that as number_as_a_string.

You can convert almost anything in Python to a string using the str() function.

3.
How do we get the last element of a string? This would correspond to the least significant digit of the number. For strings, this is simple, we can use a negative index.

Save the last element of number_as_a_string to the variable digit.

We can get the last digit of number_as_a_string from the following:

number_as_a_string[-1]
4.
Now that we have a string containing the least significant digit of number saved to the variable digit. We want to use digit as a list index for digits. Unfortunately, it needs to be an integer to do that. But that should be easy for us to do:

Set digit equal to the integer form of digit.

You can use int() to cast a string to an integer. So something like:

an_int = int(a_string)
5.
We know that digits[digit] is an empty list (because digits has ten lists and digit is a number from 0 to 9). So let’s add our number to that list!

Call .append() on digits[digit] with the argument number.

6.
Now break out of the for loop and return digits.'''

def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))

  being_sorted = to_be_sorted[:]
  digits = [[] for i in range(10)]

  # create for loop here:
  for number in being_sorted:
    number_as_a_string = str(number)
    digit = number_as_a_string[-1]
    digit = int(digit)
    digits[digit].append(number)
  return digits

'''
Rendering the List
For every iteration, radix sort renders a version of the input list that is sorted based on the digit that was just looked at. At first pass, only the ones digit is sorted. At the second pass, the tens and the ones are sorted. This goes on until the digits in the largest position of the largest number in the list are all sorted, and the list is sorted at that time.

Here we’ll be rendering the list, at first, it will just return the list sorted so just the ones digit is sorted.

Instructions
1.
Outside of our for loop which appends the numbers in our input list to the different buckets in digits, let’s render the list.

Since we know that all of our input numbers are in digits we can safely clear out being_sorted. We’ll make it an empty list and then add back in all the numbers from digits as they appear.

Assign an empty list to being_sorted.

2.
Now, create a for loop that iterates through each of our lists in digits.

Call each of these lists numeral because they each correspond to one specific numeral from 0 to 9.

3.
Now use the .extend() method (which appends all the elements of a list, instead of appending the list itself) to add the elements of numeral to being_sorted.

Use the .extend() method:

big_list = [1, 51, 801, 42, 302]
smaller_list = [15, 905]
big_list.extend(smaller_list)
4.
Unindent out of the for loop and return being_sorted.'''

def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))

  being_sorted = to_be_sorted[:]
  digits = [[] for i in range(10)]

  for number in being_sorted:
    number_as_a_string = str(number)
    digit = number_as_a_string[-1]
    digit = int(digit)
    
    digits[digit].append(number)

  # reassign being_sorted here:
    being_sorted = []

    for numeral in digits:
      being_sorted.extend(numeral)
  return being_sorted



'''
RADIX SORT: PYTHON
Iterating through Exponents
We have the interior of our radix sort, which right now goes through a list and sorts it by the first digit in each number. That’s a pretty great start actually. It won’t be hard for us to go over every digit in a number now that we can already sort by a given digit.

Instructions
1.
After defining being_sorted for the first time in the function (and before defining digits which we’ll need per iteration), create a new for loop that iterates through the range() of max_exponent.

Use the variable name exponent as a temporary variable in your for loop, it will count the current exponent we’re looking at for each number.

2.
Now indent the rest of your function after this new for loop.

(Tip: You can highlight the text in your code editor and use the Tab key to increase the indentation of code.)

3.
In our for loop we’re going to want to create the index we’ll use to get the appropriate position in the numbers we’re sorting.

First we’re going to create the position variable, which keeps track of what exponent we’re looking at. Since exponent is zero-indexed our position is always going to be one more than the exponent. Assign to it the value of exponent + 1.

4.
Now we want to create our index that we’ll be using to index into each number! This index is going to be roughly the same as position, but since we’re going to be indexing the string in reverse it needs to be negative!

Set index equal to -position.

5.
Now in the body of our loop, let’s update our digit lookup to get the digit at the given index. Where we before used number_as_a_string[-1] we’ll want to start accessing [index] instead.

Update the line of code where we first define digit to access index in number_as_a_string.

6.
Now we’ve got a sort going! At the very end of our function, de-indenting out of all the for loops (but not the function itself), return being_sorted. It will be sorted by this point!'''
def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))
  being_sorted = to_be_sorted[:]

  # Add new for-loop here:

  for exponent in range(max_exponent):

    position = exponent + 1

    index = -position

    digits = [[] for i in range(10)]

    for number in being_sorted:
      number_as_a_string = str(number)
      digit = number_as_a_string[index]
      digit = int(digit)
      
      digits[digit].append(number)

    being_sorted = []
    for numeral in digits:
      being_sorted.extend(numeral)

  return being_sorted

'''
RADIX SORT: PYTHON
Review (and Bug Fix!)
Now that we’ve finished writing our radix sort we’re finished for the day… or are we?

Instructions
1.
Now that we’ve gotten our sort working let’s test it out with some new data.

Run radix_sort on unsorted_list.

2.
What? IndexError? Did we forget something?

We did! Some of the numbers that we’re sorting are going to be shorter than other numbers.

We can fix it though! First, we should comment out the line we added to test the sort.

Add a comment with #:

neat_code = 20 + cool_function()
# This is a comment
3.
Where we defined digit to be the value of number_as_a_string at index index we need to now wrap that definition in a try block.

Add a try block and, indented in that block, leave your original definition of digit.

Add a try block to your code by adding the try keyword and an indented block:

try:
  cool_list = [0, 30, 18]

  # raises IndexError
  cool_value = cool_list[-5]
4.
After the try block, we’ll want to handle the possibility of an IndexError. What does it mean if we get an index error here?

It means the value for number at index is actually 0.

Handle the exception by adding an except IndexError block, in this case assigning digit to be 0.

Introduce an except block to your existing try block:

try:
  cool_list = [0, 30, 18]

  # raises IndexError
  cool_value = cool_list[-5]

except IndexError:
  # 5 is a cool value that will have to do.
  cool_value = 5
5.
Excellent! Now let’s try uncommenting the line where we sort unordered_list. Print out the results.

6.
Great job! We created an algorithm that:

Takes numbers in an input list.
Passes through each digit in those numbers, from least to most significant.
Looks at the values of those digits.
Buckets the input list according to those digits.
Renders the results from that bucketing.
Repeats this process until the list is sorted.
And that’s what a radix sort does! Feel free to play around with the solution code, see if there’s anything you can improve about the code or a different way of writing it you want to try.'''

def radix_sort(to_be_sorted):
  maximum_value = max(to_be_sorted)
  max_exponent = len(str(maximum_value))
  being_sorted = to_be_sorted[:]

  for exponent in range(max_exponent):
    position = exponent + 1
    index = -position

    digits = [[] for i in range(10)]

    for number in being_sorted:
      number_as_a_string = str(number)

      try:
        digit = number_as_a_string[index]
      
      except IndexError:

        digit = 0
      
      digit = int(digit)

      digits[digit].append(number)

    being_sorted = []
    for numeral in digits:
      being_sorted.extend(numeral)

  return being_sorted

unsorted_list = [830, 921, 163, 373, 961, 559, 89, 199, 535, 959, 40, 641, 355, 689, 621, 183, 182, 524, 1]

print(radix_sort(unsorted_list))


#--------------------------- practice project ---------------------------------------

'''SORTING ALGORITHMS IN PYTHON
A Sorted Tale
You recently began employment at “A Sorted Tale”, an independent bookshop. Every morning, the owner decides to sort the books in a new way.

Some of his favorite methods include:

By author name
By title
By number of characters in the title
By the reverse of the author’s name
Throughout the day, patrons of the bookshop remove books from the shelf. Given the strange ordering of the store, they do not always get the books placed back in exactly the correct location.

The owner wants you to research methods of fixing the book ordering throughout the day and sorting the books in the morning. It is currently taking too long!

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
19/20Complete
Mark the tasks as complete by checking them off
Get to know the data
1.
The owner provides the current state of the bookshelf in a comma-separated values, or csv, file. To get you started, we have provided a function load_books, defined in utils.py.

Within script.py, we are loading the books from books_small.csv. This list of 10 books makes it easier to determine how the algorithms are behaving. we’ll use this to develop the algorithms and then we’ll try them out later on the larger file.

Add a for loop to print the titles within the bookshelf.

Save your code and run it using python3 script.py in the terminal.

for book in bookshelf:
  print(book['title'])
2.
Today’s sorting is by title and looking at the bookshelf it’s pretty close. Some patrons have placed books back in slightly the wrong place.

Before we start solving the problem, we need to do a bit of data manipulation to ensure that we compare books correctly. Python’s built-in comparison operators compare letters lexicographically based on their Unicode code point. You can determine the code point of characters using Python’s built-in ord function. For example to calculate the code point for “z” you would use the following code:

ord("z")
Try this in script.py using print statements:

What is the code point of “a”?
What about “ “?
What about “A”?
print(ord("a"))
print(ord(" "))
print(ord("A"))
3.
You may have noticed that the uppercase letters have values less than their lowercase counterparts. When sorting, we don’t want to take into account the case of the letters. For example, “cats” should come before “Dogs”, even though ord("c") > ord("D") is True.

We’ll make this happen by converting everything to lowercase prior to comparison. Since we need to do this often, lets save the lowercase author and title as attributes while loading the bookshelf in utils.py:

book['author_lower']
book['title_lower']
For the author, the statement is:

book['author_lower'] = book['author'].lower()
Fix the midday errors
4.
As we noted, our books are pretty close to being sorted by title. From the sorting lessons, you may remember that bubble sort performs well for nearly sorted data such as this.

The code for performing bubble sort on an array of numbers is provided in sorts.py. However, we are sorting on books which are Python dictionaries. Further, the owner likes to change the ordering of books daily. To make the sort order flexible, add an argument comparison_function. This will allow us to pass in a custom function for comparing the order of two books.

Within sorts.py, change the bubble sort first line to:

def bubble_sort(arr, comparison_function):
5.
Our comparison_function will take two arguments, and return True if the first one is “greater than” the second.

Within the body of the bubble sort function, modify the comparison conditional statement to use the comparison_function instead of the built in operators (if arr[idx] > arr[idx + 1]:).

if comparison_function(arr[idx], arr[idx + 1]):
6.
Now that we have a bubble sort algorithm that can work on books, let’s give it a shot. Within script.py define a sort comparison function, by_title_ascending.

It should take book_a and book_b as arguments.

It should return True if the title_lower of book_a is “greater than” the title_lower of book_b and False otherwise.

def by_title_ascending(book_a, book_b):
  return book_a['title_lower'] > book_b['title_lower']
7.
Sort the bookshelf using bubble sort. Save the result as sort_1 and print the titles to the console to verify the order.

How many swaps were necessary?

sort_1 = sorts.bubble_sort(bookshelf, by_title_ascending)

for book in sort_1:
  print(book['title'])
A new sorting order
8.
The owner of the bookshop wants to sort by the author’s full name tomorrow. Define a new comparison function, by_author_ascending, within script.py.

It should take book_a and book_b as arguments.

It should return True if the author_lower of book_a is “greater than” the author_lower of book_b and False otherwise.

def by_author_ascending(book_a, book_b):
  return book_a['author_lower'] > book_b['author_lower']
9.
Our sorting algorithms will alter the original bookshelf, so create a new copy of this data, bookshelf_v1.

This does NOT create a copy:

bookshelf_v1 = bookshelf
Use:

bookshelf_v1 = bookshelf.copy()
10.
Try sorting the list of books, bookshelf_v1 using the new comparison function and bubble sort. Save the result as sort_2 and print the authors to the console to verify the order.

How many swaps are needed now?

sort_2 = sorts.bubble_sort(bookshelf_v1, by_author_ascending)
A new sorting algorithm
11.
The number of swaps is getting to be high for even a small list like this. Let’s try implementing a different type of search: quicksort.

The code for quicksort of a numeric array is in sorts.py. We need to modify it in a similar fashion that we modified bubble sort.

Add comparison_function as the final argument to the quicksort function.

def quicksort(list, start, end, comparison_function):
12.
Within the quicksort function, be sure to pass the argument for the comparison_function for the recursive calls.

Update the recursive calls to:

quicksort(list, start, less_than_pointer - 1, comparison_function)
quicksort(list, less_than_pointer + 1, end, comparison_function)
13.
The last modification we need to make to quicksort is to update the comparison conditional. It is currently using the built in comparison:

if pivot_element > list[i]:
Update this to use comparison_function.

if comparison_function(pivot_element, list[i]):
14.
Within script.py create another copy of bookshelf as bookshelf_v2.

bookshelf_v2 = bookshelf.copy()
15.
Perform quicksort on bookshelf_v2 using by_author_ascending. This implementation operates on the input directly, so does not return a list.

Print the authors in bookshelf_v2 to ensure they are now sorted correctly.

sorts.quicksort(bookshelf_v2, 0, len(bookshelf_v2) - 1, by_author_ascending)
The last sort
16.
The owner has asked for one last sorting order, sorting by the length of the sum of the number of characters in the book and author’s name.

Create a new comparison function, by_total_length. It should return True if the sum of characters in the title and author of book_a is “greater than” the sum in book_b and False otherwise.

def by_total_length(book_a, book_b):
  return len(book_a['author_lower']) + len(book_a['title_lower']) > len(book_b['author_lower']) + len(book_b['title_lower'])
17.
Load the long list of books into a new variable, long_bookshelf.

long_bookshelf = utils.load_books('books_large.csv')
18.
Run bubble sort on this algorithm using by_total_length as the comparison function. Does it seem slow?

It should take a couple of seconds to run.

19.
Comment out the bubble sort attempt and now try quicksort. Does it live up to its name?

Quicksort should be noticeably faster.

More sorting
20.
You’ve met the requirements of the project by the bookshop owner. If you’d like, play with creating your own comparison operators or other sorting functions.'''

#--------------------------- script.py------------------------------------------

import utils
import sorts

def by_title_ascending(book_a, book_b):
  if book_a['title_lower'] > book_b['title_lower']:
    return True
  else:
    #return False
    pass

def by_author_ascending(book_a, book_b):
  if book_a['author_lower'] > book_b['author_lower']:
    return True
  else:
    #return False
    pass

def by_total_length(book_a, book_b):
  sum_a = len(book_a['title']) + len(book_a['author'])
  sum_b = len(book_b['title']) + len(book_b['author'])
  if sum_a > sum_b:
    return True
  else:
    pass


bookshelf = utils.load_books('books_small.csv')
long_bookshelf = utils.load_books('books_large.csv')
# make a copy of the original list, because the bubble sorting will alter the original list.
bookshelf_v1 = bookshelf.copy()
bookshelf_v2 = bookshelf.copy()
bookshelf_v3 = bookshelf.copy()


#for book in bookshelf:
#  print(book['title'])

#print(ord("z"))
#print(ord(" "))
#print(ord("a"))
#print(ord("A"))


#bookshelf['author_lower'] = bookshelf['author'].lower()
#bookshelf['title_lower'] = bookshelf['title'].lower()

#print(type(bookshelf))


sort_1 = sorts.bubble_sort(bookshelf, by_title_ascending)
sort_2 = sorts.bubble_sort(bookshelf_v1, by_author_ascending)
#sort_3 = sorts.quicksort(bookshelf_v2, 0, len(bookshelf_v2)-1, by_author_ascending)

for book in sort_1:
  pass
  #print('sorted List')
  #print(book['title'])

for book in sort_2:
  pass
  #print('sorted List')
  #print('{}  by  {}'.format(book['title'],book['author']))

sorts.quicksort(bookshelf_v2, 0, len(bookshelf_v2)-1, by_author_ascending)
#print(bookshelf_v2)
for book in bookshelf_v2:
  #print('{}  by  {}'.format(book['title'],book['author']))
  pass

#sort4 = sorts.bubble_sort(long_bookshelf, by_total_length)
sorts.quicksort(bookshelf_v3, 0, len(bookshelf_v2)-1, by_total_length)
#print(bookshelf_v3)
for book in bookshelf_v3:
  #print('{}  by  {}'.format(book['title'],book['author']))
  pass


#-----------------------  sort.py------------------------------------------


import random

def bubble_sort(arr, comparison_function):
  swaps = 0
  sorted = False
  while not sorted:
    sorted = True
    for idx in range(len(arr) - 1):
      if comparison_function(arr[idx], arr[idx + 1]):
        sorted = False
        arr[idx], arr[idx + 1] = arr[idx + 1], arr[idx]
        swaps += 1
  print("Bubble sort: There were {0} swaps".format(swaps))
  return arr

def quicksort(list, start, end, comparison_function):
  if start >= end:
    return
  pivot_idx = random.randrange(start, end + 1)
  pivot_element = list[pivot_idx]
  list[end], list[pivot_idx] = list[pivot_idx], list[end]
  less_than_pointer = start
  for i in range(start, end):
    if comparison_function(pivot_element, list[i]):
      list[i], list[less_than_pointer] = list[less_than_pointer], list[i]
      less_than_pointer += 1
  list[end], list[less_than_pointer] = list[less_than_pointer], list[end]
  quicksort(list, start, less_than_pointer - 1, comparison_function)
  quicksort(list, less_than_pointer + 1, end, comparison_function)


#------------------------ utils.py------------------------------------------


import csv

# This code loads the current book
# shelf data from the csv file
def load_books(filename):
  bookshelf = []
  with open(filename) as file:
      shelf = csv.DictReader(file)
      for book in shelf:
          # add your code here
          book['author_lower'] = book['author'].lower()
          book['title_lower'] = book['title'].lower()          
          bookshelf.append(book)

          
  return bookshelf
  
  
#-------------------------- books_small.csv ---------------------------------

title,author
Adventures of Huckleberry Finn,Mark Twain
Best Served Cold,Joe Abercrombie
Dear Emily,Fern Michaels
Collected Poems,Robert Hayden
End Zone,Don DeLillo
Forrest Gump,Winston Groom
Gravity,Tess Gerritsen
Hiromi's Hands,Lynne Barasch
Borwegian Wood,Haruki Murakami
Middlesex: A Novel (Oprah's Book Club),Jeffrey Eugenides

#------------------------ books_large.csv------------------------------------

title,author
100 Selected Poems,e. e. cummings
100 Years of The Best American Short Stories,Lorrie Moore
"1001 Beds: Performances, Essays, and Travels (Living Out: Gay and Lesbian Autobiog)",Tim Miller
1001 Children's Books You Must Read Before You Grow Up,Julia Eccleshare
101 Famous Poems,Roy Cook
"1020 Haiku in Translation: The Heart of Basho, Buson and Issa",William R. Nelson
11/22/63: A Novel,Stephen King
12 Plays: A Portable Anthology,Janet E. Gardner
1356: A Novel,Bernard Cornwell
187 Reasons Mexicanos Can't Cross the Border: Undocuments 1971-2007,Juan Felipe Herrera
"19 Book Set: The Aubrey Maturin Series - Master and Commander, Post Captain, HMS Surprise, The Mauritius Command, Desolation Island, The Fortune of War, The Surgeon's Mate, The Ionian Mission, Treason's Harbour, The Far Side of the World + 9 More (The Aubrey - Maturin Series Set, Vol. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)",Patrick O'Brian
1956 and All That: The Making of Modern British Drama,Dan Rebellato
1Q84: 3 Volume Boxed Set (Vintage International),Haruki Murakami
2.5 Minute Ride and 101 Most Humiliating Stories,Lisa Kron
"20,000 Leagues Under the Sea (Dramatized)",Jules Verne
2000x: R.U.R. (Dramatized),Karel Capek
21 Speeches That Shaped Our World: The People and Ideas That Changed the Way We Think,Chris Abbott
21: The Final Unfinished Voyage of Jack Aubrey (Vol. Book 21)  (Aubrey/Maturin Novels),Patrick O'Brian
2666 (Spanish Edition),Roberto Bolao
3 - La cura mortal - Maze Runner (Maze Runner Trilogy) (Spanish Edition),James Dashner
30 Ten-Minute Plays from the Actors Theatre of Louisville for 2 Actors,Michael Bigelow Dixon
30 Under 30: An Anthology of Innovative Fiction by Younger Writers,Blake Butler
36 Arguments for the Existence of God: A Work of Fiction (Vintage Contemporaries),Rebecca Goldstein
365 Days / 365 Plays,Suzan-Lori Parks
"4 Book Set : The Aubrey-Maturin Series - Master and Commander, Post Captain, HMS Surprise, The Mauritius Command (The Aubrey-Maturin Series, Vol. 1, 2, 3, 4)",Patrick O'Brian
40 Short Stories: A Portable Anthology,Beverly Lawn
4000 Miles,Amy Herzog
4000 Miles and After the Revolution: Two Plays,Amy Herzog
"44 Scotland Street (44 Scotland Street Series, Book 1)",Alexander McCall Smith
47 Ronin,John Allyn
491 Days: Prisoner Number 1323/69 (Modern African Writing Series),Winnie Madikizela-Mandela
50 Essays: A Portable Anthology,Samuel Cohen
501 Must-Know Speeches,Bounty
51 Shades of Chocolate,Christopher Milano
52 Bible Characters Dramatized: Easy-to-Use Monologues for All Occasions,Kenneth W. Osbeck
60 Seconds to Shine Volume 2: 221 One-minute Monologues For Women,John Capecci
777 And Other Qabalistic Writings of Aleister Crowley: Including Gematria & Sepher Sephiroth,Aleister Crowley
"9/11 Fiction, Empathy, and Otherness",Tim Gauthier
A 30-Minute Summary of Sue Monk Kidd's The Invention of Wings,InstaRead Summaries
A Baby for Christmas (Love at The Crossroads) (Volume 2),Pat Simmons
A Banner of Love,Josephine Garner
A Barthes Reader,Roland Barthes
A Beautiful Place to Die: An Emmanuel Cooper Mystery,Malla Nunn
A Beautiful Wedding: A Novella (Beautiful Disaster Series),Jamie McGuire
A Blaze of Glory: A Novel of the Battle of Shiloh (the Civil War in the West),Jeff Shaara
A Blossom of Bright Light (A Jimmy Vega Mystery),Suzanne Chazin
A Bollywood Affair,Sonali Dev
A Book of Middle English,J. A. Burrow
A Book That Was Lost: Thirty Five Stories (Hebrew Classics),S. Y. Agnon
A Boy and His Soul (Oberon Modern Plays),Colman Domingo
A Battle Won,S. Thomas Russell
A Boy of Good Breeding: A Novel,Miriam Toews
A Brave Man Seven Storeys Tall: A Novel (P.S.),Will Chancellor
A Brief History of Seven Killings: A Novel,Marlon James
A Brilliant Madness,Robert M Drake
A Broken Girl's Journey 4: Kylie's Song (Volume 4),Niki Jilvontae
A Brownsville Tale 2 (Volume 2),Sonovia Alexander
A Carrot a Day: A Daily Dose of Recognition for Your Employees,Adrian Gostick
A Cartoon History of Texas,Patrick M. Reynolds
A Chain of Thunder: A Novel of the Siege of Vicksburg (the Civil War in the West),Jeff Shaara
A Child's Garden of Verses (Large Print Edition),Robert Louis Stevenson
A Child's Portrait of Shakespeare (Shakespeare Can Be Fun series),Lois Burdett
A Christmas Carol,Charles Dickens
A Christmas Carol,Charles Dickens
A Christmas Carol and Other Christmas Books (Oxford World's Classics),Charles Dickens
A Clockwork Orange,Anthony Burgess
A Clockwork Orange Easton Press Leatherbound,Anthony Burgess
A Colebridge Quilted Christmas (Colebridge Community),Hazelwood
A Common Life: The Wedding Story (The Mitford Years #6),Jan Karon
A Confederacy of Dunces,John Kennedy Toole
A Confederacy of Dunces Cookbook: Recipes from Ignatius J. Reilly's New Orleans,Cynthia LeJeune Nobles
A Connecticut Yankee In King Arthur's Court,Mark Twain
A Connecticut Yankee in King Arthur's Court (Dover Thrift Editions),Mark Twain
A Constellation of Vital Phenomena: A Novel,Anthony Marra
"A Corresponding Renaissance: Letters Written by Italian Women, 1375-1650",Lisa Kaborycha
A Critical Companion to Beowulf,Andy Orchard
A Cup of Christmas Tea,Tom Hegg
A Dance of Blades (Shadowdance 2),David Dalglish
A Dance of Mirrors (Shadowdance 3),David Dalglish
A Daughter of Zion (Zion Chronicles),Bodie Thoene
A Day In August,Reis Armstrong
A Death in the Family: A Detective Kubu Mystery,Michael Stanley
A Decent Ride: A Novel,Irvine Welsh
A Deeper Love Inside: The Porsche Santiaga Story,Sister Souljah
"A Diary With Reminiscences Of The War And Refugee Life In The Shenandoah Valley, 1860-1865",Cornelia McDonald
A Dictionary of Critical Theory (Oxford Paperback Reference),Ian Buchanan
A Dictionary of Haiku: Second Edition,Jane Reichhold
A Dictionary of Northern Mythology,Rudolf Simek
A Different Kind of Courage,Sarah Holman
A Dirty Job: A Novel,Christopher Moore
"A Dirty Shame: J.J. Graves Mystery, Book 2",Liliana Hart
A Dog's Purpose,W. Bruce Cameron
A Doll's House (Dover Thrift Editions),Henrik Ibsen
A Dollar Outta Fifteen Cent 3: Mo' Money...Mo' Problems,Caroline McGill
A Dream Play,August Strindberg
A Face in the Crowd,Stephen King
A Faint Cold Fear (Grant County Mysteries),Karin Slaughter
A Family Guide To Narnia: Biblical Truths in C.S. Lewis's The Chronicles of Narnia,Christin Ditchfield
A Farewell To Arms,Ernest Hemingway
A Farewell to Arms: The Hemingway Library Edition,Ernest Hemingway
"A Fatal Grace (Three Pines Mysteries, No. 2)",Louise Penny
A Feast of Snakes: A Novel,Harry Crews
A Free State: A Novel,Tom Piazza
A Funny Thing Happened to Me on My Way Through the Bible: A Collection of Humorous Sketches and Monologues Based on Familiar Bible Stories (Lillenas Drama Resources),Martha Bolton
A Garden of Marvels: Tales of Wonder from Early Medieval China,"Robert, Ford Campany"
A Gentle Creature and Other Stories: White Nights; A Gentle Creature; The Dream of a Ridiculous Man (Oxford World's Classics),Fyodor Dostoevsky
A German General on the Eastern Front: The Letters and Diaries of Gotthard Heinrici 1941-1942,Johanne Hurter
A Glossary of Literary Terms,M.H. Abrams
A Glossary of Literary Terms,M.H. Abrams
A Good Man Is Hard to Find and Other Stories,Flannery O'Connor
A Good Yarn (A Blossom Street Novel),Debbie Macomber
A Grain of Wheat (Penguin African Writers),Ngugi wa Thiong'o
"A Gravity's Rainbow Companion: Sources and Contexts for Pynchon's Novel, 2nd Edition",Steven Weisenburger
A Guide for the Perplexed: A Novel,Dara Horn
A Haiku Garden: Selections from the Everyday Photo Haiku Project,Patrick J Harris
A Haiku Perspective,Annette Rochelle Aben
A Handbook of Critical Approaches to Literature,Wilfred Guerin
A Handbook to Literature (12th Edition),William Harmon
A Handful of Dust,Evelyn Waugh
A Hanging at Cinder Bottom: A Novel,Glenn Taylor
"A Hard, Cruel Shore: An Alan Lewrie Naval Adventure (Alan Lewrie Naval Adventures)",Dewey Lambdin
A Heart's Betrayal (A Journey of the Heart),Colleen Coble
A Heart's Disguise (A Journey of the Heart),Colleen Coble
A Heart's Obsession (A Journey of the Heart),Colleen Coble
A Herzen Reader,Alexander Herzen
A High Wind in Jamaica (New York Review Books Classics),Richard Hughes
A History of Illuminated Manuscripts,Christopher De Hamel
A History of Old Norse Poetry and Poetics,Margaret Clunies Ross
A History of the English Speaking Peoples (4 Volume Set),Winston S. Churchill
A History of the Present Illness: Stories,Louise Aronson
A House Divided (A Reverend Curtis Black Novel),Kimberla Lawson Roby
A House of My Own: Stories from My Life,Sandra Cisneros
A Hustler's Wife (Urban Books),Nikki Turner
"A Jane Austen Education: How Six Novels Taught Me About Love, Friendship, and the Things That Really Matter",William Deresiewicz
"A Jesuit Off-Broadway: Behind the Scenes with Faith, Doubt, Forgiveness, and More",James Martin SJ
A Jewish Spirit in the Wild: Stories of Jewish life in South Africa from the late 19th century to 1979,Abraham Rosen
A Journal of the Plague Year (Dover Thrift Editions),Daniel Defoe
A Journey to the End of the Millennium - A Novel of the Middle Ages,A. B. Yehoshua
A King's Ransom: A Novel,Sharon Kay Penman
A Lesson in Hope: A Novel,Philip Gulley
A King's Trade: An Alan Lewrie Naval Adventure (Alan Lewrie Naval Adventures),Dewey Lambdin
A Kiss Remembered,Sandra Brown
A Legacy (New York Review Books Classics),Sybille Bedford
A Lesson Before Dying (Oprah's Book Club),Ernest J. Gaines
"A Lick of Frost (Meredith Gentry, Book 6)",Laurell K. Hamilton
A Life in Letters (Penguin Classics),Anton Chekhov
A Light in the Wilderness: A Novel,Jane Kirkpatrick
A Lineage of Grace: Five Stories of Unlikely Women Who Changed Eternity,Francine Rivers
A Little Life: A Novel,Hanya Yanagihara
A Little Tour through European Poetry,John Taylor
A Man Called Ove: A Novel,Fredrik Backman
A Man for All Seasons: A Play in Two Acts,Robert Bolt
A Man of the People,Chinua Achebe
A Man Without a Country,Kurt Vonnegut
A Man's Worth (Urban Christian),Nikita Lynnette Nichols
A Manual for Cleaning Women: Selected Stories,Lucia Berlin
A Map of Betrayal: A Novel (Vintage International),Ha Jin
A Matter of Heart (Lone Star Brides) (Volume 3),Tracie Peterson
A Memory of Violets: A Novel of London's Flower Sellers,Hazel Gaynor
A Mercy,Toni Morrison
A Midsummer Night's Dream,William Shakespeare
A Midsummer Night's Dream,William Shakespeare
A Midsummer Night's Dream (Barnes & Noble Shakespeare),William Shakespeare
A Midsummer Night's Dream (Calla Editions),William Shakespeare
A Midsummer Night's Dream (Cambridge School Shakespeare),Rex Gibson
A Midsummer Night's Dream (Folger Shakespeare Library),William Shakespeare
A Midsummer Night's Dream (Shakespeare Made Easy),William Shakespeare
A Midsummer Night's Dream (The Pelican Shakespeare),William Shakespeare
A Midsummer Night's Dream for Kids (Shakespeare Can Be Fun!),Lois Burdett
A Midsummer Night's Dream The Graphic Novel: Original Text (Shakespeare Range),William Shakespeare
A Midsummer Night's Dream: Sixty-Minute Shakespeare Series,Cass Foster
A Midsummer Night's Dream: The Oxford Shakespeare,William Shakespeare
"A Million Guilty Pleasures: Million Dollar Duet, Book 2",C. L. Parker
A Mind at Peace,Ahmet Hamdi Tanpinar
A Mind Awake: An Anthology of C. S. Lewis,C. S. Lewis
A Modern Grammar for Biblical Hebrew Workbook,Duane A. Garrett
"A Modern Witch: A Modern Witch, Book 1",Debora Geary
A Modest Proposal and Other Satirical Works (Dover Thrift Editions),Jonathan Swift
A Moment in Time (Lone Star Brides) (Volume 2),Tracie Peterson
A Month in the Country (New York Review Books Classics),J.L. Carr
A Mother's Kisses: A Novel,Bruce Jay Friedman
A Naked Tree: Love Sonnets to C. S. Lewis and Other Poems,Joy Davidman
"A Narrative of a Revolutionary Soldier: Some Adventures, Dangers, and Sufferings of Joseph Plumb Martin (Signet Classics)",Joseph Plumb Martin
A Nasty Bit of Rough: A Novel,David Feherty
A Number,Caryl Churchill
A Pale View of Hills,Kazuo Ishiguro
A Perfect Crime,A Yi
A Perfect Life: A Novel,Danielle Steel
A Perfect Life: A Novel (Random House Large Print),Danielle Steel
A Philosophical Walking Tour with C.S. Lewis: Why It Did Not Include Rome,Stewart Goetz
A Place Where the Sea Remembers and Related Readings (Literature connections),Sandra Benitez
A Pledge of Silence,Flora J. Solomon
A Poet of the Invisible World: A Novel,Michael Golding
A Poetry Handbook,Mary Oliver
A Portrait of the Artist As a Young Man,James Joyce
A Portrait of the Artist as a Young Man (Penguin Classics),James Joyce
A Possibility of Violence: A Novel,D. A. Mishani
A Prayer for Owen Meany: A Novel,John Irving
A Private Revenge,Richard Woodman
A Promise Kept,Robin Lee Hatcher
A Question of Death: An Illustrated Phryne Fisher Anthology,Kerry Greenwood
"A Question of Mercy: A Play Based on the Essay by Richard Selzer (Rabe, David)",David Rabe
A Question of Proof,Joseph Amiel
A Quilter's Holiday: An Elm Creek Quilts Novel (The Elm Creek Quilts),Jennifer Chiaverini
A Rag Doll's Heart,Maggie Simmons
A Raisin in the Sun,Lorraine Hansberry
A Raisin In The Sun: And Related Readings,Lorraine Hansberry
A Raisin in the Sun: with Connections (HRW Library),Lorraine Hansberry
A Redbird Christmas: A Novel,Fannie Flagg
A Remarkable Kindness: A Novel,Diana Bletter
A Replacement Life: A Novel (P.S.),Boris Fishman
A Reunion of Ghosts: A Novel,Judith Claire Mitchell
A Rhetoric for Writing Teachers,Erika Lindemann
A River Runs Through It,Norman Maclean
"A River Runs Through It and Other Stories, Twenty-fifth Anniversary Edition",Norman Maclean
"A Roman Army Reader: Twenty-One Selections from Literary, Epigraphic, and Other Documents (Bc Latin Readers)",Dexter Hoyos
A Room of One's Own,Virginia Woolf
A Room of One's Own (Annotated),Virginia Woolf
"A Sailor of Austria: In Which, Without Really Intending to, Otto Prohaska Becomes Official War Hero No. 27 of the Habsburg Empire (The Otto Prohaska Novels)",John Biggins
A Sand County Almanac and Sketches Here and There,Aldo Leopold
A Sand County Almanac: And Sketches Here and There (Outdoor Essays & Reflections),Aldo Leopold
A Science Fiction Cookbook: And Guide to Edible Niceties,Nicole Lynn Roach
"A Scots Quair: Sunset Song, Cloud Howe, Grey Granite",Lewis Grassic Gibbon
"A Sea of Words, Third Edition: A Lexicon and Companion to the Complete Seafaring Tales of Patrick O'Brian",Dean King
A Second Daniel (In the Den of the English Lion) (Volume 1),Neal Roberts
A Second Helping: A Blessings Novel (Blessings Series),Beverly Jenkins
A Sentimental Journey: Sentimental Journey: Memoirs 1917-1922 (Russian Literature Series),Viktor Shklovsky
A Separate Peace,John Knowles
A Short History of Ancient Greece (I.B. Tauris Short Histories),P. J. Rhodes
A Short History of Indians in Canada: Stories,Thomas King
A Short History of Tractors in Ukrainian,Marina Lewycka
A Single Man: A Novel (FSG Classics),Christopher Isherwood
A Single Thread (Cobbled Court Quilts),Marie Bostwick
A Skeleton Key to Finnegans Wake: Unlocking James Joyce's Masterwork (The Collected Works of Joseph Campbell),Joseph Campbell
A Sky Without Eagles,Jack Donovan
A Slip of the Keyboard: Collected Nonfiction,Terry Pratchett
A Small Greek World: Networks in the Ancient Mediterranean (Greeks Overseas),Irad Malkin
A Small Story about the Sky,Alberto Ros
A Soft Place to Land: A Novel,Susan Rebecca White
A Soldier's Play (Dramabook),Charles Fuller
"A Solemn Pleasure: To Imagine, Witness, and Write (The Art of the Essay)",Melissa Pritchard
A Southern Girl: A Novel (Story River Books),John Warley
A Spear of Summer Grass,Deanna Raybourn
A Spool of Blue Thread: A novel,Anne Tyler
A Sportsman's Notebook (Everyman's Library),Ivan Turgenev
A Spot of Bother,Mark Haddon
"A Storm of Swords (A Song of Ice and Fire, Book 3)",George R. R. Martin
A Story as Sharp as a Knife: The Classical Haida Mythtellers and Their World (Masterworks of the Classical Haida Mythtellers),Robert Bringhurst
A Strangeness in My Mind: A novel,Orhan Pamuk
A Streetcar Named Desire.,Tennessee Williams
A Suitable Boy: A Novel,Vikram Seth
A Suitable Boy: A Novel (Modern Classics),Vikram Seth
A Tale of three Kings: A Study in Brokenness,Gene Edwards
A Taste for Chaos: The Art of Literary Improvisation,Randy Fertel
A Taste of Honey: Stories,Jabari Asim
A Texas Hill Country Christmas,William W. Johnstone
A Theatre of Envy,Rene Girard
A Thousand Mornings: Poems,Mary Oliver
A Thousand Pardons: A Novel,Jonathan Dee
A Thousand Splendid Suns,Khaled Hosseini
A Thread of Truth (Cobbled Court Quilts),Marie Bostwick
A Thread So Thin (Cobbled Court Quilts),Marie Bostwick
A Timeshare,Margaret Ross
A Town Called Valentine: A Valentine Valley Novel,Emma Cane
A Traveled First Lady: Writings of Louisa Catherine Adams,Louisa Catherine Adams
A Treacherous Paradise,Henning Mankell
"A Treasury of Irish Myth, Legend & Folklore (Fairy and Folk Tales of the Irish Peasantry / Cuchulain of Muirthemne)",Isabella Augusta Gregory
A Treasury of Jewish Folklore,Nathan Ausubel
A Tree Grows in Brooklyn (Modern Classics),Betty Smith
A Trip to the Stars: A Novel,Nicholas Christopher
"A True History of the Three Brave Indian Spies, John Cherry, Andrew and Adam Poe, Who Wiped Out Big Foot and His Two Brothers, Styled Sons of the Half King (Classic Reprint)",A. W. Poe
A Turn in the Road (A Blossom Street Novel),Debbie Macomber
A Vaquero of the Brush Country: The Life and Times of John D. Young,John D. Young
A View From the Bridge.,Arthur Miller
A View From the Pew: A Collection of Sketches and Monologues About Church Life- From the Potluck to the Bored Meeting,Martha Bolton
A Vintage Affair: A Novel (Random House Reader's Circle),Isabel Wolff
A Vision: The Revised 1937 Edition: The Collected Works of W.B. Yeats Volume XIV,William Butler Yeats
A Voice Full of Cities: The Collected Essays of Robert Kelly,Robert Kelly
A Voyage for Madmen,Peter Nichols
A Waka Anthology - Volume Two: Grasses of Remembrance (Parts A & B) (v. 2),Edwin A. Cranston
A Week in Winter,Maeve Binchy
A White Tea Bowl: 100 Haiku from 100 Years of Life,Mitsu Suzuki
A Wife's Betrayal,Miss KP
A Wild Swan: And Other Tales,Michael Cunningham
A Wilder Rose: A Novel,Susan Wittig Albert
A Window Opens: A Novel,Elisabeth Egan
A Winter Dream: A Novel,Richard Paul Evans
A Woman of Substance (Harte Family Saga),Barbara Taylor Bradford
A World Lit Only by Fire: The Medieval Mind and the Renaissance: Portrait of an Age,William Manchester
A Worthy Pursuit,Karen Witemeyer
"A Writer at War: A Soviet Journalist with the Red Army, 1941-1945",Vasily Grossman
A Writer's Diary,Fyodor Dostoevsky
A Year in the Life of William Shakespeare: 1599,James Shapiro
A Year on Ladybug Farm,Donna Ball
A Year with C. S. Lewis: Daily Readings from His Classic Works,C. S. Lewis
A Year with Hafiz: Daily Contemplations,Hafiz
A Year with Rilke: Daily Readings from the Best of Rainer Maria Rilke,Anita Barrows
A Year with Rumi: Daily Readings,Coleman Barks
A Yellow Raft in Blue Water: A Novel,Michael Dorris
A.C. Swinburne and the Singing Word,Yisrael Levin
A.D. 30: A Novel,Ted Dekker
A.D. 33: A Novel,Ted Dekker
Abandoned Secrets,Toni Larue
Abandoned: Three Short Stories,Jim Heskett
Abelard and Heloise: The Letters and Other Writings (Hackett Classics),Abelard
Abigail Adams: Letters: Library of America #275 (The Library of America),Abigail Adams
Abingdon's Speeches &  Recitations for Young Children,Abingdon Press
About Love and Other Stories (Oxford World's Classics),Anton Chekhov
Above the Waterfall: A Novel,Ron Rash
Abraham and Sarah: History's Most Fascinating Story of Faith and Love,J. SerVaas Williams
Abraham Lincoln (Classic Reprint),Wilbur Fisk Gordy
Abraham Lincoln: Great Speeches (Dover Thrift Editions),Abraham Lincoln
Abraham Lincoln: Vampire Hunter,Seth Grahame-Smith
Absurdistan: A Novel,Gary Shteyngart
Abundant Rain (Urban Christian),Vanessa Miller
Across a Hundred Mountains: A Novel,Reyna Grande
Across Five Aprils,Irene Hunt
Acting in Restoration Comedy (Applause Acting Series),Maria Aitken
Acting Scenes & Monologues For Kids!: Original Scenes and Monologues Combined Into One Very Special Book!,Bo Kane
Acting Scenes And Monologues For Young Teens: Original Scenes and Monologues Combined Into One Book,Bo Kane
Acting Up in Church: Humorous Sketches for Worship Services,M. K. Boyle
Acts of Betrayal (Urban Books),Tracie Loveless-Hill
"Acts of Intervention: Performance, Gay Culture, and AIDS (Unnatural Acts: Theorizing the Performative)",David Roman
"Ada, or Ardor: A Family Chronicle",Vladimir Nabokov
Adaptation and Appropriation (The New Critical Idiom),Julie Sanders
ADAPTED CLASSICS MACBETH SE 96C. (Globe Adapted Classics),GLOBE
Addicted to Drama,Pamala G Briscoe
Addiss: Haiga: Takebe Socho Pa,Stephen Addiss
Adios Hemingway,Leonardo Padura Fuentes
Admiral Hornblower in the West Indies (Hornblower Saga),C. S. Forester
Adolf Hitler and His Airship: An Alternate History: Adolf vs. the Canadians  Part 3 of the Hitler Chronicles,Victor Appleton
Adrienne Rich's Poetry and Prose (Norton Critical Editions),Adrienne Rich
Adulterio (Spanish Edition),Paulo Coelho
Adventures of Huckleberry Finn,Mark Twain
Adventures of Huckleberry Finn,Mark Twain
Adventures of Huckleberry Finn (Third Edition)  (Norton Critical Editions),Mark Twain
Adventures of Huckleberry Finn: and Related Readings (Literature Connections),MCDOUGAL LITTEL
Advice to Little Girls,Mark Twain
Aeneid (Hackett Classics),P. Vergilius Maro
Aeneid (Oxford World's Classics),Virgil
Aeschyli Persae (Latin Edition),Aeschylus
Aeschyli persae ad fidem manusciptorum,Aeschylus
Aeschyli Persae: Ad Fidem Manuscriptorum (1826) (Latin Edition),Aeschylus
Aeschyli Tragoediae Quae Supersunt: Persae,Aeschylus
"Aeschylus I: Oresteia: Agamemnon, The Libation Bearers, The Eumenides (The Complete Greek Tragedies) (Vol 1)",Aeschylus
"Aeschylus I: The Persians, The Seven Against Thebes, The Suppliant Maidens, Prometheus Bound (The Complete Greek Tragedies)",Aeschylus
Aeschylus II: The Oresteia (The Complete Greek Tragedies),Aeschylus
"Aeschylus, I, Persians. Seven against Thebes. Suppliants. Prometheus Bound (Loeb Classical Library)",Aeschylus
"Aeschylus, II, Oresteia: Agamemnon. Libation-Bearers. Eumenides (Loeb Classical Library)",Aeschylus
Aeschylus: Agamemnon (Cambridge Translations from Greek Drama),Aeschylus
Aesop's Fables (Word Cloud Classics),Aesop
Aesop's Fables: A Pop-Up Book of Classic Tales,Kees Moerbeek
Aesthetics: Lectures And Essays,Edward Bullough
"Affairytale, A Memoir",C.J. English
Affirming: Letters 1975-1997,Isaiah Berlin
Affrilachia: Poems by Frank X Walker,Frank X Walker
African Ceremonies: The Concise Edition,Carol Beckwith
"African Narratives of Orishas, Spirits and Other Deities - Stories from West Africa and the African Diaspora: A Journey Into the Realm of Deities, SPI",Alex Cuoco
"Afrodita: Cuentos, Recetas y Otros Afrodisiacos",Isabel Allende
After Abel and Other Stories,Michal Lemberger
"After Antiquity: Greek Language, Myth, and Metaphor (Myth and Poetics)",Margaret Alexiou
After Birth,Elisa Albert
After I Do: A Novel,Taylor Jenkins Reid
After Many a Summer Dies the Swan,Aldous Huxley
After the Circus: A Novel (The Margellos World Republic of Letters),Patrick Modiano
After the Parade: A Novel,Lori Ostlund
Again to Carthage: A Novel,John L. Parker Jr.
Against Interpretation: And Other Essays,Susan Sontag
Age of Arousal,Linda Griffiths
Age of Fools,William A Cook
Agnes of God,John Pielmeier
Ahmed the Philosopher: Thirty-Four Short Plays for Children and Everyone Else,Alain Badiou
Aias (Greek Tragedy in New Translations),Sophocles
Aischylos Agamemnon (1885),Aeschylus
Ajax: With Notes Critical and Explanatory,Sophocles
"Akiane: Her Life, Her Art, Her Poetry",Akiane Kramarik
Ak: The Years of Childhood,Wole Soyinka
Albion's Seed: Four British Folkways in America (America: a cultural history),David Hackett Fischer
Alburquerque: A Novel,Rudolfo Anaya
Alcestis,Euripides Euripides
Alcestis,"John Hampden Haydon, Amy M, Euripides, William Sheldon Hadley"
Alcestis,Richard (trans) EURIPIDES / ALDINGTON
Aleph (Espaol) (Spanish Edition),Paulo Coelho
Aleph (Vintage International),Paulo Coelho
Alex Cross's Trial,James Patterson
Alexandrian Summer,Yitzhak Gormezano Goren
Alibi: A Novel,Joseph Kanon
Alice I Have Been: A Novel (Random House Reader's Circle),Melanie Benjamin
Alice in Plunderland,Steve McCaffery
Alice in Tumblr-land: And Other Fairy Tales for a New Generation,Tim Manley
Alice in Wonderland (Third Edition)  (Norton Critical Editions),Lewis Carroll
Alice's Adventures in Wonderland,Lewis Carroll
Alice's Adventures in Wonderland & Through the Looking-Glass (Bantam Classics),Lewis Carroll
Alice's Adventures in Wonderland and Through the Looking-Glass (Penguin Classics),Lewis Carroll
Alice's Adventures in Wonderland and Through the Looking-Glass: 150th-Anniversary Edition (Penguin Classics Deluxe Edition),Lewis Carroll
Alice's Adventures in Wonderland Decoded: The Full Text of Lewis Carroll's Novel with its Many Hidden Meanings Revealed,David Day
"Alinor, la Reine aux deux couronnes (French Edition)",Eric Brown
All Art Is Propaganda,George Orwell
All Backs Were Turned (Rebel Lit),Marek Hlasko
All Fall Down: A Novel,Jennifer Weiner
All He Ever Desired (The Kowalskis),Shannon Stacey
All I Love and Know: A Novel,Judith Frank
"All in the Timing, Six One-Act Comedies - Acting Edition",David Ives
All in the Timing: Fourteen Plays,David Ives
All Involved: A Novel,Ryan Gattis
All My Friends Are Still Dead,Jory John
All My Puny Sorrows,Miriam Toews
All My Sons (Penguin Classics),Arthur Miller
All Other Nights: A Novel,Dara Horn
All Our Happy Days Are Stupid,Sheila Heti
All Our Names,Dinaw Mengestu
All Quiet on the Western Front,Erich Maria Remarque
All She Ever Wanted,Lynn Austin
All That Followed: A Novel,Gabriel Urza
All That Glitters,Thomas Tryon
All That Is: A Novel (Vintage International),James Salter
"All That You've Seen Here Is God: New Versions of Four Greek Tragedies Sophocles' Ajax, Philoctetes, Women of Trachis; Aeschylus' Prometheus Bound (Vintage Original)",Sophocles
All the Best: My Life in Letters and Other Writings,George H.W. Bush
All the Brave Fellows (Revolution at Sea Saga #5),James L. Nelson
All the Greek Verbs (Greek Language),N. Marinone
"All the Pretty Horses (The Border Trilogy, Book 1)",Cormac McCarthy
All the Rage: A Quest,Martin Moran
All the Single Ladies: A Novel,Dorothea Benton Frank
All the Stars in the Heavens: A Novel,Adriana Trigiani
"All The Wild That Remains: Edward Abbey, Wallace Stegner, and the American West",David Gessner
All the World,Liz Garton Scanlon
All Things New,Lynn Austin
All Year Long!: Funny Readers Theatre for Life's Special Times,Diana R. Jenkins
All's Well that Ends Well: The Oxford Shakespeare (Oxford World's Classics),William Shakespeare
Allen and Greenough's New Latin Grammar (Dover Language Guides),James B Greenough
Allusion and Intertext: Dynamics of Appropriation in Roman Poetry (Roman Literature and its Contexts),Stephen Hinds
Almanac of the Dead,Leslie Marmon Silko
Almost Famous Women: Stories,Megan Mayhew Bergman
"Almost, Maine",John Cariani
Alone with the Horrors: The Great Short Fiction of Ramsey Campbell 1961-1991,Ramsey Campbell
Already Home,Susan Mallery
Altar of Eden,James Rollins
Altneuland: The Old-New-Land,Theodor Herzl
Always Coca-Cola,Alexandra Chreiteh
Ama: A Story of the Atlantic Slave Trade,Manu Herbstein
Ambition and Survival: Becoming a Poet,Christian Wiman
America Deceived III: Department of Homeland Security Warning: Possession of this novel may result in a targeted drone strike,E.A. Blayre III
American Dervish: A Novel,Ayad Akhtar
American Elsewhere,Robert Jackson Bennett
American Hero,Bess Wohl
"American Indian Stories, Legends, and Other Writings (Penguin Classics)",Zitkala-Sa
American Indian Trickster Tales (Myths and Legends),Richard Erdoes
American Literature (EZ-101 Study Keys),Francis E. Skipp
American Primitive,Mary Oliver
American Psycho,Bret Easton Ellis
Americanah,Chimamanda Ngozi Adichie
Americanah,Chimamanda Ngozi Adichie
"Amerika: The Missing Person: A New Translation, Based on the Restored Text (The Schocken Kafka Library)",Franz Kafka
Amistad Mountain & Other Stories,Sigifredo Cavazos
Among the Gods (Chronicles of the Kings #5) (Volume 5),Lynn Austin
Among the Ten Thousand Things: A Novel,Julia Pierpont
Amoroso (Alfonzo) (Volume 16),S.W. Frank
Amulet,Roberto Bolao
Amuse Bouche (Russell Quant Mysteries),Anthony Bidulka
Amy Falls Down: A Novel,Jincy Willett
"Amy Lynn, The Lady Of Castle Dunn (Volume 3)",Jack July
An (Im)possible Life: Poesia y Testimonio in the Borderlands,Elvira Prieto
An Absent Mind,Eric Rill
An Anthropologist On Mars: Seven Paradoxical Tales,Oliver Sacks
An Apple a Day ...,Julian Gough
An Echo in the Bone: A Novel (Outlander),Diana Gabaldon
An Ecology of World Literature: From Antiquity to the Present Day,Alexander Beecroft
An Elephant in the Garden,Michael Morpurgo
An Elm Creek Quilts Sampler: The First Three Novels in the Popular Series (The Elm Creek Quilts),Jennifer Chiaverini
An English Ghost Story,Kim Newman
An Enticing Misconception: A Book of Prose and Poetry,Ruv Burns
An Equal Music: A Novel,Vikram Seth
An Ideal Husband (Dover Thrift Editions),Oscar Wilde
An Iliad,Lisa Peterson
An Iliad,Lisa Peterson
An Improbable Truth: The Paranormal Adventures of Sherlock Holmes,A.C. Thompson
An Inspector Calls (Philip Allan Literature Guide for Gcse),J. B. Priestley
An Introduction to Literature (16th Edition),Sylvan Barnet
An Introduction to Poetry (13th Edition),X. J. Kennedy
An Irish Country Christmas (Irish Country Books),Patrick Taylor
An Irish Country Courtship: A Novel (Irish Country Books),Patrick Taylor
An Irish Country Village (Irish Country Books),Patrick Taylor
An Irish Country Wedding: A Novel (Irish Country Books),Patrick Taylor
An Irish Doctor in Love and at Sea: An Irish Country Novel (Irish Country Books),Patrick Taylor
An Irish Doctor in Peace and at War: An Irish Country Novel (Irish Country Books),Patrick Taylor
An Obedient Father,Akhil Sharma
An Occult Physiology: Eight Lectures By Rudolf Steiner,Rudolf Steiner
An Octoroon,Branden Jacobs-jenkins
An Oresteia: Agamemnon by Aiskhylos; Elektra by Sophokles; Orestes by Euripides,Aeschylus
An Untamed Heart,Lauraine Snelling
An Untamed Land (Red River of the North #1),Lauraine Snelling
Ana of California: A Novel,Andi Teran
Anacalypsis - The Saitic Isis: Languages Nations and Religions (v. 1 & 2),Godfrey Higgins
Anadarko: A Kiowa Country Mystery,Tom Holm
"Analogies at War: Korea, Munich, Dien Bien Phu, and the Vietnam Decisions of 1965",Yuen Foong Khong
Anansi Boys,Neil Gaiman
Ancient Blood: A Navajo Nation Mystery (Volume 3),R. Allen Chappell
Ancient Egyptian Literature: Volume I: The Old and Middle Kingdoms,Miriam Lichtheim
Ancient Near Eastern Thought and the Old Testament: Introducing the Conceptual World of the Hebrew Bible,John H. Walton
Ancient Sorceries and Other Weird Stories (Penguin Classics),Algernon Blackwood
And Eternity (Book Seven of Incarnations of Immortality),Piers Anthony
And Still I Rise,Maya Angelou
And the Birds Rained Down,Jocelyne Saucier
And West Is West,Ron Childress
And Yet...: Essays,Christopher Hitchens
And You Call Yourself a Christian (Urban Books),E.N. Joy
Andersen's Fairy Tales,Hans Christian Andersen
Andrew's Brain: A Novel,E.L. Doctorow
Android Karenina (Quirk Classic),Leo Tolstoy
Anecdotal Shakespeare: A New Performance History,Paul Menzer
Angel of Storms (Millennium's Rule),Trudi Canavan
Angelmaker (Vintage Contemporaries),Nick Harkaway
Angelopolis: A Novel (Angelology Series),Danielle Trussoni
"Angels in America, Part One: Millennium Approaches",Tony Kushner
"Angels in America, Part Two: Perestroika",Tony Kushner
Angels Make Their Hope Here,Breena Clarke
Anglo-Saxon Community in J.R.R. Tolkien's the Lord of the Rings,Deborah a. Higgens
Animal,K'wan
Animal 2: The Omen,K'wan
Animal 3: Revelations,K'wan
Animal Farm,George Orwell
Animal Farm and Related Readings,George Orwell
Animal Farm: Centennial Edition,George Orwell
Animals,Emma Jane Unsworth
Anna Karenina (Modern Library Classics),Leo Tolstoy
Anna Karenina (Oxford World's Classics),Leo Tolstoy
"Anne of Avonlea, Large-Print Edition",Lucy Maud Montgomery
Anne of Avonlea: Anne of Green Gables Part 2,Lucy Maud Montgomery
Annie Freeman's Fabulous Traveling Funeral,Kris Radish
Annie John: A Novel,Jamaica Kincaid
Anno Dracula,Kim Newman
Anno Dracula: Johnny Alucard,Kim Newman
Anno Dracula: The Bloody Red Baron,Kim Newman
Anointed,Patricia Haley
Another,Yukito Ayatsuji
Another Country,James Baldwin
Another Day,David Levithan
Another Man's Moccasins: A Walt Longmire Mystery (A Longmire Mystery),Craig Johnson
Another Piece of My Heart,Jane Green
Another Woman's Daughter (U.S edition),Fiona Sussman
Anselm of Canterbury: The Major Works (Oxford World's Classics),St. Anselm
Anthills of the Savannah,Chinua Achebe
Anthology Of Classical Myth: Primary Sources in Translation,Stephen Trzaskoma
Anthology of Living Theater,Edwin Wilson
Anthroposophy in the Light of Goeth's Faust: Writings and Lectures from Mid-1880s to 1916: of Spiritual-Scientific Commentaries on Goethe's Faust (The Collected Works of Rudolf Steiner),Rudolf Steiner
Antigone,Jean Anouilh
Antigone,Sophocles
Antigone - Activity Pack,Sophocles
Antigone (Dover Thrift Editions),Sophocles
Antigone (Greek Tragedy in New Translations),Sophocles
Antigone (Hackett Classics),Sophocles
"Antigone (Methuen Drama, Methuen Student Edition)",Jean Anouilh
"Antigone, Oedipus the King, Electra (Oxford World's Classics)",Sophocles
Antigone: A New Translation,Sophocles
Antigone: In Plain and Simple English,Sophocles
Antigonick (New Directions Paperbook),Anne Carson
Antologa (Poesia) (Spanish Edition),Len De Greiff
"Anton Chekhov Early Short Stories, 1883-1888 (Modern Library)",Anton Chekhov
"Anton Chekhov Later Short Stories, 1888-1903 (Modern Library)",Anton Chekhov
Anton Chekhov: A Life,Donald Rayfield
Anton Chekhov's Selected Plays (Norton Critical Editions),Anton Chekhov
Anton Chekhov's Short Stories (Norton Critical Editions),Anton Chekhov
Antonio's Will,Yasmin Tirado-Chiodini
Antony & Cleopatra (No Fear Shakespeare),SparkNotes
Antony and Cleopatra (Folger Shakespeare Library),William Shakespeare
Antony and Cleopatra: Oxford School Shakespeare (Oxford School Shakespeare Series),William Shakespeare
Any Other Name: A Longmire Mystery,Craig Johnson
Anything Goes,John Barrowman
Apart at the Seams (Cobbled Court Quilts),Marie Bostwick
Apathy and Other Small Victories,Paul Neilan
Aphrodite: A Memoir of the Senses,Isabel Allende
Apocalypse Cow,Michael Logan
"Apollodorus:  The Library, Volume I: Books 1-3.9 (Loeb Classical Library no. 121)",Apollodorus
"Apollodorus: The Library, Vol. 2: Book 3.10-16 / Epitome (Loeb Classical Library, No. 122) (Volume II)",Apollodorus
"Applause First Folio of Shakespeare in Modern Type: Comedies, Histories & Tragedies (Applause First Folio Editions)",William Shakespeare
Application for Release from the Dream: Poems,Tony Hoagland
Approaching Hysteria,Mark S. Micale
Appropriate and Other Plays,Branden Jacobs-Jenkins
April Morning,Howard Fast
"Apuleius: Metamorphoses (The Golden Ass), Volume II, Books 7-11 (Loeb Classical Library No. 453)",Apuleius
Arabian Love Poems: Full Arabic and English Texts (Three Continents Press),Nizar Qabbani
Aratus: Phaenomena (Cambridge Classical Texts and Commentaries),Aratus
Arcadia: A Play,Tom Stoppard
Argonautika: The Voyage of Jason and the Argonauts,Mary Zimmerman
Arguably: Essays by Christopher Hitchens,Christopher Hitchens
Arguing about Literature: A Guide and Reader,John Schilb
"Aristophanes 1: Clouds, Wasps, Birds (Hackett Classics)",Aristophanes
Aristophanes Clouds,Aristophanes
Aristophanes: Lysistrata (Focus Classical Library),Aristophanes
Aristophanes' Clouds,Aristophanes
Aristotle on Comedy: Towards a Reconstruction of Poetics II,Richard Janko
Aristotle on Life and Death,R.A.H. King
Aristotle On Poetics,Aristotle
Ark: A Dane Maddock Adventure (Dane Maddock Adventures) (Volume 7),David Wood
Armageddon: A Novel of Berlin,Leon Uris
Around the World in 80 Days,Jules Verne
Around the World in Eighty Days (Dover Thrift Editions),Jules Verne
Arrow of God,Chinua Achebe
Arsenic and Old Lace - Acting Edition,Joseph Kesselring
Art and Myth in Ancient Greece (World of Art),Thomas H. Carpenter
Arthur Miller: Collected Plays 1944-1961 (Library of America),Arthur Miller
Arthur Rimbaud: Complete Works,Arthur Rimbaud
Articulos Desarticulados Y Cuentos No Contados (Spanish Edition),Leonardo Guzman
As A Driven Leaf,Milton Steinberg
"As Always, Julia: The Letters of Julia Child and Avis DeVoto",Joan Reardon
"As Bill Sees It (The A.A. Way of Life, Selected writings of AA's co-founder (LARGE PRINT)) (The A.A. Way of Life, Selected writings of AA's co-founder (LARGE PRINT))",Bill Willson
"As Consciousness Is Harnessed to Flesh: Journals and Notebooks, 1964-1980",Susan Sontag
As I Lay Dying: The Corrected Text,William Faulkner
As You Like It (Arden Shakespeare: Third Series),William Shakespeare
As You Like It (Dover Thrift Editions),William Shakespeare
As You Like It (Folger Shakespeare Library),William Shakespeare
As You Like It (No Fear Shakespeare),SparkNotes
As You Like It (Norton Critical Editions),William Shakespeare
As You Like It (Saddleback's Illustrated Classics),William Shakespeare
Asclepius: The Perfect Discourse of Hermes Trismegistus,Clement Salaman
Ash,Malinda Lo
Ashes and Seeds,Michelle Greenblatt
Asimov's Guide to Shakespeare: A Guide to Understanding and Enjoying the Works of Shakespeare,Isaac Asimov
Asking for Trouble: A Novel,Elizabeth Young
Asking the Right Questions (11th Edition),M. Neil Browne
Assassins (Violators: The Coalition) (Volume 2),Nancy Brooks
Associated Press Stylebook 2015 and Briefing on Media Law,The Associated Press
Astonish Me (Vintage Contemporaries),Maggie Shipstead
Astor Place Vintage: A Novel,Stephanie Lehmann
Astray,Emma Donoghue
Astrid and Veronika,Linda Olsson
At Blackwater Pond: Mary Oliver reads Mary Oliver,Mary Oliver
"At Grave's End: Night Huntress, Book 3",Jeaniene Frost
"At Home in Mitford (The Mitford Years, Book 1)",Jan Karon
At Home on Ladybug Farm,Donna Ball
At Home with Jane Austen,Kim Wilson
At Least Once: A Novel (While We Wait) (Volume 1),D. M. Cuffie
At Swim-Two-Birds (Irish Literature),Flann O'Brien
At the Water's Edge: A Novel,Sara Gruen
Atlantis and Lemuria,Rudolf Steiner
Atlas Shrugged,Ayn Rand
Atomic Robo: The Everything Explodes Collection,Brian Clevinger
"Atys, Tragedie En Musique, Ornee D'Entrees de Ballet, de Machines, de Changemens de Theatre (Litterature) (French Edition)",Philippe Quinault
Auden: Poems (Everyman's Library Pocket Poets),W. H. Auden
August Wilson Century Cycle,August Wilson
August: Osage County - Acting Edition,Tracy Letts
Augustus (New York Review Books Classics),John Williams
Aunt Julia and the Scriptwriter: A Novel,Mario Vargas Llosa
Auntie Mame - Acting Edition,Jerome Lawrence and Robert E. Lee
Austerlitz (Modern Library Paperbacks),W.G. Sebald
Autobiography of a Corpse (New York Review Books Classics),Sigizmund Krzhizhanovsky
"Autobiography of Mark Twain, Volume 2: The Complete and Authoritative Edition (Mark Twain Papers)",Mark Twain
"Autobiography of Mark Twain, Volume 3: The Complete and Authoritative Edition (Mark Twain Papers)",Mark Twain
"Autobiography of Mark Twain: The Complete and Authoritative Edition, Vol. 1",Mark Twain
Autobiography of Red,Anne Carson
Autumn Brides: A Year of Weddings Novella Collection,Kathryn Springer
Autumn: Aftermath (Autumn series 5),David Moody
Avenue of Mysteries,John Irving
Averno: Poems,Louise Glck
Awakening Osiris: The Egyptian Book of the Dead,Normandi Ellis
Away: A Novel,Amy Bloom
Aztec and Maya Myths (Legendary Past),Karl Taube
B,Sarah Kay
B-More Careful: A Novel,Shannon Holmes
"Baby, You're the Best (The Crystal Series)",Mary B. Morrison
Bacchae,Euripides
Bacchae,Euripides
Bacchae,Euripides
Bacchae (Dover Thrift Editions),Euripides
Bacchae (Focus Classical Library),Euripides
Bacchae and Other Plays: Iphigenia among the Taurians; Bacchae; Iphigenia at Aulis; Rhesus (Oxford World's Classics),Euripides
Back Channel,Stephen L. Carter
Back Roads to Far Towns: Basho's Oku-No-Hosomichi (Ecco Travels),Basho Matsuo
"Backpack Literature: An Introduction to Fiction, Poetry, Drama, and Writing (5th Edition)",X. J. Kennedy
Bad Apple (The Baddest Chick) Part 1,Nisa Santiago
Bad Bitch (Bitch Series),Joy Deja King
Bad Blood,Mary Monroe
Bad Boys Do (The Donovan Family),Victoria Dahl
Bad Jews,Joshua Harmon
Bad Men: A Thriller,John Connolly
Bad Romeo (The Starcrossed Series),Leisa Rayven
Badenheim 1939,Aharon Appelfeld
Baking Cakes in Kigali: A Novel,Gaile Parkin
Bakkhai (Greek Tragedy in New Translations),Euripides
Balls,Julian Tepper
Balm: A Novel,Dolen Perkins-Valdez
Balseros (Spanish Edition),Ernesto Ochoa
Balzac and the Little Chinese Seamstress: A Novel,Dai Sijie
Band of Sisters,Cathy Gohlke
Bang the Drum Slowly (Second Edition),Mark Harris
Banker,Dick Francis
Banquet for the Damned,Adam Nevill
Barbara the Slut and Other People,Lauren Holmes
Barefoot Heart: Stories of a Migrant Child,Elva Trevino Hart
Barefoot: A Novel,Elin Hilderbrand
Barely a Lady (The Drake's Rakes series),Eileen Dreyer
Barney's Version (Vintage International),Mordecai Richler
Barometer Rising,Hugh Maclennan
"Bartleby, The Scrivener A Story of Wall-Street",Herman Melville
Baseball Dads,Matthew S. Hiley
Basho and His Interpreters: Selected Hokku with Commentary,Basho Matsuo
Basho: The Complete Haiku,Matsuo Basho
Basho's Narrow Road: Spring and Autumn Passages (Rock Spring Collection of Japanese Literature),Matsuo Basho
Bastard Out of Carolina: A Novel,Dorothy Allison
Bastards of the Reagan Era (Stahlecker Selections),Reginald Dwayne Betts
Bathed in Blood (Rogue Angel),Alex Archer
Battle Cry,Leon Uris
Battleborn: Stories,Claire Vaye Watkins
Battlemage (Age of Darkness),Stephen Aryan
Be Careful What You Pray For: A Novel (The Reverend Curtis Black Series),Kimberla Lawson Roby
Be Careful What You Wish For (The Clifton Chronicles),Jeffrey Archer
"Be Still My Vampire Heart (Love at Stake, Book 3)",Kerrelyn Sparks
Beasts of No Nation: A Novel,Uzodinma Iweala
Beasts of No Nation: A Novel (P.S.),Uzodinma Iweala
Beatrix Potter the Complete Tales (Peter Rabbit),Beatrix Potter
Beatrix Potter's Gardening Life: The Plants and Places That Inspired the Classic Children's Tales,Marta McDowell
Beautiful Chaos,Robert M. Drake
Beautiful Chaos,Robert M. Drake
Beautiful Day: A Novel,Elin Hilderbrand
Beautiful Oblivion,Addison Moore
Beautiful Thing - Acting Edition,Jonathan Harvey
Bed of Roses: Bride Quartet,Nora Roberts
"Bedford Introduction to Literature: Reading, Thinking, Writing",Michael Meyer
Before The Muses: An Anthology Of Akkadian Literature,Benjamin R. Foster
Before We Set Sail,Chika Ezeanya
Behind the Beauty 3,YaYa Grant
Behold the Man (The Jerusalem Chronicles),Bodie and Brock Thoene
"Bell, Book and Candle: A Comedy in Three Acts",John van Druten
Bella Fortuna,Rosanna Chiofalo
Bella Poldark,Winston Graham
Belleville,Amy Herzog
Bellman & Black: A Novel,Diane Setterfield
Bellocq's Ophelia: Poems,Natasha Trethewey
Beloved,Toni Morrison
Below Zero (A Joe Pickett Novel),C. J. Box
"Ben-Hur: A Tale of the Christ, Complete and Unabridged",Lew Wallace
Bendigo Shafter: A Novel,Louis L'Amour
Beneath Still Waters (Rogue Angel),Alex Archer
Beneath the Dark Ice,Greig Beck
BENITA: prey for him,Virginia Tranel
Beowulf - Autotypes Of The Unique Cotton Manuscript Vitellius A XV In The British Museum,Julius Zupitza
Beowulf (Broadview Literary Texts) (Broadview Literary Texts Series),Roy Liuzza
Beowulf (Signet Classics),Anonymous
Beowulf: A Dual-Language Edition,Howell D. Chickering
Beowulf: A New Telling,Robert Nye
Beowulf: A New Translation,Seamus Heaney
Beowulf: A Prose Translation (Penguin Classics)paperback,Anonymous
Beowulf: A Translation and Commentary,J.R.R. Tolkien
Berlin Diary,William L. Shirer
Bertie's Guide to Life and Mothers (44 Scotland Street Series),Alexander McCall Smith
Beside a Burning Sea,John Shors
Best Food Writing 2015,Holly Hughes
Best Friends Forever,Kimberla Lawson Roby
"Best Ghost Stories of Algernon Blackwood (Dover Mystery, Detective, & Other Fiction)",Algernon Blackwood
Best Ghost Stories of J. S. LeFanu,J. Sheridan Le Fanu
Best Loved Poems of the American People,Hazel Felleman
Best Russian Short Stories,Various
Best Served Cold,Joe Abercrombie
Best Tales of the Yukon,Robert Service
Best-Loved Folktales of the World (The Anchor folktale library),Joanna Cole
"Bestiary: Being an English Version of the Bodleian Library, Oxford, MS Bodley 764",Richard Barber
Betrayal,Harold Pinter
Betrayal (Exposed Series),Naomi Chase
Betsey Brown: A Novel,Ntozake Shange
Better Off Without Him,Dee Ernst
Better Than Chocolate (Life in Icicle Falls),Sheila Roberts
Between Distant Modernities: Performing Exceptionality in Francoist Spain and the Jim Crow South,Brittany Powell Kennedy
Between Friends,Amos Oz
"Between Heaven and Texas (A Too Much, Texas Novel)",Marie Bostwick
"Between Levinas and Lacan: Self, Other, Ethics",Mari Ruti
"Between Parentheses: Essays, Articles and Speeches, 1998-2003",Roberto Bolano
Between Riverside and Crazy,Stephen Adly Guirgis
Between Sisters,Kristin Hannah
Between Sisters: A Novel (Random House Reader's Circle),Kristin Hannah
Between the Bridge and the River,Craig Ferguson
Between the World and Me,Ta-Nehisi Coates
Beyond Black: A Novel,Hilary Mantel
Beyond Bolao: The Global Latin American Novel (Literature Now),Hctor Hoyos
Beyond the Great Snow Mountains: Stories,Louis L'Amour
Big Cherry Holler: A Novel (Ballantine Reader's Circle),Adriana Trigiani
Big Game Fishing Journal,Speedy Publishing LLC
Big Girls Don't Cry,Brenda Novak
Big Sky Wedding,Linda Lael Miller
Big Stone Gap (Movie Tie-in Edition): A Novel,Adriana Trigiani
Billy Lynn's Long Halftime Walk,Ben Fountain
Biloxi Blues,Neil Simon
Binocular Vision: New & Selected Stories,Edith Pearlman
Birds and Other Plays (Oxford World's Classics),Aristophanes
Birds of Paradise Lost,Andrew Lam
Birthday Letters: Poems,Ted Hughes
Bitch A New Beginning (Bitch Series),Joy Deja King
Bitter Creek (The Montana Mysteries Featuring Gabriel Du Pr),Peter Bowen
Bitter Eden: A Novel,Tatamkhulu Afrika
Bittersweet Dreams (Forbidden),V.C. Andrews
Bittersweet: A Novel,Colleen McCullough
Black ButterFly,Robert M. Drake
Black Cat Journal: 160 Page Lined Journal/Notebook,Mahtava Journals
Black Culture and Black Consciousness: Afro-American Folk Thought from Slavery to Freedom,Lawrence W. Levine
Black Diamond,Brittani Williams
Black Diamond 2: Nicety,Brittani Williams
Black Gangster,Donald Goines
Black Girl Lost,Donald Goines
"Black Men, Obsolete, Single, Dangerous?: The Afrikan American Family in Transition",Haki R Madhubuti
Black Ops (Presidential Agent),W.E.B. Griffin
Black Scarface (Volume 1),Jimmy DaSaint
Black Scarface 3 The Wrath of Face,Jimmy DaSaint
Black Scarface II The Rise of an American Kingpin,Jimmy DaSaint
Black Scarface IV: Live A King...Die A Legend (Volume 4),Jimmy DaSaint
Black Swan Green,David Mitchell
Black Tuesday (Area 51: Time Patrol),Bob Mayer
Blackberry Days of Summer: A Novel (Zane Presents),Ruth P. Watson
Blake and Tradition,Kathleen Raine
Blake or The Huts of America,Martin R. Delany
Bleachers: A Novel,John Grisham
Bleeding Edge: A Novel,Thomas Pynchon
"Bless Me, Ultima",Rudolfo Anaya
Blessing the Boats: New and Selected Poems 1988-2000 (American Poets Continuum),Lucille Clifton
Blind Descent (Anna Pigeon),Nevada Barr
Blind Faith,Ben Elton
Blind Your Ponies,Stanley Gordon West
Blindsided (Sisterhood),Fern Michaels
Bliss,Olivier Choinire
Bliss House: A Novel,Laura Benedict
Bliss: A Novel,Shay Mitchell
"Blithe Spirit, Hay Fever, Private Lives: Three Plays",Noel Coward
Block Party 5k1: Diplomatic Immunity (Volume 1),Al-Saadiq Banks
Block Party 5k1: Diplomatic Immunity (Volume 2),Al-Saadiq Banks
Blockade Billy,Stephen King
Blonde Eskimo: A Novel,Kristen Hunt
"Blonde Hair, Blue Eyes",Karin Slaughter
Blood and Beauty: The Borgias; A Novel,Sarah Dunant
"Blood and Steel 2: The Wehrmacht Archive - Retreat to the Reich, September to December 1944",Donald E. Graves
Blood Girls (Nunatak Fiction),Meira Cook
Blood Harvest,Sharon Bolton
Blood Knot and Other Plays,Athol Fugard
Blood Meridian: Or the Evening Redness in the West,Cormac McCarthy
Blood of Elves,Andrzej Sapkowski
Blood of My Brother II: The Face Off,Zoe Woods
Blood on the church house steps,Avery Bond
Blood on the Forge (New York Review Books Classics),William Attaway
Blood Orchid (Holly Barker),Stuart Woods
Blood Red Tide (Deathlands),James Axler
"Blood Shadows: Blackthorn, Book 1",Lindsay J. Pryor
Bloodletting and Miraculous Cures: Stories,Vincent Lam
Bloodshed of the Mountain Man,William W. Johnstone
Bloodstream,Tess Gerritsen
Blow-Up: And Other Stories,Julio Cortazar
"Blowhard: A Steampunk Fairy Tale: The Clockwork Republic Series, Volume 1",Katina French
Blue at the Mizzen (Vol. Book 20)  (Aubrey/Maturin Novels),Patrick O'Brian
"Blue Gold: A Kurt Austin Adventure (A Novel from the NUMA Files, Book 2)",Paul Kemprecos
Blue Horses: Poems,Mary Oliver
Blue Iris: Poems and Essays,Mary Oliver
Blue Jeans and Coffee Beans,Joanne DeMaio
Blue Lines Up In Arms,James Craig Atchison
"Blue Shoes and Happiness (No. 1 Ladies Detective Agency, Book 7)",Alexander McCall Smith
Bluebeard: A Novel (Delta Fiction),Kurt Vonnegut
Bluets,Maggie Nelson
Bodega Dreams,Ernesto Quinonez
Body Awareness,Annie Baker
Body Surfing: A Novel,Anita Shreve
"Bolder Than Bus: Original Poems, Art, Photography, and Music from the Furthur 50th Anniversary Tour to the Undiscovered Planet",Jessica Charnley
Bone,Fae Myenne Ng
Bone Black: Memories of Girlhood,bell hooks
Bones of Home and Other Plays,Charlene A. Donaghy
Boo (Vintage Contemporaries),Neil Smith
"Book Lust To Go: Recommended Reading for Travelers, Vagabonds, and Dreamers",Nancy Pearl
"Book Lust: Recommended Reading for Every Mood, Moment, and Reason",Nancy Pearl
"Book of Haikus (Poets, Penguin)",Jack Kerouac
Book of Hours,Kevin Young
Book of the Hopi,Frank Waters
BOOK REVIEW: All the Light We Cannot See,J.T. Salrich
Books I've Read: Books I Want to Read,Annabel Fraser
"Books of Blood, Vols. 1-3",Clive Barker
"Books of Blood, Vols. 4-6 (v. 2)",Clive Barker
Bootycandy,Robert O'Hara
"Booze, Broads, and Blackjack: A Deadly Combination",Carl Nicita
Borderlands / La Frontera: The New Mestiza,Gloria Anzalda
Borges: Selected Poems,Jorge Luis Borges
Boricuas: Influential Puerto Rican Writings - An Anthology,Roberto Santiago
Born in Fire: Irish Born Trilogy,Nora Roberts
Boss Bitch (Bitch Series),Joy Deja King
Boss Divas,De'nesha Diamond
Boy Who Fell into a Book,Alan Ayckbourn
Boy with Thorn (Pitt Poetry Series),Rickey Laurentiis
"Boy, Snow, Bird: A Novel",Helen Oyeyemi
"Boy, Snow, Bird: A Novel",Helen Oyeyemi
Boy's Life,Robert McCammon
Boyhood: Scenes From Provincial Life,J. M. Coetzee
Boys in the Band,Mart Crowley
Bradbury Stories: 100 of His Most Celebrated Tales,Ray Bradbury
Braided Creek: A Conversation in Poetry,Jim Harrison
Bram Stoker - Dracula (Readers' Guides to Essential Criticism),William Hughes
Bratya Karamazovy - EEEE EE (Russian Edition),Fyodor Dostoevsky
Brave Enough,Cheryl Strayed
Brave New World and Brave New World Revisited,Aldous Huxley
Bread Alone: A Novel,Judith Ryan Hendricks
Breakdown on Bethlehem Street: A Christmas Play,Frank Ramirez
Breakfast at Tiffany's and Three Stories,Truman Capote
Breaking into Japanese Literature: Seven Modern Classics in Parallel Text,Giles Murray
"Breaking News (Godmothers, Book 5) (The Godmothers)",Fern Michaels
Bream Gives Me Hiccups,Jesse Eisenberg
Breath: A Novel,Tim Winton
"Brewer's Dictionary of Phrase and Fable, Seventeenth Edition",John Ayto
"Brian Friel: Plays 2: Dancing at Lughnasa, Fathers and Sons, Making History, Wonderful Tennessee and Molly Sweeney (Contemporary Classics (Faber & Faber)) (v. 2)",Brian Friel
Brick Shakespeare: Four Tragedies & Four Comedies,John McCann
Breathers: A Zombie's Lament,S.G. Browne
"Brick Shakespeare: The ComediesEEA Midsummer NightEEs Dream, The Tempest, Much Ado About Nothing, and The Taming of the Shrew",John McCann
Brickstone,Sim Ciarlo
Brida: A Novel (P.S.),Paulo Coelho
Bride of Pendorric,Victoria Holt
Bridge to Haven,Francine Rivers
Bridget Jones: Mad About the Boy,Helen Fielding
Bridget Jones: Mad About the Boy (Vintage Contemporaries),Helen Fielding
Bridget Jones: the Edge of Reason,Helen Fielding
Brief Interviews with Hideous Men,David Foster Wallace
Bright Dead Things: Poems,Ada Limn
"Bright Lights, Big City",Jay McInerney
Bright Lines: A Novel,Tanwi Nandini Islam
Brightest Heaven of Invention: A Christian Guide To Six Shakespeare Plays,Peter J. Leithart
"Bring Up the Bodies (Wolf Hall, Book 2)",Hilary Mantel
British Library Desk Diary 2016,British Library
British Literature of World War I,Angela K Smith
Broadway Babies Say Goodnight: Musicals Then and Now,Mark Steyn
Broadway Bound,Neil Simon
Broken Hierarchies: Poems 1952-2012,Geoffrey Hill
Brooklyn,Colm Toibin
Brooklyn Fictions: The Contemporary Urban Community in a Global Age (Bloomsbury Studies in the City),James Peacock
Brotherhood of Evil (The Family Jensen),William W. Johnstone
"Brown Girl, Brownstones",Paule Marshall
"Browsings: A Year of Reading, Collecting, and Living with Books",Michael Dirda
Brush Talks from Dream Brook,Shen Kuo
Buddenbrooks: The Decline of a Family,Thomas Mann
Buddhist Animal Wisdom Stories,Mark W. McGinnis
"Buddhist Tales of India, China, and Japan: Indian Section",Yoshiko Dykstra
Buffalo Bird Woman's Garden: Agriculture of the Hidatsa Indians (Borealis Books),Gilbert Wilson
Buffalo Trail: A Novel of the American West,Jeff Guinn
Building: Letters 1960-1975,Isaiah Berlin
Bulfinch's Mythology (Leather-bound Classics),Thomas Bulfinch
Bullets in the Washing Machine,Melissa Littles
Burger's Daughter,Nadine Gordimer
Burning Desire (Dark Kings),Donna Grant
Burning Down George Orwell's House,Andrew Ervin
Burro Genius: A Memoir,Victor Villasenor
Busker: Avazkhan-e Doregard (Persian Edition),Moniro Ravanipour
But Beautiful: A Book About Jazz,Geoff Dyer
"Butch Queens Up in Pumps: Gender, Performance, and Ballroom Culture in Detroit (Triangulations: Lesbian/Gay/Queer Theater/Drama/Performance)",Marlon  M. Bailey
Butcher's Crossing (New York Review Books Classics),John Williams
Butterball (Hesperus Classics),Guy de Maupassant
By Blood We Live,Glen Duncan
By Night in Chile,Roberto Bolao
By The Book: Stories and Pictures,Diane Schoemperlen
"By the Way, Meet Vera Stark (TCG Edition)",Lynn Nottage
Byron's Poetry and Prose (Norton Critical Edition),George Gordon Byron
C. S. Lewis at War: The Dramatic Story Behind Mere Christianity (Radio Theatre),C. S. Lewis
"C. S. Lewis Signature Classics: Mere Christianity, The Screwtape Letters, A Grief Observed, The Problem of Pain, Miracles, and The Great Divorce (Boxed Set)",C. S. Lewis
"C.S. Lewis: The Signature Classics Audio Collection: The Problem of Pain, The Screwtape Letters, The Great Divorce, Mere Christianity",C. S. Lewis
Caballo de Troya 1. Jerusaln (NE) (Caballo De Troya / Trojan Horse) (Spanish Edition),Juan Jos Bentez
Caffe Cino: The Birthplace of Off-Off-Broadway (Theater in the Americas),Wendell C. Stone
Calculating God,Robert J. Sawyer
Caleb,Charles Alverson
"Calico Joe by Grisham, John [Hardcover]",John.. Grisham
Calico Joe: A Novel,John Grisham
California's Wild Edge,Tom Killion
Call It Sleep: A Novel,Henry Roth
Calligraphy Lesson: The Collected Stories,Mikhail Shishkin
Calloustown,George Singleton
Calming The Anxiety Within (The Healing Journal Series),Kaitlyn Storm
Camera Lucida: Reflections on Photography,Roland Barthes
Can I Taste It?,David Weaver
Can such things be?: A Collection of Supernatural Fiction,Ambrose Bierce
Canadian Fiction: A Guide to Reading Interests (Genreflecting Advisory Series),Sharron Smith
Candace Reign (Zane Presents),Sharai Robbin
Candide,Voltaire
Candide (A Norton Critical Edition),Voltaire
Candide (Dover Thrift Editions),Voltaire
Candies: A Comedy Composite,Basil H. Johnston
Candy: A Novel of Love and Addiction,Luke Davies
Cane (New Edition),Jean Toomer
Cannibal (A Jack Sigler Thriller Book 7) (Volume 7),Jeremy Robinson
Canterbury Tales by Chaucer,Geoffrey Chaucer
Canyons: A Novel,Samuel Western
Cape Horn and Other Stories from the End of the World (Discoveries),Francisco Coloane
Capitalism: The Unknown Ideal,Ayn Rand
Capone Bloodline: A T-Bone Capone Adventure,Tom Belton
Captain Blood (Penguin Classics),Rafael Sabatini
Captains Courageous (Dramatized),Rudyard Kipling
Capturing the Moon: Classic and Modern Jewish Tales,Edward Feinstein
Cara Massimina: A Novel (Duckworth and the Italian Girls),Tim Parks
Caramelo,Sandra Cisneros
Caramelo: En Espanol (Spanish Edition),Sandra Cisneros
Carey's Trade,Gregory Ast
Caribbee: A Kydd Sea Adventure (Kydd Sea Adventures),Julian Stockwin
Cariboo Magi,Lucia Frangione
Carmilla,J. Sheridan LeFanu
Carmilla: A Critical Edition (Irish Studies),Joseph Le Fanu
Carol (Movie Tie-In),Patricia Highsmith
"Carolina Israelite: How Harry Golden Made Us Care about Jews, the South, and Civil Rights",Kimberly Marlowe Hartnett
Carribbean Discourse: Selected Essays (Caribbean and African Literature),Edouard Glissant
Carried Forward By Hope (# 6 in the Bregdan Chronicles Historical Fiction Romance Series) (Volume 6),Ginny Dye
"Carry On, Jeeves (A Jeeves and Bertie Novel)",P. G. Wodehouse
"Carrying Albert Home: The Somewhat True Story of A Man, His Wife, and Her Alligator",Homer Hickam
Cartel 3: The Last Chapter (The Cartel),Ashley and JaQuavis
Casca #01: Eternal Mercenary,Barry Sadler
Casting the Runes and Other Ghost Stories (Oxford World's Classics),M. R. James
Cat on a Hot Tin Roof,Tennessee Williams
Cat Out of Hell,Lynne Truss
Catalog of Unabashed Gratitude (Pitt Poetry Series),Ross Gay
Catch-22,Joseph Heller
Cathedral of the Black Madonna: The Druids and the Mysteries of Chartres,Jean Markale
Catherine de Valois:  A Play in Three Acts (The Legendary Women of World History) (Volume 2),Laurel A. Rockefeller
Cattle King for a Day (Western Short Stories Collection),L. Ron Hubbard
Catullus,John Ferguson
Catullus and the Poetics of Roman Manhood,David Wray
"Catullus, Tibullus, Pervigilium Veneris (Loeb Classical Library No. 6)",Gaius Valerius Catullus
Caucasia: A Novel,Danzy Senna
"Celebrate Christmas: Easy Dramas, Speeches, and Recitations for Children",Peggy Augustine
Celia's House,D.E. Stevenson
Censorship and the Limits of the Literary: A Global View,Nicole Moore
Centennial: A Novel,James A. Michener
Ceremonies in Dark Old Men: A Play,Lonne Elder III
Ceremony: (Penguin Classics Deluxe Edition),Leslie Marmon Silko
Certain Dark Things: Stories,M.J. Pack
Certain Prey,John Sandford
Changing the Subject: Art and Attention in the Internet Age,Sven Birkerts
Chango's Fire: A Novel,Ernesto Quinonez
Charlie and the Chocolate Factory: a Play,Roald Dahl
Charlotte Bront: A Fiery Heart,Claire Harman
Charlotte's Web (Trophy Newbery),E. B. White
"Chased: A Novella: Titan, Book 3.5",Cristin Harber
Chasing Utopia: A Hybrid,Nikki Giovanni
Chekhov: The Essential Plays (Modern Library Classics),Anton Chekhov
Chekhov's Three Sisters and Woolf's Orlando: Two Renderings for the Stage,Virginia Woolf
Chelsea Girls: A Novel,Eileen Myles
Chemical Theatre,Charles Nicholl
Chemistry,Steven S. Zumdahl
"Chemistry, 11th Edition",Raymond Chang
Cherry,Mary Karr
Chesapeake: A Novel,James A. Michener
Chester Creek Ravine: Haiku,Bart Sutter
"CHEW Omnivore Edition, Vol. 2",John Layman
Chicago Stories (Prairie State Books),James T. Farrell
Childe Harold's Pilgrimage,Lord Byron
Children of Paradise: A Novel (P.S.),Fred D'Aguiar
Children of the Days: A Calendar of Human History,Eduardo Galeano
Chilly Scenes of Winter,Ann Beattie
China Dolls: A Novel,Lisa See
China Men,Maxine Hong Kingston
China Rich Girlfriend: A Novel,Kevin Kwan
"Chinese Link: Beginning Chinese, Simplified Character Version, Level 1/Part 1 (2nd Edition)",Sue-mei Wu
"Chinese Link: Beginning Chinese, Simplified Character Version, Level 1/Part 2 (2nd Edition)",Sue-mei Wu
"Chinese Link: Beginning Chinese, Traditional Character Version, Level 1/Part 2 (2nd Edition)",Sue-mei Wu
Chinese Mythology: An Introduction,Anne M. Birrell
Chinglish (TCG Edition),David Henry Hwang
Chippewa Customs (Publications of the Minnesota Historical Society),Frances Densmore
Chiyo-ni: Woman Haiku Master,Patricia Donegan
Choices (Cole),Noah Gordon
Choir Boy,Tarell Alvin Mccraney
Choir Boy,Tarell Alvin McCraney
Choose Your Own Misery: The Office Adventure,Mike MacDonald
Christ the Lord: Out of Egypt: A Novel,Anne Rice
Christmas at Lilac Cottage: A perfect romance to curl up by the fire with (White Cliff Bay) (Volume 1),Holly Martin
Christmas at Thompson Hall: And Other Christmas Stories (Penguin Christmas Classics),Anthony Trollope
Christmas at Tiffany's: A Novel,Karen Swan
Christmas Bells: A Novel,Jennifer Chiaverini
Christmas Bliss: A Novel,Mary Kay Andrews
Christmas Gifts: A Children's Christmas Play,Valerie Howard
Christmas Lights,Valerie Howard
Christmas on Stage: An Anthology of Royalty-Free Christmas Plays for All Ages,Theodore O. Zapel
Christmas Program Builder No. 50,Paul M. Miller
Christmas Program Builder No. 64: Creative Resources for Program Directors (Lillenas Drama),Heidi Petak
Christopher Durang Volume I: 27 Short Plays,Christopher Durang
Christopher Marlowe: The Complete Plays,Christopher Marlowe
Chronicle of a Death Foretold,Gabriel Garca Mrquez
Chronicle of the Abbey of Bury St. Edmunds (Oxford World's Classics),Jocelin of Brakelond
Cicero and the Jurists,Jill Harries
Cicero: A Portrait (Bristol Classical Paperbacks),Elizabeth Rawson
Cicero: Catilinarians (Cambridge Greek and Latin Classics),Marcus Tullius Cicero
Cicero: On the Orator: Book 3. On Fate. Stoic Paradoxes. On the Divisions of Oratory: A. Rhetorical Treatises (Loeb Classical Library No. 349) (English and Latin Edition),Cicero
Cinnamon and Gunpowder: A Novel,Eli Brown
Cicero: Rhetorica ad Herennium (Loeb Classical Library No. 403) (English and Latin Edition),Cicero
Cien aos de soledad (Spanish Edition),Gabriel Garca Mrquez
Cinderella (Dramatized),Brothers Grimm
Cinderella Skeleton,Robert D. San Souci
Circle of Friends,Maeve Binchy
Circling the Sun: A Novel,Paula McLain
Cities of Salt,Abdelrahman Munif
Cities of the Plain: Border Trilogy (3),Cormac McCarthy
Citizen: An American Lyric,Claudia Rankine
City of Clowns,Daniel Alarcn
City of God,Gil Cuadros
City of God (Penguin Classics),Augustine of Hippo
City of Lost Dreams: A Novel,Magnus Flyte
City of Thieves: A Novel,David Benioff
City on Fire: A novel,Garth Risk Hallberg
City On Fire: A Novelette,Mandy De Sandra
Civil Disobedience and Other Essays (Dover Thrift Editions),Henry David Thoreau
Civil War Stories (Dover Thrift Editions),Ambrose Bierce
CivilWarLand in Bad Decline,George Saunders
Claire of the Sea Light (Vintage Contemporaries),Edwidge Danticat
Clara and Mr. Tiffany: A Novel,Susan Vreeland
Clarissa Pinkola Estes Live: Theatre of the Imagination,Clarissa Pinkola Ests
"Clases de literatura.  Berkeley, 1980 (Spanish Edition)",Julio Cortzar
Clash of Eagles: The Clash of Eagles Trilogy Book I,Alan Smale
Classic Crews: A Harry Crews Reader,Harry Crews
Classic Goosebumps #7: Be Careful What You Wish For,R.L. Stine
"Classic Myths to Read Aloud: The Great Stories of Greek and Roman Mythology, Specially Arranged for Children Five and Up by an Educational Expert",William F. Russell
Classical Greek Prose: A Basic Vocabulary,Malcolm Campbell
Classical Monologues For Men (Audition Speeches),Chrys Salt
Classical Mythology,Mark Morford
Classical Mythology: A Very Short Introduction,Helen Morales
Classical Tragedy - Greek and Roman: Eight Plays in Authoritative Modern Translations,Aeschylus
"Classics Reimagined, Edgar Allan Poe: Stories & Poems",Edgar Allan Poe
"Classics Reimagined, The Wonderful Wizard of Oz",L. Frank Baum
Classics: A Very Short Introduction,Mary Beard
Claudian and the Roman Epic Tradition,Catherine Ware
Cleaning Nabokov's House: A Novel,Leslie Daniels
Clear Light of Day,Anita Desai
Cleopatra's Shadows,Emily Holleman
CliffsComplete Macbeth,William Shakespeare
CliffsComplete Romeo and Juliet,William Shakespeare
CliffsComplete Shakespeare's Hamlet,William Shakespeare
CliffsNotes on Bradbury's Fahrenheit 451,Kristi Hiner
CliffsNotes on Dickens' A Tale of Two Cities (Cliffsnotes Literature Guides),Marie Kalil
CliffsNotes on Fitzgerald's The Great Gatsby (Cliffsnotes Literature Guides),Kate Maurer
CliffsNotes on Golding's Lord of the Flies (Cliffsnotes Literature),Maureen Kelly
CliffsNotes on Homer's Odyssey (Cliffsnotes Literature Guides),Stanley P Baldwin
CliffsNotes on Rand's Anthem (Cliffsnotes Literature Guides),Andrew Bernstein
CliffsNotes on Rand's Atlas Shrugged (Cliffsnotes Literature Guides),Andrew Bernstein
CliffsNotes on Salinger's The Catcher in the Rye (Cliffsnotes Literature Guides),Stanley P. Baldwin
CliffsNotes on Shakespeare's Hamlet (Cliffsnotes Literature Guides),Carla Lynn Stockton
CliffsNotes on Shakespeare's Julius Caesar (Cliffsnotes Literature Guides),James E Vickers
CliffsNotes on Shakespeare's Macbeth (Cliffsnotes Literature),Alex Went
CliffsNotes on Shakespeare's Romeo and Juliet (Cliffsnotes Literature),Annaliese F Connolly
CliffsNotes on Shelley's Frankenstein (Cliffsnotes Literature Guides),Jeff Coghill
Climbing Parnassus: A New Apologia for Greek and Latin,Tracy Lee Simmons
Clipboard Christmas Skits,Tom Spence
"Close Your Eyes, Hold Hands (Vintage Contemporaries)",Chris Bohjalian
Closing Time: The Sequel to Catch-22,Joseph Heller
"Clotel: Or, The President's Daughter: A Narrative of Slave Life in the United States (Bedford Cultural Editions Series)",William Wells Wells Brown
Cloud 9,Caryl Churchill
Cloud Atlas: A Novel (Modern Library),David Mitchell
Cloudstreet: A Novel,Tim Winton
Clybourne Park,Bruce Norris
Coastal Disturbances: Four Plays,Tina Howe
Cobra Trap (Modesty Blaise series),Peter O'Donnell
Coca Kola (The Baddest Chick) Part 2,Nisa Santiago
Cocaine and Champagne: Road To My Recovery,Sherrie Lueder
Coeur de Lion,Ariana Reines
Cold Comfort Farm,Stella Gibbons
Cold Comfort Farm (Penguin Classics Deluxe Edition),Stella Gibbons
Cold-Cocked: On Hockey,Lorna Jackson
Coldheart Canyon: A Hollywood Ghost Story,Clive Barker
Coleridge's Poetry and Prose (Norton Critical Editions),Samuel Taylor Coleridge
"Colic Solved: The Essential Guide to Infant Reflux and the Care of Your Crying, Difficult-to- Soothe Baby",Bryan Vartabedian
Collateral: A Novel,Ellen Hopkins
Collected Fictions,Jorge Luis Borges
Collected Ghost Stories (Oxford World's Classics),M. R. James
Collected Haiku of Yosa Buson,Yosa Buson
Collected Poems,Jack Gilbert
Collected Poems,Philip Larkin
Collected Poems,Robert Hayden
Collected Short Stories: of Percival Christopher Wren (Volume 2),P. C. Wren
Collected Shorter Fiction: Volume 1 (Everyman's Library),Leo Tolstoy
Collected Stories,Gabriel Garcia Marquez
"College Essays That Made a Difference, 6th Edition (College Admissions Guides)",Princeton Review
Color My Fro: A Natural Hair Coloring Book for Big Hair Lovers of All Ages,Crystal Swain-Bates
"Comanche Moon (Lonesome Dove Story, Book 2)",Larry McMurtry
"Combat Ops (Tom Clancy's Ghost Recon, Book 2)",David Michaels
Combined and Uneven Development: Towards a New Theory of World-Literature (Postcolonialism Across the Disciplines LUP),Sharae Deckard
"Come On, Rain",Karen Hesse
Come Rain or Come Shine (A Mitford Novel),Jan Karon
Comedia & Drama (Dionisios) (Volume 1) (Spanish Edition),Carmen Resino
Comedy Scenes for Student Actors: Short Sketches for Young Performers,Laurie Allen
Cometh the Hour (The Clifton Chronicles),Jeffrey Archer
Comfort: A Novel of the Reverse Underground Railroad,H. A. Maxson
Coming to Rosemont,Barbara Hinske
Commodore Hornblower (Hornblower Saga),C. S. Forester
"Common Liar: Essay on ""Antony and Cleopatra"" (Study in English)",Janet Adelman
Como agua para chocolate (Spanish Edition),Laura Esquivel
"Compact Literature: Reading, Reacting, Writing",Laurie G. Kirszner
Comparative Literature 108: Myths and Mythologies Package for Pennsylvania State University,Sidney Aboul-Hosn
Comparative Religion For Dummies,William P. Lazarus
"Compendium of Roman History / Res Gestae Divi Augusti (Loeb Classical Library, No. 152)",Velleius Paterculus
Complete Greek Tragedies Euripides,Euripedes
Complete Greek Tragedies: Aeschylus I,trans Aeschylus / Richmond Lattimore
"Complete Harley 2253 Manuscript, Volume 1 (Middle English Texts)",Susanna Fein
Complete Letters (Oxford World's Classics),Pliny the Younger
Complete Plays of Aristophanes (Bantam Classics),Aristophanes
Complete Poems and Selected Letters of John Keats (Modern Library Classics),John Keats
Complete Poems and Songs of Robert Burns,Robert Burns
Complete Poems of Whitman (Wordsworth Poetry) (Wordsworth Poetry Library),Walt Whitman
"Complete Poems, 1904-1962 (Liveright Classics)",E. E. Cummings
Complete Sonnets and Poems: The Oxford Shakespeare The Complete Sonnets and Poems (Oxford World's Classics),William Shakespeare
Complete Stories (Penguin Classics),Dorothy Parker
Complete Stories and Poems of Edgar Allan Poe,Edgar Allan Poe
Complete Works of Oscar Wilde (Collins Classics),Oscar Wilde
Complete Works of William Shakespeare (Leather-bound Classics),William Shakespeare
Comprehensive Chess Endings Volume 4 Pawn Endings,Yuri Averbakh
Concepts In Solids: Lectures On The Theory Of Solids,Philip Warren Anderson
Concerning the Book that is the Body of the Beloved,Gregory Orr
Concrete Situations (Situations Series) (Volume 1),Crystal Darks
Conde de Montecristo (Coleccion los Inmortales) (Spanish Edition),Alejandro Dumas
Conduit: [A Collection of Poems and Short Stories by Jon Goode],Jon Goode
Confession of the Lioness: A Novel,Mia Couto
Confessions of a Crap Artist,Philip K. Dick
Confessions of a First Lady,Denora M Boone
Confessions of a First Lady 2,Denora M Boone
Confessions of a Mask,Yukio Mishima
Confessions of a Preachers Wife (Urban Christian),Mikasenoja
Confessions of a Wild Child (Lucky: the Early Years),Jackie Collins
Conflict Resolution for Holy Beings: Poems,Joy Harjo
Connemara: Listening to the Wind,Tim Robinson
Conqueror: A Novel of Kublai Khan (The Khan Dynasty),Conn Iggulden
Consequences Of A SideChick: SideChicks,Vladimir Dubois
Consider the Lobster and Other Essays,David Foster Wallace
Consorts of the Caliphs: Women and the Court of Baghdad (Library of Arabic Literature),Ibn al-Sai
Constantinople and the West in Medieval French Literature: Renewal and Utopia (Gallica),Rima Devereaux
Constellation Myths: with Aratus's Phaenomena (Oxford World's Classics),Eratosthenes
Constellations: A Play,Nick Payne
Consumed by Fire (The Fire Series),Anne Stuart
Contact Harvest (Halo),Joseph Staten
Contemporary Chicana Literature: (Re)Writing the Maternal Script,Cristina Herrera
Contested Will: Who Wrote Shakespeare?,James Shapiro
"Continuum: New And Selected Poems, Revised Edition",Mari Evans
Contrition,Maura Weiler
Copenhagen,Michael Frayn
Copper and Stone: stories,Bethany Snyder
Copper Sun,Sharon M. Draper
Coram Boy (Nick Hern Books),Jamila Gavin
Corduroy Mansions (Corduroy Mansions Series),Alexander McCall Smith
Coriolanus (The New Cambridge Shakespeare),William Shakespeare
Coriolanus: Oxford School Shakespeare (Oxford School Shakespeare Series),William Shakespeare
Coronado's Children: Tales of Lost Mines and Buried Treasures of the Southwest (Barker Texas History Center Series),J. Frank Dobie
Corregidora,Gayl Jones
Corrupt City (Urban Books),Tra Verdejo
Corsair (The Oregon Files),Clive Cussler
Cougar Club,Dark Chocolate
Could You Ever Live Without?,David Jones
"Count Magnus and Other Ghost Stories (The Complete Ghost Stories of M. R. James, Vol. 1)",M. R. James
Count the Waves: Poems,Sandra Beasley
"Countdown City: The Last Policeman, Book 2",Ben H. Winters
Countdown: M Day,Tom Kratman
Counternarratives,John Keene
Courage: The Backbone of Leadership,Gus Lee
Coven Thirteen Motorcycle Club: Volume One: Phoenix Fire (Volume 1),C Leigh Addison
Covenant of War (Lion of War Series),Cliff Graham
Coyote Wisdom: The Power of Story in Healing,Lewis Mehl-Madrona
Cracking India: A Novel,Bapsi Sidhwa
Cracks in Her Foundation,Shani Mixon
Cracks In The Sidewalk,Bette Lee Crosby
Cranford (Hardcover Classics),Elizabeth Gaskell
Crash Course: Essays From Where Writing and Life Collide,Robin Black
Crave Radiance: New and Selected Poems 1990-2010,Elizabeth Alexander
Crazy Horse's Girlfriend,Erika T. Wurth
Crazy In Luv 2: Blood Don't Make You Family,La'Tonya West
Crazy Rich Asians,Kevin Kwan
Creating a Scene in Corinth: A Simulation,Reta Halteman Finger
Creation Myths,Marie-Louise Von Franz
Creatures of a Day: And Other Tales of Psychotherapy,Irvin D. Yalom
"Cries for Help, Various: Stories",Padgett Powell
Crime and Punishment,Fyodor Dostoyevsky
Crime and Punishment: Pevear & Volokhonsky Translation (Vintage Classics),Fyodor Dostoevsky
Crimes of the Heart.,Beth Henley
Critique of Pure Reason (Penguin Classics),Immanuel Kant
Cromwell's Place in History: Founded on Six Lectures Delivered in the University of Oxford (Classic Reprint),Samuel Rawson Gardiner
Crooked,Catherine Trieschmann
Crooked Heart: A Novel,Lissa Evans
Crossing Delancey: A Romantic Comedy,Susan Sandler
Crossing to Safety (Modern Library Classics),Wallace Earle Stegner
Crossings: Nietzsche and the Space of Tragedy,John Sallis
Crossroads (Urban Books),Skyy
Crow: From the Life and Songs of the Crow,Ted Hughes
Crowned Heads,Thomas Tryon
"Crowned: Becoming the Woman of my Dreams: The Missing Things Were Goddess Wings: Poems, Prayers, and Love Letters",Sherry Sharp
Crowning Glory (Urban Christian),Pat Simmons
Crumbs from the Table of Joy and Other Plays,Lynn Nottage
Crusade (Destroyermen),Taylor Anderson
Crush (Yale Series of Younger Poets),Richard Siken
"Cry, the Beloved Country",Alan Paton
Cryptos,James R Wylder
Crnica de una muerte anunciada (Spanish Edition),Gabriel Garca Mrquez
Cuatro dias de enero (Best Seller (Debolsillo)) (Spanish Edition),Jordi Sierra
Cuatro ruedas compartidas (Spanish Edition),Mauro Hernandez
Cuentos de Amor de Locura y de Muerte,Horacio Quiroga
Cuentos de Amor de Locura y de Muerte (Spanish Edition),Horacio Quiroga
Cuentos De La Alhambra (1888) (Spanish Edition),Washington Irving
Cultural Amnesia: Necessary Memories from History and the Arts,Clive James
Cultural Intelligence: A Guide to Working with People from Other Cultures,Brooks Peterson
Cupid and Psyche: An Adaptation from The Golden Ass of Apuleius (Latin Edition),Apuleius
Cure for the Common Breakup (Black Dog Bay Novel),Beth Kendrick
Curiosity,Alberto Manguel
Curious Lives: Adventures from the Ferret Chronicles,Richard Bach
Cutting for Stone,Abraham Verghese
Cymbeline (The New Cambridge Shakespeare),William Shakespeare
D'aulaire's Book of Greek Myths,Ingri d'Aulaire
Damaged Goods,Nikki Urban
Damascus Nights,Rafik Schami
Damned,Chuck Palahniuk
Damsels in Distress (Urban Books),Nikita Lynnette Nichols
Dance Hall of the Dead,Tony Hillerman
Dance Me to the End of Love (Art & Poetry),Leonard Cohen
Dancer from the Dance: A Novel,Andrew Holleran
Dancer: A Novel (Picador Modern Classics),Colum McCann
Dancing at Lughnasa,Brian Friel
"Dancing at the Edge of the World: Thoughts on Words, Women, Places",Ursula K. Le Guin
Dancing at the Harvest Moon,K.C. McKinnon
Dancing Dogs: Stories,Jon Katz
Dancing with Butterflies: A Novel,Reyna Grande
Dandelion Through the Crack,Kiyo Sato
Dangerous to Go Alone!: an anthology of gamer poetry,CB Droege
Dangerous Work: Diary of an Arctic Adventure,Arthur Conan Doyle
"Dangerously In Love: ""Blame It on the Streets"" (Volume 1)",Aletta H.
Daniel X: Watch the Skies,James Patterson
"Dante, Poet of the Desert: History and Allegory in the DIVINE COMEDY",Giuseppe Mazzotta
"Dark Celebration: A Carpathian Reunion (The Carpathians (Dark) Series, Book 14)",Christine Feehan
Dark Chaos (# 4 in the Bregdan Chronicles Historical Fiction Romance Series) (Volume 4),Ginny Dye
Dark Infidelity,Shawn Starr
Dark Night of the Soul (Dover Thrift Editions),St. John of the Cross
Dark Sparkler,Amber Tamblyn
Dark Sweet: A Collection of Poetry,Keishi Ando
Dark Watch (The Oregon Files),Clive Cussler
Darkest Flame (Dark Kings),Donna Grant
Das Zeichen der Vier: Ein Sherlock Holmes Roman (German Edition),Arthur Conan Doyle
Dashing Through the Snow - Acting Edition,"Nicholas Hope, and Jamie Wooten Jessie Jones"
Dashing Through the Snow: A Christmas Novel (Random House Large Print),Debbie Macomber
Daughter of Fortune: A Novel,Isabel Allende
Daughters of Copper Woman,Anne Cameron
David Copperfield (Penguin Classics),Charles Dickens
"David Foster Wallace's Infinite Jest: A Reader's Guide, 2nd Edition",Stephen J. Burn
Dawn,Elie Wiesel
Day After Night: A Novel,Anita Diamant
Day: A Novel,Elie Wiesel
Days Of Poetry: My writing,Genna Beth Strachan
De Nerval: Selected Writings (Penguin Classics),Gerard de Nerval
De Profundis,Oscar Wilde
De Profundis and Other Prison Writings (Penguin Classics),Oscar Wilde
De Shootinest Gent'man & Other Tales,Nash Buckingham
Dead Cert,Dick Francis
Dead Man's Cell Phone (TCG Edition),Sarah Ruhl
Dead Man's Hand: An Anthology of the Weird West,John Joseph Adams
Dead Man's Walk (Lonesome Dove),Larry McMurtry
Dead Men's Boots,Mike Carey
Dead Shot: A Sniper Novel,Jack Coughlin
Dead Six,Larry Correia
Dead Solid Perfect,Dan Jenkins
Deadlight Hall: A haunted house mystery (A Nell West and Michael Flint Haunted House Story),Sarah Rayne
Deadline,Sandra Brown
Deadly Deals (Sisterhood),Fern Michaels
Deadtown Abbey: An Undead Homage,Sean Hoade
Deaf Sentence: A Novel,David Lodge
Deal Breaker: The First Myron Bolitar Novel,Harlan Coben
Dear and Glorious Physician: A Novel about Saint Luke,Taylor Caldwell
Dear Emily,Fern Michaels
Dear Miss Breed: True Stories of the Japanese American Incarceration During World War II and a Librarian Who Made a Difference,Joanne Oppenheim
"Dear Santa: Children's Christmas Letters and Wish Lists, 1870 - 1920",Chronicle Books
Death and Taxes: Hydriotaphia and Other Plays,Tony Kushner
Death and the King's Horseman: A Play,Wole Soyinka
"Death and the King's Horseman: Authoritative Text, Backgrounds and Contexts, Criticism, Norton",Wole Soyinka
Death and the Maiden,Ariel Dorfman
Death at Tammany Hall (A Gilded Age Mystery),Charles O'Brien
Death Before Compline: Short Stories,Sharan Newman
Death Before Wicket: A Phryne Fisher Mystery,Kerry Greenwood
Death by Facebook,Everett Peacock
Death by Water,Kenzaburo Oe
Death Comes for the Archbishop (Vintage Classics),Willa Cather
Death Defying Acts,Woody Allen
Death du Jour (Temperance Brennan Novels),Kathy Reichs
Death in the Afternoon,Ernest Hemingway
Death in the Andes: A Novel,Mario Vargas Llosa
Death in Venice (Dover Thrift Editions),Thomas Mann
Death of a Salesman (Penguin Plays),Arthur Miller
Death of Kings (Saxon Tales),Bernard Cornwell
Death or Liberty: African Americans and Revolutionary America,Douglas R. Egerton
Death Rides the River: A Joshua Miller Adventure (Joshua Miller Series) (Volume 2),Wayne Lincourt
Death Traps: The Survival of an American Armored Division in World War II,Belton Y. Cooper
Debbie Doesn't Do It Anymore (Vintage Crime/Black Lizard),Walter Mosley
December (Seagull Books - The German List),Alexander Kluge
Decolonising the Mind (Studies in African Literature),Ngugi Wa Thiong'O
Decolonising the Mind: The Politics of Language in African Literature,Ngugi wa Thiong'o
"Decreation: Poetry, Essays, Opera",Anne Carson
Deep Down True: A Novel,Juliette Fay
Defence Speeches (Oxford World's Classics),Cicero
Del amor y otros demonios (Spanish Edition),Gabriel Garca Mrquez
Delicious!: A Novel,Ruth Reichl
Deliverance (Modern Library 100 Best Novels),James Dickey
"Demelza: A Novel of Cornwall, 1788-1790 (Poldark)",Winston Graham
"Demons (Everyman's Library, 182)",Fyodor Dostoevsky
Demons (Penguin Classics),Fyodor Dostoevsky
Demons: A Novel in Three Parts (Vintage Classics),Fyodor Dostoevsky
Der Kleine Prinz (German),Antoine de Saint-Exupry
Der Lehnsmann und das Hexenweib (German Edition),Annika Stinner
Der Traum ein Leben: Dramatisches Mrchen in vier Aufzgen (German Edition),Franz Grillparzer
Der Wildtdter (TREDITION CLASSICS) (German Edition),James Fenimore Cooper
Derailed,Dave Jackson
Descent to the Goddess: A Way of Initiation for Women,Sylvia Brinton Perera
Desdemona (Oberon Modern Plays),Toni Morrison
"Desert Sun, Red Blood",E. W. Farnsworth
"Desire and Anxiety: Circulations of Sexuality in Shakespearean Drama (Gender, Culture, Difference)",Valerie Traub
Desire and the Female Therapist: Engendered Gazes in Psychotherapy and Art Therapy,Joy Schaverien
"Desolation Island  (The Aubrey/Maturin Novels, Book 5)",Patrick O'Brian
Desperately Seeking Exclusivity (Volume 1),Christopher Markland
Detroit '67,Dominique Morisseau
Developing Minds: An American Ghost Story,Jonathan LaPoma
Deviants,Peter Kline
Devices and Desires (Engineer Trilogy),K. J. Parker
Devil Knows: A Tale of Murder and Madness in America's First Century,David Joseph Kolb
Devil on the Cross (Heinemann African Writers Series),Ngugi wa Thiong'o
"Devil's Gate (Numa Files, Book 9)",Clive Cussler
Devils (Oxford World's Classics),Fyodor Dostoevsky
Dewdrops on a Lotus Leaf: Zen Poems of Ryokan,Ryokan
Dia's Story Cloth: The Hmong People's Journey of Freedom,Dia Cha
Diamond Head: A Novel,Cecily Wong
Diaries 1969-1979: The Python Years (Michael Palin Diaries),Michael Palin
"Diaries: Diary and Autobiographical Writings of Louisa Catherine Adams, Volumes 1 and 2: 1778-1849 (Adams Papers)",Louisa Catherine Adams
Diary (The Margellos World Republic of Letters),Witold Gombrowicz
Diary of a Mad Bride,Laura Wolf
Diary of a Mad Diva,Joan Rivers
Diary Of An Oxygen Thief,Anonymous
"Diasporic Dis(locations): Indo-Caribbean Women Writers Negotiate the ""Kala Pani""",Brinda J. Mehta
Dick Francis's Damage,Felix Francis
Dick Francis's Gamble,Felix Francis
Dickens at Christmas (Vintage Classics),Charles Dickens
Dickinson: Poems (Everyman's Library Pocket Poets),Emily Dickinson
Dickinson's Misery: A Theory of Lyric Reading,Virginia Jackson
Dictee,Theresa Hak Kyung Cha
Did You Ever Have A Family,Bill Clegg
Dien Cai Dau (Wesleyan Poetry Series),Yusef Komunyakaa
Different Seasons (Signet),Stephen King
Digest (Stahlecker Selections),Gregory Pardlo
Digging into Literature,Joanna Wolfe
Digital Mammals,Luiz Mauricio Azevedo
Dilemma of a Ghost and Anowa,Ama Ata Aidoo
Dime quin soy (Spanish Edition),Julia Navarro
Dinner with Buddha,Roland Merullo
Din Bahane': The Navajo Creation Story,Paul G. Zolbrod
"Dionysius of Halicarnassus: Roman Antiquities, Volume VI. Books 9.25-10 (Loeb Classical Library No. 378)",Dionysius of Halicarnassus
Directed by Desire: The Collected Poems of June Jordan,June Jordan
Dirty Divorce part 4 (The Dirty Divorce) (Volume 4),Miss KP
"Dirty Little Secrets: A J.J. Graves Mystery, Book 1",Liliana Hart
Dirty Money (Urban Books),Ashley and JaQuavis
Dirty Pretty Things,Michael Faudet
Dirty Rush,Taylor Bell
"Disappearing Acts: Spectacles of Gender and Nationalism in Argentina's ""Dirty War""",Diana Taylor
Disappearing Man,Doug Peterson
"Disarming: Reign of Blood, Book 2",Alexia Purdy
Disco for the Departed (A Dr. Siri Paiboun Mystery),Colin Cotterill
Disgraced: A Play,Ayad Akhtar
Disgruntled: A Novel,Asali Solomon
Disney Fairies Storybook Collection Special Edition,Disney Book Group
"Dispara, yo ya estoy muerto (Spanish Edition)",Julia Navarro
Distant Neighbors: The Selected Letters of Wendell Berry and Gary Snyder,Gary Snyder
Divine Misdemeanors: A Novel (Merry Gentry),Laurell K. Hamilton
Divine Secrets of the Ya-Ya Sisterhood: A Novel (The Ya-Ya Series),Rebecca Wells
Diving into the Wreck: Poems 1971-1972,Adrienne Rich
Divinity School,Alicia Jo Rabins
Divisadero,Michael Ondaatje
Divorce Turkish Style (Kati Hirschel Murder Mystery),Esmahan Aykol
Doctor Thorne (Oxford World's Classics),Anthony Trollope
Doctor Who: City of Death,Douglas Adams
Doctor Zhivago,Boris Pasternak
Doctor Zhivago (Vintage International),Boris Pasternak
Doctors: A Novel,Erich Segal
Dog Sees God: Confessions of a Teenage Blockhead - Acting Edition,Bert V. Royal
Dog Songs: Poems,Mary Oliver
Dollbaby: A Novel,Laura Lane McNeal
Dollhouse: A Novel,Kim Kardashian
Dominoes,Susan Emshwiller
Don Juan Tenorio (Clasicos de la literatura series),Jose Zorrilla
Don Quijote de la Mancha (Spanish Edition),Miguel Cervantes
Don Quijote de la Mancha (Spanish Edition),Miguel de Cervantes
"Don Quijote: Legacy Edition (Cervantes & Co.) (Spanish Edition) (European Masterpieces, Cervantes & Co. Spanish Classics)",Miguel de Cervantes Saavedra
Don Quixote,Miguel de Cervantes
Don Quixote of La Mancha (Restless Classics),Miguel de Cervantes
Don't Ever Get Old (Buck Schatz Series),Daniel Friedman
"Don't Jump: Sex, Drugs, Rock 'N Roll... And My Fucking Mother",Vicki Abelson
Don't Let Me Go,Catherine Ryan Hyde
Don't Make Me Wait (Urban Books),Shana Burton
Don't: A Manual Of Mistakes And Improprieties More Or Less Prevalent In Conduct And Speech (1884),Censor
Donald Duk,Frank Chin
Doomed,Chuck Palahniuk
Dopefiend,Donald Goines
Dopeman: Memoirs of a Snitch:: Part 3 of Dopeman's Trilogy,JaQuavis Coleman
Down For Him: A Hood Love Story,Jada Pullen
Down These Mean Streets,Piri Thomas
"Dr Seuss Collection 20 Books Set Pack (The Cat in the Hat, Green Eggs and Ham, Fox in Socks, One Fish Two Fish Red Fish Blue Fish, How the Grinch Stole Christmas!, Oh the Places You'll Go!, the Cat in the Hat Comes Back, Dr. Seuss' Abc, Dr. Seuss ..)",Dr. Seuss
Dr. Faustus (Dover Thrift Editions),Christopher Marlowe
Dr. Seuss Pops Up,Dr. Seuss
Dracula,Bram Stoker
Dracula: A Play in Two Acts,Bram Stoker
Dracula: Writer's Digest Annotated Classics,Bram Stoker
Dracula's Guest and Other Weird Tales (Penguin Classics),Bram Stoker
Dragon (Dirk Pitt Adventure),Clive Cussler
Dragon Warrior (Midnight Bay),Janet Chapman
Dragonfish: A Novel,Vu Tran
Dragonvein: Book Two,Brian D. Anderson
Drama Essentials: An Anthology of Plays,Matthew Roudane
Drama Ministry,Steve Pederson
"Drama, Drinks and Double Faults: The Skinny about Tennis Fanatics That No One Has Had the Balls to Say . . . 'Til Now!",Mary Moses
"Drama, Skits, & Sketches 3",Youth Specialties
Drama: A Pocket Anthology (Penguin Academics Series) (5th Edition),R. S. Gwynn
Dreams in a Time of War: A Childhood Memoir,Ngugi wa Thiong'o
Dreams Of My Mothers: A Story Of Love Transcendent,Joel L. A. Peterson
Dreams of the Red Phoenix,Virginia Pye
Dreamtigers (Texas Pan American Series),Jorge Luis Borges
Drifting House,Krys Lee
Drink Cultura: Chicanismo,Jos Antonio Burciaga
Driven To Be Loved (Carmen Sisters),Pat Simmons
Driving the King: A Novel,Ravi Howard
Drone Command (A Troy Pearce Novel),Mike Maden
Drone String: Poems,Sherry Cook Stanforth
Drones (The Maliviziati Series.) (Volume 1),Johnny Ray
Drown,Junot Diaz
Drowned Boy: Stories (Mary McCarthy Prize in Short Fiction),Jerry Gabriel
Drowning Ruth: A Novel (Oprah's Book Club),Christina Schwarz
Drunk Enough to Say I Love You?,Caryl Churchill
Dry Bones: A Walt Longmire Mystery (Walt Longmire Mysteries),Craig Johnson
Dryland,Sara Jaffe
Dubliners,James Joyce
Dubliners (Dover Thrift Editions),James Joyce
Duende: Poems,Tracy K. Smith
Dumpling Field: Haiku Of Issa,Lucien Stryk
Dust,Yvonne Adhiambo Owuor
Dusty Locks and the Three Bears,Susan Lowell
Dysfluencies: On Speech Disorders in Modern Literature,Chris Eagle
Each Shining Hour: A Novel of Watervalley,Jeff High
Each Thing Unblurred is Broken,Andrea Baker
Early One Morning,Virginia Baily
Early Warning: A novel,Jane Smiley
"Earth Is My Mother, Sky Is My Father: Space, Time, and Astronomy in Navajo Sandpainting",Trudy Griffin-Pierce
Earth Medicine: Ancestor's Ways of Harmony for Many Moons,Jamie Sams
East of Acre Lane,Alex Wheatle
East Of Eden - John Steinbeck Centennial Edition (1902-2002),"John; With an Introduction by Wyatt, David Steinbeck"
East of Eden (Penguin Twentieth Century Classics),John Steinbeck
Ebenezer Scrooge: Ghost Hunter,Jaqueline Kyle
Echoes of a Distant Summer,Guy Johnson
"Echoes: Tired, Worn Out and Over It. Ignoring the Echoes and Listening to God's Voice.",Stephanie DeLores Moore
Ecocriticism on the Edge: The Anthropocene as a Threshold Concept,Timothy Clark
Edda (Everyman's Library),Snorri Sturluson
Edgar Allan Poe: Complete Tales and Poems,Edgar Allan Poe
EDGE OF WONDER: Notes From The Wildness Of Being,Victoria Erickson
Edipo Rey / Oedipus the King (Catedra Base / Base Cathedra) (Spanish Edition),Sophocles
Edith and Winnifred Eaton: CHINATOWN MISSIONS AND JAPANESE ROMANCES (Asian American Experience),Dominika Ferens
"Edith Stein: Letters to Roman Ingarden (Stein, Edith//the Collected Works of Edith Stein)",Edith Stein
Edmund Burke: Selected Writings and Speeches,Edmund Burke
Egeria's Travels,John Wilkinson
"Egyptian Mythology: A Guide to the Gods, Goddesses, and Traditions of Ancient Egypt",Geraldine Pinch
Egyptian Proverbs (Tem T Tchaas),Muata Ashby
Eichmann in Jerusalem: A Report on the Banality of Evil (Penguin Classics),Hannah Arendt
Eight Months on Ghazzah Street: A Novel,Hilary Mantel
El Alquimista: Una Fabula Para Seguir Tus Suenos,Paulo Coelho
El amor en los tiempos del clera (Oprah #59) (Spanish Edition),Gabriel Garca Mrquez
El Arroyo de la Llorona y otros cuentos,Sandra Cisneros
El Borak and Other Desert Adventures,Robert E. Howard
El Caballero De La Armadura Oxidada / the Knight in Rusty Armor (Spanish Edition),Robert Fisher
El caballero de los Siete Reinos [Knight of the Seven Kingdoms-Spanish] (A Vintage Espaol Original) (Spanish Edition),George R. R. Martin
El Conde Lucanor (Spanish Edition),Don Juan Manuel
El Corazon de un Artista (Spanish Edition),Rory Noland
El coronel no tiene quien le escriba (Spanish Edition),Gabriel Garca Mrquez
El Diario de Ana Frank (Anne Frank: The Diary of a Young Girl) (Spanish Edition),Ana Frank
El hombre que amaba a los perros (Coleccion Andanzas) (Spanish Edition),Leonardo Padura
El laberinto de la soledad,Octavio Paz
El Leon Bruja y el Ropero (Narnia) (Spanish Edition),C. S. Lewis
El maestro y Margarita (Spanish Edition),Mijal Bulgkov
El murmullo de las abejas (Spanish Edition),Sofa Segovia
El Presagio: El misterio ancestral que guarda el secreto del futuro del mundo (Spanish Edition),Jonathan Cahn
El profeta rojo (Spanish Edition),Orson Scott Card
El secreto del Bamb: Una fbula (Spanish Edition),Ismael Cala
El senor de las moscas / Lord of the Flies (Spanish Edition),William Golding
El Senor Presidente,Miguel Angel Asturias
El tiempo entre costuras: Una novela (Atria Espanol) (Spanish Edition),Mara Dueas
El Tunel / The Tunnel (Spanish Edition),Ernesto Sabato
El Viaje de Su Vida (Nivel 1 / Libro D) (Spanish Edition),Lisa Ray Turner
Eldorado Red,Donald Goines
Electra (Greek Tragedy in New Translations),Sophocles
Electra and Other Plays (Penguin Classics),Sophocles
Elegy for a Broken Machine: Poems,Patrick Phillips
Elephant Prince: The Story of Ganesh,Amy Novesky
Eleven Minutes: A Novel (P.S.),Paulo Coelho
Elias' Proverbs,Daniel Molyneux
Elijah in Jerusalem,Michael D. O'Brien
"Elizabeth Bishop: Poems, Prose and Letters (Library of America)",Elizabeth Bishop
Elizabeth Street,Laurie Fabiano
"Elliot, A Soldier's Fugue",Quiara Alegra Hudes
"Elmer Rice: Three Plays: The Adding Machine, Street Scene and Dream Girl",Elmer Rice
Emerson: Essays and Lectures: Nature: Addresses and Lectures / Essays: First and Second Series / Representative Men / English Traits / The Conduct of Life (Library of America),Ralph Waldo Emerson
Emerson: The Mind on Fire (Centennial Books),Robert D. Richardson
Emily Dickinson:  A Biography,Connie Ann Kirk
Emily Dickinson: Selected Letters,Emily Dickinson
Emily's Hope,Ellen Gable
Emma (Dover Thrift Editions),Jane Austen
Emma (Fourth Edition)  (Norton Critical Editions),Jane Austen
Emma (Penguin Classics),Jane Austen
"Emma, la cautiva (Spanish Edition)",Csar Aira
Emotionally Weird: A Novel,Kate Atkinson
Empire and Honor (Honor Bound),W.E.B. Griffin
Empire and Memory: The Representation of the Roman Republic in Imperial Culture (Roman Literature and its Contexts),Alain M. Gowing
Empire of Chance: The Napoleonic Wars and the Disorder of Things,Anders Engberg-Pedersen
Empire of Gold: A Novel (Nina Wilde and Eddie Chase),Andy McDermott
Empire of Kalman the Cripple (Library of Modern Jewish Literature),Yehuda Elberg
Empire of Self: A Life of Gore Vidal,Jay Parini
"En busca de la verdad / In search of truth: Discursos, Cartas De Lector, Entrevistas, Artculos / Speeches, Reader Letters, Interviews, Articles (Spanish Edition)",Thomas Bernhard
En el tiempo de las mariposas (Spanish Edition),Julia Alvarez
Enacting the Word: Using Drama in Preaching,James O. Chatham
Enchantress: A Novel of Rav Hisda's Daughter,Maggie Anton
End Of The Rainbow (Modern Plays),Peter Quilter
End Zone,Don DeLillo
Ends of Assimilation: The Formation of Chicano Literature,John Alba Cutler
Enemy In The Ashes,William W. Johnstone
Enemy Women,Paulette Jiles
English Romantic Poetry: An Anthology (Dover Thrift Editions),William Blake
Enigma of China: An Inspector Chen Novel (Inspector Chen Cao),Qiu Xiaolong
Envy (New York Review Books Classics),Yuri Olesha
Epigrams: With parallel Latin text (Oxford World's Classics),Martial
Epistemology of the Closet,Eve Kosofsky Sedgwick
Epitaph: A Novel of the O.K. Corral,Mary Doria Russell
Erasure: A Novel,Percival Everett
Eric Carle's Animals Animals,Eric Carle
Eros the Bittersweet (Canadian Literature),Anne Carson
Erratic Facts,Kay Ryan
Escape From The Ashes,William W. Johnstone
Escape Velocity,Charles Portis
Espresso Tales,Alexander McCall Smith
Essays (Everyman's Library Classics & Contemporary Classics),George Orwell
Essential Literary Terms: A Brief Norton Guide with Exercises,Sharon Hamilton
Essential Shakespeare Handbook,Leslie Dunton-Downer
Essie's Roses,Michelle Muriel
Estrategias Tematicas Y Narrativas En La Novela Feminizada De Maria De Zayas: Spa,Pilar Alcalde
Eternity's Sunrise: The Imaginative World of William Blake,Leo Damrosch
"Eudora Welty : Stories, Essays & Memoir (Library of America, 102)",Eudora Welty
Eugene O'Neill : Complete Plays 1913-1920 (Library of America),Eugene O'Neill
Eugene O'Neill : Complete Plays 1932-1943 (Library of America),Eugene O'Neill
Eugene Onegin (Russian Edition),Alexander Pushkin
Euphoria,Lily King
"Euripides IV: Helen, The Phoenician Women, Orestes (The Complete Greek Tragedies)",Euripides
"Euripides V: Bacchae, Iphigenia in Aulis, The Cyclops, Rhesus (The Complete Greek Tragedies)",Euripides
"Euripides V: Electra, The Phoenician Women, The Bacchae (The Complete Greek Tragedies) (Vol 5)",Euripides
"Euripides I: Alcestis, Medea, The Children of Heracles, Hippolytus (The Complete Greek Tragedies)",Euripides
"Euripides I: Alcestis, The Medea, The Heracleidae, Hippolytus (The Complete Greek Tragedies) (Vol 3)",Euripides
"Euripides III: Heracles, The Trojan Women, Iphigenia among the Taurians, Ion (The Complete Greek Tragedies)",Euripides
"Euripides, Volume III. Suppliant Women. Electra. Heracles (Loeb Classical Library No. 9)",Euripides
"Euripides, Volume IV. Trojan Women. Iphigenia among the Taurians. Ion (Loeb Classical Library No. 10)",Euripides
Euripides: Bacchae (Duckworth Companions to Greek & Roman Tragedy),Sophie Mills
Euripides: Hippolytus (Duckworth Companions to Greek & Roman Tragedy),Sophie Mills
Euripides: Medea (Cambridge Greek and Latin Classics) (Greek and English Edition),Euripides
Euripides: Medea (Cambridge Translations from Greek Drama),Euripides
Euripides: Medea (Duckworth Companions to Greek & Roman Tragedy),William Allan
"Euripides: Medea, Hippolytus, Heracles, Bacchae",Euripides
Euripides: Suppliant Women (Classical Texts) (Ancient Greek Edition),James Morwood
Euripides: Suppliant Women (Companions to Greek and Roman Tragedy),Ian C. Storey
Euripides: Trojan Women (Duckworth Companions to Greek & Roman Tragedy),Barbara Goff
Euripides' Hippolytus,Euripides
"European Proverbs in 55 Languages with Equivalents in Arabic, Persian, Sanskrit, Chinese and Japanese",Gyula Paczolay
Eurydice,Sarah Ruhl
Eve: A Novel,WM. Paul Young
Eve's Hollywood (New Yorkreview Books Classics),Eve Babitz
Even in Darkness: A Novel,Barbara Stark-Nemon
Evening Stars (Blackberry Island),Susan Mallery
Ever Yours: The Essential Letters,Vincent van Gogh
Evergreen Falls: A Novel,Kimberley Freeman
Every Closed Eye Ain't 'Sleep,MaRita Teague
Every Day,David Levithan
Every Day Is for the Thief: Fiction,Teju Cole
Every Thug Needs A Lady,Wahida Clark
Everybody Rise: A Novel,Stephanie Clifford
Everyman and Other Miracle and Morality Plays (Dover Thrift Editions),Anonymous
Everyone I Love is a Stranger to Someone,Annelyse Gelman
Everything and Nothing (New Directions Pearls),Jorge Luis Borges
Everything Begins and Ends at the Kentucky Club,Benjamin Alire Senz
Everything Flows (New York Review Books Classics),Vasily Grossman
Everything I Never Told You,Celeste Ng
Everything Is Illuminated: A Novel,Jonathan Safran Foer
Everything's Eventual: 14 Dark Tales,Stephen King
Evidence: Poems,Mary Oliver
Evolution of an Unorthodox Rabbi,John Moscowitz
Ex Libris: Confessions of a Common Reader,Anne Fadiman
Exclusive,Sandra Brown
Exclusive (The Godmothers),Fern Michaels
"Excursions with Thoreau: Philosophy, Poetry, Religion",Edward F. Mooney
Exemplary Stories (Oxford World's Classics),Miguel de Cervantes
Exemplary Traits: Reading Characterization in Roman Poetry,J. Mira Seo
Existentialism Is a Humanism,Jean-Paul Sartre
"Exploring Literature: Writing and Arguing about Fiction, Poetry, Drama, and the Essay, 5th Edition",Frank Madden
Exploring the Northern Tradition,Galina Krasskova
Explosion in a Cathedral,Alejo Carpentier
Extracting the Stone of Madness: Poems 1962 - 1972,Alejandra Pizarnik
"Extravagant Postcolonialism: Modernism and Modernity in Anglophone Fiction, 1958-1988",Brian T. May
Extreme Metaphors,J.G Ballard
Eyes Like Mine,Lauren Cecile
Eyes Only,Fern Michaels
Eyes: Novellas and Stories,William H. Gass
Eyewitness Christmas,Shell Isenhoff
Ezra Pound: Poet: Volume III: The Tragic Years 1939-1972,A. David Moody
F for Effort: More of the Very Best Totally Wrong Test Answers,Richard Benson
Face Off - The Baddest Chick Part 4,Nisa Santiago
Faces of the Game,Mandi Mac
Faces of the Game 2 (Volume 2),Mandi Mac
Facing Unpleasant Facts,George Orwell
Faggots,Larry Kramer
Fahrenheit 451,Ray Bradbury
"Fair Share Divorce for Women, Second Edition: The Definitive Guide to Creating a Winning Solution",Kathleen A. Miller
Fairy Godmothers Inc.,Jenniffer Wardell
Fairy-Faith in Celtic Countries (Library of the Mystic Arts),Walter Evans-Wentz
Faith and Fat Chances: A Novel,Carla Trujillo
Faith Healer,Brian Friel
Faithful and Virtuous Night: Poems,Louise Glck
Fake Fruit Factory,Patrick Wensink
Fala,Dana Kittendorf
Fall Leaves,Loretta Holland
Fall of Giants: Book One of the Century Trilogy,Ken Follett
Fall of Giants: Book One of the Century Trilogy,Ken Follett
Fall of Poppies: Stories of Love and the Great War,Heather Webb
Falling For a Drug Dealer (Volume 1),Melikia Gaino
Falling for You,Jill Mansell
Fallout,Ellen Hopkins
Family Affair LP,Debbie Macomber
"Family Furnishings: Selected Stories, 1995-2014 (Vintage International)",Alice Munro
Family Life: A Novel,Akhil Sharma
Family of Lies,Mary Monroe
Fans of the Impossible Life,Kate Scelsa
Fantasmagoria,Rick Wayne
Far Away (Nick Hern Books Drama Classics),Caryl Churchill
Far from the Madding Crowd (Penguin Classics),Thomas Hardy
Fast Animal,Tim Seibles
Fat City (New York Review Books Classics),Leonard Gardner
Fat Pig: A Play,Neil LaBute
"Fatal Decision: Edith Cavell, World War I Nurse",Terri Arthur
Fate Is the Hunter: A Pilot's Memoir,Ernest K. Gann
Fatelessness,Imre Kertesz
Fates and Furies: A Novel,Lauren Groff
"Father Comes Home From the Wars (Parts 1, 2 & 3)",Suzan-Lori Parks
Father Marquette and the Great Rivers (Vision Book),August Derleth
Fathers and Sons (Oxford World's Classics),Ivan Turgenev
Fathers and Sons (Penguin Classics),Ivan Turgenev
"Faust, Part One (Oxford World's Classics) (Pt. 1)",J. W. von Goethe
Faust: Part Two (Oxford World's Classics) (Pt. 2),J. W. von Goethe
Favorite Folktales from Around the World (The Pantheon Fairy Tale and Folklore Library),Jane Yolen
Favorite Novels and Stories: Four-Book Collection (Dover Thrift Editions),Jack London
Favorite Poems (Dover Thrift Editions),William Wordsworth
Fear and Loathing at Rolling Stone: The Essential Writing of Hunter S. Thompson,Hunter S. Thompson
Fear of Dying: A Novel,Erica Jong
Federal Agent (Violators: The Coalition) (Volume 3),Nancy Brooks
"Feed Your Vow, Poems for Falling into Fullness",Brooke McNamara
Feet of Clay: A Novel of Discworld,Terry Pratchett
Fefu and Her Friends,Maria Irene Fornes
Felicity: Poems,Mary Oliver
Female Hustler,Joy Deja King
Female Hustler Part 2,Joy Deja King
Fences,August Wilson
Fences (August Wilson Century Cycle),August Wilson
Fever 1793,Laurie Halse Anderson
Fever at Dawn,Pter Grdos
Fevre Dream,George R. R. Martin
"Feydeau Plays: 1: Heart's Desire Hotel, Sauce for the Goose, The One That Got Away, Now You See it, Pig in a Poke (World Classics) (Vol 1)",Georges Feydeau
Ficciones (Spanish Edition),Jorge Luis Borges
Fields of Fire,James Webb
Fierce and True: Plays for Teen Audiences,ChildrenEEs Theatre Company
Fierce Day,Rose Styron
Fiercombe Manor,Kate Riordan
Fifteen One-Act Plays,Sam Shepard
Fifteen Years,Kendra Norman-Bellamy
Fifty Letters of Pliny,Pliny the Younger
Fifty Shades of Dorian Gray (Classic),Nicole Audrey Spector
"Fighting for Rome: Poets and Caesars, History and Civil War",John Henderson
Figures of Speech Used in the Bible,E. W. Bullinger
Filth,Irvine Welsh
Finale: A Novel of the Reagan Years,Thomas Mallon
Finding Amos,J.D. Mason
Finding Emma (Finding Emma Series),Steena Holmes
Finding Freedom: Writings from Death Row,Jarvis Jay Masters
Finding Out: A Novel,Sheryn MacMunn
Finding Soul on the Path of Orisa: A West African Spiritual Tradition,Tobe Melora Correal
Finding the Dream: Dream Trilogy,Nora Roberts
Finding Them Gone: Visiting China's Poets of the Past,Red Pine
Fingersmith,Sarah Waters
Finish This Book,Keri Smith
Finishing Forty,Sean Patrick Brennan
Fire,Sebastian Junger
Fire Ice (The NUMA Files),Clive Cussler
Fire in Beulah,Rilla Askew
Fire in the Head: Shamanism and the Celtic Spirit,Tom Cowan
Fire in the Hole: Stories,Elmore Leonard
Fire in the Treetops: Celebrating Twenty-Five Years  of Haiku North America,"Michael Dylan Welch, Editor"
Fire Rising (Dark Kings),Donna Grant
Firefly Lane,Kristin Hannah
Firestorm (Anna Pigeon),Nevada Barr
First Frost,Sarah Addison Allen
First Love and Other Stories (Oxford World's Classics),Ivan Turgenev
Fishing Stories (Everyman's Pocket Classics),Henry Hughes
Five Ghosts Volume 1: The Haunting of Fabian Gray TP,Frank J. Barbiere
Five Great Greek Tragedies (Dover Thrift Editions),Sophocles
Five Great Science Fiction Novels (Dover Thrift Editions),H. G. Wells
Five Smooth Stones: A Novel (Rediscovered Classics),Ann Fairbairn
Five Women Wearing the Same Dress,Alan Ball
"Five Years of My Life, 1894-1899 (Classic Reprint)",Alfred Dreyfus
Flaming Iguanas: An Illustrated All-Girl Road Novel Thing,Erika Lopez
Flannery O'Connor : Collected Works : Wise Blood / A Good Man Is Hard to Find / The Violent Bear It Away / Everything that Rises Must Converge / Essays & Letters (Library of America),Flannery O'Connor
Flat Water Tuesday: A Novel,Ron Irwin
Flight Behavior: A Novel,Barbara Kingsolver
Flight of the Sparrow: A Novel of Early America,Amy Belding Brown
Flight: A Novel,Sherman Alexie
Flint: A Novel,Louis L'Amour
Flirt: The Interviews,Lorna Jackson
Flood of Fire: A Novel (The Ibis Trilogy),Amitav Ghosh
Flower Fairies of the Autumn,Cicely Mary Barker
Flowers and Stone,Jan Sikes
Flowers for Algernon,Daniel Keyes
Flowers in the Attic (Dollanganger),V.C. Andrews
Flowers in the Attic /  Petals on the Wind / If There Be Thorns / Seeds of Yesterday / Garden of Shadows,Virginia Andrews
Fludd: A Novel,Hilary Mantel
Fly Away,Kristin Hannah
Flyin' West and Other Plays,Pearl Cleage
Flying Changes: A Novel (Riding Lessons),Sara Gruen
Flying Colours (Hornblower Saga),C. S. Forester
Flying Too High : a Phryne Fisher Mystery,Kerry Greenwood
Folk Medicine in Southern Appalachia,Anthony Cavender
Folk Medicine: A Vermont Doctor's Guide to Good Health,D. C. Jarvis
"Folktales and Fairy Tales [4 volumes]: Traditions and Texts from around the World, 2nd Edition",Donald Haase Ph.D.
"Folktales on Stage: Children's Plays for Readers Theater, with 16 Reader's Theatre Play Scripts from World Folk and Fairy Tales and Legends, Including Asian, African, Middle Eastern, & Native American",Aaron Shepard
Follies of God: Tennessee Williams and the Women of the Fog,James Grissom
Fool for Love - Acting Edition,Sam Shepard
Fool for Love and Other Plays,Sam Shepard
Fools Crow (Penguin Classics),James Welch
For All My Walking,Santoka Taneda
For Love of the Game,Michael Shaara
For One More Day Large Print Edition,Mitch Albom
For the Love of 2am: Poetry For Insomniacs,Zena A. White
For the Sake of Love (Urban Books),Dwan Abrams
For Today I Am a Boy,Kim Fu
For Whom the Bell Tolls,Ernest Hemingway
For Whom The Bell Tolls,Ernest Hemingway
For Your Love: A Blessings Novel,Beverly Jenkins
Forbidden Acts: Pioneering Gay & Lesbian Plays of the 20th Century,Ben Hodges
Force Majeure: A Novel,Bruce Wagner
Fore!: The Best of Wodehouse on Golf (P.G. Wodehouse Collection),P. G. Wodehouse
"Foreign Gods, Inc.",Okey Ndibe
Foreign Influence: A Thriller (The Scot Harvath Series),Brad Thor
Forensics Duo Series Volume 4: Duo Practice and Competition Thirty-five 8-10 Minute Original Dramatic Plays for Two Females,Ira Brodsky
Forest Primeval: Poems,Vievee Francis
Forever a Hustler's Wife: A Novel (Nikki Turner Original),Nikki Turner
Forever an Ex: A Novel,Victoria Christopher Murray
Forever Human,Tom Conyers
"Forever, Interrupted: A Novel",Taylor Jenkins Reid
Forgiven (Urban Christian),Vanessa Miller
Forgotten (Forsaken) (Volume 3),Vanessa Miller
Forgotten Country,Catherine Chung
Form Line of Battle! (The Bolitho Novels) (Volume 9),Alexander Kent
Forrest Gump,Winston Groom
Forsaken (Urban Books),Vanessa Miller
Fortune Smiles: Stories,Adam Johnson
Forty Stories (Vintage Classics),Anton Chekhov
Foundations Of The Republic: Speeches And Addresses,Calvin Coolidge
Four Comedies,Aristophanes
Four Comedies: The Braggart Soldier; The Brothers Menaechmus; The Haunted House; The Pot of Gold (Oxford World's Classics),Plautus
"Four Greek Plays: The Agamemnon, The Oedipus Rex, The Alcestis, The Birds",Aeschylus and Sophocles
"Four Major Plays: Lysistrata, The Acharnians, The Birds, The Clouds",Aristophanes
Four Plays,Samuel D. Hunter
Four Plays by Aristophanes: The Birds; The Clouds; The Frogs; Lysistrata (Meridian Classics),Aristophanes
"Four Tragedies: Ajax, Women of Trachis, Electra, Philoctetes",Sophocles
Four-Legged Girl: Poems,Diane Seuss
Fourplay: A Novel,Jane Moore
Fourth of July Creek: A Novel,Smith Henderson
Foxfire 10,Inc. Foxfire Fund
Foxfire 9,Inc. Foxfire Fund
Fragile Things: Short Fictions and Wonders,Neil Gaiman
"Fragments: Poems, Intimate Notes, Letters",Marilyn Monroe
Francesco: Una vida entre el cielo y la tierra (Spanish Edition),Yohana Garca
Francis Bacon: The Major Works (Oxford World's Classics),Francis Bacon
Frank N' Goat: A Tale of Freakish Friendship,Jessica Watts
Frankenstein,Mary Shelley
Frankenstein (Hardcover Classics),Mary Shelley
Frankenstein (Second Edition)  (Norton Critical Editions),Mary Shelley
Frankenstein Makes a Sandwich,Adam Rex
Frankenstein: (Penguin Classics Deluxe Edition),Mary Shelley
Frankenstein: IT Support,James Livingood
Frankie and Johnny in the Claire de Lune,Terrence McNally
Franny and Zooey,J. D. Salinger
Franz Kafka: The Complete Stories,Franz Kafka
Freaks I've Met,Donald Jans
"Frederick Law Olmsted: Writings on Landscape, Culture, and Society: (Library of America #270)",Frederick Law Olmsted
Free to Be...You and Me (The 35th Anniversary Edition),Marlo Thomas and Friends
Freedom from Fear: And Other Writings,Aung San Suu Kyi
Freedom Time: The Poetics and Politics of Black Experimental Writing (The  Callaloo African Diaspora Series),Anthony Reed
Freedom: A Novel (Oprah's Book Club),Jonathan Franzen
Freedom's Battle Being A Comprehensive Collection Of Writings And Speeches On The Present Situation,Mahatma Gandhi
Freeman,Leonard Jr. Pitts
Freezer Burn,Joe R Lansdale
French Grammar (Quickstudy: Academic),Inc. BarCharts
Freud's Rome: Psychoanalysis and Latin Poetry (Roman Literature and its Contexts),Ellen Oliensis
Friday Night Love (Days Of Grace V1),Tia McCollors
Fried Green Tomatoes at the Whistle Stop Cafe: A Novel,Fannie Flagg
Friends & Foes,ReShonda Tate Billingsley
Friends with Full Benefits,Luke Young
Friends With Partial Benefits (Friends With... Benefits Series (Book 1)),Luke Young
Frog: A Novel,Mo Yan
Frogs and Other Plays (Penguin Classics),Aristophanes
"From Distant Days: Myths, Tales, and Poetry of Ancient Mesopotamia",Benjamin R. Foster
"From Hitler's Doorstep: The Wartime Intelligence Reports of Allen Dulles, 1942-1945",Neal H. Petersen
From Olympus to Camelot: The World of European Mythology,David Leeming
From Russia with Love (James Bond Series),Ian Fleming
From Slave to Governor: the Unlikely Life of Lott Cary,Perry Thomas
From the Cincinnati Reds to the Moscow Reds: The Memoirs of Irwin Weil (Jews of Russia & Eastern Europe and Their Legacy),Irwin Weil
From the Dissident Right II: Essays 2013,John Derbyshire
From the Listening Hills: Stories,Louis L'Amour
From the New World: Poems 1976-2014,Jorie Graham
From the Soul: My Haiku and My Senryu,Opal F. Caleb
Frostgrave: Thaw of the Lich Lord,Joseph A. McCullough
Frozen Socks: New & Selected Short Poems,Alan Pizzarelli
Fucked Up Shit: A Mixtape Anthology,Berti Walker
Fugitive Colors: A Novel,Lisa Barr
Full Black: A Thriller (The Scot Harvath Series),Brad Thor
Full Circle (Urban Books),Skyy
"Full Dark, No Stars",Stephen King
Full Force and Effect (Jack Ryan),Mark Greaney
Fun Home,Jeanine Tesori
Funny 4 God: A Variety of Christian Comedy Skits,Rick Eichorn
Funny Girl: A Novel,Nick Hornby
"Gabriel Marcel's Perspectives on the Broken World: The Broken World, a Four-Act Play : Followed by Concrete Approaches to Investigating the Ontological Mystery (Marquette Studies in Philosophy)",Gabriel Marcel
Gabriel: A Poem,Edward Hirsch
Gaia Codex,Sarah Drew
"Galateo: Or, The Rules of Polite Behavior",Giovanni Della Casa
Galore,Michael Crummey
"Gambled: A Novella: Titan, Book 3.25",Cristin Harber
Game Seven,Tom Rock
Ganesha Goes to Lunch: Classics from Mystic India (Mandala Classics),Kamla K. Kapur
Gangsta (Urban Books),K'wan
Garden of Shadows (Dollanganger),V.C. Andrews
Garden Spells (Bantam Discovery),Sarah Addison Allen
Gardenias: A Novel,Faith Sullivan
GARDENS IN THE DUNES: A Novel,Leslie Marmon Silko
Gate of the Sun,Elias Khoury
Gates of Fire,Steven Pressfield
Gates of Fire: An Epic Novel of the Battle of Thermopylae,Steven Pressfield
Gates of Gold,Frank McGuinness
Gathering of Waters,Bernice L. McFadden
Gaunt's Ghosts: The Founding,Dan Abnett
Gem of the Ocean,August Wilson
Gemini: A Novel,Carol Cassella
Gems of Gemvron: Five Onyx Stones,Mr Michael J Murtuagh
Gender Trouble: Feminism and the Subversion of Identity (Routledge Classics),Judith Butler
Genealogical Fictions: Cultural Periphery and Historical Change in the Modern Novel,Jobst Welge
Genealogy of the Tragic: Greek Tragedy and German Philosophy,Joshua Billings
General A. P. Stewart: His Life And Letters,Marshall Wingfield
Genesis Revisited (Earth Chronicles),Zecharia Sitchin
George,Alex Gino
George MacDonald: An Anthology 365 Readings,George MacDonald
"George R. R. Martin's A Game of Thrones Leather-Cloth Boxed Set (Song of Ice and Fire Series): A Game of Thrones, A Clash of Kings, A Storm of Swords, A Feast for Crows, and A Dance with Dragons",George R. R. Martin
George Washington Gomez: A Mexicotexan Novel,Americo Paredes
Georgette Heyer,Jennifer Kloester
Georgette Heyer's Regency World,Jennifer Kloester
Gerard Manley Hopkins: The Major Works (Oxford World's Classics),Gerard Manley Hopkins
Geronimo Rex,Barry Hannah
Getting Back,Kelly Sinclair
Getting Over Kyle: Second Chances Series Book II (Volume 2),A. W. Myrie
Getting to Happy,Terry McMillan
Gettysburg Address and Other Writings,Abraham Lincoln
Ghetto Bastard (Animal),K'wan
Ghost Ship (The NUMA Files),Clive Cussler
Ghostly: A Collection of Ghost Stories,Audrey Niffenegger
Ghetto Love 4 (Volume 4),Sonovia Alexander
Ghost Medicine: An Ella Clah Novel,Aime Thurlo
Ghost of the Machine (ShatterGlass and WinterHeld) (Volume 1),Diana N. Logg
Ghosts of Bungo Suido,P. T. Deutermann
Ghosts of Manitowish Waters,G. M. Moore
"Giambattista Basile's The Tale of Tales, or Entertainment for Little Ones (Series in Fairy-Tale Studies)",Giambattista Basile
Gilead: A Novel,Marilynne Robinson
Gilgamesh: A Verse Narrative,Herbert Mason
Gillian's Island,Natalie Vivien
Giovanni's Room (Vintage International),James Baldwin
Girl at War: A Novel,Sara Novic
Girl in Translation,Jean Kwok
Girl Singer,Mick Carlon
Girlchild: A Novel,Tupelo Hassman
Glengarry Glen Ross: A Play,David Mamet
Glew I: Maneater (GLEW: the horse that eats people) (Volume 1),Michael R Peterson MP
Glimmer in the Darkness (Forgiveness),Nicole Hampton
Glitterwolf Magazine: Halloween Special,Matt Cresswell
Glorious,Bernice L. McFadden
Glorious: A Novel of the American West,Jeff Guinn
Go Set a Watchman: A Novel,Harper Lee
Go Tell It on the Mountain (Vintage International),James Baldwin
Go the F**k to Sleep,Adam Mansbach
God Don't Like Ugly,Mary Monroe
God Says No,James Hannaham
God's Country,Percival Everett
God's Formula,James Lepore
God's Kingdom: A Novel,Howard Frank Mosher
Goddess,Kelly Gardiner
Gods and Generals: A Novel of the Civil War (Civil War Trilogy),Jeff Shaara
Gods and Heroes of Ancient Greece (The Pantheon Fairy Tale and Folklore Library),Gustav Schwab
Gods and Kings (Chronicles of the Kings #1) (Volume 1),Lynn Austin
Gods and Myths of Northern Europe,H.R. Ellis Davidson
Gods Behaving Badly: A Novel,Marie Phillips
Going Home: A Novel of the Civil War,James D. Shipman
Going to Meet the Man: Stories,James Baldwin
Gold: (Poiema Poetry),Barbara Crooker
Gold: A Novel,Chris Cleave
Golden Age: A novel (Last Hundred Years Trilogy),Jane Smiley
Golden State: A Novel,Michelle Richmond
Goldilocks and the Three Bears: Special Edition,Robert Southey
Golf in the Kingdom,Michael J. Murphy
GOLF MAGAZINE How To Hit Every Shot,Editors of Golf Magazine
GOLF MAGAZINE'S BIG BOOK OF BASICS: Your step-by-step guide to building a complete and reliable game from the ground up WITH THE TOP 100 TEACHERS IN AMERICA,GOLF Magazine
GOLF Magazine's The Par Plan: A Revolutionary System to Shoot Your Best Score in 30 Days,GOLF Magazine
Golf: The Best Short Game Instruction Book Ever!,Editors of Golf Magazine
Golfing with God: A Novel of Heaven and Earth,Roland Merullo
Gone (Hannah Smith Novels),Randy Wayne White
Good Bones and Simple Murders,Margaret Atwood
"Good Dog: True Stories of Love, Loss, and Loyalty",Editors of Garden and Gun
Good Faeries/Bad Faeries,Brian Froud
Good People,David Lindsay-Abaire
Goth,Otsuichi
Gothic Tales (Penguin Classics),Elizabeth Gaskell
Grace - Acting Edition,Craig Wright
Grace: A Novel,Richard Paul Evans
Graceland (Today Show Pick January 2005),Chris Abani
Graceland and Asleep on the Wind,Ellen Byron
Graffiti and the Literary Landscape in Roman Pompeii,Kristina Milnor
"Graham R.: Rosamund Marriott Watson, Woman of Letters",Linda K. Hughes
Grand Central: Original Stories of Postwar Love and Reunion,Karen White
Grand Opening: A Family Business Novel (Family Business Novels),Carl Weber
Graphic Women: Life Narrative and Contemporary Comics (Gender and Culture Series),Hillary L. Chute
Grass Crown (Masters of Rome),Colleen McCullough
Gratitude,Oliver Sacks
Grave on Grand Avenue (An Officer Ellie Rush Mystery),Naomi Hirahara
Gravity,Tess Gerritsen
Gray,Pete Wentz
Great Books of the Western World,Mortimer J. Adler
Great Classic Stories: 22 Unabridged Classics,Derek Jacobi
Great Expectations (Dover Thrift Editions),Charles Dickens
Great Hunting Stories: Inspiring Adventures for Every Hunter,Steve Chapman
Great Short Works of Fyodor Dostoevsky (Harper Perennial Modern Classics),Fyodor Dostoevsky
Great Speeches (Dover Thrift Editions),Franklin Delano Roosevelt
Great Speeches of our Time,Hywel Williams
"Great Wilderness, A",Samuel D Hunter
Great with Child: Letters to a Young Mother,Beth Ann Fennelly
"Greece, in 1823 and 1824: Being a Series of Letters and Other Documents on the Greek Revolution, Written during a Visit to that Country (Cambridge Library Collection - European History)",Leicester Stanhope
Greek Drama (Bantam Classics),Moses Hadas
Greek Lexicon of the Roman and Byzantine Periods from B.C. 146 to A.D. 1100 V1,E. A. Sophocles
Greek Lexicon of the Roman and Byzantine Periods from B.C. 146 to A.D. 1100 V2,E. A. Sophocles
Greek Lyric Poetry (Bcp Greek Texts),David A. Campbell
"Greek Lyric, Volume III, Stesichorus, Ibycus, Simonides, and Others (Loeb Classical Library No. 476)",Stesichorus
Greek Lyric: Sappho and Alcaeus (Loeb Classical Library No. 142) (Volume I),Sappho
"Greek Tragedies, Volume 1",Aeschylus
"Greek Tragedies, Volume 2 The Libation Bearers (Aeschylus), Electra (Sophocles), Iphigenia in Tauris, Electra, & The Trojan Women (Euripides)",Aeschylus
"Greek Tragedy on the American Stage: Ancient Drama in the Commercial Theater, 1882-1994 (Contributions in Drama and Theatre Studies)",Karelisa Hartigan
Green Calder Grass,Janet Dailey
Green Eyes,Ari Eastman
Green Girl: A Novel (P.S.),Kate Zambreno
"Green Grass, Running Water",Thomas King
Green Hills of Africa,Ernest Hemingway
Green Skull and Crossbones Journal: 160 Page Lined Journal/Notebook,Mahtava Journals
Greenbeard,Richard James Bentley
Grey: Fifty Shades of Grey as Told By Christian By E L James | Summary & Analysis,Quick Read
Grief Lessons: Four Plays by Euripides (New York Review Books Classics),Euripides
Grimm's Complete Fairy Tales,Jacob Grimm
Grimm's Fairy Tales - Illustrated by Charles Folkard,Grimm Brothers
Growing Older with Jane Austen,Maggie Lane
Grown Folks Business: A Novel,Victoria Christopher Murray
Gruesome Playground Injuries,Rajiv Joseph
Gruesome Playground Injuries; Animals Out of Paper; Bengal Tiger at the Baghdad Zoo: Three Plays,Rajiv Joseph
Guerra y Paz (Spanish Edition),Len Tolsti
Gulliver's Travels,Jonathan Swift
Gulliver's Travels,Martin Rowson
Gulliver's Travels (Calla Editions),Jonathan Swift
Gulliver's Travels: A Signature Performance by David Hyde Pierce,Jonathan Swift
Gutenberg's Apprentice: A Novel,Alix Christie
Gutter: A Novel,K'wan
Gtz and Meyer: G?tz and Meyer (Serbian Literature),David Albahari
  
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON MACHINE LEARNING PYTHON MACHINE LEARNING PYTHON MACHINE LEARNING PYTHON MACHINE LEARNING #-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON SK LEARN PYTHON SK-LEARN PYTHON SK LEARN PYTHON SK-LEARNPYTHON SK LEARN PYTHON SK-LEARN  #-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''#Machine learning can be branched out into the following categories:
#Supervised Learning
#Unsupervised Learning

Supervised Learning is where the data is labeled and the program learns to predict the output from the input data. For instance, a supervised learning algorithm for credit card fraud detection would take as input a set of recorded transactions. For each transaction, the program would predict if it is fraudulent or not.

Supervised learning problems can be further grouped into regression and classification problems.


Regression:

In regression problems, we are trying to predict a continuous-valued output. Examples are:

What is the housing price in New York?
What is the value of cryptocurrencies?
Classification:

In classification problems, we are trying to predict a discrete number of values. Examples are:

Is this a picture of a human or a picture of an AI?
Is this email spam?
For a quick preview, we will show you an example of supervised learning.

Unsupervised Learning
Unsupervised Learning is a type of machine learning where the program learns the inherent structure of the data based on unlabeled examples.

Clustering is a common unsupervised machine learning approach that finds patterns and structures in unlabeled data by grouping them into clusters.

Some examples:

Social networks clustering topics in their news feed
Consumer sites clustering users for recommendations
Search engines to group similar objects in one cluster

Summary
We have gone over the difference between supervised and unsupervised learning:

Supervised Learning: data is labeled and the program learns to predict the output from the input data
Unsupervised Learning: data is unlabeled and the program learns to recognize the inherent structure in the input data


'''

#Linear Regression----------------------------
#Import and create the model:

from sklearn.linear_model import LinearRegression

your_model = LinearRegression()
#Fit:

your_model.fit(x_training_data, y_training_data)
#.coef_: contains the coefficients
#.intercept_: contains the intercept
#Predict:

predictions = your_model.predict(your_x_data)
#.score(): returns the coefficient of determination R²




# Naive -------------------
# Import and create the model:

from sklearn.naive_bayes import MultinomialNB

your_model = MultinomialNB()
# Fit:

your_model.fit(x_training_data, y_training_data)
# Predict:

# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data



# K-Nearest Neighbors-------------------------
# Import and create the model:

from sklearn.neigbors import KNeighborsClassifier

your_model = KNeighborsClassifier()
# Fit:

your_model.fit(x_training_data, y_training_data)
# Predict:

# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data)




# K-Means------------------------------------
# Import and create the model:

from sklearn.cluster import KMeans

your_model = KMeans(n_clusters=4, init='random')
# n_clusters: number of clusters to form and number of centroids to generate
# init: method for initialization
# k-means++: K-Means++ [default]
# random: K-Means
# random_state: the seed used by the random number generator [optional]
# Fit:

your_model.fit(x_training_data)
# Predict:

predictions = your_model.predict(your_x_data)





# Validating the Model---------------------
Import and print accuracy, recall, precision, and F1 score:

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

print(accuracy_score(true_labels, guesses))
print(recall_score(true_labels, guesses))
print(precision_score(true_labels, guesses))
print(f1_score(true_labels, guesses))
# Import and print the confusion matrix:

from sklearn.metrics import confusion_matrix

print(confusion_matrix(true_labels, guesses))



# Training Sets and Test Sets--------------------------------
from sklearn.model_selection import train_test_split

# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
# train_size: the proportion of the dataset to include in the train split
# test_size: the proportion of the dataset to include in the test split
# random_state: the seed used by the random number generator [optional]

# ################################Linear Regression########################


# Points and Lines
# In the last exercise, you were probably able to make a rough estimate about the next data point for Sandra’s lemonade stand without thinking too hard about it. For our program to make the same level of guess, we have to determine what a line would look like through those data points.

# A line is determined by its slope and its intercept. In other words, for each point y on a line we can say:

# y = m x + by=mx+b
# where m is the slope, and b is the intercept. y is a given point on the y-axis, and it corresponds to a given x on the x-axis.

# The slope is a measure of how steep the line is, while the intercept is a measure of where the line hits the y-axis.

# When we perform Linear Regression, the goal is to get the “best” m and b for our data. We will determine what “best” means in the next exercises.

# Loss
# When we think about how we can assign a slope and intercept to fit a set of points, we have to define what the best fit is.

# For each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was. You may have seen this being referred to as error.

# We can think about loss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way:


# In this example:

# For point A, the squared distance is 9 (3²)
# For point B, the squared distance is 1 (1²)
# So the total loss, with this model, is 10. If we found a line that had less loss than 10, that line would be a better model for this data.

# Gradient Descent for Intercept
# As we try to minimize loss, we take each parameter we are changing, and move it as long as we are decreasing loss. It’s like we are moving down a hill, and stop once we reach the bottom:


# The process by which we do this is called gradient descent. We move in the direction that decreases our loss the most. Gradient refers to the slope of the curve at any point.

# For example, let’s say we are trying to find the intercept for a line. We currently have a guess of 10 for the intercept. At the point of 10 on the curve, the slope is downward. Therefore, if we increase the intercept, we should be lowering the loss. So we follow the gradient downwards.


# We derive these gradients using calculus. It is not crucial to understand how we arrive at the gradient equation. To find the gradient of loss as intercept changes, the formula comes out to be:

# \frac{2}{N}\sum_{i=1}^{N}-(y_i-(mx_i+b)) 
# N
# 2
# ​	  
# i=1
# ∑
# N
# ​	 −(y 
# i
# ​	 −(mx 
# i
# ​	 +b))
# N is the number of points we have in our dataset
# m is the current gradient guess
# b is the curr


#------------------------------------------------

#---------------------------------------------------
good video to recap all this 

https://www.youtube.com/watch?v=sDv4f4s2SB8

and a good video to review "how to calculate derivative"

https://www.youtube.com/watch?v=54KiyZy145Y&t=162s

and the chain rule for derivative

https://www.youtube.com/watch?v=H-ybCx8gt-8&t=372s

#-------------------------------------------------------

#---------------- sample of a Gradient Descent 1 -----------------------

import codecademylib3_seaborn
import matplotlib.pyplot as plt

def get_gradient_at_b(x, y, b, m):
  N = len(x)
  diff = 0
  for i in range(N):
    x_val = x[i]
    y_val = y[i]
    diff += (y_val - ((m * x_val) + b))
  b_gradient = -(2/N) * diff  
  return b_gradient

def get_gradient_at_m(x, y, b, m):
  N = len(x)
  diff = 0
  for i in range(N):
      x_val = x[i]
      y_val = y[i]
      diff += x_val * (y_val - ((m * x_val) + b))
  m_gradient = -(2/N) * diff  
  return m_gradient

#Your step_gradient function here
def step_gradient(b_current, m_current, x, y, learning_rate ):
    b_gradient = get_gradient_at_b(x, y, b_current, m_current)
    m_gradient = get_gradient_at_m(x, y, b_current, m_current)
    b = b_current - (learning_rate * b_gradient)
    m = m_current - (learning_rate * m_gradient)
    return [b, m]
  
#Your gradient_descent function here:  
def gradient_descent(x, y, learning_rate, num_iterations):
  b = 0
  m = 0
  i = range(num_iterations)
  for a in i:
    b, m = step_gradient (b, m, x, y, learning_rate)
  return [b, m]

months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]

#Uncomment the line below to run your gradient_descent function
b, m = gradient_descent(months, revenue, 0.01, 1000)

#Uncomment the lines below to see the line you've settled upon!
y = [m*x + b for x in months]

plt.plot(months, revenue, "o")
plt.plot(months, y)

#---------------------------------------------------------

import codecademylib3_seaborn
from gradient_descent_funcs import gradient_descent
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("heights.csv")

X = df["height"]
y = df["weight"]

plt.plot(X, y, 'o')
#plot your line here:
b, m = gradient_descent(X, y, 0.0001, 1000)

y_predictions = [x*m + b for x in X]

plt.plot(X, y_predictions, 'o')

plt.show

#-------------------------------------------


# Scikit-Learn
# Congratulations! You’ve now built a linear regression algorithm from scratch.

# Luckily, we don’t have to do this every time we want to use linear regression. We can use Python’s scikit-learn library. Scikit-learn, or sklearn, is used specifically for Machine Learning. Inside the linear_model module, there is a LinearRegression() function we can use:

from sklearn.linear_model import LinearRegression
You can first create a LinearRegression model, and then fit it to your x and y data:

line_fitter = LinearRegression()
# line_fitter.fit(X, y)
# The .fit() method gives the model two variables that are useful to us:

# the line_fitter.coef_, which contains the slope
# the line_fitter.intercept_, which contains the intercept
# We can also use the .predict() function to pass in x-values and receive the y-values that this line would predict:

y_predicted = line_fitter.predict(X)
# Note: the num_iterations and the learning_rate that you learned about in your own implementation have default values within scikit-learn, so you don’t need to worry about setting them specifically!

#--------------------------------------------

import codecademylib3_seaborn
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np

temperature = np.array(range(60, 100, 2))
temperature = temperature.reshape(-1, 1)
sales = [65, 58, 46, 45, 44, 42, 40, 40, 36, 38, 38, 28, 30, 22, 27, 25, 25, 20, 15, 5]
         
line_fitter = LinearRegression()
line_fitter.fit(temperature, sales )
sales_predict = line_fitter.predict(temperature)

plt.plot(temperature, sales, 'o')
plt.plot(temperature, sales_predict, 'o')

plt.show()

#------------------------------------------------

Find another dataset, maybe in scikit-learn’s example datasets. Or on Kaggle, a great resource for tons of interesting data.

Try to perform linear regression on your own! If you find any cool linear correlations, make sure to share them!

As a starter, we’ve loaded in the Boston housing dataset. We made the X values the nitrogen oxides concentration (parts per 10 million), and the y values the housing prices. See if you can perform regression on these houses!

#----------------------------------------

Sklearn sample and real source of data for practice:

https://scikit-learn.org/stable/datasets/index.html

https://www.kaggle.com/datasets

#-------------------------------------

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# Boston housing dataset
boston = load_boston()

df = pd.DataFrame(boston.data, columns = boston.feature_names)

# Set the x-values to the nitrogen oxide concentration:
X = df[['NOX']]
# Y-values are the prices:
y = boston.target

# Can we do linear regression on this?

line_fitter = LinearRegression()
line_fitter.fit(X, y )
yy_predict = line_fitter.predict(X)


plt.scatter(X, y, alpha=0.4)
# Plot line here:
plt.plot(X, yy_predict)

plt.title("Boston Housing Dataset")
plt.xlabel("Nitric Oxides Concentration")
plt.ylabel("House Price ($)")
plt.show()

#----------------------------------------------

import codecademylib3_seaborn
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model

df = pd.read_csv("https://s3.amazonaws.com/codecademy-content/programs/data-science-path/linear_regression/honeyproduction.csv")

#print(df.head())
prod_per_year = df.groupby("year").totalprod.mean().reset_index()

#print(prod_per_year.head())
X = prod_per_year['year']
X = X.values.reshape(-1 ,1)

y = prod_per_year['totalprod']
#print(type(y))

regr = linear_model.LinearRegression()
regr.fit(X, y)

y_predict = regr.predict(X)

X_future = np.array(range(2013,2051))
X_future = X_future.reshape(-1, 1)

future_predict = regr.predict(X_future)

plt.scatter(X, y)
plt.plot(X, y_predict)
plt.plot(X_future, future_predict)
plt.show()
#--------------------------------------------
Multiple Linear Regression
#----------------------------------------------

import codecademylib3_seaborn

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from mpl_toolkits.mplot3d import Axes3D

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['size_sqft','building_age_yrs']]
y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state=6)

ols = LinearRegression()

ols.fit(x_train, y_train)

# Plot the figure

fig = plt.figure(1, figsize=(6, 4))
plt.clf()

elev = 43.5
azim = -110

ax = Axes3D(fig, elev=elev, azim=azim)

ax.scatter(x_train[['size_sqft']], x_train[['building_age_yrs']], y_train, c='k', marker='+')

ax.plot_surface(np.array([[0, 0], [4500, 4500]]), np.array([[0, 140], [0, 140]]), ols.predict(np.array([[0, 0, 4500, 4500], [0, 140, 0, 140]]).T).reshape((2, 2)), alpha=.7)

ax.set_xlabel('Size (ft$^2$)')
ax.set_ylabel('Building Age (Years)')
ax.set_zlabel('Rent ($)')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

# Add the code below:

plt.show()


# split Train_test set 
                                         
import codecademylib3_seaborn
import pandas as pd

# import train_test_split
from sklearn.model_selection import train_test_split

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state = 6)

print(x_train.shape)
print(x_test.shape)

print(y_train.shape)
print(y_test.shape)
 
 
 #---------------------------------------------------
 
#MULTIPLE LINEAR REGRESSION
#Multiple Linear Regression: Scikit-Learn
#Now we have the training set and the test set, let’s use scikit-learn to build the linear regression model!

#The steps for multiple linear regression in scikit-learn are identical to the steps for simple linear regression. Just like simple linear regression, we need to import LinearRegression from the linear_model module:

from sklearn.linear_model import LinearRegression
#Then, create a LinearRegression model, and then fit it to your x_train and y_train data:

mlr = LinearRegression()

mlr.fit(x_train, y_train) 
# finds the coefficients and the intercept value
#We can also use the .predict() function to pass in x-values. It returns the y-values that this plane would predict:

y_predicted = mlr.predict(x_test)
# takes values calculated by `.fit()` and the `x` values, plugs them into the multiple linear regression equation, and calculates the predicted y values. 
We will start by using two of these columns to teach you how to predict the values of the dependent variable, prices.


'''
Now we have:

x_test
x_train
y_test
y_train
and y_predict!
4.
To see this model in action, let’s test it on Sonny’s apartment in Greenpoint, Brooklyn!

Or if you reside in New York, plug in your own apartment’s values and see if you are over or underpaying!

This is a 1BR/1Bath apartment that is 620 ft². We have pulled together the data for you:

Features	Sonny’s Apartment
bedrooms	1
bathrooms	1
size_sqft	620 ft²
min_to_subway	16 min
floor	1
building_age_yrs	98 (built in 1920)
no_fee	1
has_roofdeck	0
has_washer_dryer	Yas
has_doorman	0
has_elevator	0
has_dishwasher	1
has_patio	1
has_gym	0

'''
# Sonny doesn't have an elevator so the 11th item in the list is a 0
sonny_apartment = [[1, 1, 620, 16, 1, 98, 1, 0, 1, 0, 0, 1, 1, 0]]

predict = mlr.predict(sonny_apartment)

print("Predicted rent: $%.2f" % predict)
#The result is:

#Predicted rent: $2393.58
#cAnd Sonny is only paying $2,000. Yay!

# -------------- code -----------------------
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression


streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state=6)

# Add the code here:
mlr = LinearRegression()

mlr.fit(x_train, y_train)

y_predict = mlr.predict(x_test)

# test of sonny_apartment

sonny_apartment = [[1, 1, 620, 16, 1, 98, 1, 0, 1, 0, 0, 1, 1, 0]]

predict = mlr.predict(sonny_apartment)

print("Predicted rent: $%.2f" % predict)

#plot the result

# Create a scatter plot
plt.scatter(y_test, y_predict, alpha=0.4)

# Create x-axis label and y-axis label

plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")


# Create a title

plt.title("Actual Rent vs Predicted Rent")

# show the plot
plt.show()

#---------------------------
'''
MULTIPLE LINEAR REGRESSION
Multiple Linear Regression Equation
Now that we have implemented Multiple Linear Regression, we will learn how to tune and evaluate the model. Before we do that, however, it’s essential to learn the equation behind it.

Equation 6.1 The equation for multiple linear regression that uses two independent variables is this:

y = b + m_{1}x_{1} + m_{2}x_{2}y=b+m 
1
​	 x 
1
​	 +m 
2
​	 x 
2
​	 
Equation 6.2 The equation for multiple linear regression that uses three independent variables is this:

y = b + m_{1}x_{1} + m_{2}x_{2} + m_{3}x_{3}y=b+m 
1
​	 x 
1
​	 +m 
2
​	 x 
2
​	 +m 
3
​	 x 
3
​	 
Equation 6.3 As a result, since multiple linear regression can use any number of independent variables, its general equation becomes:

y = b + m_{1}x_{1} + m_{2}x_{2} + ... + m_{n}x_{n}y=b+m 
1
​	 x 
1
​	 +m 
2
​	 x 
2
​	 +...+m 
n
​	 x 
n
​	 
Here, m1, m2, m3, … mn refer to the coefficients, and b refers to the intercept that you want to find. You can plug these values back into the equation to compute the predicted y values.

Remember, with sklearn‘s LinearRegression() method, we can get these values with ease.

The .fit() method gives the model two variables that are useful to us:

.coef_, which contains the coefficients
.intercept_, which contains the intercept
After performing multiple linear regression, you can print the coefficients using .coef_.

Coefficients are most helpful in determining which independent variable carries more weight. For example, a coefficient of -1.345 will impact the rent more than a coefficient of 0.238, with the former impacting prices negatively and latter positively.
'''
# code ---

mlr = LinearRegression()

model=mlr.fit(x_train, y_train)

y_predict = mlr.predict(x_test)

# Input code here:

print(mlr.coef_)

print(mlr.intercept_)

'''MULTIPLE LINEAR REGRESSION
Correlations
In our Manhattan model, we used 14 variables, so there are 14 coefficients:

[ -302.73009383  1199.3859951  4.79976742  -24.28993151  24.19824177  -7.58272473  -140.90664773  48.85017415  191.4257324  -151.11453388  89.408889  -57.89714551  -19.31948556  -38.92369828 ]]
bedrooms - number of bedrooms
bathrooms - number of bathrooms
size_sqft - size in square feet
min_to_subway - distance from subway station in minutes
floor - floor number
building_age_yrs - building’s age in years
no_fee - has no broker fee (0 for fee, 1 for no fee)
has_roofdeck - has roof deck (0 for no, 1 for yes)
has_washer_dryer - has in-unit washer/dryer (0/1)
has_doorman - has doorman (0/1)
has_elevator - has elevator (0/1)
has_dishwasher - has dishwasher (0/1)
has_patio - has patio (0/1)
has_gym - has gym (0/1)
To see if there are any features that don’t affect price linearly, let’s graph the different features against rent.

Interpreting graphs

In regression, the independent variables will either have a positive linear relationship to the dependent variable, a negative linear relationship, or no relationship. A negative linear relationship means that as X values increase, Y values will decrease. Similarly, a positive linear relationship means that as X values increase, Y values will also increase.

Graphically, when you see a downward trend, it means a negative linear relationship exists. When you find an upward trend, it indicates a positive linear relationship. Here are two graphs indicating positive and negative linear relationships:

'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

# Input code here:

plt.scatter(df[['size_sqft']], df[['rent']], alpha=0.4)

plt.show()

#------------------------
"""
MULTIPLE LINEAR REGRESSION
Evaluating the Model's Accuracy
When trying to evaluate the accuracy of our multiple linear regression model, one technique we can use is Residual Analysis.

The difference between the actual value y, and the predicted value ŷ is the residual e. The equation is:

e = y - ŷ
​	 
In the StreetEasy dataset, y is the actual rent and the ŷ is the predicted rent. The real y values should be pretty close to these predicted y values.

sklearn‘s linear_model.LinearRegression comes with a .score() method that returns the coefficient of determination R² of the prediction.

The coefficient R² is defined as:

1 - u / v
​	 
where u is the residual sum of squares:

((y - y_predict) ** 2).sum()
and v is the total sum of squares (TSS):

((y - y.mean()) ** 2).sum()
The TSS tells you how much variation there is in the y variable.

R² is the percentage variation in y explained by all the x variables together.

For example, say we are trying to predict rent based on the size_sqft and the bedrooms in the apartment and the R² for our model is 0.72 — that means that all the x variables (square feet and number of bedrooms) together explain 72% variation in y (rent).

Now let’s say we add another x variable, building’s age, to our model. By adding this third relevant x variable, the R² is expected to go up. Let say the new R² is 0.95. This means that square feet, number of bedrooms and age of the building together explain 95% of the variation in the rent.
w
The best possible R² is 1.00 (and it can be negative because the model can be arbitrarily worse). Usually, a R² of 0.70 is considered good.

"""
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state=6)

mlr = LinearRegression()

model=mlr.fit(x_train, y_train)

y_predict = mlr.predict(x_test)

# Input code here:
print("Train score:")
print(mlr.score(x_train, y_train ))

print("Test score:")
print(mlr.score(x_test, y_test))

residuals = y_predict - y_test

plt.scatter(y_predict, residuals, alpha=0.4)
plt.title('Residual Analysis')

plt.show()

#-----------------------------------
"""
MULTIPLE LINEAR REGRESSION
Rebuild the Model
Now let’s rebuild the model using the new features as well as evaluate the new model to see if we improved!

For Manhattan, the scores returned:

Train score: 0.772546055982
Test score:  0.805037197536
For Brooklyn, the scores returned:

Train score: 0.613221453798
Test score:  0.584349923873
For Queens, the scores returned:

Train score: 0.665836031009
Test score:  0.665170319781
For whichever borough you used, let’s see if we can improve these scores!

Instructions
1.
Print the coefficients again to see which ones are strongest.

2.
Currently the x should look something like:

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]
Remove some of the features that don’t have strong correlations and see if your scores improved!

Post your best model in the Slack channel!

There is no right answer! Try building a model using different features!

"""

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['bedrooms', 'bathrooms', 'size_sqft',  'floor', 'building_age_yrs', 'has_roofdeck', 'has_washer_dryer',  'has_elevator', 'has_gym']]

#x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state=6)

lm = LinearRegression()

model = lm.fit(x_train, y_train)

y_predict= lm.predict(x_test)

print("Train score:")
print(lm.score(x_train, y_train))

print("Test score:")
print(lm.score(x_test, y_test))

plt.scatter(y_test, y_predict)
plt.plot(range(20000), range(20000))

plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")
plt.title("Actual Rent vs Predicted Rent")

plt.show()

# zoe_apartment = [[1, 1, 620, 16, 1, 98, 0, 0, 1, 0, 0, 0, 1, 0]]
# predict = model.predict(zoe_apartment)
# print("Predicted rent: $%.2f" % predict)

#-------Review-------------

"""
MULTIPLE LINEAR REGRESSION
Review
Great work! Let’s review the concepts before you move on:

Multiple Linear Regression uses two or more variables to make predictions about another variable:
y = b + m_{1}x_{1} + m_{2}x_{2} + ... + m_{n}x_{n}y=b+m 
1
​	 x 
1
​	 +m 
2
​	 x 
2
​	 +...+m 
n
​	 x 
n
​	 
Multiple linear regression uses a set of independent variables and a dependent variable. It uses these variables to learn how to find optimal parameters. It takes a labeled dataset and learns from it. Once we confirm that it’s learned correctly, we can then use it to make predictions by plugging in new x values.
We can use scikit-learn’s LinearRegression() to perform multiple linear regression.
Residual Analysis is used to evaluate the regression model’s accuracy. In other words, it’s used to see if the model has learned the coefficients correctly.
Scikit-learn’s linear_model.LinearRegression comes with a .score() method that returns the coefficient of determination R² of the prediction. The best score is 1.0.

"""

#------------------------------------------

'''
Regression vs. Classification
Learn about the two types of Supervised Learning algorithms.

Machine Learning is a set of many different techniques that are each suited to answering different types of questions.

We have previously divided algorithms into two groups — Supervised Learning vs Unsupervised Learning. Supervised learning algorithms use labeled data as input while unsupervised learning algorithms use unlabeled data. However, we can further distinguish machine learning algorithms by the output they produce. In terms of output, two main types of machine learning models exist: those for regression and those for classification.

Regression
Regression is used to predict outputs that are continuous. The outputs are quantities that can be flexibly determined based on the inputs of the model rather than being confined to a set of possible labels.

For example:

Predict the height of a potted plant from the amount of rainfall
Predict salary based on someone’s age and availability of high-speed internet
Predict a car’s MPG (miles per gallon) based on size and model year
Regression GIF


Linear regression is the most popular regression algorithm. It is often underrated because of its relative simplicity. In a business setting, it could be used to predict the likelihood that a customer will churn or the revenue a customer will generate. More complex models may fit this data better, at the cost of losing simplicity.

Classification
Classification is used to predict a discrete label. The outputs fall under a finite set of possible outcomes. Many situations have only two possible outcomes. This is called binary classification (True/False, 0 or 1, Hotdog / not Hotdog).

For example:

Predict whether an email is spam or not
Predict whether it will rain or not
Predict whether a user is a power user or a casual user
Multi-label classification is when there are multiple possible outcomes. It is useful for customer segmentation, image categorization, and sentiment analysis for understanding text. To perform these classifications, we use models like Naive Bayes, K-Nearest Neighbors, and SVMs.

Classification GIF


Choosing a model is a critical step in the Machine Learning process. It is important that the model fits the question at hand. When you choose the right model, you are already one step closer to getting meaningful and interesting results.
'''


#------------------------------
'''c
DISTANCE FORMULA
Representing Points
In this lesson, you will learn three different ways to define the distance between two points:

Euclidean Distance
Manhattan Distance
Hamming Distance
Before diving into the distance formulas, it is first important to consider how to represent points in your code.

In this exercise, we will use a list, where each item in the list represents a dimension of the point. For example, the point (5, 8) could be represented in Python like this:

pt1 = [5, 8]
Points aren’t limited to just two dimensions. For example, a five-dimensional point could be represented as [4, 8, 15, 16, 23].

Ultimately, we want to find the distance between two points. We’ll be writing functions that look like this:

distance([1, 2, 3], [5, 8, 9])
Note that we can only find the difference between two points if they have the same number of dimensions!
'''

'''DISTANCE FORMULA
Euclidean Distance
Euclidean Distance is the most commonly used distance formula. To find the Euclidean distance between two points, we first calculate the squared distance between each dimension. If we add up all of these squared differences and take the square root, we’ve computed the Euclidean distance.

Let’s take a look at the equation that represents what we just learned:

\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+\ldots+(a_n - b_n)^2} 
(a 
1
​	 −b 
1
​	 ) 
2
 +(a 
2
​	 −b 
2
​	 ) 
2
 +…+(a 
n
​	 −b 
n
​	 ) 
2
 
​	 
The image below shows a visual of Euclidean distance being calculated:

The Euclidean distance between two points.

d = \sqrt{(a_1-b_1)^2+(a_2-b_2)^2}d= 
(a 
1
​	 −b 
1
​	 ) 
2
 +(a 
2
​	 −b 
2
​	 ) 
2
 
​	'''

#------------------
def euclidean_distance(pt1, pt2):
  distance = 0
  for n in range(len(pt1)):
    distance += (pt1[n] - pt2[n])**2
  distance = distance ** 0.5
  return distance

print(euclidean_distance(([1,2]),([4,0])))

print(euclidean_distance(([5,4,3]),([1,7,9])))

print(euclidean_distance(([1,2]),([4,0])))

'''

DISTANCE FORMULA
Manhattan Distance
Manhattan Distance is extremely similar to Euclidean distance. Rather than summing the squared difference between each dimension, we instead sum the absolute value of the difference between each dimension. It’s called Manhattan distance because it’s similar to how you might navigate when walking city blocks. If you’ve ever wondered “how many blocks will it take me to get from point A to point B”, you’ve computed the Manhattan distance.

The equation is shown below:

\mid a_1 - b_1 \mid + \mid a_2 - b_2 \mid + \ldots + \mid a_n - b_n \mid∣a 
1
​	 −b 
1
​	 ∣+∣a 
2
​	 −b 
2
​	 ∣+…+∣a 
n
​	 −b 
n
​	 ∣
Note that Manhattan distance will always be greater than or equal to Euclidean distance. Take a look at the image below visualizing Manhattan Distance:

The Manhattan distance between two points.

d = \mid a_1 - b_1 \mid + \mid a_2 - b_2 \midd=∣a 
1
​	 −b 
1
​	 ∣+∣a 
2
​	 −b 
2
​	 ∣

'''
'''
DISTANCE FORMULA
Hamming Distance
Hamming Distance is another slightly different variation on the distance formula. Instead of finding the difference of each dimension, Hamming distance only cares about whether the dimensions are exactly equal. When finding the Hamming distance between two points, add one for every dimension that has different values.

Hamming distance is used in spell checking algorithms. For example, the Hamming distance between the word “there” and the typo “thete” is one. Each letter is a dimension, and each dimension has the same value except for one.
'''
def euclidean_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += (pt1[i] - pt2[i]) ** 2
  return distance ** 0.5

def manhattan_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += abs(pt1[i] - pt2[i])
  return distance

def hamming_distance(pt1,pt2):
  distance = 0
  for i in range(len(pt1)):
    if pt1[i] != pt2[i]:
      distance += 1
  return distance
  
 
'''
DISTANCE FORMULA
SciPy Distances
Now that you’ve written these three distance formulas yourself, let’s look at how to use them using Python’s SciPy library:

Euclidean Distance .euclidean()
Manhattan Distance .cityblock()
Hamming Distance .hamming()
There are a few noteworthy details to talk about:

First, the scipy implementation of Manhattan distance is called cityblock(). Remember, computing Manhattan distance is like asking how many blocks away you are from a point.

Second, the scipy implementation of Hamming distance will always return a number between 0 an 1. Rather than summing the number of differences in dimensions, this implementation sums those differences and then divides by the total number of dimensions. For example, in your implementation, the Hamming distance between [1, 2, 3] and [7, 2, -10] would be 2. In scipy‘s version, it would be 2/3.

'''
from scipy.spatial import distance

def euclidean_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += (pt1[i] - pt2[i]) ** 2
  return distance ** 0.5

def manhattan_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += abs(pt1[i] - pt2[i])
  return distance

def hamming_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    if pt1[i] != pt2[i]:
      distance += 1
  return distance

print(euclidean_distance([1, 2], [4, 0]))
print(manhattan_distance([1, 2], [4, 0]))
print(hamming_distance([5, 4, 9], [1, 7, 9]))

print(distance.euclidean([1, 2], [4, 0]))
print(distance.cityblock([1, 2], [4, 0]))
print(distance.hamming([5, 4, 9], [1, 7, 9]))


codecademy.com/courses/machine-learning/articles/normalization
'''
Normalization
This article describes why normalization is necessary. It also demonstrates the pros and cons of min-max normalization and z-score normalization.

Why Normalize?
Many machine learning algorithms attempt to find trends in the data by comparing features of data points. However, there is an issue when the features are on drastically different scales.

For example, consider a dataset of houses. Two potential features might be the number of rooms in the house, and the total age of the house in years. A machine learning algorithm could try to predict which house would be best for you. However, when the algorithm compares data points, the feature with the larger scale will completely dominate the other. Take a look at the image below:

Data points on the y-axis range from 0 to 20. Data points on the x-axis range from 0 to 100
When the data looks squished like that, we know we have a problem. The machine learning algorithm should realize that there is a huge difference between a house with 2 rooms and a house with 20 rooms. But right now, because two houses can be 100 years apart, the difference in the number of rooms contributes less to the overall difference.

As a more extreme example, imagine what the graph would look like if the x-axis was the cost of the house. The data would look even more squished; the difference in the number of rooms would be even less relevant because the cost of two houses could have a difference of thousands of dollars.

The goal of normalization is to make every datapoint have the same scale so each feature is equally important. The image below shows the same house data normalized using min-max normalization.

Data points on the y-axis range from 0 to 1. Data points on the x-axis range from 0 to 1
Min-Max Normalization
Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.

For example, if the minimum value of a feature was 20, and the maximum value was 40, then 30 would be transformed to about 0.5 since it is halfway between 20 and 40. The formula is as follows:

\frac{value - min}{max - min} 
max−min
value−min
​	 
Min-max normalization has one fairly significant downside: it does not handle outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before! Take a look at the image below to see an example of this.

Almost all normalized data points have an x value between 0 and 0.4
Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.

Z-Score Normalization
Z-score normalization is a strategy of normalizing data that avoids this outlier issue. The formula for Z-score normalization is below:

\frac{value - \mu}{\sigma} 
σ
value−μ
​	 
Here, μ is the mean value of the feature and σ is the standard deviation of the feature. If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature. If the unnormalized data had a large standard deviation, the normalized values will be closer to 0.

Take a look at the graph below. This is the same data as before, but this time we’re using z-score normalization.

All points have a similar range in both the x and y dimensions
While the data still looks squished, notice that the points are now on roughly the same scale for both features — almost all points are between -2 and 2 on both the x-axis and y-axis. The only potential downside is that the features aren’t on the exact same scale.

With min-max normalization, we were guaranteed to reshape both of our features to be between 0 and 1. Using z-score normalization, the x-axis now has a range from about -1.5 to 1.5 while the y-axis has a range from about -2 to 2. This is certainly better than before; the x-axis, which previously had a range of 0 to 40, is no longer dominating the y-axis.

Review
Normalizing your data is an essential part of machine learning. You might have an amazing dataset with many great features, but if you forget to normalize, one of those features might completely dominate the others. It’s like you’re throwing away almost all of your information! Normalizing solves this problem. In this article, you learned the following techniques to normalize:

Min-max normalization: Guarantees all features will have the exact same scale but does not handle outliers well.
Z-score normalization: Handles outliers, but does not produce normalized data with the exact same scale.

'''
'''
Training Set vs Validation Set vs Test Set
This article teaches the importance of splitting a data set into training, validation and test sets.

Testing Our Model
Supervised machine learning algorithms are amazing tools capable of making predictions and classifications. However, it is important to ask yourself how accurate those predictions are. After all, it’s possible that every prediction your classifier makes is actually wrong! Luckily, we can leverage the fact that supervised machine learning algorithms, by definition, have a dataset of pre-labeled datapoints. In order to test the effectiveness of your algorithm, we’ll split this data into:

training set
validation set
test set
Training Set vs Validation Set
The training set is the data that the algorithm will learn from. Learning looks different depending on which algorithm you are using. For example, when using Linear Regression, the points in the training set are used to draw the line of best fit. In K-Nearest Neighbors, the points in the training set are the points that could be the neighbors.

After training using the training set, the points in the validation set are used to compute the accuracy or error of the classifier. The key insight here is that we know the true labels of every point in the validation set, but we’re temporarily going to pretend like we don’t. We can use every point in the validation set as input to our classifier. We’ll then receive a classification for that point. We can now peek at the true label of the validation point and see whether we got it right or not. If we do this for every point in the validation set, we can compute the validation error!

Validation error might not be the only metric we’re interested in. A better way of judging the effectiveness of a machine learning algorithm is to compute its precision, recall, and F1 score.

How to Split
Figuring out how much of your data should be split into your validation set is a tricky question. If your training set is too small, then your algorithm might not have enough data to effectively learn. On the other hand, if your validation set is too small, then your accuracy, precision, recall, and F1 score could have a large variance. You might happen to get a really lucky or a really unlucky split! In general, putting 80% of your data in the training set, and 20% of your data in the validation set is a good place to start.

N-Fold Cross-Validation
Sometimes your dataset is so small, that splitting it 80/20 will still result in a large amount of variance. One solution to this is to perform N-Fold Cross-Validation. The central idea here is that we’re going to do this entire process N times and average the accuracy. For example, in 10-fold cross-validation, we’ll make the validation set the first 10% of the data and calculate accuracy, precision, recall and F1 score. We’ll then make the validation set the second 10% of the data and calculate these statistics again. We can do this process 10 times, and every time the validation set will be a different chunk of the data. If we then average all of the accuracies, we will have a better sense of how our model does on average.

Cross Validation


Changing The Model / Test Set
Understanding the accuracy of your model is invaluable because you can begin to tune the parameters of your model to increase its performance. For example, in the K-Nearest Neighbors algorithm, you can watch what happens to accuracy as you increase or decrease K. (You can try out all of this in our K-Nearest Neighbors lesson!)

Once you’re happy with your model’s performance, it is time to introduce the test set. This is part of your data that you partitioned away at the very start of your experiment. It’s meant to be a substitute for the data in the real world that you’re actually interested in classifying. It functions very similarly to the validation set, except you never touched this data while building or tuning your model. By finding the accuracy, precision, recall, and F1 score on the test set, you get a good understanding of how well your algorithm will do in the real world.

'''
'''
ACCURACY, RECALL, PRECISION, AND F1 SCORE
Accuracy
After creating a machine learning algorithm capable of making classifications, the next step in the process is to calculate its predictive power. In order to calculate these statistics, we’ll need to split our data into a training set and validation set.

Let’s say you’re using a machine learning algorithm to try to predict whether or not you will get above a B on a test. The features of your data could be something like:

The number of hours you studied this week.
The number of hours you watched Netflix this week.
The time you went to bed the night before the test.
Your average in the class before taking the test.
The simplest way of reporting the effectiveness of an algorithm is by calculating its accuracy. Accuracy is calculated by finding the total number of correctly classified points and dividing by the total number of points.

In other words, accuracy can be defined as:

(True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives)

Let’s define those terms in the context of our grade example :

True Positive: The algorithm predicted you would get above a B, and you did.
True Negative: The algorithm predicted you would get below a B, and you did.
False Positive: The algorithm predicted you would get above a B, and you didn’t.
False Negative: The algorithm predicted you would get below a B, and you didn’t.
Let’s calculate the accuracy of a classification algorithm!

ACCURACY, RECALL, PRECISION, AND F1 SCORE
Recall
Accuracy can be an extremely misleading statistic depending on your data. Consider the example of an algorithm that is trying to predict whether or not there will be over 3 feet of snow on the ground tomorrow. We can write a pretty accurate classifier right now: always predict False. This classifier will be incredibly accurate — there are hardly ever many days with that much snow. But this classifier never finds the information we’re actually interested in.

In this situation, the statistic that would be helpful is recall. Recall measures the percentage of relevant items that your classifier found. In this example, recall is the number of snow days the algorithm correctly predicted divided by the total number of snow days. Another way of saying this is:

True Positives / (True Positives + False Negatives)
Our algorithm that always predicts False might have a very high accuracy, but it never will find any True Positives, so its recall is 0. This makes sense; recall should be very low for such an absurd classifier.

Precision
Unfortunately, recall isn’t a perfect statistic either. For example, we could create a snow day classifier that always returns True. This would have low accuracy, but its recall would be 1 because it would be able to accurately find every snow day. But this classifier is just as nonsensical as the one before! The statistic that will help demonstrate that this algorithm is flawed is precision.

In the snow day example, precision is the number of snow days the algorithm correctly predicted divided by the number of times it predicted there would be a snow day. The formula for precision is below:

True Positives / (True Positives + False Positives)
The algorithm that predicts every day is a snow day has recall of 1, but it will have very low precision. It correctly predicts every snow day, but there are tons of false positives as well.

Precision and recall are statistics that are on opposite ends of a scale. If one goes down, the other will go up.

F1 Score
It is useful to consider the precision and recall of an algorithm, however, we still don’t have one number that can sufficiently describe how effective our algorithm is. This is the job of the F1 score — F1 score is the harmonic mean of precision and recall. The harmonic mean of a group of numbers is a way to average them together. The formula for F1 score is below:

F1 = 2 \* (precision \* recall) / (precision + recall)
The F1 score combines both precision and recall into a single statistic. We use the harmonic mean rather than the traditional arithmetic mean because we want the F1 score to have a low value when either precision or recall is 0.

For example, consider a classifier where recall = 1 and precision = 0.01. We know that there is most likely a problem with this classifier since the precision is so low, and so we want the F1 score to reflect that.

If we took the arithmetic mean, we’d get:

(1 + 0.01) / 2 = 0.505
That looks way too high! But if we calculate the harmonic mean, we get:

2 * (1 * 0.01) / (1 + 0.01) = 0.019
That’s much better! The F1 score is now accurately describing the effectiveness of this classifier.

Review
You’ve now learned many different ways to analyze the predictive power of your algorithm. Some of the key insights for this course include:

Classifying a single point can result in a true positive (truth = 1, guess = 1), a true negative (truth = 0, guess = 0), a false positive (truth = 0, guess = 1), or a false negative (truth = 1, guess = 0).
Accuracy measures how many classifications your algorithm got correct out of every classification it made.
Recall measures the percentage of the relevant items your classifier was able to successfully find.
Precision measures the percentage of items your classifier found that were actually relevant.
Precision and recall are tied to each other. As one goes up, the other will go down.
F1 score is a combination of precision and recall.
F1 score will be low if either precision or recall is low.
The decision to use precision, recall, or F1 score ultimately comes down to the context of your classification. Maybe you don’t care if your classifier has a lot of false positives. If that’s the case, precision doesn’t matter as much.

As long as you have an understanding of what question you’re trying to answer, you should be able to determine which statistic is most relevant to you.

The Python library scikit-learn has some functions that will calculate these statistics for you!


'''


#---------------- code 1 -----------------------

labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]
guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]

true_positives = 0
true_negatives = 0
false_positives = 0
false_negatives = 0

for i in range(len(guesses)):
  #True Positives
  if labels[i] == 1 and guesses[i] == 1:
    true_positives += 1
  #True Negatives
  if labels[i] == 0 and guesses[i] == 0:
    true_negatives += 1
  #False Positives
  if labels[i] == 0 and guesses[i] == 1:
    false_positives += 1
  #False Negatives
  if labels[i] == 1 and guesses[i] == 0:
    false_negatives += 1
    
accuracy = (true_positives + true_negatives) / len(guesses)
print(accuracy)

recall = true_positives / (true_positives + false_negatives)
print(recall)

precision = true_positives / (true_positives + false_positives)
print(precision)

f_1 = 2*precision*recall / (precision + recall)
print(f_1)

'''
Python’s scikit-learn library has functions that will find accuracy, recall, precision, and F1 score for you. They all take two parameters — a list of the true labels and a list of the predicted classifications.

Call accuracy_score() using the correct parameters and print the results.

Call the three other functions and print the results. The name of those functions are:

recall_score()
precision_score()
f1_score()

'''

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]
guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]

print(accuracy_score(labels, guesses))
print(recall_score(labels, guesses))
print(precision_score(labels, guesses))
print(f1_score(labels, guesses))



'''
The Dangers of Overfitting
Learn about how to recognize when your model is fitting too closely to the training data.

Often in Machine Learning, we feed a huge amount of data to an algorithm that then learns how to classify that input based on rules it creates. The data we feed into this algorithm, the training data, is hugely important. The rules created by the program will be determined by looking at every example in the training data.

Overfitting occurs when we have fit our model’s parameters too closely to the training data:

Image of overfitting

When we overfit, we are assuming that everything we see in the training data is exactly how it will appear in the real world. Instead, we want to be modeling trends that show us the general behavior of a variable:

Image of good fit

That said, when we find trends in training data, all we are doing is replicating trends that already exist. Our model will learn to replicate data from the real world. If that data is part of a system that results in prejudices or injustices, then your machine learning algorithm will produce harmful results as well. Some people say that Machine Learning can be a GIGO process — Garbage In, Garbage Out.

We can imagine an example where an ad agency is creating an algorithm to display the right job recommendations to the right people. If they use a training set of the kinds of people who have high paying jobs to determine which people to show ads for high paying jobs to, the model will probably learn to make decisions that leave out historically underrepresented groups of people.

This problem is fundamentally a problem with overfitting to our training set. If we overfit to training sets with underrepresentation, we only create more underrepresentation. How do we tackle this problem?

Inspect Training Data First
Find the important aggregate statistics for the variables you’re measuring. Find the mean and median of different variables. Use groupby to inspect the aggregate statistics for different groups of data, and see how they differ. These are the trends that your machine learning model will replicate.

Visualize your training data and look for outstanding patterns.

Compare the aggregate statistics from your specific training set to aggregate statistics from other sources. Does your training set seem to follow the trends that are universally present?

Collect Data Thoughtfully
If you have the power to control the way your data is collected, i.e. if you’re the one collecting the data, make sure that you are sampling from all groups.

Imagine for a massively multiplayer app, rewards and hotspots are set by an algorithm that trains on frequencies of user actions. If the people using the app overwhelmingly are in one area, the app will continuously get better and better for people in that area.

Some neighborhoods/areas might be undersampled, or have significantly less datapoints, so the algorithm will fit to the oversampled population. Imagine this clustering forming:

bad clustering

The small cluster in the bottom left would probably be a cluster of its own, if it had a comparable amount of samples to the other two clusters. To solve this, we can specifically oversample areas that are undersampled, and add more datapoints there. Conversely, we can undersample groups that are over-represented in our training set.

Try to Augment the Training Data
In our Bayes’ Theorem lesson we discussed that when we have a small total number of an event, this will affect how reliably we can guess if the event will occur. Many systems built to detect fraud suffer from this problem. Suppose we were creating a machine learning model to detect fraudulent credit card activity. On the aggregate, there are very few fraudulent transactions, so the model can reach a very high accuracy by simply predicting that every transaction is legitimate. This approach would not solve our problem very well.

One technique is to identify a fraudulent transaction and make many copies of it in the training set, with small variations in the feature data. We can imagine that if our training set has only 2 examples of fraud, the algorithm will overfit to only identify a transaction as fraudulent if it has the exact characteristics of those couple of examples. When we augment the training data with more fraudulent examples, mildly altered from the ones we know, we reduce the amount of overfitting.

Data augmentation is used most often in image classification techniques. Often, we add copies of each picture with an added rotation, shearing, or color jittering.

Let’s imagine we have a huge dataset of animals, and we’re trying to classify which animal is which. We may only have one instance of an alpaca:

alpaca

but we know that this image, sheared:

alpaca sheared

and this image rotated:

alpaca upside-down

are all also examples of an alpaca. When we add these examples of augmented data to our training set, the model won’t overfit as much.

Try Restricting the Featureset
If one of your features is more heavily affecting the parameters of the model, try to run your model without that feature.

For example, let’s say you are writing a program to determine if someone’s loan application should be accepted or rejected. Your model shows that the most significant variable is their race — with all other features the same, the model has a much higher chance of producing an “accept” prediction on an application from a white applicant than on a non-white applicant. This parameter weight may be a sign that the training data contained racial bias. We can try to train the model again, with the race data removed from the featureset.

Reflection
Machine Learning algorithms always must introduce a bias as a function of being programs that are trying to make assumptions and rules by looking at data.

Sometimes the best way to deal with the introduction of bias in a training set is to just acknowledge that it is there. As we try to compensate for the bias, our methods of compensation themselves introduce a bias. It is important to find a balance. The most important thing is to mention the existence of bias in your results, and make sure that all stakeholders know that it exists, so that it is taken into consideration with the decisions made from your model’s results.'''








'''
K-NEAREST NEIGHBORS
K-Nearest Neighbors Classifier
K-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories.

Consider the image to the right. This image is complicated, but for now, let’s just focus on where the data points are being placed. Every data point — whether its color is red, green, or white — has an x value and a y value. As a result, it can be plotted on this two-dimensional graph.

Next, let’s consider the color of the data. The color represents the class that the K-Nearest Neighbor algorithm is trying to classify. In this image, data points can either have the class green or the class red. If a data point is white, this means that it doesn’t have a class yet. The purpose of the algorithm is to classify these unknown points.

Finally, consider the expanding circle around the white point. This circle is finding the k nearest neighbors to the white point. When k = 3, the circle is fairly small. Two of the three nearest neighbors are green, and one is red. So in this case, the algorithm would classify the white point as green. However, when we increase k to 5, the circle expands, and the classification changes. Three of the nearest neighbors are red and two are green, so now the white point will be classified as red.

This is the central idea behind the K-Nearest Neighbor algorithm. If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it’s nearest neighbors, and classify it.

Instructions
Before moving on to the next exercise, consider the image below:

If k = 1, what would the class of the question mark be?
If k = 5, what would it be?
2D Visualization
Note that rather than using colors, in this image, the class is denoted by the shape of each point.

'''
#cheat sheet ---------------------------------

'''
K-Nearest Neighbors
The K-Nearest Neighbors algorithm is a supervised machine learning algorithm for labeling an unknown data point given existing labeled data.

The nearness of points is typically determined by using distance algorithms such as the Euclidean distance formula based on parameters of the data. The algorithm will classify a point based on the labels of the K nearest neighbor points, where the value of K can be specified.

Elbow Curve Validation Technique in K-Nearest Neighbor Algorithm
Choosing an optimal k value in KNN determines the number of neighbors we look at when we assign a value to any new observation.

For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train and validation set. When k increases, validation error decreases and then starts increasing in a “U” shape. An optimal value of k can be determined from the elbow curve of the validation error.

K-Nearest Neighbors Underfitting and Overfitting
The value of k in the KNN algorithm is related to the error rate of the model. A small value of k could lead to overfitting as well as a big value of k can lead to underfitting. Overfitting imply that the model is well on the training data but has poor performance when new data is coming. Underfitting refers to a model that is not good on the training data and also cannot be generalized to predict new data.

KNN Classification Algorithm in Scikit Learn
Scikit-learn is a very popular Machine Learning library in Python which provides a KNeighborsClassifier object which performs the KNN classification. The n_neighbors parameter passed to the KNeighborsClassifier object sets the desired k value that checks the k closest neighbors for each unclassified point.

The object provides a .fit() method which takes in training data and a .predict() method which returns the classification of a set of data points.

from sklearn.neighbors import KNeighborsClassifier

KNNClassifier = KNeighborsClassifier(n_neighbors=5)
KNNClassifier.fit(X_train, y_train)
KNNClassifier.predict(X_test)
Euclidean Distance
The Euclidean Distance between two points can be computed, knowing the coordinates of those points.

On a 2-D plane, the distance between two points p and q is the square-root of the sum of the squares of the difference between their x and y components. Remember the Pythagorean Theorem: a^2 + b^2 = c^2 ?

We can write a function to compute this distance. Let’s assume that points are represented by tuples of the form (x_coord, y_coord). Also remember that computing the square-root of some value n can be done in a couple of ways: math.sqrt(n), using the math library, or n ** 0.5 (n raised to the power of 1/2).

def distance(p1, p2):
  x_diff_squared = (p1[0] - p2[0]) ** 2
  y_diff_squared = (p1[1] - p2[1]) ** 2
  return (x_diff_squared + y_diff_squared) ** 0.5

distance( (0, 0), (3, 4) )      # => 5.0

'''
'''

Distance Between Points - 2D
In the first exercise, we were able to visualize the dataset and estimate the k nearest neighbors of an unknown point. But a computer isn’t going to be able to do that!

We need to define what it means for two points to be close together or far apart. To do this, we’re going to use the Distance Formula.

For this example, the data has two dimensions:

The length of the movie
The movie’s release date
Consider Star Wars and Raiders of the Lost Ark. Star Wars is 125 minutes long and was released in 1977. Raiders of the Lost Ark is 115 minutes long and was released in 1981.

The distance between the movies is computed below:

distance formula example

K-NEAREST NEIGHBORS
Distance Between Points - 3D
Making a movie rating predictor based on just the length and release date of movies is pretty limited. There are so many more interesting pieces of data about movies that we could use! So let’s add another dimension.

Let’s say this third dimension is the movie’s budget. We now have to find the distance between these two points in three dimensions.

3D graph
What if we’re not happy with just three dimensions? Unfortunately, it becomes pretty difficult to visualize points in dimensions higher than 3. But that doesn’t mean we can’t find the distance between them.

The generalized distance formula between points A and B is as follows:

\sqrt{(A_1-B_1)^2+(A_2-B_2)^2+ \dots+(A_n-B_n)^2} 

​	 
Here, A1-B1 is the difference between the first feature of each point. An-Bn is the difference between the last feature of each point.

Using this formula, we can find the K-Nearest Neighbors of a point in N-dimensional space! We now can use as much information about our movies as we want.

We will eventually use these distances to find the nearest neighbors to an unlabeled point.

Data with Different Scales: Normalization
In the next three lessons, we’ll implement the three steps of the K-Nearest Neighbor Algorithm:

Normalize the data
Find the k nearest neighbors
Classify the new point based on those neighbors
When we added the dimension of budget, you might have realized there are some problems with the way our data currently looks.

Consider the two dimensions of release date and budget. The maximum difference between two movies’ release dates is about 125 years (The Lumière Brothers were making movies in the 1890s). However, the difference between two movies’ budget can be millions of dollars.

The problem is that the distance formula treats all dimensions equally, regardless of their scale. If two movies came out 70 years apart, that should be a pretty big deal. However, right now, that’s exactly equivalent to two movies that have a difference in budget of 70 dollars. The difference in one year is exactly equal to the difference in one dollar of budget. That’s absurd!

Another way of thinking about this is that the budget completely outweighs the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 70 years apart is essentially meaningless compared to the difference in millions in the other dimension.

The solution to this problem is to normalize the data so every value is between 0 and 1. In this lesson, we’re going to be using min-max normalization.

'''
release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]

def min_max_normalize(lst):
  minimum = min(lst)
  maximum = max(lst)
  normalized = []
  for i in range(len(lst)):
    normalized.append((lst[i] - minimum) / (maximum - minimum))
  return normalized

print(min_max_normalize(release_dates))

'''
K-NEAREST NEIGHBORS
Finding the Nearest Neighbors
The K-Nearest Neighbor Algorithm:

Normalize the data
Find the k nearest neighbors
Classify the new point based on those neighbors
Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!

To do this, we want to find the k nearest neighbors of the unclassified point. In a few exercises, we’ll learn how to properly choose k, but for now, let’s choose a number that seems somewhat reasonable. Let’s choose 5.

In order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.

It might look something like this:

[
  [0.30, 'Superman II'],
  [0.31, 'Finding Nemo'],
  ...
  ...
  [0.38, 'Blazing Saddles']
]
In this example, the unknown movie has a distance of 0.30 to Superman II.

In the next exercise, we’ll use the labels associated with these movies to classify the unlabeled point.
'''
#--------------------------------
'''
Instructions
1.
Begin by running the program. We’ve imported and normalized a movie dataset for you and printed the data for the movie Bruce Almighty. Each movie in the dataset has three features:

the normalized budget (dollars)
the normalized duration (minutes)
the normalized release year.
We’ve also imported the labels associated with every movie in the dataset. The label associated with Bruce Almighty is a 0, indicating that it is a bad movie. Remember, a bad movie had a rating less than 7.0 on IMDb.

Comment out the two print lines after you have run the program.

If you want to see some more of the data, the following line of code will print 20 movies along with their data.

print(list(movie_dataset.items())[:20])
2.
Create a function called classify that has three parameters: the data point you want to classify named unknown, the dataset you are using to classify it named dataset, and k, the number of neighbors you are interested in.

For now put pass inside your function.

def classify(unknown, dataset, k):
'''

from movies import movie_dataset, movie_labels

print(movie_dataset['Bruce Almighty'])
print(movie_labels['Bruce Almighty'])

#print(list(movie_dataset.items())[:20])
#print(list(movie_labels.items())[:20])

#print(len(movie_dataset))

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def classify(unknown, dataset, k):
  distances = []
  
  for title in dataset:
    distance_to_point = distance(dataset[title], unknown)
    distances.append([distance_to_point, title])
  
  distances.sort()
  
  neighbors = distances[0:k]
  
  return neighbors

print(classify([.4, .2, .9], movie_dataset, 5) )
'''
K-NEAREST NEIGHBORS
Count Neighbors
The K-Nearest Neighbor Algorithm:

Normalize the data
Find the k nearest neighbors
Classify the new point based on those neighbors
We’ve now found the k nearest neighbors, and have stored them in a list that looks like this:

[
  [0.083, 'Lady Vengeance'],
  [0.236, 'Steamboy'],
  ...
  ...
  [0.331, 'Godzilla 2000']
]
Our goal now is to count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad.

In order to find the class of each of the labels, we’ll need to look at our movie_labels dataset. For example, movie_labels['Akira'] would give us 1 because Akira is classified as a good movie.

You may be wondering what happens if there’s a tie. What if k = 8 and four neighbors were good and four neighbors were bad? There are different strategies, but one way to break the tie would be to choose the class of the closest point.
'''
from movies import movie_dataset, movie_labels

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def classify(unknown, dataset, labels, k ):
  distances = []
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]

  print(neighbors)

  num_good = 0
  num_bad = 0
  title = 0

  for movie in neighbors:
    title = movie[1]
    if movie_labels[title] == 1:
      num_good += 1
    else:
      num_bad += 1


  if num_good > num_bad:
    return 1
  else:
    return 0

print(classify([.45, .2, .5], movie_dataset, movie_labels, 5))

print(classify([.4, .2, .9], movie_dataset, movie_labels, 5))

#------------------------------
'''
K-NEAREST NEIGHBORS
Classify Your Favorite Movie
Nice work! Your classifier is now able to predict whether a movie will be good or bad. So far, we’ve only tested this on a completely random point [.4, .2, .9]. In this exercise we’re going to pick a real movie, normalize it, and run it through our classifier to see what it predicts!

In the instructions below, we are going to be testing our classifier using the 2017 movie Call Me By Your Name. Feel free to pick your favorite movie instead!

Instructions
1.
To begin, we want to make sure the movie that we want to classify isn’t already in our database. This is important because we don’t want one of the nearest neighbors to be itself!

You can do this by using the in keyword.

Begin by printing if the title of your movie is in movie_dataset. This should print False.

For Call Me By Your Name, we would do the following:

print("Call Me By Your Name" in movie_dataset)
2.
Once you confirm your movie is not in your database, we need to make a datapoint for your movie. Create a variable named my_movie and set it equal to a list of three numbers. They should be:

The movie’s budget (dollars)
The movie’s runtime (minutes)
The year the movie was released
Make sure to put the information in that order.

If you want to use Call Me By Your Name, the budget was 350,000 dollars, the runtime was 132 minutes, and the movie was released in 2017.

For Call Me By Your Name, our code looks like this:

my_movie = [3500000, 132, 2017]
3.
Next, we want to normalize this datapoint. We’ve included the function normalize_point which takes a datapoint as a parameter and returns the point normalized. Create a variable called normalized_my_movie and set it equal to the normalized value of my_movie. Print the result!

The call to the function should look like this:

normalized_my_movie = normalize_point(my_movie)
4.
Finally, call classify with the following parameters:

normalized_my_movie
movie_dataset
movie_labels
5
Print the result? Did your classifier think your movie was good or bad?

classify(normalized_my_movie, movie_dataset, movie_labels, 5)

'''
from movies import movie_dataset, movie_labels, normalize_point

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def classify(unknown, dataset, labels, k):
  distances = []
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]
  num_good = 0
  num_bad = 0
  for neighbor in neighbors:
    title = neighbor[1]
    if labels[title] == 0:
      num_bad += 1
    elif labels[title] == 1:
      num_good += 1
  if num_good > num_bad:
    return 1
  else:
    return 0

name = "Call Me By Your Name"

print(name in movie_dataset)

my_movie = [350000, 132, 2017]

normalized_my_movie = normalize_point(my_movie)

print(classify(normalized_my_movie,movie_dataset,movie_labels, 5 ))


'''
K-NEAREST NEIGHBORS
Training and Validation Sets
You’ve now built your first K Nearest Neighbors algorithm capable of classification. You can feed your program a never-before-seen movie and it can predict whether its IMDb rating was above or below 7.0. However, we’re not done yet. We now need to report how effective our algorithm is. After all, it’s possible our predictions are totally wrong!

As with most machine learning algorithms, we have split our data into a training set and validation set.

Once these sets are created, we will want to use every point in the validation set as input to the K Nearest Neighbor algorithm. We will take a movie from the validation set, compare it to all the movies in the training set, find the K Nearest Neighbors, and make a prediction. After making that prediction, we can then peek at the real answer (found in the validation labels) to see if our classifier got the answer correct.

If we do this for every movie in the validation set, we can count the number of times the classifier got the answer right and the number of times it got it wrong. Using those two numbers, we can compute the validation accuracy.

Validation accuracy will change depending on what K we use. In the next exercise, we’ll use the validation accuracy to pick the best possible K for our classifier.

Instructions
1.
We’ve imported training_set, training_labels, validation_set, and validation_labels. Let’s take a look at one of the movies in validation_set.

The movie "Bee Movie" is in validation_set. Print out the data associated with Bee Movie. Print Bee Movie ‘s label as well (which can be found in validation_labels).

Is Bee Movie a good or bad movie?

To print the data about Bee Movie, do the following:

print(validation_set["Bee Movie"])
Do the same for Bee Movie‘s label.

2.
Let’s have our classifier predict whether Bee Movie is good or bad using k = 5. Call the classify function using the following parameters:

Bee Movie‘s data
training_set
training_labels
5
Store the results in a variable named guess and print guess.

Bee Movie‘s data can be found using validation_set["Bee Movie"]. Use that as the first parameter of the classify function.

3.
Let’s check to see if our classification got it right. If guess is equal to Bee Movie‘s real class (found in validation_labels), print "Correct!". Otherwise, print "Wrong!".

To check if the guess was right, use:

if guess == validation_labels["Bee Movie"]:

'''

from movies import training_set, training_labels, validation_set, validation_labels

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def classify(unknown, dataset, labels, k):
  distances = []
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]
  num_good = 0
  num_bad = 0
  for neighbor in neighbors:
    title = neighbor[1]
    if labels[title] == 0:
      num_bad += 1
    elif labels[title] == 1:
      num_good += 1
  if num_good > num_bad:
    return 1
  else:
    return 0

name = 'Bee Movie'
print(validation_set[name])
print(validation_labels[name])

guess = classify(validation_set[name], training_set, training_labels, 5)

print (guess)

if guess == validation_labels[name]:
  print('Correct!')
else:
  print('Wrong!') 

'''
K-NEAREST NEIGHBORS
Choosing K
In the previous exercise, we found that our classifier got one point in the training set correct. Now we can test every point to calculate the validation accuracy.

The validation accuracy changes as k changes. The first situation that will be useful to consider is when k is very small. Let’s say k = 1. We would expect the validation accuracy to be fairly low due to overfitting. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point. Consider the image below.

colored dots with a single outlier

The dark blue point in the top left corner of the graph looks like a fairly significant outlier. When k = 1, all points in that general area will be classified as dark blue when it should probably be classified as green. Our classifier has relied too heavily on the small quirks in the training data.

On the other hand, if k is very large, our classifier will suffer from underfitting. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set. Imagine you have 100 points in your training set and you set k = 100. Every single unknown point will be classified in the same exact way. The distances between the points don’t matter at all! This is an extreme example, however, it demonstrates how the classifier can lose understanding of the training data if k is too big.'''
'''
Instructions
1.
Begin by creating a function called find_validation_accuracy that takes five parameters. The parameters should be training_set, training_labels, validation_set, validation_labels, and k.

def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):
2.
Create a variable called num_correct and have it begin at 0.0. Loop through the movies of validation_set, and call classify using each movie’s data, the training_set, the training_labels, and k. Store the result in a variable called guess. For now, return guess outside of your loop.

Remember, the movie’s data can be found by using validation_set[title].

Your for loop should look like

for title in validation_set:
Inside the for loop, you can call classify using

classify(validation_set[title], training_set, training_labels, k)
3.
Inside the for loop, compare guess to the corresponding label in validation_labels. If they were equal, add 1 to num_correct. For now, outside of the for loop, return num_correct

The label that you want to compare guess to is validation_labels[title].

4.
Outside the for loop return the validation error. This should be num_correct divided by the total number of points in the validation set.

len(validation_set) will give you the number of points in the validation set.

5.
Call find_validation_accuracy with k = 3. Print the results The code should take a couple of seconds to run.

print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, 3))'''

from movies import training_set, training_labels, validation_set, validation_labels

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def classify(unknown, dataset, labels, k):
  distances = []
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]
  num_good = 0
  num_bad = 0
  for neighbor in neighbors:
    title = neighbor[1]
    if labels[title] == 0:
      num_bad += 1
    elif labels[title] == 1:
      num_good += 1
  if num_good > num_bad:
    return 1
  else:
    return 0

def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):
  num_correct = 0.0
  for movie in validation_set:
    guess = classify(validation_set[movie], training_set, training_labels, k)
    if guess == validation_labels[movie]:
      num_correct += 1
  return num_correct / len(validation_set)

print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels,3))

'''
K-NEAREST NEIGHBORS
Graph of K
The graph to the right shows the validation accuracy of our movie classifier as k increases. When k is small, overfitting occurs and the accuracy is relatively low. On the other hand, when k gets too large, underfitting occurs and accuracy starts to drop.

--------------------------------------------------------------

K-NEAREST NEIGHBORS
Using sklearn
You’ve now written your own K-Nearest Neighbor classifier from scratch! However, rather than writing your own classifier every time, you can use Python’s sklearn library. sklearn is a Python library specifically used for Machine Learning. It has an amazing number of features, but for now, we’re only going to investigate its K-Nearest Neighbor classifier.

There are a couple of steps we’ll need to go through in order to use the library. First, you need to create a KNeighborsClassifier object. This object takes one parameter - k. For example, the code below will create a classifier where k = 3

classifier = KNeighborsClassifier(n_neighbors = 3)
Next, we’ll need to train our classifier. The .fit() method takes two parameters. The first is a list of points, and the second is the labels associated with those points. So for our movie example, we might have something like this

training_points = [
  [0.5, 0.2, 0.1],
  [0.9, 0.7, 0.3],
  [0.4, 0.5, 0.7]
]

training_labels = [0, 1, 1]
classifier.fit(training_points, training_labels)
Finally, after training the model, we can classify new points. The .predict() method takes a list of points that you want to classify. It returns a list of its guesses for those points.

unknown_points = [
  [0.2, 0.1, 0.7],
  [0.4, 0.7, 0.6],
  [0.5, 0.8, 0.1]
]

guesses = classifier.predict(unknown_points)'''

from movies import movie_dataset, labels
from sklearn.neighbors import KNeighborsClassifier

classifier = KNeighborsClassifier(n_neighbors  = 5)

training_points = movie_dataset
training_labels  = labels

classifier.fit(training_points, training_labels)

unknown_points = [
[.45, .2, .5],
[.25, .8, .9],
[.1, .1, .9]]

print(classifier.predict(unknown_points))

#-----------------------------------------------------
'''
K-NEAREST NEIGHBORS
Review
Congratulations! You just implemented your very own classifier from scratch and used Python’s sklearn library. In this lesson, you learned some techniques very specific to the K-Nearest Neighbor algorithm, but some general machine learning techniques as well. Some of the major takeaways from this lesson include:

Data with n features can be conceptualized as points lying in n-dimensional space.
Data points can be compared by using the distance formula. Data points that are similar will have a smaller distance between them.
A point with an unknown class can be classified by finding the k nearest neighbors
To verify the effectiveness of a classifier, data with known classes can be split into a training set and a validation set. Validation error can then be calculated.
Classifiers have parameters that can be tuned to increase their effectiveness. In the case of K-Nearest Neighbors, k can be changed.
A classifier can be trained improperly and suffer from overfitting or underfitting. In the case of K-Nearest Neighbors, a low k often leads to overfitting and a large k often leads to underfitting.
Python’s sklearn library can be used for many classification and machine learning algorithms.
To the right is an interactive visualization of K-Nearest Neighbors. If you move your mouse over the canvas, the location of your mouse will be classified as either green or blue. The nearest neighbors to your mouse are highlighted in yellow. Use the slider to change k to see how the boundaries of the classification change.

If you find any interesting patterns, share it with us on Twitter!
'''
'''
K-NEAREST NEIGHBOR REGRESSOR
Regression
The K-Nearest Neighbors algorithm is a powerful supervised machine learning algorithm typically used for classification. However, it can also perform regression.

In this lesson, we will use the movie dataset that was used in the K-Nearest Neighbors classifier lesson. However, instead of classifying a new movie as either good or bad, we are now going to predict its IMDb rating as a real number.

This process is almost identical to classification, except for the final step. Once again, we are going to find the k nearest neighbors of the new movie by using the distance formula. However, instead of counting the number of good and bad neighbors, the regressor averages their IMDb ratings.

For example, if the three nearest neighbors to an unrated movie have ratings of 5.0, 9.2, and 6.8, then we could predict that this new movie will have a rating of 7.0.
'''



from movies import movie_dataset, movie_ratings

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def predict(unknown, dataset, movie_ratings, k):
  distances = []
  sum_rating = 0
  avg_rating = 0
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]


  for neighbor in neighbors:
    title = neighbor[1]
    rating = movie_ratings[title]
    sum_rating += rating
  avg_rating = sum_rating / (len(neighbors))
  return avg_rating



print(movie_dataset['Life of Pi'])
print(movie_ratings['Life of Pi'])
print(predict([0.016, 0.300, 1.022], movie_dataset, movie_ratings, 5))

'''
K-NEAREST NEIGHBOR REGRESSOR
Weighted Regression
We’re off to a good start, but we can be even more clever in the way that we compute the average. We can compute a weighted average based on how close each neighbor is.

Let’s say we’re trying to predict the rating of movie X and we’ve found its three nearest neighbors. Consider the following table:

Movie	Rating	Distance to movie X
A	5.0	3.2
B	6.8	11.5
C	9.0	1.1

If we find the mean, the predicted rating for X would be 6.93. However, movie X is most similar to movie C, so movie C’s rating should be more important when computing the average. Using a weighted average, we can find movie X’s rating:

(5.0/3.2 + 6.8 / 11.5 + 9.0 / 1.1) / (1/3.2 + 1 / 11.5 + 1 / 1.1) = 7.9

The numerator is the sum of every rating divided by their respective distances. The denominator is the sum of one over every distance. Even though the ratings are the same as before, the weighted average has now gone up to 7.9.

'''
from movies import movie_dataset, movie_ratings

def distance(movie1, movie2):
  squared_difference = 0
  for i in range(len(movie1)):
    squared_difference += (movie1[i] - movie2[i]) ** 2
  final_distance = squared_difference ** 0.5
  return final_distance

def predict(unknown, dataset, movie_ratings, k):
  distances = []
  numerator = 0
  denominator = 0
  #Looping through all points in the dataset
  for title in dataset:
    movie = dataset[title]
    distance_to_point = distance(movie, unknown)
    #Adding the distance and point associated with that distance
    distances.append([distance_to_point, title])
  distances.sort()
  #Taking only the k closest points
  neighbors = distances[0:k]
  for neighbor in neighbors:
        numerator += (movie_ratings[neighbor[1]] )/ neighbor[0] 
    denominator += 1/neighbor[0]
  return numerator / denominator

print(predict([0.016, 0.300, 1.022], movie_dataset, movie_ratings, k = 5 ))

'''
K-NEAREST NEIGHBOR REGRESSOR
Scikit-learn
Now that you’ve written your own K-Nearest Neighbor regression model, let’s take a look at scikit-learn’s implementation. The KNeighborsRegressor class is very similar to KNeighborsClassifier.

We first need to create the regressor. We can use the parameter n_neighbors to define our value for k.

We can also choose whether or not to use a weighted average using the parameter weights. If weights equals "uniform", all neighbors will be considered equally in the average. If weights equals "distance", then a weighted average is used.
'''
classifier = KNeighborsRegressor(n_neighbors = 3, weights = "distance")'''
Next, we need to fit the model to our training data using the .fit() method. .fit() takes two parameters. The first is a list of points, and the second is a list of values associated with those points.
'''
training_points = [
  [0.5, 0.2, 0.1],
  [0.9, 0.7, 0.3],
  [0.4, 0.5, 0.7]
]

training_labels = [5.0, 6.8, 9.0]
classifier.fit(training_points, training_labels)'''
Finally, we can make predictions on new data points using the .predict() method. .predict() takes a list of points and returns a list of predictions for those points.
'''
unknown_points = [
  [0.2, 0.1, 0.7],
  [0.4, 0.7, 0.6],
  [0.5, 0.8, 0.1]
]

guesses = classifier.predict(unknown_points)'''
'''

#--- code ---
from movies import movie_dataset, movie_ratings
from sklearn.neighbors import KNeighborsRegressor

regressor = KNeighborsRegressor(n_neighbors = 5, weights = "distance")
regressor.fit(movie_dataset, movie_ratings)

#print(regressor.predict([0.016, 0.300, 1.022]))
#print(regressor.predict([0.0004092981, 0.283, 1.0112]))
#print(regressor.predict([0.00687649, 0.235, 1.0112]))

print(regressor.predict([
  
  [0.00687649, 0.235, 1.0112],
  [0.0004092981, 0.283, 1.0112],
  [0.00687649, 0.235, 1.0112]
]))







'''
LOGISTIC REGRESSION
Introduction
When an email lands in your inbox, how does your email service know whether it’s a real email or spam? This evaluation is made billions of times per day, and one way it can be done is with Logistic Regression. Logistic Regression is a supervised machine learning algorithm that uses regression to predict the continuous probability, ranging from 0 to 1, of a data sample belonging to a specific category, or class. Then, based on that probability, the sample is classified as belonging to the more probable class, ultimately making Logistic Regression a classification algorithm.

In our spam filtering example, a Logistic Regression model would predict the probability of an incoming email being spam. If that predicted probability is greater than or equal to 0.5, the email is classified as spam. We would call spam the positive class, with the label 1, since the positive class is the class our model is looking to detect. If the predicted probability is less than 0.5, the email is classified as ham (a real email). We would call ham the negative class, with the label 0. This act of deciding which of two classes a data sample belongs to is called binary classification.

Some other examples of what we can classify with Logistic Regression include:

Disease survival —Will a patient, 5 years after treatment for a disease, still be alive?
Customer conversion —Will a customer arriving on a sign-up page enroll in a service?
In this lesson you will learn how to perform Logistic Regression and use it to make classifications on your own data!

If you are unfamiliar with Linear Regression, we recommend you go check out our Linear Regression course before proceeding to Logistic Regression. If you are familiar, let’s dive in!
'''
import codecademylib3_seaborn
import numpy as np
import matplotlib.pyplot as plt
from exam import hours_studied, passed_exam, math_courses_taken

# Scatter plot of exam passage vs number of hours studied
plt.scatter(hours_studied.ravel(), passed_exam, color='black', zorder=20)
plt.ylabel('passed/failed')
plt.xlabel('hours studied')

plt.show()

'''
LOGISTIC REGRESSION
Linear Regression Approach
With the data from Codecademy University, we want to predict whether each student will pass their final exam. And the first step to making that prediction is to predict the probability of each student passing. Why not use a Linear Regression model for the prediction, you might ask? Let’s give it a try.

Recall that in Linear Regression, we fit a regression line of the following form to the data:

y = b_{0} + b_{1}x_{1} + b_{2}x_{2} +\cdots + b_{n}x_{n}y=b 
0
​	 +b 
1
​	 x 
1
​	 +b 
2
​	 x 
2
​	 +⋯+b 
n
​	 x 
n
​	 
where

y is the value we are trying to predict
b_0 is the intercept of the regression line
b_1, b_2, … b_n are the coefficients of the features x_1, x_2, … x_n of the regression line
For our data points y is either 1 (passing), or 0 (failing), and we have one feature, num_hours_studied. Below we fit a Linear Regression model to our data and plotted the results, with the line of best fit in red.

Linear Regression Model on Exam Data
A problem quickly arises. For low values of num_hours_studied the regression line predicts negative probabilities of passing, and for high values of num_hours_studied the regression line predicts probabilities of passing greater than 1. These probabilities are meaningless! We get these meaningless probabilities since the output of a Linear Regression model ranges from -∞ to +∞.'''

'''
LOGISTIC REGRESSION
Logistic Regression
We saw that the output of a Linear Regression model does not provide the probabilities we need to predict whether a student passes the final exam. Step in Logistic Regression!

In Logistic Regression we are also looking to find coefficients for our features, but this time we are fitting a logistic curve to the data so that we can predict probabilities. Described below is an overview of how Logistic Regression works. Don’t worry if something does not make complete sense right away, we will dig into each of these steps in further detail in the remaining exercises!

To predict the probability of a data sample belonging to a class, we:

initialize all feature coefficients and intercept to 0
multiply each of the feature coefficients by their respective feature value to get what is known as the log-odds
place the log-odds into the sigmoid function to link the output to the range [0,1], giving us a probability
By comparing the predicted probabilities to the actual classes of our data points, we can evaluate how well our model makes predictions and use gradient descent to update the coefficients and find the best ones for our model.

To then make a final classification, we use a classification threshold to determine whether the data sample belongs to the positive class or the negative class.'''

import codecademylib3_seaborn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from exam import hours_studied, passed_exam
from plotter import plot_data

# Create logistic regression model
model = LogisticRegression()
model.fit(hours_studied,passed_exam)

# Plug sample data into fitted model
sample_x = np.linspace(-16.65, 33.35, 300).reshape(-1,1)
probability = model.predict_proba(sample_x)[:,1]

# Function to plot exam data and logistic regression curve
plot_data(model)

# Show the plot
plt.show()

# Lowest and highest probabilities
lowest = 0

highest = 1


'''
LOGISTIC REGRESSION
Log-Odds
In Linear Regression we multiply the coefficients of our features by their respective feature values and add the intercept, resulting in our prediction, which can range from -∞ to +∞. In Logistic Regression, we make the same multiplication of feature coefficients and feature values and add the intercept, but instead of the prediction, we get what is called the log-odds.

The log-odds are another way of expressing the probability of a sample belonging to the positive class, or a student passing the exam. In probability, we calculate the odds of an event occurring as follows:

Odds = \frac{P(event\ occurring)}{P(event\ not\ occurring)}Odds= 
P(event not occurring)
P(event occurring)
​	 
The odds tell us how many more times likely an event is to occur than not occur. If a student will pass the exam with probability 0.7, they will fail with probability 1 - 0.7 = 0.3. We can then calculate the odds of passing as:

Odds\ of\ passing = \frac{0.7}{0.3} = 2.\overline{33}Odds of passing= 
0.3
0.7
​	 =2. 
33
 
The log-odds are then understood as the logarithm of the odds!

Log\ odds\ of\ passing = log(2.\overline{33}) = 0.847Log odds of passing=log(2. 
33
 )=0.847
For our Logistic Regression model, however, we calculate the log-odds, represented by z below, by summing the product of each feature value by its respective coefficient and adding the intercept. This allows us to map our feature values to a measure of how likely it is that a data sample belongs to the positive class.

z = b_{0}+b_{1}x_{1} + \cdots + b_{n}x_{n}z=b 
0
​	 +b 
1
​	 x 
1
​	 +⋯+b 
n
​	 x 
n
​	 
b_0 is the intercept
b_1, b_2, … b_n are the coefficients of the features x_1, x_2, … x_n
This kind of multiplication and summing is known as a dot product.

We can perform a dot product using numpy‘s np.dot() method! Given feature matrix features, coefficient vector coefficients, and an intercept, we can calculate the log-odds in numpy as follows:

log_odds = np.dot(features, coefficients) + intercept
np.dot() will take each row, or student, in features and multiply each individual feature value by its respective coefficient in coefficients, summing the result, as shown below.

Matrix Multiplication
We then add in the intercept to get the log-odds!'''

import numpy as np
from exam import hours_studied, calculated_coefficients, intercept

# Create your log_odds() function here

def log_odds(features, coefficients, intercept):
  return np.dot(features, coefficients) + intercept

# Calculate the log-odds for the Codecademy University data here
calculated_log_odds = log_odds(hours_studied, calculated_coefficients, intercept)

print(calculated_log_odds)

'''
LOGISTIC REGRESSION
Sigmoid Function
How did our Logistic Regression model create the S-shaped curve we previously saw? The answer is the Sigmoid Function.

Sigmoid Function
The Sigmoid Function is a special case of the more general Logistic Function, where Logistic Regression gets its name. Why is the Sigmoid Function so important? By plugging the log-odds into the Sigmoid Function, defined below, we map the log-odds z to the range [0,1].

h(z)=\frac{1}{1+e^{-z}}h(z)= 
1+e 
−z
 
1
​	 
e^(-z) is the exponential function, which can be written in numpy as np.exp(-z)
This enables our Logistic Regression model to output the probability of a sample belonging to the positive class, or in our case, a student passing the final exam!'''

import codecademylib3_seaborn
import numpy as np
from exam import calculated_log_odds

# Create your sigmoid function here
def sigmoid(z):
  denominator = 1 + np.exp(-z)
  return 1/denominator

# Calculate the sigmoid of the log-odds here
probabilities = sigmoid(calculated_log_odds)
print(probabilities)

'''
LOGISTIC REGRESSION
Log-Loss I
Now that we understand how a Logistic Regression model makes its probability predictions, what coefficients and intercept should we use in our model to best predict whether a student will pass the exam? To answer this question we need a way to evaluate how well a given model fits the data we have.

The function used to evaluate the performance of a machine learning model is called a loss function, or a cost function. To evaluate how “good a fit” a model is, we calculate the loss for each data sample (how wrong the model’s prediction was) and then average the loss across all samples. The loss function for Logistic Regression, known as Log Loss, is given below:

-\frac{1}{m}\sum_{i=1}^{m} [y^{(i)}log(h(z^{(i)})) + (1-y^{(i)})log(1-h(z^{(i)}))]− 
m
1
​	  
i=1
∑
m
​	 [y 
(i)
 log(h(z 
(i)
 ))+(1−y 
(i)
 )log(1−h(z 
(i)
 ))]
m is the total number of data samples
y_i is the class of data sample i
z_i is the log-odds of sample i
h(z_i) is the sigmoid of the log-odds of sample i, which is the probability of sample i belonging to the positive class
The log-loss function might seem scary, but don’t worry, we are going to break it down in the next exercise!

The goal of our Logistic Regression model is to find the feature coefficients and intercept, which shape the logistic function, that minimize log-loss for our training data!

'''
'''
LOGISTIC REGRESSION
Log Loss II
J(\mathbf{b}) = -\frac{1}{m}\sum_{i=1}^{m} [y^{(i)}log(h(z^{(i)})) + (1-y^{(i)})log(1-h(z^{(i)}))]J(b)=− 
m
1
​	  
i=1
∑
m
​	 [y 
(i)
 log(h(z 
(i)
 ))+(1−y 
(i)
 )log(1−h(z 
(i)
 ))]
Let’s go ahead and break down our log-loss function into two separate parts so it begins to make more sense. Consider the case when a data sample has class y = 1, or for our data when a student passed the exam. The right-side of the equation drops out because we end up with 1 - 1 (or 0) multiplied by some value. The loss for that individual student becomes:

loss_{y=1} = -log(h(z^{(i)}))loss 
y=1
​	 =−log(h(z 
(i)
 ))
The loss for a student who passed the exam is just the log of the probability the student passed the exam!

And for a student who fails the exam, where a sample has class y = 0, the left-side of the equation drops out and the loss for that student becomes:

loss_{y = 0} = -log(1-h(z^{(i)}))loss 
y=0
​	 =−log(1−h(z 
(i)
 ))
The loss for a student who failed the exam is the log of one minus the probability the student passed the exam, which is just the log of the probability the student failed the exam!

Let’s take a closer look at what is going on with our loss function by graphing the loss of individual samples when the class label is y = 1 and y = 0.

Log Loss for Positive and Negative Samples
Let’s go back to our Codecademy University data and consider four possible cases:

Class	Model Probability y = 1	Correct?	Loss
y = 1	High	Yes	Low
y = 1	Low	No	High
y = 0	High	No	High
y = 0	Low	Yes	Low

From the graphs and the table you can see that confident correct predictions result in small losses, while confident incorrect predictions result in large losses that approach infinity. This makes sense! We want to punish our model with an increasing loss as it makes progressively incorrect predictions, and we want to reward the model with a small loss as it makes correct predictions.

Just like in Linear Regression, we can then use gradient descent to find the coefficients that minimize log-loss across all of our training data.
'''

import numpy as np
from exam import passed_exam, probabilities, probabilities_2

# Function to calculate log-loss
def log_loss(probabilities,actual_class):
  return np.sum(-(1/actual_class.shape[0])*(actual_class*np.log(probabilities) + (1-actual_class)*np.log(1-probabilities)))

# Print passed_exam here
print(passed_exam)


# Calculate and print loss_1 here
loss_1 = log_loss(probabilities,passed_exam )
print(loss_1)

# Calculate and print loss_2 here
loss_2 = log_loss(probabilities_2,passed_exam )
print(loss_2)

'''

Classification Thresholding
Many machine learning algorithms, including Logistic Regression, spit out a classification probability as their result. Once we have this probability, we need to make a decision on what class the sample belongs to. This is where the classification threshold comes in!

The default threshold for many algorithms is 0.5. If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the classification of the sample is the positive class. If the predicted probability of an observation belonging to the positive class is less than the threshold, 0.5, the classification of the sample is the negative class.

Threshold at 0.5
We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a Logistic Regression model that classifies whether or not an individual has cancer, we want to be more sensitive to the positive cases, signifying the presence of cancer, than the negative cases.

In order to ensure that most patients with cancer are identified, we can move the classification threshold down to 0.3 or 0.4, increasing the sensitivity of our model to predicting a positive cancer classification. While this might result in more overall misclassifications, we are now missing fewer of the cases we are trying to detect: actual cancer patients.

Threshold at 0.4

'''
'''
Instructions
1.
Let’s use all the knowledge we’ve gathered to create a function that performs thresholding and makes class predictions! Define a function predict_class() that takes a features matrix, a coefficients vector, an intercept, and a threshold as parameters. Return threshold.

2.
In predict_class(), calculate the log-odds using the log_odds() function we defined earlier. Store the result in calculated_log_odds, and return calculated_log_odds.

3.
Still in predict_class(), find the probabilities that the samples belong to the positive class. Create a variable probabilities, and give it the value returned by calling sigmoid() on calculated_log_odds. Return probabilities.

4.
Return 1 for all values within probabilities equal to or above threshold, and 0 for all values below threshold.

Since we are working with numpy objects, we can compare all the values in an array with some threshold using the following syntax:

np.where(array_to_check >= threshold, 1, 0)
If a value in array_to_check is above threshold, the output is 1. If a value in array_to_check is below threshold, the output is 0.

5.
Let’s make final classifications on our Codecademy University data to see which students passed the exam. Use the predict_class() function with hours_studied, calculated_coefficients, intercept, and a threshold of 0.5 as parameters. Store the results in final_results, and print final_results.
'''

import numpy as np
from exam import hours_studied, calculated_coefficients, intercept

def log_odds(features, coefficients,intercept):
  return np.dot(features,coefficients) + intercept

def sigmoid(z):
    denominator = 1 + np.exp(-z)
    return 1/denominator

# Create predict_class() function here

def predict_class(features, coefficients, intercept, threshold):
  calculated_log_odds = log_odds(features, coefficients, intercept)
  probabilities = sigmoid(calculated_log_odds)
  
  return np.where(probabilities >= threshold, 1, 0) 
  
# Make final classifications on Codecademy University data here

final_results = predict_class(hours_studied, calculated_coefficients,intercept, 0.5) 

print(final_results)


'''
LOGISTIC REGRESSION
Scikit-Learn
Now that you know the inner workings of how Logistic Regression works, let’s learn how to easily and quickly create Logistic Regression models with sklearn! sklearn is a Python library that helps build, train, and evaluate Machine Learning models.

To take advantage of sklearn‘s abilities, we can begin by creating a LogisticRegression object.

model = LogisticRegression()
After creating the object, we need to fit our model on the data. When we fit the model with sklearn it will perform gradient descent, repeatedly updating the coefficients of our model in order to minimize the log-loss. We train — or fit — the model using the .fit() method, which takes two parameters. The first is a matrix of features, and the second is a matrix of class labels.

model.fit(features, labels)
Now that the model is trained, we can access a few useful attributes of the LogisticRegression object.

model.coef_ is a vector of the coefficients of each feature
model.intercept_ is the intercept b_0
With our trained model we are able to predict whether new data points belong to the positive class using the .predict() method! .predict() takes a matrix of features as a parameter and returns a vector of labels 1 or 0 for each sample. In making its predictions, sklearn uses a classification threshold of 0.5.

model.predict(features)
If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class, we can use the .predict_proba() method. predict_proba() also takes a matrix of features as a parameter and returns a vector of probabilities, ranging from 0 to 1, for each sample.

model.predict_proba(features)
Before proceeding, one important note is that sklearn‘s Logistic Regression implementation requires feature data to be normalized. Normalization scales all feature data to vary over the same range. sklearn‘s Logistic Regression requires normalized feature data due to a technique called Regularization that it uses under the hood. Regularization is out of the scope of this lesson, but in order to ensure the best results from our model, we will be using a normalized version of the data from our Codecademy University example.

'''
'''
Instructions
1.
Let’s build, train and evaluate a Logistic Regression model in sklearn for our Codecademy University data! We’ve imported sklearn and the LogisiticRegression classifier for you. Create a Logistic Regression model named model.

2.
Train the model using hours_studied_scaled as the training features and passed_exam as the training labels.

3.
Save the coefficients of the model to the variable calculated_coefficients, and the intercept of the model to intercept. Print calculated_coefficients and intercept.

4.
The next semester a group of students in the Introductory Machine Learning course want to predict their final exam scores based on how much they intended to study for the exam. The number of hours each student thinks they will study, normalized, is given in guessed_hours_scaled. Use model to predict the probability that each student will pass the final exam, and save the probabilities to passed_predictions.

5.
That same semester, the Data Science department decides to update the final exam passage model to consider two features instead of just one. During the final exam, students were asked to estimate how much time they spent studying, as well as how many previous math courses they have taken. The student responses, along with their exam results, were split into training and test sets. The training features, normalized, are given to you in exam_features_scaled_train, and the students’ results on the final are given in passed_exam_2_train.

Create a new Logistic Regression model named model_2 and train it on exam_features_scaled_train and passed_exam_2_train.

6.
Use the model you just trained to predict whether each student in the test set, exam_features_scaled_test, will pass the exam and save the predictions to passed_predictions_2. Print passed_predictions_2.

Compare the predictions to the actual student performance on the exam in the test set. How well did your model do?

To make predictions, call model_2.predict() on exam_features_scaled_test.

Print passed_exam_2_test to see how well your model performed!

'''

import numpy as np
from sklearn.linear_model import LogisticRegression
from exam import hours_studied_scaled, passed_exam, exam_features_scaled_train, exam_features_scaled_test, passed_exam_2_train, passed_exam_2_test, guessed_hours_scaled

# Create and fit logistic regression model here
model = LogisticRegression()
model.fit(hours_studied_scaled, passed_exam)

# Save the model coefficients and intercept here
calculated_coefficients = model.coef_
intercept = model.intercept_
print(calculated_coefficients)
print(intercept)

# Predict the probabilities of passing for next semester's students here
passed_predictions = model.predict_proba(guessed_hours_scaled)
print(passed_predictions)

passed_predictions1 = model.predict(guessed_hours_scaled)
print(passed_predictions1)

# Create a new model on the training data with two features here
model_2 = LogisticRegression()
model_2.fit(exam_features_scaled_train , passed_exam_2_train)

# Predict whether the students will pass here
passed_predictions_2 = model_2.predict(exam_features_scaled_test)
'''
LOGISTIC REGRESSION
Feature Importance
One of the defining features of Logistic Regression is the interpretability we have from the feature coefficients. How to handle interpreting the coefficients depends on the kind of data you are working with (normalized or not) and the specific implementation of Logistic Regression you are using. We’ll discuss how to interpret the feature coefficients from a model created in sklearn with normalized feature data.

Since our data is normalized, all features vary over the same range. Given this understanding, we can compare the feature coefficients’ magnitudes and signs to determine which features have the greatest impact on class prediction, and if that impact is positive or negative.

Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class
Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class
Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class
Given cancer data, a logistic regression model can let us know what features are most important for predicting survival after, for example, five years from diagnosis. Knowing these features can lead to a better understanding of outcomes, and even lives saved!
'''
'''
Instructions
1.
Let’s revisit the sklearn Logistic Regression model we fit to our exam data in the last exercise. Remember, the two features in the new model are the number of hours studied and the number of previous math courses taken.

Using the model, given to you as model_2 in the code editor, save the feature coefficients to the variable coefficients.

2.
In order to visualize the coefficients, let’s pull them out of the numpy array in which they are currently stored. With numpys tolist() method we can convert the array into a list and grab the values we want to visualize.

Below your original assignment of coefficients, update coefficients to equal coefficients.tolist()[0].

3.
Create a bar graph comparing the feature coefficients with matplotlib‘s plt.bar() method. Which feature appears to be more important in determining whether or not a student will pass the Introductory Machine Learning final exam?
'''

import codecademylib3_seaborn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from exam import exam_features_scaled, passed_exam_2

# Train a sklearn logistic regression model on the normalized exam data
model_2 = LogisticRegression()
model_2.fit(exam_features_scaled,passed_exam_2)

# Assign and update coefficients
coefficients = model_2.coef_
coefficients = coefficients.tolist()[0]

# Plot bar graph
#plt.bar(range(len(coefficients)),coefficients)
#plt.show()

plt.bar([1,2],coefficients)
plt.xticks([1,2],['hours studied','math courses taken'])
plt.xlabel('feature')
plt.ylabel('coefficient')

plt.show()

'''
LOGISTIC REGRESSION
Review
Congratulations! You just learned how a Logistic Regression model works and how to fit one to a dataset. Class is over, and the final exam for Codecademy University’s Introductory Machine Learning is around the corner. Do you predict that you will pass? Let’s do some review to make sure.

Logistic Regression is used to perform binary classification, predicting whether a data sample belongs to a positive (present) class, labeled 1 and the negative (absent) class, labeled 0.
The Sigmoid Function bounds the product of feature values and their coefficients, known as the log-odds, to the range [0,1], providing the probability of a sample belonging to the positive class.
A loss function measures how well a machine learning model makes predictions. The loss function of Logistic Regression is log-loss.
A Classification Threshold is used to determine the probabilistic cutoff for where a data sample is classified as belonging to a positive or negative class. The standard cutoff for Logistic Regression is 0.5, but the threshold can be higher or lower depending on the nature of the data and the situation.
Scikit-learn has a Logistic Regression implementation that allows you to fit a model to your data, find the feature coefficients, and make predictions on new data samples.
The coefficients determined by a Logistic Regression model can be used to interpret the relative importance of each feature in predicting the class of a data sample.

'''
'''
DECISION TREES
Decision Trees
Decision trees are machine learning models that try to find patterns in the features of data points. Take a look at the tree on this page. This tree tries to predict whether a student will get an A on their next test.

By asking questions like “What is the student’s average grade in the class” the decision tree tries to get a better understanding of their chances on the next test.

In order to make a classification, this classifier needs a data point with four features:

The student’s average grade in the class.
The number of hours the student plans on studying for the test.
The number of hours the student plans on sleeping the night before the test.
Whether or not the student plans on cheating.
For example, let’s say that somebody has a “B” average in the class, studied for more than 3 hours, slept less than 5 hours before the test, and doesn’t plan to cheat. If we start at the top of the tree and take the correct path based on that data, we’ll arrive at a leaf node that predicts the person will not get an A on the next test.

In this course, you’ll learn how to create a tree like this!

DECISION TREES
Making Decision Trees
If we’re given this magic tree, it seems relatively easy to make classifications. But how do these trees get created in the first place? Decision trees are supervised machine learning models, which means that they’re created from a training set of labeled data. Creating the tree is where the learning in machine learning happens.

Take a look at the gif on this page. We begin with every point in the training set at the top of the tree. These training points have labels — the red points represent students that didn’t get an A on a test and the green points represent students that did get an A on a test .

We then decide to split the data into smaller groups based on a feature. For example, that feature could be something like their average grade in the class. Students with an A average would go into one set, students with a B average would go into another subset, and so on.

Once we have these subsets, we repeat the process — we split the data in each subset again on a different feature.

Eventually, we reach a point where we decide to stop splitting the data into smaller groups. We’ve reached a leaf of the tree. We can now count up the labels of the data in that leaf. If an unlabeled point reaches that leaf, it will be classified as the majority label.

We can now make a tree, but how did we know which features to split the data set with? After all, if we started by splitting the data based on the number of hours they slept the night before the test, we’d end up with a very different tree that would produce very different results. How do we know which tree is best? We’ll tackle this question soon!

'''

'''

DECISION TREES
Cars
In this lesson, we’ll create a decision tree build off of a dataset about cars. When considering buying a car, what factors go into making that decision?

Each car can fall into four different classes which represent how satisfied someone would be with purchasing the car — unacc (unacceptable), acc (acceptable), good, vgood.

Each car has 6 features:

The price of the car which can be "vhigh", "high", "med", or "low".
The cost of maintaining the car which can be "vhigh", "high", "med", or "low".
The number of doors which can be "2", "3", "4", "5more".
The number of people the car can hold which can be "2", "4", or "more".
The size of the trunk which can be "small", "med", or "big".
The safety rating of the car which can be "low", "med", or "high".
We’ve imported a dataset of cars behind the scenes and created a decision tree using that data. In this lesson, you’ll learn how to build that tree yourself, but for now, let’s see what the tree can do!

Instructions

Create a variable named car. We’re going to be feeding car into tree, the decision tree we’ve made behind the scenes. car should be a list of six items — one value for each feature.

Try to make is a car that you think would have the label vgood and we’ll see if the decision tree agrees with you!

Make sure your features are in the order listed above.

Here’s the start of the definition of a car.

car = ["low", "med", "3", ____, ____, ____]
2.
Call classify() using car and tree as parameters. Print the result.

Did the decision tree classify car as you expected?

Feel free to change the features of car to see how tree reacts.

'''
'''
DECISION TREES
Gini Impurity
Consider the two trees below. Which tree would be more useful as a model that tries to predict whether someone would get an A in a class?

A tree where the leaf nodes have different types of classificationA tree where the leaf nodes have only one type of classification
Let’s say you use the top tree. You’ll end up at a leaf node where the label is up for debate. The training data has labels from both classes! If you use the bottom tree, you’ll end up at a leaf where there’s only one type of label. There’s no debate at all! We’d be much more confident about our classification if we used the bottom tree.

This idea can be quantified by calculating the Gini impurity of a set of data points. To find the Gini impurity, start at 1 and subtract the squared percentage of each label in the set. For example, if a data set had three items of class A and one item of class B, the Gini impurity of the set would be

1 - \bigg(\frac{3}{4}\bigg)^2 - \bigg(\frac{1}{4}\bigg)^2 = 0.3751−( 
4
3
​	 ) 
2
 −( 
4
1
​	 ) 
2
 =0.375
If a data set has only one class, you’d end up with a Gini impurity of 0. The lower the impurity, the better the decision tree!

'''
'''
Instructions
1.
Let’s find the Gini impurity of the set of labels we’ve given you.

Let’s start by creating a variable named impurity and set it to 1.

2.
We now want to count up how many times every unique label is in the dataset. Python’s Counter object can do this quickly.

For example, given the following code:

lst = ["A", "A", "B"]
counts = Counter(lst)
would result in counts storing this object:

Counter({"A": 2, "B": 1})
Create a counter object of labels‘ items named label_counts.

Print out label_counts to see if it matches what you expect.

Fill in labels as the parameter:

label_counts = Counter(___)
3.
Let’s find the probability of each label given the dataset. Loop through each label in label_counts.

Inside the for loop, create a variable named probability_of_label. Set it equal to the label count divided by the total number of labels in the dataset.

For every label, the count associated with that label can be found at label_counts[label].

We can find the total number of labels in the dataset with len(labels).

Your for loop should look something like this:

for label in label_counts:
  probability_of_label = ____/____
4.
We now want to take probability_of_label, square it, and subtract it from impurity.

Inside the for loop, subtract probability_of_label squared from impurity.

In Python, you can square x by using x ** 2.

You can use -= to subtract from impurity:

impurity -= _____
5.
Outside of the for loop, print impurity.

Test out some of the other labels that we’ve given you by uncommenting them. Which one do you expect to have the lowest impurity?

In the next exercise, we’ll put all of your code into a function. If you want a challenge, try creating the function yourself! Ours is named gini(), takes labels as a parameter, and returns impurity.

The dataset that has only one type of label should have an impurity of 0.'''

from collections import Counter

labels = ["unacc", "unacc", "acc", "acc", "good", "good"]
#labels = ["unacc","unacc","unacc", "good", "vgood", "vgood"]
#labels = ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc"]

impurity = 1

label_counts = Counter(labels)

print(label_counts)

for lable in label_counts:
  probability_of_label = label_counts[lable] / len(labels)
  print(lable,probability_of_label)
  impurity += -1*(probability_of_label**2)
  
print(impurity)

'''
DECISION TREES
Information Gain
We know that we want to end up with leaves with a low Gini Impurity, but we still need to figure out which features to split on in order to achieve this. For example, is it better if we split our dataset of students based on how much sleep they got or how much time they spent studying?

To answer this question, we can calculate the information gain of splitting the data on a certain feature. Information gain measures difference in the impurity of the data before and after the split. For example, let’s say you had a dataset with an impurity of 0.5. After splitting the data based on a feature, you end up with three groups with impurities 0, 0.375, and 0. The information gain of splitting the data in that way is 0.5 - 0 - 0.375 - 0 = 0.125.


Not bad! By splitting the data in that way, we’ve gained some information about how the data is structured — the datasets after the split are purer than they were before the split. The higher the information gain the better — if information gain is 0, then splitting the data on that feature was useless! Unfortunately, right now it’s possible for information gain to be negative. In the next exercise, we’ll calculate weighted information gain to fix that problem.
'''
'''
Instructions
1.
We’ve given you a set of labels named unsplit_labels and two different ways of splitting those labels into smaller subsets. Let’s calculate the information gain of splitting the labels in this way.

At the bottom of your code, begin by creating a variable named info_gain. info_gain should start at the Gini impurity of the unsplit_labels.

Call the gini() function we’ve given you with unsplit_labels as a parameter. Store the result in info_gain.

2.
We now want to subtract the impurity of each subset in split_labels_1 from info_gain.

Loop through every subset in split_labels_1. We want to change the value of info_gain.

For every subset, calculate the Gini impurity and subtract it from info_gain.

Your for loop might look something like this:

for subset in split_labels_1:
  info_gain -= _______
3.
Outside of your loop, print info_gain.

We’ve given you a second way to split the data. Instead of looping through the subsets in split_labels_1, loop through the subsets in split_labels_2.

Which split resulted in more information gain?

Once again, in the next exercise, we’ll put the code you wrote into a function named information_gain that takes unsplit_labels and split_labels as parameters.

The second method of splitting the data should have slightly more information gain.'''

from collections import Counter

unsplit_labels = ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc", "good", "good", "good", "good", "vgood", "vgood", "vgood"]

split_labels_1 = [
  ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc", "good", "good", "vgood"], 
  [ "good", "good"], 
  ["vgood", "vgood"]
]

split_labels_2 = [
  ["unacc", "unacc", "unacc", "unacc","unacc", "unacc", "good", "good", "good", "good"], 
  ["vgood", "vgood", "vgood"]
]

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

info_gain = gini(unsplit_labels)

print(info_gain)
for subset in split_labels_1:
  print(gini(subset))
  info_gain -= gini(subset)
print(info_gain)

info_gain2 = gini(unsplit_labels)

print(info_gain2)
for subset in split_labels_2:
  print(gini(subset))
  info_gain2 -= gini(subset)
print(info_gain2)

'''
DECISION TREES
Weighted Information Gain
We’re not quite done calculating the information gain of a set of objects. The sizes of the subset that get created after the split are important too! For example, the image below shows two sets with the same impurity. Which set would you rather have in your decision tree?


Both of these sets are perfectly pure, but the purity of the second set is much more meaningful. Because there are so many items in the second set, we can be confident that whatever we did to produce this set wasn’t an accident.

It might be helpful to think about the inverse as well. Consider these two sets with the same impurity:


Both of these sets are completely impure. However, that impurity is much more meaningful in the set with more instances. We know that we are going to have to do a lot more work in order to completely separate the two classes. Meanwhile, the impurity of the set with two items isn’t as important. We know that we’ll only need to split the set one more time in order to make two pure sets.

Let’s modify the formula for information gain to reflect the fact that the size of the set is relevant. Instead of simply subtracting the impurity of each set, we’ll subtract the weighted impurity of each of the split sets. If the data before the split contained 20 items and one of the resulting splits contained 2 items, then the weighted impurity of that subset would be 2/20 * impurity. We’re lowering the importance of the impurity of sets with few elements.


Now that we can calculate the information gain using weighted impurity, let’s do that for every possible feature. If we do this, we can find the best feature to split the data on.'''

'''
Instructions
1.
Let’s update the information_gain function to make it calculate weighted information gain.

When subtracting the impurity of a subset from info_gain, first multiply the impurity by the correct percentage.

The percentage should be the number of labels in the subset, len(subset), divided by the number of labels before the split, len(starting_labels).

Multiply gini(subset) by len(subset)/len(starting_labels).

2.
We’ve given you a split() function along with ten cars and the car_labels associated with those cars.

After your information_gain() function, call split() using cars, car_labels and 3 as a parameter. This will split the data based on the third index (That feature was the number of people the car could hold).

split() returns two lists. Create two variables named split_data and split_labels and set them equal to the result of the split function.

We’ll explore what these variables contain in a second!

In Python, functions can return more than one value. When this happens, you can do something like this:

a, b = function_that_returns_two_things()
Do this with your split() function.

3.
Take a look at what these variables are. Begin by printing split_data. It’s kind of hard to tell what’s going on there! There are so many lists of lists!

Try printing the length of split_data. What do you think this is telling you?

Also try printing split_data[0]. What do you notice about the items at index 3 of all these lists? (Remember, when we called split, we used 3 as the split index).

Try printing split_data[1]. What do you notice about the items at index 3 of these lists?

len(split_data) is telling you how many subsets the original data set was split into. In this case, when we split the dataset using index 3, we split it into 3 subsets.

When you print each subset, you’ll see that the value at index 3 of each car in the subset is the same. We’ve basically created three subsets — cars could hold "2" people, cars that could hold "4" people, and cars that could hold "more" people.

4.
We now know that split_data contains the cars split into different subsets. split_labels contains the labels of those cars split into different subsets.

Use those split labels to find the information gain of splitting on index 3! Remember, the information_gain() function takes a list of the labels before the split (car_labels), and a list of the subsets of labels after the split (split_labels).

Call this function and print the result! How did we do when we split the function on index 3?

Print the results of information_gain(car_labels, split_labels)

5.
We found the information gain when splitting on feature 3. Let’s do the same for every possible feature.

Loop through all of the features of our data to find the best one to split on! Each car has six features, so we want to loop through the indices 0 through 5.

Inside your for loop, call split() using the unsplit data, the unsplit labels, and the index that you’re looping through.

Call information_gain() using the resulting split labels and print the results. Which feature produces the most information gain?

Your for loop might look something like this:

for i in range(0, 6):
  split_data, split_labels = split(____, ____, i)
  print(information_gain(____, ____)
  '''
from collections import Counter

cars = [['med', 'low', '3', '4', 'med', 'med'], ['med', 'vhigh', '4', 'more', 'small', 'high'], ['high', 'med', '3', '2', 'med', 'low'], ['med', 'low', '4', '4', 'med', 'low'], ['med', 'low', '5more', '2', 'big', 'med'], ['med', 'med', '2', 'more', 'big', 'high'], ['med', 'med', '2', 'more', 'med', 'med'], ['vhigh', 'vhigh', '2', '2', 'med', 'low'], ['high', 'med', '4', '2', 'big', 'low'], ['low', 'low', '2', '4', 'big', 'med']]

car_labels = ['acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good']

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    # Multiply gini(subset) by the correct percentage below
    info_gain -= gini(subset)*len(subset)/len(starting_labels)
  return info_gain

split_data , split_labels = split(cars, car_labels, 3) 
#print(len(split_data))

#print(split_data[0])

#print(split_data[1])

#print(split_labe   `ls)

print(information_gain(car_labe      QQQQ   qlswrsdsswqawdsxawqewsadsaxszals , split_labels))

for i in range(len(cars[0])):
  split_data_a , split_labels_a = split(cars, car_labels, i)
    #print(i)  
  
'''
DECISION TREES
Recursive Tree Building
Now that we can find the best feature to split the dataset, we can repeat this process again and again to create the full tree. This is a recursive algorithm! We start with every data point from the training set, find the best feature to split the data, split the data based on that feature, and then recursively repeat the process again on each subset that was created from the split.

We’ll stop the recursion when we can no longer find a feature that results in any information gain. In other words, we want to create a leaf of the tree when we can’t find a way to split the data that makes purer subsets.

The leaf should keep track of the classes of the data points from the training set that ended up in the leaf. In our implementation, we’ll use a Counter object to keep track of the counts of labels.

We’ll use these counts to make predictions about new data that we give the tree.
'''
#script.py-------------------------------

from tree import *

car_data = [['med', 'low', '3', '4', 'med', 'med'], ['med', 'vhigh', '4', 'more', 'small', 'high'], ['high', 'med', '3', '2', 'med', 'low'], ['med', 'low', '4', '4', 'med', 'low'], ['med', 'low', '5more', '2', 'big', 'med'], ['med', 'med', '2', 'more', 'big', 'high'], ['med', 'med', '2', 'more', 'med', 'med'], ['vhigh', 'vhigh', '2', '2', 'med', 'low'], ['high', 'med', '4', '2', 'big', 'low'], ['low', 'low', '2', '4', 'big', 'med']]

car_labels = ['acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good']

def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    for feature in range(len(dataset[0])):
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain


def build_tree(data, labels):
  best_feature, best_gain = find_best_split(data, labels)
  
  if best_gain == 0:
    return Counter(labels)
  
  data_subsets, label_subsets = split(data, labels, best_feature)

  branches = []

  for i in range(len(data_subsets)):
    branch = build_tree(data_subsets[i], label_subsets[i])
    branches.append(branch)

  return branches

tree = build_tree(car_data, car_labels)
print_tree(tree)

#tree.py-------------------------------
from collections import Counter

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain  

class Leaf:
    def __init__(self, labels):
        self.predictions = Counter(labels)

class Internal_Node:
    def __init__(self,
                 feature,
                 branches):
        self.feature = feature
        self.branches = branches

def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Counter):
        print (spacing + str(node))
        return

    # Print the question at this node
    print (spacing + "Splitting")

    # Call this function recursively on the true branch
    for i in range(len(node)):
        print (spacing + '--> Branch ' + str(i)+':')
        print_tree(node[i], spacing + "  ")

'''
DECISION TREES
Classifying New Data
We can finally use our tree as a classifier! Given a new data point, we start at the top of the tree and follow the path of the tree until we hit a leaf. Once we get to a leaf, we’ll use the classes of the points from the training set to make a classification.

We’ve slightly changed the way our build_tree() function works. Instead of returning a list of branches or a Counter object, the build_tree() function now returns a Leaf object or an Internal_Node object. We’ll explain how to use these objects in the instructions!

Let’s write a function that will use our tree to classify new points!
'''
'''
Instructions
1.
We’ve created a tree named tree using a lot of car data. Use the print_tree() function with tree as a parameter to see it.

Notice that the tree now knows which feature was used to split the data. This new information is contained in the Leaf and Internal_Node classes. This will come in handy when we write our classify function!

Comment out printing the tree once you get a sense of how large it is!

Call print_tree(tree).

2.
Let’s start writing the classify() function. classify() should take a datapoint and a tree as a parameter.

The first thing classify should do is check to see if we’re at a leaf.

Check to see if tree is a Leaf by using the isinstance() function.

For example, isinstance(a, list) will be True if a is a list. You should check if tree is a Leaf.

If we’ve found a Leaf, that means we want to return the label with the highest count. The label counts are stored in tree.labels.

You could find the label with the largest count by using a for loop, or by using this rather complicated line of code:

return max(tree.labels.items(), key=operator.itemgetter(1))[0]
Your if statement should look like this:

if isinstance(tree, Counter):
Then return the label with the highest count.

3.
If we’re not at a leaf, we want to find the branch that corresponds to our data point. For example, if we’re splitting on index 0 and our data point is ['med', 'low', '4', '2', 'big', 'low'], we want to find the branch that contains all of the points with med at index 0.

To start, let’s find datapoint‘s value of the feature we’re looking for. If datapoint were the example above, and the feature we’re interested is 0, this would be med.

Outside the if statement, create a variable named value and set it equal to datapoint[tree.feature]. tree.feature contains the index of the feature that we’re splitting on, so datapoint[tree.feature] is the value at that index.

To help us check your code, return value.

4.
Start by deleting return value.

Let’s now loop through all of the branches in the tree to find the one that has all the data points with value at the correct index.

Your loop should look like this:

for branch in tree.branches:
Next, inside the loop, check to see if branch.value is equal to value. If it is, we’ve found the branch that we’re looking for! We want to now recursively call classify() on that branch:

return classify(datapoint, branch)
We know that one of these branches will be the one we’re looking for, so we know that this return statement will happen once.

Your final function should look something like this. Fill in the if statement near the bottom of the function.

def classify(datapoint, tree):
  if isinstance(tree, Leaf):
    return max(tree.labels.items(), key=operator.itemgetter(1))[0]
  answer = datapoint[tree.feature]
  for branch in tree.branches:
    if ____ == ____:
      return classify(datapoint, branch)
5.
Finally, outside of your function, call classify() using test_point and tree as parameters. Print the results. You should see a classification for this new point.
'''
#--------------------- script.py-------------------------------
from tree import *
import operator

test_point = ['vhigh', 'low', '3', '4', 'med', 'med']

#print_tree(tree)

def classify(datapoint, tree):
  if isinstance(tree, Leaf) == True:
    return max(tree.labels.items(), key=operator.itemgetter(1))[0]
  
  value = datapoint[tree.feature]
  
  for branch in tree.branches:
    if branch.value == value:
      return classify(datapoint, branch)

test = classify(test_point, tree)
print(test)

#---------------------tree.py-------------------------------

from collections import Counter

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain  

class Leaf:
    def __init__(self, labels, value):
        self.labels = Counter(labels)
        self.value = value

class Internal_Node:
    def __init__(self,
                 feature,
                 branches,
                 value):
        self.feature = feature
        self.branches = branches
        self.value = value

        
def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    for feature in range(len(dataset[0])):
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain

def build_tree(data, labels, value = ""):
  best_feature, best_gain = find_best_split(data, labels)
  if best_gain == 0:
    return Leaf(Counter(labels), value)
  data_subsets, label_subsets = split(data, labels, best_feature)
  branches = []
  for i in range(len(data_subsets)):
    branch = build_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][best_feature])
    branches.append(branch)
  return Internal_Node(best_feature, branches, value)
        
        
def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Leaf):
        print (spacing + str(node.labels))
        return

    # Print the question at this node
    print (spacing + "Splitting on " + question_dict[node.feature])

    # Call this function recursively on the true branch
    for i in range(len(node.branches)):
        print (spacing + '--> Branch ' + node.branches[i].value+':')
        print_tree(node.branches[i], spacing + "  ")
        
cars = [['high', 'low', '5more', '4', 'big', 'high'], ['high', 'vhigh', '4', 'more', 'med', 'med'], ['high', 'med', '4', '2', 'med', 'high'], ['low', 'vhigh', '4', '2', 'med', 'med'], ['vhigh', 'low', '5more', '2', 'small', 'low'], ['vhigh', 'high', '5more', '4', 'small', 'low'], ['med', 'med', '2', 'more', 'small', 'med'], ['med', 'med', '2', 'more', 'small', 'high'], ['med', 'low', '2', '4', 'med', 'high'], ['high', 'vhigh', '4', '4', 'small', 'low'], ['vhigh', 'low', '5more', 'more', 'med', 'med'], ['vhigh', 'vhigh', '3', 'more', 'big', 'med'], ['high', 'med', '4', '4', 'small', 'high'], ['med', 'med', '5more', 'more', 'med', 'high'], ['low', 'vhigh', '4', 'more', 'small', 'med'], ['high', 'med', '4', '2', 'big', 'low'], ['vhigh', 'vhigh', '5more', '2', 'med', 'med'], ['low', 'vhigh', '2', '2', 'big', 'high'], ['med', 'high', '2', '4', 'med', 'low'], ['vhigh', 'med', '2', '4', 'med', 'low'], ['low', 'high', '3', '4', 'med', 'high'], ['med', 'low', '5more', '4', 'med', 'high'], ['high', 'high', '3', '2', 'big', 'low'], ['low', 'vhigh', '2', '4', 'big', 'low'], ['high', 'low', '4', '2', 'small', 'high'], ['vhigh', 'med', '5more', 'more', 'big', 'high'], ['vhigh', 'med', '5more', '2', 'small', 'low'], ['vhigh', 'med', '5more', '2', 'big', 'low'], ['med', 'vhigh', '4', 'more', 'med', 'high'], ['low', 'high', '2', 'more', 'small', 'low'], ['med', 'vhigh', '2', 'more', 'med', 'high'], ['low', 'vhigh', '5more', '2', 'small', 'high'], ['med', 'med', '4', '2', 'med', 'low'], ['med', 'low', '4', '4', 'big', 'high'], ['high', 'vhigh', '3', 'more', 'big', 'high'], ['high', 'high', '4', 'more', 'med', 'low'], ['vhigh', 'high', '5more', '2', 'small', 'low'], ['high', 'high', '3', '4', 'med', 'med'], ['high', 'low', '5more', '4', 'small', 'low'], ['low', 'vhigh', '5more', '2', 'med', 'high'], ['med', 'high', '3', '4', 'big', 'high'], ['med', 'low', '4', '2', 'big', 'low'], ['med', 'high', '2', '2', 'med', 'low'], ['low', 'vhigh', '3', 'more', 'med', 'high'], ['vhigh', 'low', '3', '4', 'big', 'low'], ['high', 'high', '2', 'more', 'big', 'med'], ['low', 'vhigh', '3', '2', 'med', 'low'], ['low', 'med', '5more', 'more', 'med', 'med'], ['high', 'med', '3', 'more', 'small', 'high'], ['high', 'med', '5more', 'more', 'big', 'high'], ['med', 'vhigh', '2', '2', 'med', 'high'], ['low', 'high', '3', '2', 'big', 'high'], ['vhigh', 'high', '3', 'more', 'big', 'low'], ['vhigh', 'med', '2', '2', 'big', 'low'], ['high', 'vhigh', '4', '4', 'big', 'med'], ['low', 'med', '4', 'more', 'med', 'low'], ['high', 'high', '3', '4', 'small', 'med'], ['med', 'low', '3', '4', 'small', 'high'], ['vhigh', 'vhigh', '5more', '2', 'big', 'low'], ['vhigh', 'med', '3', 'more', 'med', 'high'], ['high', 'low', '2', '4', 'med', 'high'], ['low', 'high', '4', '2', 'small', 'med'], ['high', 'med', '2', '4', 'med', 'high'], ['low', 'med', '3', '4', 'big', 'high'], ['high', 'low', '4', 'more', 'big', 'low'], ['high', 'low', '5more', '2', 'med', 'low'], ['low', 'high', '2', 'more', 'small', 'high'], ['med', 'high', '2', '2', 'big', 'high'], ['med', 'high', '3', '4', 'small', 'high'], ['high', 'high', '3', '4', 'med', 'high'], ['vhigh', 'med', '5more', '4', 'med', 'high'], ['vhigh', 'med', '4', '4', 'small', 'high'], ['high', 'low', '4', 'more', 'big', 'med'], ['high', 'med', '2', 'more', 'big', 'low'], ['low', 'vhigh', '3', '2', 'med', 'high'], ['vhigh', 'vhigh', '5more', '2', 'big', 'high'], ['low', 'high', '4', '4', 'med', 'high'], ['high', 'low', '4', 'more', 'big', 'high'], ['med', 'vhigh', '5more', '2', 'small', 'low'], ['high', 'med', '4', '4', 'med', 'low'], ['med', 'med', '3', '2', 'small', 'med'], ['vhigh', 'low', '3', 'more', 'med', 'high'], ['high', 'low', '2', '2', 'small', 'med'], ['med', 'med', '5more', 'more', 'big', 'high'], ['high', 'vhigh', '5more', '4', 'small', 'high'], ['med', 'med', '5more', 'more', 'small', 'high'], ['high', 'low', '4', '2', 'med', 'high'], ['low', 'high', '4', '2', 'big', 'high'], ['low', 'vhigh', '2', '4', 'med', 'med'], ['low', 'med', '5more', '2', 'big', 'high'], ['vhigh', 'vhigh', '5more', '4', 'big', 'low'], ['vhigh', 'med', '4', '2', 'small', 'high'], ['med', 'high', '4', '2', 'med', 'med'], ['high', 'vhigh', '3', '4', 'small', 'low'], ['low', 'low', '5more', 'more', 'big', 'low'], ['vhigh', 'high', '3', '2', 'big', 'med'], ['high', 'high', '3', '4', 'big', 'med'], ['low', 'high', '5more', '4', 'small', 'med'], ['vhigh', 'med', '4', '4', 'med', 'high'], ['med', 'vhigh', '4', '2', 'small', 'med'], ['med', 'med', '3', '2', 'big', 'high'], ['low', 'high', '4', '2', 'small', 'high'], ['vhigh', 'med', '2', '4', 'med', 'high'], ['high', 'med', '2', '2', 'small', 'med'], ['vhigh', 'low', '4', '2', 'big', 'med'], ['low', 'vhigh', '4', 'more', 'big', 'high'], ['low', 'high', '2', '2', 'big', 'low'], ['vhigh', 'low', '5more', '4', 'big', 'med'], ['med', 'vhigh', '5more', '4', 'med', 'med'], ['med', 'med', '2', '2', 'small', 'low'], ['med', 'med', '2', '2', 'med', 'med'], ['low', 'med', '3', 'more', 'med', 'low'], ['med', 'high', '4', '4', 'big', 'high'], ['vhigh', 'vhigh', '2', '2', 'big', 'med'], ['high', 'med', '5more', '2', 'small', 'high'], ['low', 'high', '5more', '2', 'small', 'high'], ['low', 'med', '2', 'more', 'small', 'low'], ['low', 'high', '5more', '2', 'med', 'med'], ['high', 'med', '5more', '4', 'big', 'low'], ['vhigh', 'low', '3', 'more', 'big', 'high'], ['med', 'vhigh', '5more', 'more', 'med', 'low'], ['vhigh', 'med', '5more', '2', 'small', 'high'], ['low', 'vhigh', '2', '4', 'med', 'high'], ['med', 'low', '2', 'more', 'med', 'low'], ['high', 'low', '3', '2', 'med', 'high'], ['low', 'med', '4', 'more', 'big', 'low'], ['low', 'vhigh', '2', '4', 'big', 'high'], ['low', 'med', '3', '4', 'small', 'low'], ['low', 'med', '4', 'more', 'small', 'high'], ['med', 'low', '3', 'more', 'med', 'med'], ['high', 'med', '2', 'more', 'small', 'low'], ['med', 'vhigh', '4', 'more', 'med', 'low'], ['med', 'vhigh', '5more', '2', 'med', 'high'], ['med', 'vhigh', '3', '2', 'big', 'low'], ['vhigh', 'high', '5more', '2', 'big', 'high'], ['low', 'high', '3', '4', 'big', 'med'], ['high', 'high', '2', '2', 'med', 'low'], ['high', 'vhigh', '5more', '2', 'med', 'low'], ['vhigh', 'high', '5more', 'more', 'small', 'high'], ['high', 'low', '5more', 'more', 'big', 'low'], ['vhigh', 'low', '2', '4', 'med', 'high'], ['vhigh', 'vhigh', '3', 'more', 'small', 'high'], ['high', 'low', '2', 'more', 'med', 'low'], ['high', 'high', '3', 'more', 'small', 'med'], ['low', 'vhigh', '2', '2', 'big', 'low'], ['low', 'vhigh', '5more', '4', 'med', 'low'], ['med', 'vhigh', '4', '4', 'med', 'high'], ['vhigh', 'low', '2', 'more', 'small', 'med'], ['low', 'low', '5more', '4', 'small', 'low'], ['high', 'vhigh', '4', '4', 'med', 'med'], ['low', 'vhigh', '2', 'more', 'small', 'med'], ['high', 'high', '4', '4', 'small', 'med'], ['low', 'low', '4', '4', 'small', 'low'], ['high', 'high', '3', 'more', 'med', 'med'], ['high', 'med', '3', 'more', 'small', 'low'], ['med', 'vhigh', '3', '4', 'small', 'med'], ['high', 'high', '4', '2', 'med', 'med'], ['med', 'med', '3', '2', 'med', 'med'], ['vhigh', 'med', '5more', '2', 'big', 'high'], ['low', 'high', '3', '4', 'med', 'low'], ['low', 'low', '4', '4', 'big', 'med'], ['low', 'high', '2', 'more', 'med', 'high'], ['high', 'low', '4', '4', 'med', 'med'], ['low', 'vhigh', '2', '2', 'big', 'med'], ['high', 'vhigh', '4', '2', 'big', 'low'], ['vhigh', 'high', '4', '4', 'med', 'med'], ['vhigh', 'high', '2', 'more', 'small', 'high'], ['vhigh', 'vhigh', '2', 'more', 'med', 'med'], ['vhigh', 'high', '4', 'more', 'small', 'med'], ['high', 'high', '3', '2', 'med', 'high'], ['high', 'high', '4', 'more', 'big', 'low'], ['low', 'med', '4', '2', 'small', 'med'], ['med', 'vhigh', '3', 'more', 'big', 'low'], ['low', 'vhigh', '2', 'more', 'big', 'high'], ['high', 'high', '4', '2', 'med', 'high'], ['low', 'med', '4', '2', 'med', 'med'], ['vhigh', 'low', '5more', '4', 'big', 'low'], ['high', 'vhigh', '4', '4', 'small', 'high'], ['med', 'med', '2', '2', 'big', 'high'], ['high', 'med', '3', 'more', 'med', 'low'], ['low', 'med', '3', '2', 'small', 'high'], ['vhigh', 'med', '4', 'more', 'small', 'low'], ['med', 'vhigh', '3', '4', 'big', 'med'], ['low', 'low', '2', '2', 'med', 'low'], ['med', 'high', '4', '2', 'small', 'med'], ['high', 'high', '4', '4', 'med', 'high'], ['med', 'low', '5more', 'more', 'big', 'low'], ['vhigh', 'med', '2', '4', 'small', 'low'], ['vhigh', 'low', '3', '4', 'big', 'med'], ['vhigh', 'vhigh', '5more', '4', 'big', 'high'], ['med', 'med', '5more', '4', 'med', 'high'], ['vhigh', 'high', '2', 'more', 'small', 'med'], ['med', 'med', '4', 'more', 'small', 'med'], ['high', 'low', '2', '4', 'big', 'med'], ['high', 'low', '2', 'more', 'big', 'high'], ['high', 'high', '2', '4', 'small', 'high'], ['high', 'high', '4', '2', 'big', 'med'], ['low', 'vhigh', '5more', '2', 'small', 'med'], ['high', 'med', '4', '2', 'small', 'low'], ['low', 'med', '4', '4', 'med', 'high'], ['vhigh', 'high', '5more', '4', 'med', 'low'], ['high', 'med', '5more', '4', 'med', 'high'], ['vhigh', 'med', '3', 'more', 'med', 'med'], ['med', 'low', '3', '4', 'med', 'low'], ['vhigh', 'med', '3', '2', 'big', 'med'], ['vhigh', 'low', '2', '4', 'small', 'high'], ['high', 'high', '3', 'more', 'big', 'med'], ['high', 'med', '3', 'more', 'med', 'med'], ['vhigh', 'high', '5more', 'more', 'big', 'med'], ['vhigh', 'low', '4', 'more', 'small', 'high'], ['med', 'med', '3', '2', 'small', 'high'], ['vhigh', 'low', '4', '4', 'small', 'low'], ['med', 'high', '2', '4', 'small', 'low'], ['high', 'high', '2', 'more', 'med', 'med'], ['vhigh', 'low', '4', 'more', 'small', 'med'], ['med', 'low', '3', '4', 'big', 'med'], ['med', 'high', '2', '2', 'med', 'high'], ['low', 'vhigh', '3', 'more', 'big', 'med'], ['vhigh', 'high', '2', '4', 'small', 'med'], ['med', 'low', '3', '2', 'med', 'med'], ['high', 'low', '5more', '2', 'small', 'med'], ['high', 'vhigh', '3', '2', 'med', 'low'], ['vhigh', 'low', '2', '2', 'big', 'med'], ['high', 'vhigh', '3', 'more', 'small', 'high'], ['vhigh', 'low', '3', '4', 'med', 'med'], ['high', 'vhigh', '4', '4', 'small', 'med'], ['high', 'low', '5more', '4', 'med', 'high'], ['high', 'low', '4', '2', 'med', 'low'], ['low', 'med', '5more', '4', 'small', 'low'], ['vhigh', 'vhigh', '3', '2', 'big', 'low'], ['vhigh', 'low', '4', '4', 'big', 'high'], ['med', 'low', '5more', '2', 'small', 'med'], ['med', 'vhigh', '5more', 'more', 'small', 'high'], ['med', 'med', '2', 'more', 'big', 'med'], ['vhigh', 'high', '2', '2', 'small', 'low'], ['vhigh', 'vhigh', '5more', '2', 'med', 'high'], ['med', 'high', '3', 'more', 'small', 'med'], ['low', 'high', '2', 'more', 'med', 'med'], ['vhigh', 'med', '3', 'more', 'med', 'low'], ['vhigh', 'med', '3', '4', 'big', 'med'], ['low', 'low', '4', '4', 'big', 'high'], ['high', 'high', '3', '4', 'big', 'high'], ['med', 'high', '5more', '4', 'big', 'low'], ['vhigh', 'high', '3', '4', 'small', 'low'], ['high', 'vhigh', '3', 'more', 'small', 'med'], ['med', 'low', '4', '4', 'big', 'low'], ['low', 'vhigh', '5more', '2', 'big', 'high'], ['med', 'high', '4', '2', 'med', 'high'], ['med', 'med', '4', '2', 'big', 'low'], ['vhigh', 'low', '4', '2', 'med', 'high'], ['vhigh', 'vhigh', '4', 'more', 'big', 'high'], ['vhigh', 'vhigh', '3', '2', 'small', 'low'], ['low', 'vhigh', '5more', '4', 'small', 'low'], ['med', 'med', '2', 'more', 'small', 'low'], ['high', 'med', '4', 'more', 'med', 'low'], ['vhigh', 'low', '4', '4', 'big', 'med'], ['vhigh', 'low', '2', '4', 'big', 'low'], ['med', 'high', '3', '2', 'med', 'low'], ['low', 'vhigh', '5more', '4', 'small', 'med'], ['low', 'med', '3', '2', 'big', 'med'], ['vhigh', 'high', '3', 'more', 'big', 'med'], ['vhigh', 'med', '5more', '4', 'big', 'low'], ['med', 'low', '5more', '2', 'med', 'high'], ['high', 'high', '2', 'more', 'small', 'low'], ['low', 'vhigh', '3', '2', 'big', 'low'], ['vhigh', 'vhigh', '3', '4', 'big', 'high'], ['high', 'med', '5more', 'more', 'big', 'low'], ['vhigh', 'high', '3', '2', 'small', 'high'], ['med', 'high', '5more', '2', 'big', 'low'], ['med', 'low', '5more', 'more', 'small', 'med'], ['low', 'med', '3', '4', 'med', 'low'], ['med', 'low', '5more', '2', 'small', 'high'], ['low', 'vhigh', '3', '2', 'small', 'low'], ['med', 'low', '3', '2', 'small', 'med'], ['vhigh', 'low', '2', '4', 'med', 'med'], ['low', 'low', '5more', '2', 'small', 'low'], ['high', 'vhigh', '2', '4', 'big', 'high'], ['low', 'vhigh', '4', 'more', 'med', 'med'], ['vhigh', 'med', '3', '4', 'small', 'high'], ['high', 'low', '5more', 'more', 'big', 'high'], ['high', 'high', '4', 'more', 'small', 'med'], ['vhigh', 'vhigh', '2', 'more', 'small', 'med'], ['vhigh', 'high', '5more', 'more', 'med', 'low'], ['med', 'med', '4', 'more', 'big', 'low'], ['vhigh', 'med', '2', '2', 'big', 'med'], ['low', 'med', '4', '4', 'small', 'med'], ['med', 'vhigh', '3', '2', 'small', 'med'], ['vhigh', 'high', '4', '4', 'small', 'low'], ['med', 'high', '2', '4', 'med', 'med'], ['low', 'low', '2', 'more', 'small', 'low'], ['high', 'med', '2', '4', 'small', 'med'], ['med', 'vhigh', '3', '2', 'med', 'med'], ['high', 'med', '3', '2', 'med', 'med'], ['low', 'low', '2', '4', 'med', 'high'], ['med', 'med', '3', '4', 'small', 'med'], ['vhigh', 'low', '2', '4', 'small', 'med'], ['vhigh', 'high', '4', 'more', 'small', 'low'], ['vhigh', 'low', '5more', '2', 'med', 'med'], ['med', 'low', '2', '2', 'med', 'high'], ['med', 'high', '2', '4', 'small', 'high'], ['vhigh', 'vhigh', '4', '2', 'med', 'med'], ['vhigh', 'vhigh', '4', 'more', 'med', 'high'], ['high', 'med', '4', 'more', 'med', 'high'], ['vhigh', 'high', '3', '4', 'big', 'high'], ['low', 'vhigh', '5more', 'more', 'med', 'low'], ['high', 'vhigh', '3', '4', 'small', 'med'], ['vhigh', 'high', '4', 'more', 'med', 'low'], ['med', 'low', '5more', 'more', 'small', 'high'], ['low', 'low', '4', '4', 'med', 'med'], ['vhigh', 'vhigh', '4', '4', 'big', 'med'], ['high', 'high', '2', 'more', 'big', 'high'], ['med', 'vhigh', '2', '2', 'small', 'med'], ['vhigh', 'vhigh', '3', '4', 'small', 'med'], ['low', 'vhigh', '3', '2', 'big', 'med'], ['low', 'vhigh', '2', '4', 'small', 'med'], ['high', 'med', '2', '2', 'big', 'low'], ['high', 'med', '3', 'more', 'small', 'med'], ['low', 'low', '5more', '2', 'big', 'high'], ['low', 'vhigh', '2', '2', 'med', 'low'], ['vhigh', 'low', '5more', '4', 'med', 'low'], ['low', 'low', '5more', '4', 'big', 'low'], ['vhigh', 'vhigh', '4', '4', 'med', 'med'], ['low', 'low', '2', 'more', 'med', 'low'], ['med', 'med', '4', '2', 'med', 'med'], ['low', 'high', '4', '4', 'med', 'med'], ['vhigh', 'med', '3', '4', 'big', 'low'], ['low', 'high', '5more', 'more', 'small', 'high'], ['high', 'vhigh', '2', '2', 'big', 'med'], ['high', 'high', '4', '4', 'big', 'med'], ['high', 'med', '5more', '4', 'small', 'high'], ['low', 'high', '5more', '2', 'big', 'med'], ['med', 'low', '2', 'more', 'small', 'low'], ['vhigh', 'vhigh', '2', '4', 'med', 'high'], ['high', 'high', '5more', '2', 'med', 'med'], ['vhigh', 'vhigh', '4', 'more', 'big', 'med'], ['vhigh', 'vhigh', '5more', '4', 'big', 'med'], ['high', 'med', '4', 'more', 'big', 'med'], ['low', 'med', '5more', 'more', 'med', 'low'], ['vhigh', 'low', '2', 'more', 'big', 'high'], ['med', 'med', '4', 'more', 'small', 'low'], ['med', 'med', '3', '4', 'med', 'med'], ['med', 'low', '5more', 'more', 'med', 'med'], ['low', 'high', '5more', '4', 'big', 'low'], ['high', 'med', '2', '2', 'small', 'high'], ['med', 'vhigh', '4', '4', 'big', 'med'], ['low', 'med', '4', '2', 'med', 'high'], ['low', 'vhigh', '3', 'more', 'small', 'high'], ['high', 'low', '2', '4', 'small', 'med'], ['high', 'high', '5more', 'more', 'big', 'low'], ['low', 'vhigh', '5more', '4', 'small', 'high'], ['med', 'med', '3', '2', 'med', 'low'], ['vhigh', 'low', '5more', 'more', 'small', 'low'], ['med', 'med', '2', '2', 'med', 'low'], ['med', 'high', '3', '4', 'med', 'med'], ['low', 'high', '3', '4', 'small', 'low'], ['med', 'vhigh', '3', '4', 'med', 'med'], ['low', 'low', '2', '4', 'big', 'high'], ['low', 'low', '3', 'more', 'big', 'low'], ['vhigh', 'med', '4', '4', 'small', 'med'], ['vhigh', 'vhigh', '2', '4', 'med', 'med'], ['vhigh', 'high', '3', '4', 'small', 'high'], ['high', 'low', '4', 'more', 'med', 'low'], ['low', 'med', '5more', 'more', 'med', 'high'], ['high', 'vhigh', '4', '4', 'med', 'low'], ['vhigh', 'low', '4', 'more', 'big', 'low'], ['med', 'vhigh', '3', 'more', 'med', 'med'], ['low', 'med', '5more', '4', 'med', 'low'], ['vhigh', 'vhigh', '4', 'more', 'med', 'low'], ['vhigh', 'low', '5more', 'more', 'small', 'med'], ['med', 'med', '4', '4', 'small', 'high'], ['low', 'low', '3', '4', 'small', 'high'], ['high', 'low', '2', '4', 'med', 'low'], ['high', 'low', '3', '4', 'med', 'med'], ['vhigh', 'vhigh', '5more', '4', 'small', 'low'], ['high', 'med', '4', 'more', 'small', 'high'], ['vhigh', 'vhigh', '3', '4', 'small', 'high'], ['med', 'high', '3', 'more', 'big', 'low'], ['med', 'low', '5more', '2', 'small', 'low'], ['vhigh', 'med', '2', 'more', 'big', 'med'], ['low', 'med', '2', '4', 'big', 'med'], ['vhigh', 'med', '4', 'more', 'med', 'med'], ['high', 'low', '3', '4', 'small', 'low'], ['low', 'vhigh', '5more', '4', 'big', 'low'], ['med', 'low', '5more', 'more', 'big', 'high'], ['vhigh', 'high', '4', 'more', 'med', 'med'], ['vhigh', 'vhigh', '2', '2', 'med', 'high'], ['low', 'low', '5more', '2', 'med', 'high'], ['high', 'low', '4', '2', 'med', 'med'], ['high', 'low', '3', 'more', 'med', 'high'], ['high', 'med', '3', '4', 'med', 'low'], ['med', 'vhigh', '3', '2', 'med', 'low'], ['high', 'med', '5more', '2', 'med', 'med'], ['high', 'low', '4', '2', 'small', 'low'], ['med', 'high', '5more', '4', 'med', 'low'], ['vhigh', 'med', '2', '4', 'big', 'high'], ['low', 'vhigh', '2', '2', 'med', 'high'], ['vhigh', 'med', '5more', 'more', 'med', 'low'], ['med', 'low', '4', '2', 'small', 'high'], ['vhigh', 'high', '2', '2', 'med', 'low'], ['low', 'high', '5more', 'more', 'med', 'low'], ['low', 'low', '2', '4', 'small', 'low'], ['low', 'high', '5more', '4', 'med', 'med'], ['med', 'low', '3', 'more', 'med', 'low'], ['high', 'low', '3', '4', 'small', 'med'], ['high', 'high', '2', '2', 'small', 'high'], ['high', 'low', '3', '4', 'med', 'high'], ['low', 'med', '2', 'more', 'med', 'med'], ['low', 'med', '3', '4', 'med', 'med'], ['med', 'high', '4', 'more', 'small', 'high'], ['high', 'med', '2', 'more', 'small', 'med'], ['low', 'low', '4', 'more', 'med', 'low'], ['med', 'high', '5more', '2', 'med', 'low'], ['high', 'low', '2', 'more', 'med', 'high'], ['high', 'high', '3', '4', 'small', 'low'], ['med', 'vhigh', '5more', '4', 'big', 'med'], ['high', 'low', '4', '4', 'big', 'med'], ['low', 'high', '5more', '2', 'small', 'low'], ['med', 'vhigh', '2', 'more', 'med', 'med'], ['low', 'med', '5more', '4', 'small', 'high'], ['vhigh', 'vhigh', '2', '2', 'small', 'low'], ['vhigh', 'vhigh', '5more', '2', 'small', 'med'], ['low', 'med', '2', '2', 'big', 'med'], ['low', 'low', '5more', '4', 'big', 'med'], ['high', 'low', '5more', 'more', 'big', 'med'], ['low', 'vhigh', '3', '4', 'small', 'med'], ['low', 'low', '2', '2', 'small', 'high'], ['vhigh', 'high', '2', '4', 'med', 'med'], ['med', 'low', '5more', '4', 'big', 'high'], ['med', 'high', '4', '2', 'big', 'high'], ['low', 'low', '4', 'more', 'big', 'high'], ['low', 'low', '5more', 'more', 'big', 'high'], ['med', 'low', '3', '2', 'small', 'high'], ['high', 'med', '4', '4', 'med', 'high'], ['med', 'vhigh', '2', 'more', 'med', 'low'], ['med', 'vhigh', '4', '4', 'big', 'low'], ['med', 'low', '3', '4', 'small', 'low'], ['low', 'med', '4', '4', 'big', 'low'], ['high', 'vhigh', '5more', 'more', 'big', 'high'], ['high', 'med', '2', '2', 'big', 'med'], ['med', 'high', '3', '2', 'big', 'med'], ['high', 'vhigh', '2', '2', 'small', 'low'], ['high', 'high', '5more', '4', 'med', 'high'], ['low', 'med', '4', 'more', 'small', 'low'], ['high', 'high', '4', '2', 'small', 'high'], ['vhigh', 'med', '4', 'more', 'med', 'high'], ['high', 'med', '2', 'more', 'med', 'med'], ['med', 'med', '3', 'more', 'big', 'med'], ['low', 'high', '3', '2', 'big', 'low'], ['high', 'med', '3', '4', 'small', 'low'], ['vhigh', 'low', '4', '4', 'med', 'low'], ['low', 'vhigh', '2', '2', 'small', 'high'], ['med', 'low', '2', '2', 'big', 'med'], ['low', 'low', '3', '2', 'big', 'low'], ['med', 'vhigh', '2', 'more', 'small', 'high'], ['vhigh', 'med', '4', 'more', 'small', 'high'], ['med', 'med', '3', '4', 'big', 'low'], ['med', 'vhigh', '2', '4', 'med', 'low'], ['high', 'high', '4', '4', 'big', 'low'], ['med', 'med', '2', '4', 'med', 'high'], ['vhigh', 'high', '4', '2', 'big', 'high'], ['high', 'low', '2', 'more', 'small', 'med'], ['vhigh', 'high', '4', '2', 'med', 'high'], ['vhigh', 'med', '5more', '4', 'big', 'med'], ['low', 'low', '4', '2', 'small', 'med'], ['vhigh', 'high', '2', '2', 'big', 'low'], ['low', 'med', '4', 'more', 'med', 'high'], ['med', 'high', '5more', 'more', 'big', 'high'], ['low', 'med', '5more', '2', 'small', 'low'], ['vhigh', 'low', '2', '2', 'med', 'low'], ['med', 'vhigh', '2', '4', 'small', 'med'], ['low', 'vhigh', '5more', '4', 'med', 'high'], ['vhigh', 'vhigh', '2', '2', 'small', 'high'], ['low', 'med', '2', '4', 'big', 'high'], ['high', 'vhigh', '3', '2', 'small', 'low'], ['vhigh', 'low', '2', '4', 'small', 'low'], ['med', 'high', '3', '2', 'big', 'low'], ['high', 'vhigh', '4', 'more', 'small', 'high'], ['vhigh', 'high', '4', '2', 'big', 'low'], ['vhigh', 'vhigh', '2', '2', 'med', 'med'], ['high', 'low', '2', '2', 'small', 'low'], ['vhigh', 'low', '3', '4', 'small', 'high'], ['vhigh', 'low', '3', '4', 'med', 'high'], ['med', 'high', '3', 'more', 'med', 'med'], ['med', 'med', '5more', '2', 'small', 'low'], ['med', 'vhigh', '5more', 'more', 'big', 'low'], ['vhigh', 'vhigh', '2', '4', 'big', 'low'], ['high', 'med', '4', 'more', 'small', 'med'], ['low', 'high', '4', '4', 'small', 'high'], ['med', 'low', '4', '4', 'big', 'med'], ['low', 'med', '3', '2', 'big', 'high'], ['high', 'vhigh', '2', 'more', 'med', 'low'], ['low', 'med', '2', 'more', 'small', 'high'], ['low', 'med', '5more', '2', 'big', 'low'], ['high', 'high', '4', '2', 'big', 'low'], ['high', 'med', '3', '2', 'big', 'med'], ['med', 'vhigh', '5more', 'more', 'big', 'high'], ['high', 'high', '5more', 'more', 'med', 'med'], ['vhigh', 'med', '5more', '4', 'small', 'med'], ['low', 'high', '5more', '4', 'med', 'low'], ['high', 'high', '2', '4', 'med', 'high'], ['high', 'med', '5more', 'more', 'small', 'low'], ['high', 'high', '3', 'more', 'big', 'low'], ['high', 'vhigh', '3', '4', 'med', 'high'], ['low', 'high', '4', '4', 'big', 'low'], ['vhigh', 'low', '4', '2', 'med', 'med'], ['vhigh', 'vhigh', '5more', '4', 'small', 'med'], ['low', 'vhigh', '4', '4', 'small', 'low'], ['vhigh', 'low', '3', 'more', 'big', 'low'], ['vhigh', 'high', '4', '2', 'small', 'low'], ['high', 'high', '3', '2', 'small', 'low'], ['vhigh', 'high', '4', '2', 'med', 'low'], ['high', 'low', '2', '2', 'med', 'low'], ['low', 'med', '4', 'more', 'big', 'high'], ['vhigh', 'high', '2', '4', 'small', 'low'], ['low', 'low', '5more', '2', 'small', 'high'], ['low', 'low', '3', '2', 'small', 'high'], ['med', 'med', '2', '2', 'big', 'med'], ['high', 'high', '5more', '4', 'small', 'high'], ['vhigh', 'low', '5more', '2', 'med', 'high'], ['vhigh', 'vhigh', '2', '4', 'small', 'high'], ['med', 'low', '4', '2', 'med', 'low'], ['low', 'high', '4', '4', 'big', 'med'], ['low', 'low', '2', 'more', 'big', 'low'], ['vhigh', 'low', '5more', '4', 'small', 'low'], ['high', 'low', '5more', '4', 'small', 'med'], ['vhigh', 'med', '4', '2', 'small', 'low'], ['high', 'low', '2', '2', 'small', 'high'], ['low', 'vhigh', '3', '4', 'small', 'low'], ['low', 'med', '4', '4', 'small', 'low'], ['low', 'med', '2', '4', 'big', 'low'], ['med', 'med', '2', '4', 'big', 'low'], ['vhigh', 'high', '4', '4', 'big', 'med'], ['vhigh', 'med', '2', 'more', 'med', 'high'], ['low', 'high', '4', 'more', 'big', 'med'], ['low', 'med', '4', '2', 'big', 'low'], ['high', 'med', '2', '2', 'med', 'high'], ['low', 'high', '2', 'more', 'big', 'high'], ['high', 'vhigh', '3', '2', 'med', 'med'], ['vhigh', 'low', '4', 'more', 'med', 'low'], ['low', 'vhigh', '4', '4', 'med', 'low'], ['high', 'low', '5more', '2', 'big', 'high'], ['high', 'vhigh', '5more', 'more', 'small', 'high'], ['high', 'med', '5more', '2', 'small', 'med'], ['med', 'low', '4', 'more', 'big', 'high'], ['med', 'high', '2', 'more', 'big', 'high'], ['high', 'med', '4', 'more', 'big', 'low'], ['low', 'high', '2', '2', 'med', 'high'], ['high', 'vhigh', '5more', '2', 'med', 'med'], ['vhigh', 'high', '2', '2', 'med', 'med'], ['med', 'vhigh', '2', 'more', 'big', 'high'], ['vhigh', 'low', '3', 'more', 'small', 'med'], ['vhigh', 'med', '4', 'more', 'big', 'med'], ['med', 'low', '3', '4', 'med', 'med'], ['med', 'low', '3', '4', 'med', 'high'], ['med', 'med', '5more', '2', 'big', 'med'], ['med', 'med', '3', 'more', 'med', 'low'], ['low', 'low', '4', '4', 'med', 'low'], ['high', 'vhigh', '5more', 'more', 'med', 'low'], ['med', 'high', '4', '4', 'med', 'low'], ['low', 'high', '4', 'more', 'med', 'low'], ['low', 'high', '2', '4', 'small', 'high'], ['vhigh', 'med', '3', '4', 'small', 'med'], ['med', 'med', '4', '4', 'small', 'low'], ['low', 'med', '2', 'more', 'big', 'med'], ['high', 'vhigh', '4', '2', 'small', 'high'], ['low', 'low', '5more', '2', 'med', 'low'], ['med', 'vhigh', '4', '2', 'med', 'low'], ['low', 'med', '4', '4', 'big', 'med'], ['high', 'vhigh', '2', '2', 'med', 'med'], ['vhigh', 'vhigh', '3', '2', 'small', 'med'], ['med', 'med', '5more', '2', 'small', 'high'], ['low', 'high', '2', '2', 'med', 'med'], ['high', 'med', '5more', 'more', 'small', 'med'], ['med', 'vhigh', '2', 'more', 'small', 'med'], ['vhigh', 'med', '4', '2', 'med', 'high'], ['high', 'high', '5more', 'more', 'big', 'med'], ['high', 'vhigh', '2', '2', 'small', 'med'], ['low', 'high', '2', 'more', 'big', 'med'], ['med', 'vhigh', '3', '2', 'small', 'low'], ['high', 'low', '3', '4', 'small', 'high'], ['high', 'vhigh', '2', 'more', 'small', 'high'], ['vhigh', 'med', '3', '4', 'med', 'med'], ['med', 'vhigh', '2', '4', 'med', 'med'], ['high', 'low', '2', '4', 'big', 'low'], ['low', 'med', '2', '4', 'med', 'high'], ['vhigh', 'med', '3', '4', 'med', 'high'], ['low', 'high', '4', 'more', 'small', 'med'], ['med', 'low', '4', '2', 'small', 'med'], ['vhigh', 'low', '3', '2', 'big', 'high'], ['vhigh', 'high', '2', 'more', 'med', 'med'], ['med', 'med', '4', '2', 'med', 'high'], ['med', 'low', '5more', '4', 'small', 'med'], ['high', 'vhigh', '2', 'more', 'big', 'low'], ['med', 'low', '4', 'more', 'big', 'med'], ['high', 'vhigh', '2', 'more', 'small', 'low'], ['med', 'med', '3', '4', 'big', 'high'], ['low', 'low', '5more', 'more', 'big', 'med'], ['low', 'med', '3', '2', 'med', 'med'], ['med', 'high', '2', 'more', 'small', 'high'], ['med', 'med', '3', '4', 'med', 'low'], ['high', 'vhigh', '3', '4', 'small', 'high'], ['low', 'med', '3', '4', 'small', 'med'], ['med', 'med', '2', '2', 'small', 'med'], ['low', 'low', '2', '2', 'small', 'med'], ['low', 'vhigh', '4', '2', 'big', 'low'], ['med', 'vhigh', '5more', '4', 'med', 'high'], ['med', 'vhigh', '4', '2', 'med', 'med'], ['med', 'vhigh', '5more', '2', 'small', 'med'], ['high', 'vhigh', '5more', '4', 'big', 'med'], ['low', 'med', '3', 'more', 'med', 'med'], ['vhigh', 'vhigh', '3', 'more', 'big', 'high'], ['low', 'vhigh', '3', '2', 'small', 'med'], ['low', 'vhigh', '4', '4', 'med', 'med'], ['med', 'med', '4', '4', 'small', 'med'], ['med', 'med', '3', 'more', 'big', 'low'], ['vhigh', 'vhigh', '5more', '2', 'small', 'low'], ['vhigh', 'low', '5more', '4', 'small', 'med'], ['med', 'high', '3', '4', 'med', 'high'], ['vhigh', 'vhigh', '5more', 'more', 'big', 'low'], ['med', 'med', '2', '4', 'big', 'high'], ['high', 'high', '2', 'more', 'med', 'high'], ['low', 'med', '5more', '2', 'med', 'high'], ['vhigh', 'med', '3', '2', 'med', 'high'], ['med', 'vhigh', '2', '4', 'med', 'high'], ['high', 'high', '4', '4', 'big', 'high'], ['vhigh', 'high', '2', '2', 'med', 'high'], ['low', 'med', '2', '2', 'small', 'med'], ['low', 'high', '5more', '2', 'med', 'low'], ['vhigh', 'low', '5more', '2', 'big', 'med'], ['vhigh', 'med', '2', '2', 'big', 'high'], ['high', 'high', '3', '4', 'med', 'low'], ['low', 'med', '2', 'more', 'med', 'high'], ['vhigh', 'vhigh', '3', '4', 'med', 'high'], ['vhigh', 'vhigh', '2', 'more', 'small', 'high'], ['vhigh', 'med', '5more', '2', 'med', 'med'], ['med', 'low', '2', '2', 'med', 'low'], ['low', 'low', '4', '4', 'small', 'med'], ['low', 'high', '3', '2', 'small', 'high'], ['med', 'vhigh', '2', '2', 'small', 'low'], ['vhigh', 'vhigh', '3', 'more', 'small', 'med'], ['high', 'high', '5more', '2', 'big', 'med'], ['high', 'low', '3', '2', 'small', 'high'], ['vhigh', 'high', '4', 'more', 'big', 'low'], ['vhigh', 'med', '3', '2', 'small', 'high'], ['high', 'low', '3', '4', 'med', 'low'], ['high', 'vhigh', '2', '2', 'big', 'low'], ['low', 'high', '4', 'more', 'small', 'low'], ['high', 'high', '5more', '2', 'med', 'low'], ['low', 'high', '5more', '2', 'med', 'high'], ['med', 'med', '2', '2', 'small', 'high'], ['vhigh', 'vhigh', '4', 'more', 'big', 'low'], ['med', 'high', '4', '4', 'small', 'low'], ['high', 'high', '2', '2', 'big', 'med'], ['med', 'med', '3', 'more', 'small', 'low'], ['low', 'med', '3', '4', 'small', 'high'], ['high', 'low', '2', 'more', 'big', 'low'], ['high', 'vhigh', '2', '4', 'med', 'low'], ['med', 'med', '3', 'more', 'big', 'high'], ['vhigh', 'vhigh', '3', 'more', 'small', 'low'], ['vhigh', 'vhigh', '2', 'more', 'big', 'high'], ['vhigh', 'high', '3', 'more', 'small', 'low'], ['high', 'high', '4', 'more', 'small', 'high'], ['high', 'vhigh', '5more', '2', 'big', 'high'], ['high', 'low', '3', '2', 'big', 'high'], ['high', 'vhigh', '4', '2', 'med', 'low'], ['med', 'low', '4', '4', 'med', 'low'], ['med', 'vhigh', '2', '2', 'med', 'med'], ['low', 'high', '3', 'more', 'big', 'med'], ['vhigh', 'low', '3', '2', 'med', 'high'], ['high', 'high', '5more', '2', 'small', 'high'], ['med', 'low', '5more', 'more', 'big', 'med'], ['vhigh', 'low', '3', '4', 'big', 'high'], ['high', 'high', '4', 'more', 'big', 'high'], ['vhigh', 'vhigh', '5more', 'more', 'small', 'low'], ['med', 'vhigh', '5more', '4', 'big', 'high'], ['med', 'high', '5more', 'more', 'big', 'med'], ['high', 'high', '3', '2', 'small', 'high'], ['med', 'vhigh', '3', 'more', 'med', 'high'], ['low', 'high', '4', 'more', 'big', 'high'], ['med', 'med', '4', 'more', 'med', 'high'], ['high', 'med', '3', '2', 'small', 'med'], ['med', 'high', '2', '2', 'small', 'med'], ['vhigh', 'med', '5more', 'more', 'small', 'high'], ['med', 'vhigh', '2', '4', 'small', 'low'], ['med', 'vhigh', '3', '4', 'small', 'low'], ['high', 'vhigh', '2', '4', 'big', 'low'], ['vhigh', 'high', '3', '2', 'med', 'med'], ['high', 'med', '3', '4', 'small', 'high'], ['low', 'vhigh', '4', '4', 'big', 'low'], ['med', 'high', '3', '2', 'small', 'low'], ['low', 'low', '3', '2', 'med', 'med'], ['low', 'vhigh', '2', 'more', 'med', 'med'], ['low', 'high', '3', '2', 'med', 'low'], ['vhigh', 'med', '5more', '2', 'med', 'high'], ['high', 'med', '2', '4', 'med', 'med'], ['med', 'med', '3', 'more', 'med', 'high'], ['low', 'high', '2', '4', 'med', 'high'], ['med', 'high', '3', 'more', 'small', 'low'], ['low', 'low', '5more', 'more', 'small', 'high'], ['vhigh', 'med', '3', '2', 'med', 'med'], ['vhigh', 'low', '2', '2', 'med', 'high'], ['vhigh', 'high', '5more', 'more', 'med', 'med'], ['low', 'vhigh', '3', '4', 'big', 'med'], ['low', 'low', '2', '4', 'med', 'med'], ['med', 'high', '4', '2', 'small', 'low'], ['vhigh', 'vhigh', '3', '4', 'big', 'low'], ['med', 'high', '3', '4', 'med', 'low'], ['vhigh', 'vhigh', '3', '2', 'med', 'low'], ['vhigh', 'vhigh', '2', 'more', 'big', 'med'], ['med', 'vhigh', '4', 'more', 'big', 'med'], ['vhigh', 'med', '2', '4', 'small', 'med'], ['high', 'vhigh', '3', '4', 'med', 'low'], ['vhigh', 'vhigh', '4', '4', 'big', 'high'], ['med', 'high', '5more', '4', 'small', 'high'], ['med', 'med', '2', '2', 'med', 'high'], ['high', 'vhigh', '5more', 'more', 'small', 'med'], ['low', 'vhigh', '2', '2', 'small', 'med'], ['med', 'low', '3', '4', 'small', 'med'], ['vhigh', 'low', '3', 'more', 'med', 'med'], ['vhigh', 'vhigh', '5more', 'more', 'med', 'med'], ['low', 'med', '4', 'more', 'med', 'med'], ['high', 'vhigh', '5more', '4', 'med', 'high'], ['vhigh', 'med', '2', 'more', 'small', 'high'], ['vhigh', 'low', '5more', '2', 'big', 'low'], ['high', 'low', '5more', '4', 'big', 'low'], ['low', 'vhigh', '3', 'more', 'med', 'med'], ['vhigh', 'low', '4', 'more', 'small', 'low'], ['vhigh', 'vhigh', '4', '2', 'small', 'med'], ['med', 'low', '2', 'more', 'big', 'low'], ['low', 'med', '3', '2', 'med', 'low'], ['med', 'high', '5more', 'more', 'med', 'low'], ['high', 'high', '4', 'more', 'med', 'high'], ['vhigh', 'vhigh', '5more', '4', 'small', 'high'], ['med', 'high', '2', '2', 'big', 'med'], ['high', 'high', '2', '2', 'med', 'high'], ['med', 'low', '3', '4', 'big', 'low'], ['med', 'vhigh', '3', 'more', 'small', 'low'], ['vhigh', 'med', '3', '2', 'med', 'low'], ['med', 'low', '2', '4', 'med', 'med'], ['med', 'vhigh', '5more', '2', 'small', 'high'], ['vhigh', 'low', '4', '4', 'small', 'high'], ['med', 'vhigh', '3', 'more', 'big', 'med'], ['vhigh', 'low', '4', 'more', 'big', 'high'], ['med', 'vhigh', '4', '2', 'small', 'high'], ['med', 'vhigh', '2', '2', 'big', 'high'], ['low', 'high', '2', '2', 'small', 'high'], ['high', 'vhigh', '2', '4', 'med', 'high'], ['low', 'high', '5more', '4', 'big', 'med'], ['high', 'high', '5more', '4', 'big', 'low'], ['med', 'vhigh', '4', '2', 'med', 'high'], ['vhigh', 'med', '3', '4', 'med', 'low'], ['high', 'med', '3', '2', 'big', 'low'], ['low', 'med', '2', 'more', 'big', 'low'], ['low', 'med', '3', '2', 'small', 'med'], ['med', 'med', '4', '2', 'small', 'high'], ['vhigh', 'med', '2', '2', 'small', 'high'], ['high', 'med', '2', '4', 'small', 'high'], ['vhigh', 'med', '2', 'more', 'big', 'low'], ['vhigh', 'low', '5more', '4', 'small', 'high'], ['low', 'high', '5more', 'more', 'big', 'low'], ['high', 'low', '5more', 'more', 'small', 'high'], ['low', 'vhigh', '2', 'more', 'med', 'low'], ['high', 'low', '4', '4', 'small', 'med'], ['high', 'high', '4', '4', 'small', 'high'], ['med', 'med', '3', '2', 'med', 'high'], ['high', 'med', '4', 'more', 'small', 'low'], ['low', 'low', '2', '2', 'big', 'low'], ['low', 'high', '2', '4', 'big', 'low'], ['vhigh', 'med', '2', '4', 'big', 'med'], ['high', 'low', '4', '2', 'small', 'med'], ['low', 'low', '3', 'more', 'small', 'low'], ['med', 'high', '5more', '2', 'med', 'high'], ['vhigh', 'med', '3', '2', 'small', 'low'], ['high', 'vhigh', '4', '2', 'med', 'med'], ['low', 'med', '3', '4', 'med', 'high'], ['vhigh', 'vhigh', '5more', '2', 'med', 'low'], ['med', 'high', '4', 'more', 'big', 'low'], ['low', 'high', '3', '2', 'big', 'med'], ['high', 'vhigh', '2', 'more', 'big', 'med'], ['high', 'high', '4', '2', 'big', 'high'], ['med', 'high', '5more', '4', 'small', 'low'], ['vhigh', 'vhigh', '4', '4', 'med', 'low'], ['med', 'med', '2', '4', 'small', 'med'], ['med', 'med', '5more', 'more', 'big', 'med'], ['low', 'low', '2', '2', 'med', 'high'], ['med', 'high', '2', '2', 'small', 'high'], ['low', 'med', '5more', '4', 'med', 'high'], ['low', 'high', '3', '4', 'big', 'high'], ['vhigh', 'high', '3', '2', 'small', 'low'], ['high', 'high', '3', 'more', 'med', 'low'], ['med', 'vhigh', '4', '2', 'big', 'high'], ['med', 'med', '2', '4', 'med', 'low'], ['med', 'low', '2', '2', 'small', 'med'], ['high', 'med', '4', 'more', 'big', 'high'], ['high', 'vhigh', '3', 'more', 'med', 'med'], ['vhigh', 'low', '5more', 'more', 'big', 'low'], ['low', 'low', '4', 'more', 'big', 'low'], ['med', 'high', '4', '4', 'small', 'high'], ['vhigh', 'low', '3', '2', 'small', 'low'], ['high', 'med', '3', '2', 'med', 'high'], ['low', 'low', '3', '4', 'big', 'med'], ['med', 'high', '4', 'more', 'med', 'high'], ['med', 'low', '3', 'more', 'small', 'low'], ['vhigh', 'low', '2', '2', 'small', 'high'], ['vhigh', 'vhigh', '4', '2', 'med', 'high'], ['med', 'med', '4', 'more', 'med', 'med'], ['vhigh', 'high', '5more', '4', 'med', 'med'], ['vhigh', 'vhigh', '3', '2', 'small', 'high'], ['high', 'low', '3', 'more', 'big', 'low'], ['vhigh', 'vhigh', '2', '4', 'med', 'low'], ['low', 'med', '5more', '2', 'med', 'low'], ['low', 'med', '3', '2', 'big', 'low'], ['high', 'high', '2', '2', 'big', 'high'], ['vhigh', 'high', '5more', 'more', 'med', 'high'], ['vhigh', 'med', '5more', 'more', 'small', 'low'], ['med', 'high', '3', '4', 'small', 'low'], ['high', 'low', '5more', '4', 'med', 'med'], ['high', 'high', '3', 'more', 'med', 'high'], ['med', 'med', '5more', '2', 'med', 'low'], ['high', 'med', '2', 'more', 'med', 'low'], ['med', 'med', '3', 'more', 'small', 'med'], ['high', 'low', '3', '2', 'med', 'low'], ['low', 'high', '4', '2', 'med', 'high'], ['high', 'vhigh', '3', 'more', 'med', 'high'], ['med', 'high', '2', '4', 'big', 'high'], ['low', 'vhigh', '3', 'more', 'small', 'med'], ['vhigh', 'low', '4', '2', 'small', 'low'], ['high', 'low', '5more', '4', 'small', 'high'], ['low', 'high', '4', '4', 'small', 'low'], ['vhigh', 'med', '5more', 'more', 'small', 'med'], ['med', 'high', '3', '4', 'small', 'med'], ['low', 'vhigh', '2', 'more', 'big', 'med'], ['low', 'low', '5more', '2', 'big', 'med'], ['high', 'low', '2', '4', 'big', 'high'], ['low', 'vhigh', '2', 'more', 'small', 'high'], ['high', 'vhigh', '2', 'more', 'med', 'high'], ['med', 'med', '4', '4', 'big', 'med'], ['high', 'high', '4', '2', 'small', 'low'], ['vhigh', 'high', '5more', '2', 'big', 'low'], ['high', 'high', '5more', '2', 'big', 'high'], ['low', 'vhigh', '3', '4', 'med', 'med'], ['high', 'high', '5more', '2', 'big', 'low'], ['med', 'vhigh', '5more', '2', 'med', 'med'], ['low', 'vhigh', '5more', 'more', 'med', 'med'], ['med', 'high', '4', '4', 'small', 'med'], ['high', 'vhigh', '3', '2', 'big', 'low'], ['high', 'vhigh', '2', 'more', 'big', 'high'], ['low', 'low', '5more', '2', 'big', 'low'], ['vhigh', 'high', '4', '4', 'med', 'low'], ['high', 'med', '4', '2', 'big', 'med'], ['vhigh', 'high', '2', 'more', 'med', 'high'], ['low', 'low', '3', '4', 'big', 'low'], ['high', 'vhigh', '2', '2', 'big', 'high'], ['med', 'low', '4', 'more', 'med', 'low'], ['low', 'low', '4', '4', 'big', 'low'], ['high', 'vhigh', '5more', 'more', 'big', 'med'], ['low', 'vhigh', '4', '2', 'small', 'high'], ['vhigh', 'low', '4', 'more', 'med', 'high'], ['low', 'low', '3', 'more', 'small', 'high'], ['med', 'low', '2', 'more', 'big', 'high'], ['vhigh', 'med', '3', 'more', 'big', 'low'], ['vhigh', 'low', '2', '4', 'big', 'high'], ['vhigh', 'low', '5more', 'more', 'med', 'high'], ['vhigh', 'med', '2', '2', 'med', 'low'], ['vhigh', 'vhigh', '2', '4', 'big', 'med'], ['low', 'vhigh', '5more', '2', 'big', 'med'], ['high', 'med', '5more', 'more', 'med', 'med'], ['low', 'med', '2', 'more', 'big', 'high'], ['med', 'vhigh', '3', '2', 'big', 'high'], ['vhigh', 'high', '2', '4', 'big', 'med'], ['high', 'med', '3', '2', 'small', 'low'], ['low', 'vhigh', '4', '4', 'big', 'med'], ['med', 'high', '5more', '4', 'med', 'high'], ['vhigh', 'vhigh', '3', '2', 'big', 'med'], ['med', 'low', '5more', 'more', 'small', 'low'], ['med', 'low', '2', '2', 'big', 'low'], ['low', 'med', '5more', 'more', 'small', 'high'], ['vhigh', 'low', '5more', '4', 'big', 'high'], ['low', 'low', '5more', '2', 'med', 'med'], ['med', 'med', '2', 'more', 'big', 'low'], ['low', 'high', '5more', 'more', 'big', 'med'], ['med', 'vhigh', '2', '2', 'big', 'low'], ['vhigh', 'med', '4', '4', 'med', 'med'], ['high', 'low', '5more', '2', 'small', 'high'], ['low', 'low', '5more', '4', 'med', 'med'], ['med', 'low', '3', '2', 'big', 'med'], ['low', 'low', '3', '2', 'small', 'med'], ['vhigh', 'high', '3', 'more', 'big', 'high'], ['low', 'low', '5more', '2', 'small', 'med'], ['vhigh', 'med', '5more', '2', 'small', 'med'], ['med', 'med', '3', '4', 'small', 'high'], ['med', 'med', '5more', '4', 'big', 'med'], ['med', 'low', '4', '4', 'small', 'low'], ['high', 'med', '4', '2', 'small', 'med'], ['low', 'low', '4', '2', 'med', 'low'], ['med', 'low', '3', '2', 'med', 'high'], ['low', 'high', '3', '2', 'small', 'low'], ['high', 'high', '2', '4', 'big', 'high'], ['high', 'med', '4', '2', 'big', 'high'], ['high', 'med', '2', '2', 'med', 'low'], ['low', 'vhigh', '5more', '4', 'med', 'med'], ['low', 'low', '2', '4', 'big', 'med'], ['vhigh', 'high', '5more', '4', 'small', 'high'], ['high', 'med', '4', '2', 'med', 'low'], ['low', 'med', '3', 'more', 'small', 'med'], ['low', 'vhigh', '4', '4', 'big', 'high'], ['high', 'high', '4', 'more', 'small', 'low'], ['med', 'med', '2', 'more', 'med', 'high'], ['high', 'low', '5more', 'more', 'small', 'low'], ['med', 'med', '5more', '4', 'small', 'high'], ['high', 'low', '5more', '2', 'med', 'high'], ['med', 'vhigh', '2', '4', 'big', 'high'], ['low', 'med', '2', '2', 'small', 'high'], ['high', 'med', '5more', 'more', 'big', 'med'], ['low', 'med', '4', '2', 'big', 'med'], ['high', 'high', '2', '4', 'med', 'low'], ['high', 'vhigh', '4', '2', 'small', 'low'], ['low', 'low', '5more', 'more', 'med', 'high'], ['med', 'high', '3', '4', 'big', 'low'], ['vhigh', 'med', '3', 'more', 'big', 'med'], ['high', 'low', '2', '2', 'med', 'med'], ['vhigh', 'vhigh', '2', 'more', 'big', 'low'], ['low', 'med', '5more', '4', 'big', 'low'], ['low', 'vhigh', '3', 'more', 'big', 'low'], ['high', 'med', '2', '4', 'med', 'low'], ['low', 'high', '3', 'more', 'big', 'high'], ['low', 'high', '2', '4', 'big', 'med'], ['vhigh', 'low', '4', 'more', 'med', 'med'], ['vhigh', 'high', '5more', '4', 'small', 'med'], ['low', 'low', '2', '2', 'small', 'low'], ['med', 'vhigh', '5more', 'more', 'small', 'med'], ['high', 'low', '2', '4', 'med', 'med'], ['high', 'high', '2', 'more', 'big', 'low'], ['high', 'high', '4', 'more', 'med', 'med'], ['vhigh', 'vhigh', '3', '2', 'med', 'med'], ['vhigh', 'vhigh', '5more', 'more', 'big', 'med'], ['low', 'vhigh', '3', '2', 'small', 'high'], ['high', 'high', '2', 'more', 'small', 'high'], ['high', 'med', '4', '4', 'med', 'med'], ['vhigh', 'high', '3', '2', 'med', 'high'], ['high', 'med', '4', '4', 'big', 'high'], ['low', 'high', '3', '4', 'small', 'med'], ['vhigh', 'med', '2', 'more', 'small', 'low'], ['low', 'vhigh', '5more', '2', 'big', 'low'], ['high', 'vhigh', '4', '4', 'med', 'high'], ['med', 'low', '2', 'more', 'small', 'high'], ['low', 'med', '2', 'more', 'med', 'low'], ['low', 'low', '2', '2', 'med', 'med'], ['vhigh', 'med', '4', '2', 'big', 'high'], ['med', 'med', '2', 'more', 'big', 'high'], ['vhigh', 'vhigh', '5more', 'more', 'med', 'low'], ['high', 'high', '3', '2', 'big', 'high'], ['med', 'med', '4', '2', 'small', 'med'], ['high', 'low', '4', 'more', 'small', 'high'], ['med', 'med', '5more', 'more', 'big', 'low'], ['high', 'low', '4', '2', 'big', 'low'], ['low', 'low', '3', 'more', 'med', 'low'], ['vhigh', 'low', '5more', '2', 'small', 'high'], ['vhigh', 'high', '2', 'more', 'big', 'med'], ['med', 'med', '5more', '2', 'big', 'high'], ['vhigh', 'high', '3', '4', 'med', 'low'], ['med', 'low', '4', 'more', 'med', 'med'], ['vhigh', 'low', '2', 'more', 'big', 'low'], ['vhigh', 'med', '5more', '2', 'big', 'med'], ['vhigh', 'high', '5more', '4', 'big', 'high'], ['vhigh', 'low', '4', '2', 'big', 'high'], ['vhigh', 'high', '4', '4', 'big', 'low'], ['low', 'vhigh', '4', 'more', 'small', 'high'], ['high', 'high', '5more', 'more', 'med', 'low'], ['vhigh', 'high', '3', 'more', 'small', 'high'], ['low', 'high', '4', '2', 'big', 'med'], ['low', 'med', '5more', 'more', 'big', 'high'], ['vhigh', 'vhigh', '5more', 'more', 'big', 'high'], ['low', 'med', '3', 'more', 'big', 'med'], ['med', 'low', '5more', '4', 'big', 'low'], ['high', 'med', '3', '4', 'big', 'med'], ['med', 'vhigh', '4', 'more', 'small', 'high'], ['low', 'vhigh', '3', '2', 'big', 'high'], ['med', 'low', '4', '2', 'big', 'med'], ['high', 'vhigh', '5more', 'more', 'big', 'low'], ['low', 'high', '2', '2', 'small', 'low'], ['med', 'low', '2', 'more', 'big', 'med'], ['med', 'vhigh', '4', '2', 'big', 'med'], ['vhigh', 'low', '4', 'more', 'big', 'med'], ['med', 'vhigh', '2', '4', 'big', 'low'], ['high', 'med', '2', '4', 'big', 'low'], ['high', 'high', '5more', '4', 'big', 'med'], ['vhigh', 'low', '2', 'more', 'small', 'high'], ['med', 'med', '4', '4', 'med', 'high'], ['med', 'low', '2', '2', 'big', 'high'], ['vhigh', 'med', '2', '2', 'med', 'med'], ['med', 'med', '5more', 'more', 'med', 'low'], ['vhigh', 'vhigh', '4', '2', 'small', 'low'], ['high', 'low', '4', '2', 'big', 'med'], ['vhigh', 'med', '2', '2', 'small', 'low'], ['low', 'med', '5more', 'more', 'big', 'med'], ['low', 'high', '3', 'more', 'small', 'high'], ['vhigh', 'med', '3', '4', 'small', 'low'], ['vhigh', 'high', '2', '4', 'small', 'high'], ['high', 'high', '2', '4', 'med', 'med'], ['med', 'low', '3', 'more', 'med', 'high'], ['vhigh', 'vhigh', '2', '2', 'small', 'med'], ['high', 'vhigh', '4', '4', 'big', 'high'], ['vhigh', 'low', '4', '2', 'small', 'high'], ['vhigh', 'high', '3', '2', 'big', 'high'], ['vhigh', 'med', '3', '2', 'small', 'med'], ['med', 'vhigh', '3', '2', 'med', 'high'], ['high', 'high', '2', '2', 'small', 'med'], ['low', 'high', '5more', 'more', 'big', 'high'], ['vhigh', 'low', '2', 'more', 'med', 'low'], ['high', 'vhigh', '5more', 'more', 'med', 'med'], ['high', 'low', '4', 'more', 'med', 'high'], ['low', 'high', '2', '2', 'small', 'med'], ['low', 'vhigh', '4', 'more', 'small', 'low'], ['med', 'low', '3', 'more', 'small', 'med'], ['med', 'med', '4', 'more', 'small', 'high'], ['low', 'high', '4', '2', 'small', 'low'], ['low', 'low', '5more', 'more', 'med', 'med'], ['vhigh', 'vhigh', '4', '2', 'big', 'med'], ['high', 'high', '2', '4', 'big', 'low'], ['med', 'med', '2', '4', 'med', 'med'], ['high', 'med', '4', '4', 'small', 'med'], ['vhigh', 'high', '3', '2', 'big', 'low'], ['vhigh', 'vhigh', '4', '2', 'big', 'high'], ['vhigh', 'med', '4', '4', 'big', 'low'], ['med', 'high', '2', '4', 'med', 'high'], ['vhigh', 'high', '2', 'more', 'big', 'low'], ['high', 'vhigh', '4', '2', 'big', 'high'], ['med', 'high', '2', '2', 'small', 'low'], ['vhigh', 'vhigh', '5more', 'more', 'small', 'high'], ['med', 'vhigh', '5more', '2', 'big', 'high'], ['high', 'med', '3', '4', 'med', 'med'], ['vhigh', 'high', '4', 'more', 'big', 'high'], ['low', 'vhigh', '3', 'more', 'med', 'low'], ['low', 'vhigh', '3', '4', 'med', 'low'], ['low', 'med', '4', '2', 'med', 'low'], ['vhigh', 'low', '3', '2', 'med', 'low'], ['high', 'vhigh', '2', 'more', 'small', 'med'], ['med', 'med', '3', '2', 'big', 'low'], ['low', 'med', '4', 'more', 'big', 'med'], ['low', 'high', '5more', '4', 'med', 'high'], ['vhigh', 'vhigh', '2', 'more', 'small', 'low'], ['low', 'low', '3', '2', 'big', 'high'], ['low', 'vhigh', '5more', '4', 'big', 'med'], ['med', 'low', '2', '2', 'med', 'med'], ['med', 'med', '5more', '2', 'med', 'high'], ['vhigh', 'low', '3', '2', 'big', 'med'], ['med', 'high', '3', 'more', 'big', 'med'], ['low', 'high', '2', 'more', 'small', 'med'], ['vhigh', 'med', '3', 'more', 'small', 'med'], ['low', 'med', '2', '4', 'med', 'low'], ['vhigh', 'med', '2', '2', 'med', 'high'], ['vhigh', 'vhigh', '5more', '4', 'med', 'med'], ['med', 'vhigh', '4', '4', 'small', 'low'], ['med', 'low', '4', 'more', 'med', 'high'], ['high', 'low', '5more', '2', 'big', 'low'], ['low', 'low', '5more', 'more', 'med', 'low'], ['med', 'vhigh', '4', '4', 'med', 'low'], ['high', 'low', '4', '4', 'small', 'high'], ['med', 'high', '3', '4', 'big', 'med'], ['med', 'high', '4', 'more', 'small', 'med'], ['vhigh', 'high', '4', 'more', 'med', 'high'], ['high', 'med', '2', '2', 'small', 'low'], ['med', 'low', '4', '4', 'med', 'med'], ['med', 'vhigh', '5more', 'more', 'big', 'med'], ['high', 'low', '3', 'more', 'small', 'high'], ['med', 'low', '3', '2', 'big', 'high'], ['high', 'vhigh', '3', '2', 'big', 'med'], ['low', 'vhigh', '4', '2', 'small', 'med'], ['high', 'med', '3', '4', 'big', 'high'], ['vhigh', 'med', '2', 'more', 'med', 'med'], ['low', 'med', '3', 'more', 'small', 'high'], ['high', 'med', '2', '4', 'small', 'low'], ['vhigh', 'med', '4', '2', 'small', 'med'], ['high', 'high', '2', '4', 'big', 'med'], ['med', 'vhigh', '3', '4', 'big', 'high'], ['med', 'vhigh', '2', '4', 'big', 'med'], ['vhigh', 'high', '2', 'more', 'med', 'low'], ['med', 'low', '5more', '4', 'med', 'low'], ['low', 'low', '3', '4', 'small', 'med'], ['vhigh', 'high', '2', 'more', 'small', 'low'], ['low', 'vhigh', '5more', '4', 'big', 'high'], ['high', 'med', '3', '4', 'med', 'high'], ['vhigh', 'vhigh', '5more', '4', 'med', 'high'], ['high', 'vhigh', '4', '2', 'med', 'high'], ['high', 'vhigh', '4', 'more', 'small', 'low'], ['med', 'low', '5more', '4', 'small', 'high'], ['high', 'med', '3', 'more', 'big', 'high'], ['med', 'med', '5more', 'more', 'med', 'med'], ['high', 'vhigh', '4', 'more', 'big', 'high'], ['high', 'med', '5more', '4', 'small', 'low'], ['high', 'low', '5more', 'more', 'med', 'high'], ['low', 'vhigh', '4', '4', 'small', 'med'], ['high', 'vhigh', '5more', '2', 'small', 'med'], ['high', 'med', '3', '2', 'med', 'low'], ['low', 'vhigh', '5more', 'more', 'med', 'high'], ['vhigh', 'med', '4', 'more', 'med', 'low'], ['vhigh', 'high', '5more', '2', 'med', 'high'], ['med', 'low', '2', '4', 'big', 'low'], ['vhigh', 'low', '5more', '2', 'big', 'high'], ['low', 'med', '2', '4', 'small', 'high'], ['low', 'high', '4', '4', 'big', 'high'], ['vhigh', 'med', '5more', '4', 'small', 'high'], ['med', 'med', '5more', '4', 'big', 'high'], ['low', 'vhigh', '5more', 'more', 'small', 'med'], ['low', 'vhigh', '4', 'more', 'big', 'med'], ['high', 'vhigh', '3', 'more', 'big', 'med'], ['med', 'med', '4', '2', 'small', 'low'], ['med', 'low', '4', '4', 'small', 'med'], ['med', 'vhigh', '3', '2', 'small', 'high'], ['med', 'low', '2', '4', 'small', 'med'], ['high', 'med', '5more', '2', 'big', 'low'], ['vhigh', 'low', '2', 'more', 'small', 'low'], ['low', 'low', '2', 'more', 'med', 'med'], ['vhigh', 'high', '5more', 'more', 'big', 'low'], ['vhigh', 'vhigh', '4', '2', 'med', 'low'], ['vhigh', 'med', '3', '4', 'big', 'high'], ['med', 'med', '5more', '4', 'small', 'med'], ['high', 'high', '5more', 'more', 'med', 'high'], ['vhigh', 'low', '4', '2', 'med', 'low'], ['low', 'high', '4', '2', 'med', 'low'], ['med', 'high', '2', '2', 'med', 'med'], ['med', 'vhigh', '3', '4', 'small', 'high'], ['low', 'low', '2', 'more', 'big', 'high'], ['low', 'med', '2', '2', 'med', 'high'], ['vhigh', 'low', '3', '2', 'big', 'low'], ['low', 'vhigh', '4', '2', 'big', 'med'], ['low', 'low', '4', '2', 'small', 'high'], ['low', 'low', '3', 'more', 'small', 'med'], ['high', 'med', '5more', 'more', 'med', 'high'], ['vhigh', 'high', '3', '4', 'med', 'med'], ['med', 'med', '2', '4', 'small', 'low'], ['med', 'low', '4', '2', 'med', 'high'], ['low', 'low', '3', '4', 'med', 'low'], ['high', 'med', '2', '2', 'big', 'high'], ['med', 'low', '3', '4', 'big', 'high'], ['high', 'high', '3', '2', 'big', 'med'], ['high', 'med', '3', 'more', 'big', 'med'], ['high', 'low', '4', '4', 'small', 'low'], ['high', 'low', '2', 'more', 'small', 'low'], ['med', 'med', '3', 'more', 'small', 'high'], ['low', 'high', '2', 'more', 'big', 'low'], ['med', 'med', '5more', 'more', 'small', 'med'], ['vhigh', 'med', '4', '2', 'big', 'med'], ['low', 'high', '5more', '4', 'big', 'high'], ['med', 'med', '5more', '2', 'big', 'low'], ['vhigh', 'low', '4', '2', 'small', 'med'], ['high', 'low', '2', '4', 'small', 'low'], ['vhigh', 'low', '4', '4', 'small', 'med'], ['med', 'vhigh', '5more', '4', 'big', 'low'], ['high', 'vhigh', '4', '2', 'small', 'med'], ['vhigh', 'high', '5more', '4', 'med', 'high'], ['vhigh', 'low', '2', '2', 'small', 'med'], ['high', 'med', '2', 'more', 'small', 'high'], ['low', 'med', '5more', '2', 'small', 'high'], ['high', 'vhigh', '2', '4', 'small', 'med'], ['med', 'med', '4', 'more', 'big', 'high'], ['vhigh', 'med', '4', '2', 'med', 'med'], ['low', 'vhigh', '2', '4', 'med', 'low'], ['high', 'high', '4', '2', 'med', 'low'], ['med', 'vhigh', '3', '4', 'med', 'high'], ['low', 'vhigh', '4', '4', 'med', 'high'], ['low', 'low', '3', 'more', 'big', 'med'], ['low', 'med', '4', '4', 'med', 'low'], ['low', 'vhigh', '4', '4', 'small', 'high'], ['med', 'low', '3', 'more', 'small', 'high'], ['vhigh', 'high', '3', 'more', 'med', 'high'], ['low', 'vhigh', '3', '4', 'big', 'low'], ['low', 'low', '4', 'more', 'small', 'high'], ['high', 'vhigh', '2', '2', 'med', 'high'], ['high', 'med', '5more', '4', 'big', 'high'], ['high', 'low', '2', 'more', 'small', 'high'], ['med', 'med', '5more', '4', 'med', 'low'], ['low', 'vhigh', '2', 'more', 'big', 'low'], ['vhigh', 'high', '2', '4', 'big', 'high'], ['high', 'high', '3', '2', 'small', 'med'], ['med', 'high', '5more', '4', 'big', 'high'], ['high', 'vhigh', '3', '4', 'big', 'med'], ['med', 'med', '4', '4', 'big', 'low'], ['med', 'vhigh', '4', 'more', 'small', 'med'], ['high', 'vhigh', '3', 'more', 'small', 'low'], ['low', 'med', '4', '2', 'small', 'high'], ['high', 'high', '5more', '4', 'small', 'low'], ['vhigh', 'high', '4', 'more', 'small', 'high'], ['med', 'high', '4', '2', 'big', 'med'], ['vhigh', 'med', '5more', '4', 'small', 'low'], ['low', 'low', '3', '2', 'big', 'med'], ['high', 'low', '3', 'more', 'small', 'low'], ['low', 'med', '4', 'more', 'small', 'med'], ['med', 'high', '4', '4', 'big', 'low'], ['vhigh', 'high', '5more', '2', 'small', 'med'], ['low', 'med', '2', '2', 'big', 'low'], ['low', 'vhigh', '2', '2', 'small', 'low'], ['high', 'high', '5more', 'more', 'small', 'low'], ['high', 'med', '3', '2', 'big', 'high'], ['high', 'high', '5more', '2', 'small', 'med'], ['high', 'high', '5more', 'more', 'small', 'high'], ['high', 'vhigh', '5more', '4', 'big', 'low'], ['vhigh', 'high', '3', 'more', 'med', 'med'], ['high', 'high', '4', 'more', 'big', 'med'], ['med', 'med', '2', 'more', 'med', 'med'], ['med', 'high', '2', 'more', 'small', 'low'], ['vhigh', 'med', '4', '2', 'med', 'low'], ['low', 'low', '3', '2', 'med', 'low'], ['low', 'high', '2', '4', 'med', 'med'], ['vhigh', 'low', '5more', '4', 'med', 'med'], ['med', 'vhigh', '3', '4', 'big', 'low'], ['med', 'med', '4', 'more', 'med', 'low'], ['low', 'high', '4', 'more', 'small', 'high'], ['med', 'med', '2', 'more', 'med', 'low'], ['vhigh', 'vhigh', '4', '4', 'big', 'low'], ['low', 'high', '2', '4', 'big', 'high'], ['low', 'med', '3', 'more', 'small', 'low'], ['med', 'vhigh', '5more', '4', 'med', 'low'], ['low', 'low', '2', 'more', 'small', 'med'], ['high', 'high', '3', '4', 'small', 'high'], ['vhigh', 'vhigh', '2', 'more', 'med', 'low'], ['low', 'vhigh', '2', 'more', 'med', 'high'], ['high', 'high', '4', '4', 'small', 'low'], ['med', 'low', '5more', '2', 'big', 'low'], ['high', 'low', '2', 'more', 'big', 'med'], ['med', 'high', '3', 'more', 'med', 'high'], ['vhigh', 'low', '2', '2', 'big', 'high'], ['vhigh', 'high', '2', '2', 'small', 'high'], ['vhigh', 'med', '4', '2', 'big', 'low'], ['high', 'vhigh', '5more', '4', 'small', 'low'], ['high', 'low', '4', 'more', 'small', 'low'], ['med', 'low', '4', '2', 'med', 'med'], ['high', 'med', '4', '4', 'small', 'low'], ['vhigh', 'high', '4', '2', 'small', 'med'], ['low', 'low', '4', 'more', 'med', 'high'], ['med', 'vhigh', '4', 'more', 'big', 'low'], ['med', 'high', '4', '2', 'med', 'low'], ['high', 'high', '2', '2', 'big', 'low'], ['med', 'vhigh', '4', 'more', 'med', 'med'], ['low', 'vhigh', '4', 'more', 'big', 'low'], ['med', 'high', '2', 'more', 'med', 'low'], ['low', 'high', '3', '2', 'med', 'med'], ['vhigh', 'high', '2', 'more', 'big', 'high'], ['med', 'low', '5more', '4', 'med', 'med'], ['med', 'high', '2', 'more', 'med', 'med'], ['vhigh', 'vhigh', '2', '2', 'big', 'low'], ['high', 'med', '4', '4', 'big', 'low'], ['vhigh', 'high', '3', '4', 'med', 'high'], ['vhigh', 'low', '5more', '4', 'med', 'high'], ['med', 'vhigh', '5more', '4', 'small', 'high'], ['low', 'low', '4', '2', 'big', 'high'], ['med', 'med', '4', 'more', 'big', 'med'], ['med', 'high', '4', 'more', 'big', 'med'], ['med', 'low', '5more', 'more', 'med', 'low'], ['med', 'high', '2', '4', 'big', 'low'], ['med', 'vhigh', '2', 'more', 'big', 'med'], ['low', 'med', '5more', '4', 'small', 'med'], ['vhigh', 'med', '4', '4', 'med', 'low'], ['med', 'low', '4', '4', 'small', 'high'], ['low', 'low', '3', '2', 'med', 'high'], ['vhigh', 'low', '4', '4', 'med', 'high'], ['med', 'low', '2', '2', 'small', 'low'], ['med', 'low', '2', '4', 'med', 'low'], ['med', 'low', '5more', 'more', 'med', 'high'], ['vhigh', 'low', '5more', '2', 'med', 'low'], ['low', 'high', '5more', 'more', 'small', 'low'], ['high', 'low', '3', '4', 'big', 'low'], ['vhigh', 'high', '5more', '4', 'big', 'med'], ['high', 'vhigh', '2', '4', 'small', 'low'], ['high', 'low', '3', '2', 'big', 'med'], ['low', 'high', '5more', 'more', 'med', 'high'], ['med', 'med', '5more', '2', 'med', 'med'], ['high', 'vhigh', '2', '4', 'small', 'high'], ['high', 'low', '4', '4', 'big', 'low'], ['vhigh', 'vhigh', '3', '2', 'big', 'high'], ['vhigh', 'high', '5more', '2', 'med', 'med'], ['low', 'low', '4', '4', 'med', 'high'], ['med', 'vhigh', '5more', '4', 'small', 'low'], ['med', 'high', '5more', 'more', 'small', 'med'], ['low', 'high', '3', 'more', 'small', 'low'], ['high', 'high', '5more', 'more', 'small', 'med'], ['vhigh', 'vhigh', '2', 'more', 'med', 'high'], ['high', 'low', '3', '2', 'small', 'low'], ['high', 'low', '4', '4', 'med', 'high'], ['vhigh', 'low', '4', '2', 'big', 'low'], ['med', 'low', '5more', '2', 'med', 'low'], ['med', 'med', '2', '4', 'small', 'high'], ['high', 'vhigh', '3', '2', 'small', 'med'], ['vhigh', 'high', '4', '2', 'med', 'med'], ['high', 'med', '3', '2', 'small', 'high'], ['med', 'med', '3', '4', 'med', 'high'], ['low', 'med', '4', '4', 'small', 'high'], ['med', 'vhigh', '5more', 'more', 'med', 'med'], ['low', 'high', '2', '4', 'small', 'med'], ['high', 'vhigh', '5more', '2', 'big', 'low'], ['high', 'low', '4', 'more', 'small', 'med'], ['high', 'high', '2', 'more', 'med', 'low'], ['med', 'high', '2', '4', 'big', 'med'], ['vhigh', 'high', '2', '2', 'small', 'med'], ['low', 'low', '4', '2', 'med', 'high'], ['low', 'med', '2', '2', 'med', 'med'], ['vhigh', 'low', '3', 'more', 'small', 'low'], ['high', 'low', '3', '2', 'small', 'med'], ['med', 'low', '3', '2', 'med', 'low'], ['vhigh', 'vhigh', '4', '2', 'big', 'low'], ['med', 'vhigh', '3', '4', 'med', 'low'], ['med', 'vhigh', '5more', '4', 'small', 'med'], ['vhigh', 'med', '4', 'more', 'big', 'high'], ['med', 'med', '5more', '4', 'med', 'med'], ['high', 'med', '5more', '2', 'med', 'high'], ['high', 'low', '4', '4', 'big', 'high'], ['high', 'vhigh', '3', 'more', 'med', 'low'], ['low', 'low', '2', 'more', 'med', 'high'], ['vhigh', 'med', '5more', 'more', 'big', 'med'], ['vhigh', 'low', '2', 'more', 'med', 'high'], ['high', 'vhigh', '5more', '4', 'med', 'med'], ['med', 'high', '5more', '2', 'small', 'low'], ['high', 'vhigh', '4', 'more', 'med', 'high'], ['med', 'high', '5more', '2', 'med', 'med'], ['high', 'med', '2', 'more', 'big', 'med'], ['low', 'vhigh', '3', '4', 'big', 'high'], ['low', 'low', '4', 'more', 'small', 'low'], ['med', 'med', '3', '2', 'small', 'low'], ['vhigh', 'med', '5more', 'more', 'med', 'med'], ['vhigh', 'med', '2', '4', 'med', 'med'], ['high', 'high', '3', 'more', 'small', 'high'], ['med', 'high', '5more', 'more', 'small', 'high'], ['vhigh', 'low', '3', '2', 'small', 'high'], ['med', 'vhigh', '3', '2', 'big', 'med'], ['high', 'med', '2', 'more', 'big', 'high'], ['low', 'vhigh', '4', '2', 'small', 'low'], ['high', 'low', '3', '4', 'big', 'med'], ['high', 'high', '4', '4', 'med', 'low'], ['vhigh', 'med', '3', 'more', 'small', 'high'], ['vhigh', 'vhigh', '4', 'more', 'med', 'med'], ['low', 'vhigh', '3', '2', 'med', 'med'], ['vhigh', 'high', '4', '4', 'med', 'high'], ['vhigh', 'high', '3', '2', 'small', 'med'], ['high', 'low', '5more', '4', 'big', 'med'], ['vhigh', 'med', '5more', '2', 'med', 'low'], ['med', 'med', '3', '4', 'small', 'low'], ['vhigh', 'low', '3', '4', 'small', 'low'], ['high', 'med', '5more', '2', 'small', 'low'], ['vhigh', 'high', '3', '4', 'small', 'med'], ['low', 'high', '5more', '4', 'small', 'low'], ['med', 'med', '5more', 'more', 'small', 'low'], ['med', 'low', '2', '4', 'small', 'high'], ['vhigh', 'high', '4', '4', 'small', 'med'], ['med', 'vhigh', '2', '4', 'small', 'high'], ['med', 'med', '3', '4', 'big', 'med'], ['high', 'vhigh', '5more', 'more', 'small', 'low'], ['med', 'low', '3', '2', 'big', 'low'], ['low', 'med', '5more', 'more', 'small', 'med'], ['vhigh', 'med', '5more', 'more', 'med', 'high'], ['low', 'high', '3', 'more', 'big', 'low'], ['high', 'low', '3', 'more', 'big', 'med'], ['high', 'high', '4', '4', 'med', 'med'], ['med', 'high', '3', '2', 'big', 'high'], ['high', 'low', '5more', 'more', 'med', 'low'], ['med', 'high', '2', '2', 'big', 'low'], ['low', 'high', '4', '4', 'med', 'low'], ['med', 'high', '2', 'more', 'small', 'med'], ['high', 'high', '2', '4', 'small', 'med'], ['high', 'low', '2', '4', 'small', 'high'], ['vhigh', 'med', '4', 'more', 'small', 'med'], ['med', 'vhigh', '4', 'more', 'small', 'low'], ['high', 'med', '5more', '2', 'med', 'low'], ['high', 'high', '4', '2', 'small', 'med'], ['med', 'high', '3', 'more', 'small', 'high'], ['low', 'med', '3', '4', 'big', 'low'], ['med', 'vhigh', '5more', 'more', 'small', 'low'], ['low', 'low', '2', '4', 'big', 'low'], ['med', 'low', '4', 'more', 'big', 'low'], ['low', 'high', '3', 'more', 'med', 'low'], ['low', 'high', '2', '2', 'big', 'high'], ['low', 'low', '5more', '4', 'med', 'high'], ['high', 'low', '3', '4', 'big', 'high'], ['low', 'vhigh', '2', '4', 'big', 'med'], ['high', 'med', '3', '4', 'small', 'med'], ['low', 'low', '3', '2', 'small', 'low'], ['low', 'high', '4', 'more', 'med', 'high'], ['vhigh', 'med', '3', '2', 'big', 'high'], ['low', 'vhigh', '5more', '2', 'med', 'low'], ['high', 'vhigh', '5more', '2', 'big', 'med'], ['vhigh', 'med', '3', '2', 'big', 'low'], ['high', 'low', '3', 'more', 'big', 'high'], ['high', 'med', '4', '2', 'med', 'med'], ['vhigh', 'med', '4', '4', 'big', 'med'], ['low', 'high', '3', '4', 'small', 'high'], ['high', 'low', '3', 'more', 'small', 'med'], ['low', 'med', '3', 'more', 'big', 'high'], ['med', 'med', '5more', '4', 'small', 'low'], ['vhigh', 'med', '2', 'more', 'med', 'low'], ['vhigh', 'low', '4', '4', 'big', 'low'], ['med', 'high', '3', '2', 'med', 'high'], ['high', 'vhigh', '2', '4', 'big', 'med'], ['vhigh', 'high', '3', 'more', 'med', 'low'], ['low', 'high', '2', '2', 'big', 'med'], ['high', 'low', '2', '2', 'big', 'med'], ['low', 'med', '2', '4', 'small', 'low'], ['high', 'high', '3', '4', 'big', 'low'], ['vhigh', 'vhigh', '2', '4', 'small', 'med'], ['vhigh', 'high', '3', '4', 'big', 'med'], ['med', 'low', '4', '4', 'med', 'high'], ['med', 'low', '2', '4', 'big', 'high'], ['vhigh', 'low', '5more', 'more', 'med', 'low'], ['med', 'vhigh', '5more', '2', 'med', 'low'], ['med', 'high', '2', 'more', 'med', 'high'], ['low', 'vhigh', '2', '4', 'small', 'low'], ['high', 'low', '5more', '2', 'med', 'med'], ['vhigh', 'high', '2', '4', 'med', 'high'], ['med', 'vhigh', '4', '2', 'big', 'low'], ['vhigh', 'vhigh', '3', '2', 'med', 'high'], ['low', 'med', '4', '2', 'big', 'high'], ['vhigh', 'low', '5more', '2', 'small', 'med'], ['high', 'high', '5more', '2', 'med', 'high'], ['low', 'high', '3', 'more', 'med', 'high'], ['vhigh', 'high', '5more', 'more', 'small', 'low'], ['high', 'low', '5more', '2', 'big', 'med'], ['low', 'vhigh', '5more', 'more', 'big', 'high'], ['low', 'high', '2', 'more', 'med', 'low'], ['low', 'med', '3', '2', 'small', 'low'], ['high', 'low', '2', '2', 'big', 'low'], ['low', 'high', '2', '4', 'small', 'low'], ['high', 'high', '3', 'more', 'big', 'high'], ['low', 'med', '5more', '4', 'big', 'high'], ['med', 'low', '4', '2', 'big', 'high'], ['vhigh', 'med', '2', 'more', 'big', 'high'], ['med', 'high', '5more', '4', 'small', 'med'], ['vhigh', 'vhigh', '3', '4', 'med', 'low'], ['high', 'med', '5more', 'more', 'small', 'high'], ['low', 'low', '2', '2', 'big', 'med'], ['low', 'vhigh', '4', '2', 'big', 'high'], ['vhigh', 'high', '5more', '2', 'med', 'low'], ['vhigh', 'low', '3', 'more', 'small', 'high'], ['low', 'high', '3', '2', 'med', 'high'], ['low', 'low', '3', 'more', 'med', 'med'], ['high', 'low', '5more', 'more', 'small', 'med'], ['vhigh', 'vhigh', '5more', 'more', 'med', 'high'], ['low', 'med', '5more', '2', 'small', 'med'], ['low', 'low', '5more', '4', 'small', 'high'], ['low', 'med', '4', '2', 'small', 'low'], ['med', 'high', '4', 'more', 'med', 'low'], ['med', 'vhigh', '2', '2', 'big', 'med'], ['med', 'vhigh', '2', 'more', 'big', 'low'], ['vhigh', 'low', '5more', 'more', 'big', 'med'], ['low', 'vhigh', '5more', 'more', 'big', 'med'], ['high', 'vhigh', '2', '2', 'med', 'low'], ['low', 'low', '4', 'more', 'small', 'med'], ['low', 'vhigh', '2', 'more', 'small', 'low'], ['med', 'med', '2', '4', 'big', 'med'], ['high', 'high', '5more', '4', 'med', 'med'], ['med', 'vhigh', '4', '4', 'small', 'high'], ['med', 'vhigh', '5more', '2', 'big', 'low'], ['low', 'low', '5more', 'more', 'small', 'low'], ['med', 'high', '5more', '4', 'med', 'med'], ['vhigh', 'vhigh', '3', 'more', 'med', 'high'], ['high', 'vhigh', '2', '4', 'med', 'med'], ['vhigh', 'high', '5more', 'more', 'big', 'high'], ['low', 'high', '5more', '2', 'big', 'low'], ['low', 'low', '4', '2', 'small', 'low'], ['vhigh', 'vhigh', '3', '4', 'med', 'med'], ['vhigh', 'high', '5more', '4', 'big', 'low'], ['low', 'med', '2', '4', 'small', 'med'], ['high', 'vhigh', '4', '2', 'big', 'med'], ['vhigh', 'high', '4', '4', 'big', 'high'], ['low', 'med', '5more', 'more', 'small', 'low'], ['vhigh', 'med', '2', '4', 'small', 'high'], ['vhigh', 'low', '2', '2', 'med', 'med'], ['high', 'low', '5more', '2', 'small', 'low'], ['high', 'low', '3', '2', 'med', 'med'], ['vhigh', 'vhigh', '4', 'more', 'small', 'low'], ['vhigh', 'med', '5more', '4', 'med', 'low'], ['vhigh', 'vhigh', '2', '4', 'small', 'low'], ['med', 'high', '5more', 'more', 'big', 'low'], ['high', 'high', '3', '2', 'med', 'low'], ['low', 'low', '2', '2', 'big', 'high'], ['low', 'med', '5more', '4', 'big', 'med'], ['high', 'vhigh', '5more', '4', 'med', 'low'], ['low', 'low', '4', '2', 'med', 'med'], ['low', 'high', '3', 'more', 'med', 'med'], ['med', 'low', '5more', '2', 'big', 'high'], ['med', 'low', '5more', '4', 'small', 'low'], ['high', 'vhigh', '3', '2', 'big', 'high'], ['med', 'low', '2', 'more', 'med', 'med'], ['vhigh', 'high', '4', '2', 'small', 'high'], ['high', 'vhigh', '5more', '2', 'small', 'low'], ['vhigh', 'high', '3', '4', 'big', 'low'], ['med', 'vhigh', '3', 'more', 'small', 'med'], ['low', 'high', '4', '2', 'med', 'med'], ['high', 'vhigh', '3', '4', 'big', 'high'], ['high', 'vhigh', '4', 'more', 'small', 'med'], ['vhigh', 'vhigh', '4', '4', 'small', 'med'], ['low', 'high', '3', 'more', 'small', 'med'], ['med', 'high', '4', '4', 'med', 'high'], ['high', 'vhigh', '3', '4', 'big', 'low'], ['low', 'vhigh', '2', '2', 'med', 'med'], ['med', 'vhigh', '4', '4', 'small', 'med'], ['vhigh', 'high', '5more', '2', 'big', 'med'], ['vhigh', 'low', '2', 'more', 'big', 'med'], ['low', 'low', '3', '4', 'med', 'high'], ['vhigh', 'med', '3', 'more', 'big', 'high'], ['vhigh', 'vhigh', '4', '4', 'med', 'high'], ['high', 'vhigh', '5more', '2', 'med', 'high'], ['low', 'med', '4', '4', 'med', 'med'], ['low', 'vhigh', '4', 'more', 'med', 'high'], ['vhigh', 'med', '4', '4', 'big', 'high'], ['med', 'med', '4', '2', 'big', 'high'], ['low', 'low', '2', '4', 'med', 'low'], ['vhigh', 'vhigh', '3', '4', 'small', 'low'], ['low', 'med', '5more', '2', 'med', 'med'], ['high', 'low', '5more', 'more', 'med', 'med'], ['med', 'low', '3', 'more', 'big', 'high'], ['high', 'vhigh', '5more', 'more', 'med', 'high'], ['high', 'vhigh', '5more', '4', 'big', 'high'], ['high', 'med', '3', 'more', 'med', 'high'], ['vhigh', 'med', '4', 'more', 'big', 'low'], ['high', 'med', '2', 'more', 'med', 'high'], ['med', 'high', '5more', '2', 'small', 'med'], ['med', 'high', '2', 'more', 'big', 'med'], ['high', 'low', '3', 'more', 'med', 'low'], ['med', 'low', '2', '4', 'small', 'low'], ['med', 'high', '5more', '2', 'big', 'high'], ['low', 'med', '3', 'more', 'big', 'low'], ['med', 'vhigh', '3', 'more', 'big', 'high'], ['vhigh', 'vhigh', '3', 'more', 'med', 'med'], ['vhigh', 'high', '5more', 'more', 'small', 'med'], ['high', 'high', '5more', 'more', 'big', 'high'], ['low', 'high', '2', '2', 'med', 'low'], ['low', 'vhigh', '4', '2', 'med', 'low'], ['vhigh', 'high', '5more', '2', 'small', 'high'], ['high', 'vhigh', '3', '2', 'small', 'high'], ['med', 'vhigh', '5more', 'more', 'med', 'high'], ['vhigh', 'med', '5more', '4', 'big', 'high'], ['low', 'high', '3', '4', 'med', 'med'], ['med', 'low', '5more', '2', 'med', 'med'], ['high', 'low', '4', '4', 'med', 'low'], ['high', 'high', '2', '2', 'small', 'low'], ['vhigh', 'vhigh', '5more', '4', 'med', 'low'], ['low', 'med', '2', '2', 'med', 'low'], ['med', 'low', '3', 'more', 'big', 'low'], ['med', 'med', '3', 'more', 'med', 'med'], ['med', 'vhigh', '4', '2', 'small', 'low'], ['low', 'low', '5more', '4', 'big', 'high'], ['vhigh', 'vhigh', '4', '4', 'small', 'low'], ['med', 'high', '3', '2', 'small', 'med'], ['high', 'med', '3', '4', 'big', 'low'], ['high', 'high', '3', 'more', 'small', 'low'], ['vhigh', 'med', '2', '4', 'big', 'low'], ['low', 'med', '2', 'more', 'small', 'med'], ['low', 'med', '2', '2', 'big', 'high'], ['med', 'low', '5more', '4', 'big', 'med'], ['med', 'low', '4', 'more', 'small', 'low'], ['high', 'med', '4', '2', 'small', 'high'], ['vhigh', 'vhigh', '2', '2', 'big', 'high'], ['vhigh', 'vhigh', '4', 'more', 'small', 'high'], ['vhigh', 'med', '5more', '4', 'med', 'med'], ['high', 'vhigh', '4', 'more', 'med', 'low'], ['vhigh', 'vhigh', '5more', '2', 'big', 'med']]

car_labels = ['acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'acc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'vgood', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'vgood', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'vgood', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'vgood', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'vgood', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'vgood', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'good', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'vgood', 'acc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'good', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'acc', 'unacc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc']

tree = build_tree(cars, car_labels)

#--------------------------- END of tree.py-------------------------------
'''
DECISION TREES
Decision Trees in scikit-learn
Nice work! You’ve written a decision tree from scratch that is able to classify new points. Let’s take a look at how the Python library scikit-learn implements decision trees.

The sklearn.tree module contains the DecisionTreeClassifier class. To create a DecisionTreeClassifier object, call the constructor:

classifier = DecisionTreeClassifier()
Next, we want to create the tree based on our training data. To do this, we’ll use the .fit() method.

.fit() takes a list of data points followed by a list of the labels associated with that data. Note that when we built our tree from scratch, our data points contained strings like "vhigh" or "5more". When creating the tree using scikit-learn, it’s a good idea to map those strings to numbers. For example, for the first feature representing the price of the car, "low" would map to 1, "med" would map to 2, and so on.

classifier.fit(training_data, training_labels)
Finally, once we’ve made our tree, we can use it to classify new data points. The .predict() method takes an array of data points and will return an array of classifications for those data points.

predictions = classifier.predict(test_data)
If you’ve split your data into a test set, you can find the accuracy of the model by calling the .score() method using the test data and the test labels as parameters.

print(classifier.score(test_data, test_labels))
.score() returns the percentage of data points from the test set that it classified correctly.
'''
#-----------------script.py-------------------------------
from cars import training_points, training_labels, testing_points, testing_labels
from sklearn.tree import DecisionTreeClassifier

print(training_points[0],training_labels[0]) 

classifier = DecisionTreeClassifier()

classifier.fit(training_points, training_labels)

predictions = classifier.predict(testing_points)

print(classifier.score(testing_points, testing_labels))
#--------------------- 
import random
random.seed(1)

def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars
  
def change_data(data):
    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},
    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},
    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},
    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]

    for row in data:
        for i in range(len(dicts)):
            row[i] = dicts[i][row[i]]

    return data
  
cars = change_data(make_cars())
random.shuffle(cars)
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]

training_points = car_data[:int(len(car_data)*0.9)]
training_labels = car_labels[:int(len(car_labels)*0.9)]

testing_points = car_data[int(len(car_data)*0.9):]
testing_labels = car_labels[int(len(car_labels)*0.9):]

#---------------------------car.data-----------------------
vhigh,vhigh,2,2,small,low,unacc
vhigh,vhigh,2,2,small,med,unacc
vhigh,vhigh,2,2,small,high,unacc
vhigh,vhigh,2,2,med,low,unacc
vhigh,vhigh,2,2,med,med,unacc
vhigh,vhigh,2,2,med,high,unacc
vhigh,vhigh,2,2,big,low,unacc
vhigh,vhigh,2,2,big,med,unacc
vhigh,vhigh,2,2,big,high,unacc
vhigh,vhigh,2,4,small,low,unacc
vhigh,vhigh,2,4,small,med,unacc
vhigh,vhigh,2,4,small,high,unacc
vhigh,vhigh,2,4,med,low,unacc
vhigh,vhigh,2,4,med,med,unacc
vhigh,vhigh,2,4,med,high,unacc
vhigh,vhigh,2,4,big,low,unacc
vhigh,vhigh,2,4,big,med,unacc
vhigh,vhigh,2,4,big,high,unacc
vhigh,vhigh,2,more,small,low,unacc
vhigh,vhigh,2,more,small,med,unacc
vhigh,vhigh,2,more,small,high,unacc
vhigh,vhigh,2,more,med,low,unacc
'
'
'
'
'
'
'
'
'
#---------------------------end of car.data------------------
'''
DECISION TREES
Decision Tree Limitations
Now that we have an understanding of how decision trees are created and used, let’s talk about some of their limitations.

One problem with the way we’re currently making our decision trees is that our trees aren’t always globablly optimal. This means that there might be a better tree out there somewhere that produces better results. But wait, why did we go through all that work of finding information gain if it’s not producing the best possible tree?

Our current strategy of creating trees is greedy. We assume that the best way to create a tree is to find the feature that will result in the largest information gain right now and split on that feature. We never consider the ramifications of that split further down the tree. It’s possible that if we split on a suboptimal feature right now, we would find even better splits later on. Unfortunately, finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is a reasonable substitute.

Another problem with our trees is that they potentially overfit the data. This means that the structure of the tree is too dependent on the training data and doesn’t accurately represent the way the data in the real world looks like. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data.

One way to solve this problem is to prune the tree. The goal of pruning is to shrink the size of the tree. There are a few different pruning strategies, and we won’t go into the details of them here. scikit-learn currently doesn’t prune the tree by default, however we can dig into the code a bit to prune it ourselves.
'''
'''
Instructions
1.
We’ve created a decision tree classifier for you and printed its accuracy. Let’s see how big this tree is.

If your classifier is named classifier, you can find the depth of the tree by printing classifier.tree_.max_depth.

Print the depth of classifier‘s decision tree.

Take note of the accuracy as well.

Print classifier.tree_.max_depth.

Don’t forget the underscore at the end of tree_!

2.
classifier should have a depth of 12. Let’s prune it! When you create classifier, set the parameter max_depth equal to 11.

What is the accuracy of the classifier after pruning the tree from size 12 to size 11?

The constructor should now look like this:

classifier = DecisionTreeClassifier(random_state = 0, max_depth = ____)
Fill in the value for the new max_depth.
'''
from cars import training_points, training_labels, testing_points, testing_labels
from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(random_state = 0, max_depth = 10)
classifier.fit(training_points, training_labels)
print(classifier.score(testing_points, testing_labels))

print(classifier.tree_.max_depth)

'''

DECISION TREES
Review
Great work! In this lesson, you learned how to create decision trees and use them to make classifications. Here are some of the major takeaways:

Good decision trees have pure leaves. A leaf is pure if all of the data points in that class have the same label.
Decision trees are created using a greedy algorithm that prioritizes finding the feature that results in the largest information gain when splitting the data using that feature.
Creating an optimal decision tree is difficult. The greedy algorithm doesn’t always find the globally optimal tree.
Decision trees often suffer from overfitting. Making the tree small by pruning helps to generalize the tree so it is more accurate on data in the real world.
'''
'''
RANDOM FORESTS
Random Forest
We’ve seen that decision trees can be powerful supervised machine learning models. However, they’re not without their weaknesses — decision trees are often prone to overfitting.

We’ve discussed some strategies to minimize this problem, like pruning, but sometimes that isn’t enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.

A random forest is an ensemble machine learning technique — a random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins.

Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact.

In this lesson, we’ll learn how the trees in a random forest get created.
'''
'''
RANDOM FORESTS
Bagging
You might be wondering how the trees in the random forest get created. After all, right now, our algorithm for creating a decision tree is deterministic — given a training set, the same tree will be made every time.

Random forests create different trees using a process known as bagging. Every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.

One thing to note is that when we’re randomly selecting these 100 rows, we’re doing so with replacement. Picture putting all 100 rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag.

This means that when we’re picking our 100 random rows, we could pick the same row more than once. In fact, it’s very unlikely, but all 100 randomly picked rows could all be the same row!

Because we’re picking these rows with replacement, there’s no need to shrink our bagged training set from 1000 rows to 100. We can pick 1000 rows at random, and because we can get the same row more than once, we’ll still end up with a unique data set.

Let’s implement bagging! We’ll be using the data set of cars that we used in our decision tree lesson.'''
'''
Instructions
1.
Start by creating a tree using all of the data we’ve given you. Create a variable named tree and set it equal to the build_tree() function using car_data and car_labels as parameters.

Then call print_tree() using tree as a parameter. Scroll up to the top to see the root of the tree. Which feature is used to split the data at the root?

2.
For now, comment out printing the tree.

Let’s now implement bagging. The original dataset has 1000 items in it. We want to randomly select a subset of those with replacement.

Create a list named indices that contains 1000 random numbers between 0 and 1000. We’ll use this list to remember the 1000 cars and the 1000 labels that we’re going to build a tree with.

You can use either a for loop or list comprehension to make this list. To get a random number between 0 and 1000, use random.randint(0, 999).

If you choose to use a for loop, your code might look something like this:

indices = []
for i in range(1000):
  indices.append(_____)
If you choose to use list comprehension, your code might look like this:

indices = [_____ for i in range(1000)]
3.
Create two new lists named data_subset and labels_subset. These two lists should contain the cars and labels found at each index in indices.

Once again, you can use either a for loop or list comprehension to make these lists.

If you choose to use a for loop, your code might look something like this:

data_subset = []
labels_subset = []
for index in indices:
  data_subset.append(car_data[index])
  labels_subset.append(_____)
If you choose to use list comprehension, your code might look like this:

data_subset = [car_data[index] for index in indices]
labels_subset = [_____]
4.
Create a tree named subset_tree using the build_tree() function with data_subset and labels_subset as parameters.

Print subset_tree using the print_tree() function.

Which feature is used to split the data at the root? Is it a different feature than the feature that split the tree that was created using all of the data?

You’ve just created a new tree from the training set! If you used 1000 different indices, you’d get another different tree. You could now create a random forest by creating multiple different trees!

Fill in the correct parameters:

subset_tree = build_tree(____, ____)
Then make sure to print the tree.
'''

#--------------tree.py-------------------------------
from collections import Counter
import random
random.seed(1)

def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars
  
cars = make_cars()
random.shuffle(cars)
cars = cars[:1000]
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain  
  
class Leaf:

    def __init__(self, labels, value):
        self.predictions = Counter(labels)
        self.value = value

class Decision_Node:


    def __init__(self,
                 question,
                 branches, value):
        self.question = question
        self.branches = branches
        self.value = value
  
def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Leaf):
        print (spacing + "Predict", node.predictions)
        return

    # Print the question at this node
    print (spacing + question_dict[node.question])

    # Call this function recursively on the true branch
    for i in range(len(node.branches)):
        print (spacing + '--> Branch ' + node.branches[i].value+':')
        print_tree(node.branches[i], spacing + "  ")
        
def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    for feature in range(len(dataset[0])):
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_gain, best_feature
  
def build_tree(rows, labels, value = ""):
    gain, question = find_best_split(rows, labels)
    if gain == 0:
        return Leaf(labels, value)
    data_subsets, label_subsets = split(rows, labels, question)
    branches = []
    for i in range(len(data_subsets)):
        branch = build_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][question])
        branches.append(branch)
    return Decision_Node(question, branches, value)

#--------------------script.py-------------------------------
from tree import build_tree, print_tree, car_data, car_labels
import random
random.seed(4)

tree = build_tree(car_data, car_labels)
#print_tree(tree)

#indices = [random.randint(0, 999) x for x in range(1000)]
indices = []
for i in range(1000):
  indices.append(random.randint(0,999))

#print(indices)

data_subset = [car_data[x] for x in indices]
labels_subset = [car_labels[x] for x in indices]

subset_tree =  build_tree(data_subset, labels_subset)
print_tree(subset_tree)


#----------------------car.data------------------
vhigh,vhigh,2,2,small,low,unacc
vhigh,vhigh,2,2,small,med,unacc
vhigh,vhigh,2,2,small,high,unacc
vhigh,vhigh,2,2,med,low,unacc
vhigh,vhigh,2,2,med,med,unacc
vhigh,vhigh,2,2,med,high,unacc
vhigh,vhigh,2,2,big,low,unacc
vhigh,vhigh,2,2,big,med,unacc
vhigh,vhigh,2,2,big,high,unacc
vhigh,vhigh,2,4,small,low,unacc
vhigh,vhigh,2,4,small,med,unacc
vhigh,vhigh,2,4,small,high,unacc
vhigh,vhigh,2,4,med,low,unacc
vhigh,vhigh,2,4,med,med,unacc
vhigh,vhigh,2,4,med,high,unacc
vhigh,vhigh,2,4,big,low,unacc
vhigh,vhigh,2,4,big,med,unacc
vhigh,vhigh,2,4,big,high,unacc
vhigh,vhigh,2,more,small,low,unacc
vhigh,vhigh,2,more,small,med,unacc
vhigh,vhigh,2,more,small,high,unacc
vhigh,vhigh,2,more,med,low,unacc
vhigh,vhigh,2,more,med,med,unacc
vhigh,vhigh,2,more,med,high,unacc
vhigh,vhigh,2,more,big,low,unacc
vhigh,vhigh,2,more,big,med,unacc
vhigh,vhigh,2,more,big,high,unacc
vhigh,vhigh,3,2,small,low,unacc
vhigh,vhigh,3,2,small,med,unacc
vhigh,vhigh,3,2,small,high,unacc
vhigh,vhigh,3,2,med,low,unacc
....

#-#-#-#-#-#-#-#-#-#-#-#-#- DECISION TREE PROEJCT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-

'''

MACHINE LEARNING: SUPERVISED LEARNING 🤖
Find the Flag
Can you guess which continent this flag comes from?

Flag of Reunion
What are some of the features that would clue you in? Maybe some of the colors are good indicators. The presence or absence of certain shapes could give you a hint. In this project, we’ll use decision trees to try to predict the continent of flags based on several of these features.

We’ll explore which features are the best to use and the best way to create your decision tree.

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
12/14Complete
Mark the tasks as complete by checking them off
Investigate the Data
1.
Let’s start by seeing what the data looks like. Begin by loading the data into a variable named flags using Panda’s pd.read_csv() function. The function should take the name of the CSV file you want to load. In this case, our file is named "flags.csv".

We also want row 0 to be used as the header, so include the parameter header = 0.


Stuck? Get a hint
2.
Take a look at the names of the columns in our DataFrame. These are the features we have available to us. Print flags.columns.

Let’s also take a look at the first few rows of the dataset. Print flags.head().

3.
Many columns contain numbers that don’t make a lot of sense. For example, the third row, which represents Algeria, has a Language of 8. What exactly does that mean?

Take a look at the Attribute Information for this dataset from UCI’s Machine Learning Repository.

Using that information along with the printout of flags.head(), can you figure out what landmass Andora is on?

Creating Your Data and Labels
4.
We’re eventually going to use create a decision tree to classify what Landmass a country is on.

Create a variable named labels and set it equal to only the "Landmass" column from flags.

You can grab specific columns from a DataFrame using this syntax:

one_column = df[["A"]]
two_columns = df[["B", "C"]]
In this example, one_column will be a DataFrame of only df‘s "A" column. two_columns will be a DataFrame of the "B" and "C" columns from df.


Stuck? Get a hint
5.
We have our labels. Now we want to choose which columns will help our decision tree correctly classify those labels.

You could spend a lot of time playing with groups of columns to find the that work best. But for now, let’s see if we can predict where a country is based only on the colors of its flag.

Create a variable named data and set it equal to a DataFrame containing the following columns from flags:

"Red"
"Green"
"Blue"
"Gold"
"White"
"Black"
"Orange"

Stuck? Get a hint
6.
Finally, let’s split these DataFrames into a training set and test set using the train_test_split() function. This function should take data and labels as parameters. Also include the parameter random_state = 1.

This function returns four values. Name those values train_data, test_data, train_labels, and test_labels in that order.


Stuck? Get a hint
Make and Test the Model
7.
Create a DecisionTreeClassifier and name it tree. When you create the tree, give it the parameter random_state = 1.


Stuck? Get a hint
8.
Call tree‘s .fit() method using train_data and train_labels to fit the tree to the training data.


Stuck? Get a hint
9.
Call .score() using test_data and test_labels. Print the result.

Since there are six possible landmasses, if we randomly guessed, we’d expect to be right about 16% of the time. Did our decision tree beat randomly guessing?

Tuning the Model
10.
We now have a good baseline of how our model performs with these features. Let’s see if we can prune the tree to make it better!

Put your code that creates, trains, and tests the tree inside a for loop that has a variable named i that increases from 1 to 20.

Inside your for loop, when you create tree, give it the parameter max_depth = i.

We’ll now see a printout of how the accuracy changes depending on how large we allow the tree to be.


Stuck? Get a hint
11.
Rather than printing the score of each tree, let’s graph it! We want the x-axis to show the depth of the tree and the y-axis to show the tree’s score.

To do this, we’ll need to create a list containing all of the scores. Before the for loop, create an empty list named scores. Inside the loop, instead of printing the tree’s score, use .append() to add it to scores.


Stuck? Get a hint
12.
Let’s now plot our points. Call plt.plot() using two parameters. The first should be the points on the x-axis. In this case, that is range(1, 21). The second should be scores.

Then call plt.show().


Stuck? Get a hint
13.
Our graph doesn’t really look like we would expect it to. It seems like the depth of the tree isn’t really having an impact on its performance. This might be a good indication that we’re not using enough features.

Let’s add all the features that have to do with shapes to our data. data should now be set equal to:

flags[["Red", "Green", "Blue", "Gold",
 "White", "Black", "Orange",
 "Circles",
"Crosses","Saltires","Quarters","Sunstars",
"Crescent","Triangle"]]
What does your graph look like after making this change?

Explore on Your Own
14.
Nice work! That graph looks more like what we’d expect. If the tree is too short, we’re underfitting and not accurately representing the training data. If the tree is too big, we’re getting too specific and relying too heavily on the training data.

There are a few different ways to extend this project:

Try to classify something else! Rather than predicting the "Landmass" feature, could predict something like the "Language"?
Find a subset of features that work better than what we’re currently using. An important note is that a feature that has categorical data won’t work very well as a feature. For example, we don’t want a decision node to split nodes based on whether the value for "Language" is above or below 5.
Tune more parameters of the model. You can find a description of all the parameters you can tune in the Decision Tree Classifier documentation. For example, see what happens if you tune max_leaf_nodes. Think about whether you would be overfitting or underfitting the data based on how many leaf nodes you allow.
'''


import codecademylib3_seaborn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt


flags = pd.read_csv('flags.csv', header = 0)

#print(flags)
#print(flags.columns)

labels = flags[['Landmass']]

#data = flags[['Red', 'Green', 'Blue', 'Gold', 'White','Black', 'Orange']]
data = flags[["Red", "Green", "Blue", "Gold",
 "White", "Black", "Orange",
 "Circles",
"Crosses","Saltires","Quarters","Sunstars",
"Crescent","Triangle"]]

train_data, test_data, train_label, test_label = train_test_split(data, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

scores = []


for i in range(1, 21):

  tree = DecisionTreeClassifier(max_depth = i)
  tree.fit(train_data, train_label)
  print('Max Depth = ', i, end = '')
  print("  The Score is ", tree.score(test_data, test_label))
  scores.append(tree.score(test_data, test_label))

plt.plot(range(1,21), scores)
plt.show()

print(data.columns)

flag_to_check = pd.DataFrame ([[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], columns =  ['Red', 'Green', 'Blue', 'Gold', 'White', 'Black', 'Orange','Circles','Crosses', 'Saltires', 'Quarters', 'Sunstars', 'Crescent', 'Triangle'])
#flag_to_check = np.array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])

print(tree.predict(flag_to_check))




#-#-#-#-#-#-#-#-#-#-#-#-#- END OF DECISION TREE PROEJCT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-



'''
RANDOM FORESTS
Bagging Features
We’re now making trees based on different random subsets of our initial dataset. But we can continue to add variety to the ways our trees are created by changing the features that we use.

Recall that for our car data set, the original features were the following:

The price of the car
The cost of maintenance
The number of doors
The number of people the car can hold
The size of the trunk
The safety rating
Right now when we create a decision tree, we look at every one of those features and choose to split the data based on the feature that produces the most information gain. We could change how the tree is created by only allowing a subset of those features to be considered at each split.

For example, when finding which feature to split the data on the first time, we might randomly choose to only consider the price of the car, the number of doors, and the safety rating.

After splitting the data on the best feature from that subset, we’ll likely want to split again. For this next split, we’ll randomly select three features again to consider. This time those features might be the cost of maintenance, the number of doors, and the size of the trunk. We’ll continue this process until the tree is complete.

One question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is to randomly select the square root of the total number of features. Our car dataset doesn’t have a lot of features, so in this example, it’s difficult to follow this rule. But if we had a dataset with 25 features, we’d want to randomly select 5 features to consider at every split point.

'''
'''
Instructions
1.
We’ve given you access to the code that finds the best feature to split on. Right now, it considers all possible features. We’re going to want to change that!

For now, let’s see what the best feature to split the dataset is. At the bottom of your code, call find_best_split() using data_subset and labels_subset as parameters and print the results.

This function returns the information gain and the index of the best feature. What was the index?

That index corresponds to the features of our car. For example, if the best feature index to split on was 0, that means we’re splitting on the price of the car.

2.
We now want to modify our find_best_split() function to only consider a subset of the features. We want to pick 3 features without replacement.

The random.choice() function found in Python’s numpy module can help us do this. random.choice() returns a list of values between 0 and the first parameter. The size of the list is determined by the second parameter. And we can choose without replacement by setting replace = False.

For example, the following code would choose ten unique numbers between 0 and 100 (exclusive) and put them in a list.

lst = np.random.choice(100, 10, replace = False)
Inside find_best_split(), create a list named features that contains 3 numbers between 0 and len(dataset[0]).

Instead of looping through feature in range(len(dataset[0])), loop through feature in features.

Now that we’ve implemented feature bagging, what is the best index to use as the split index?

Fill in the correct first parameter of np.random.choice. Then make sure to loop through features.

 features = np.random.choice(____, 3, replace=False)
    for feature in features:
      #Code in the loop shouldn't change

'''

#------------ script.py-------------------------------
from tree import car_data, car_labels, split, information_gain
import random
import numpy as np
np.random.seed(10)
random.seed(4)

def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    #Create features here
    features = np.random.choice(len(dataset[0]), 3, replace = False)
    print(features)
    
    for feature in features:
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_gain, best_feature
  
indices = [random.randint(0, 999) for i in range(1000)]

data_subset = [car_data[index] for index in indices]
labels_subset = [car_labels[index] for index in indices]

print(find_best_split(data_subset, labels_subset))

#---------- tree.py -------------------------------------
from collections import Counter
import random
random.seed(1)



def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars
  
cars = make_cars()
random.shuffle(cars)
cars = cars[:1000]
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain  
  
class Leaf:

    def __init__(self, labels, value):
        self.predictions = Counter(labels)
        self.value = value

class Decision_Node:


    def __init__(self,
                 question,
                 branches, value):
        self.question = question
        self.branches = branches
        self.value = value
  
def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Leaf):
        print (spacing + "Predict", node.predictions)
        return

    # Print the question at this node
    print (spacing + question_dict[node.question])

    # Call this function recursively on the true branch
    for i in range(len(node.branches)):
        print (spacing + '--> Branch ' + node.branches[i].value+':')
        print_tree(node.branches[i], spacing + "  ")
        
'''
RANDOM FORESTS
Classify
Now that we can make different decision trees, it’s time to plant a whole forest! Let’s say we make different 8 trees using bagging and feature bagging. We can now take a new unlabeled point, give that point to each tree in the forest, and count the number of times different labels are predicted.

The trees give us their votes and the label that is predicted most often will be our final classification! For example, if we gave our random forest of 8 trees a new data point, we might get the following results:

["vgood", "vgood", "good", "vgood", "acc", "vgood", "good", "vgood"]
Since the most commonly predicted classification was "vgood", this would be the random forest’s final classification.

Let’s write some code that can classify an unlabeled point!'''

'''
Instructions
1.
At the top of your code, we’ve included a new unlabeled car named unlabeled_point that we want to classify. We’ve also created a tree named subset_tree that was created using bagging and feature bagging.

Let’s see how that tree classifies this point. Print the results of classify() using unlabeled_point and subset_tree as parameters.

Print classify(unlabeled_point, subset_tree).

2.
That’s the prediction using one tree. Let’s make 20 trees and record the prediction of each one!

Take all of your code between creating indices and the print statement you just wrote and put it in a for loop that happens 20 times.

Above your for loop, create a variable named predictions and set it equal to an empty list. Inside your for loop, instead of printing the prediction, use .append() to add it to predictions.

Finally after your for loop, print predictions.

Your loop should look like this:

for i in range(20):
  # Code that creates the tree and makes the classification.
Inside your for loop, you should now have this line instead of your print statement:

predictions.append(classify(unlabeled_point, subset_tree))
3.
We now have a list of 20 predictions — let’s find the most common one! You can find the most common element in a list by using this line of code:

max(predictions, key=predictions.count)
Outside of your for loop, store the most common element in a variable named final_prediction and print that variable.'''

#-------- script.py-------------------------------

from tree import build_tree, print_tree, car_data, car_labels, classify
import random
random.seed(4)

# The features are the price of the car, the cost of maintenance, the number of doors, the number of people the car can hold, the size of the trunk, and the safety rating
unlabeled_point = ['high', 'vhigh', '3', 'more', 'med', 'med']



predictions = []

for x in list(range(20)):

  indices = [random.randint(0, 999) for i in range(1000)]
  data_subset = [car_data[index] for index in indices]
  labels_subset = [car_labels[index] for index in indices]
  subset_tree = build_tree(data_subset, labels_subset)

  pred = classify(unlabeled_point, subset_tree)
  predictions.append(pred)

print(predictions)


final_prediction = max(predictions, key=predictions.count)
print(final_prediction)


#-------- tree.py--------------------------------------

import operator
from collections import Counter
import random
import numpy as np
np.random.seed(1)
random.seed(1)

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain

class Leaf:
    def __init__(self, labels, value):
        self.labels = Counter(labels)
        self.value = value

class Internal_Node:
    def __init__(self,
                 feature,
                 branches,
                 value):
        self.feature = feature
        self.branches = branches
        self.value = value

def find_best_split_subset(dataset, labels, num_features):
    features = np.random.choice(6, 3, replace=False)
    best_gain = 0
    best_feature = 0
    for feature in features:
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain

def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    for feature in range(len(dataset[0])):
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain

def build_tree(data, labels, value = ""):
  best_feature, best_gain = find_best_split(data, labels)
  if best_gain < 0.00000001:
    return Leaf(Counter(labels), value)
  data_subsets, label_subsets = split(data, labels, best_feature)
  branches = []
  for i in range(len(data_subsets)):
    branch = build_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][best_feature])
    branches.append(branch)
  return Internal_Node(best_feature, branches, value)

def build_tree_forest(data,labels, n_features, value=""):
    best_feature, best_gain = find_best_split_subset(data, labels, n_features)
    if best_gain < 0.00000001:
      return Leaf(Counter(labels), value)
    data_subsets, label_subsets = split(data, labels, best_feature)
    branches = []
    for i in range(len(data_subsets)):
      branch = build_tree_forest(data_subsets[i], label_subsets[i], n_features, data_subsets[i][0][best_feature])
      branches.append(branch)
    return Internal_Node(best_feature, branches, value)


def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Leaf):
        print (spacing + str(node.labels))
        return

    # Print the question at this node
    print (spacing + "Splitting on " + question_dict[node.feature])

    # Call this function recursively on the true branch
    for i in range(len(node.branches)):
        print (spacing + '--> Branch ' + node.branches[i].value+':')
        print_tree(node.branches[i], spacing + "  ")


def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars



def change_data(data):
    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},
    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},
    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},
    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]

    for row in data:
        for i in range(len(dicts)):
            row[i] = dicts[i][row[i]]

    return data


def classify(datapoint, tree):
  if isinstance(tree, Leaf):
    return max(tree.labels.items(), key=operator.itemgetter(1))[0]

  value = datapoint[tree.feature]
  for branch in tree.branches:
    if branch.value == value:
      return classify(datapoint, branch)
  #return classify(datapoint, tree.branches[random.randint(0, len(tree.branches)-1)])



cars = make_cars()
random.shuffle(cars)
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]

'''RANDOM FORESTS
Test Set
We’re now able to create a random forest, but how accurate is it compared to a single decision tree? To answer this question we’ve split our data into a training set and test set. By building our models using the training set and testing on every data point in the test set, we can calculate the accuracy of both a single decision tree and a random forest.

We’ve given you code that calculates the accuracy of a single tree. This tree was made without using any of the bagging techniques we just learned. We created the tree by using every row from the training set once and considered every feature when splitting the data rather than a random subset.

Let’s also calculate the accuracy of a random forest and see how it compares!'''

'''
Instructions
1.
Begin by taking a look at the code we’ve given you. We’ve created a single tree using the training data, looped through every point in the test set, counted the number of points the tree classified correctly and reported the percentage of correctly classified points — this percentage is known as the accuracy of the model.

Run the code to see the accuracy of the single decision tree.

2.
Right below where tree is created, create a random forest named forest using our make_random_forest() function.

This function takes three parameters — the number of trees in the forest, the training data, and the training labels. It returns a list of trees.

Create a random forest with 40 trees using training_data and training_labels.

You should also create a variable named forest_correct and start it at 0. This is the variable that will keep track of how many points in the test set the random forest correctly classifies.

Fill in the last two parameters:

forest = make_random_forest(40, ____, ____)
Don’t forget to create forest_correct as well!

3.
For every data point in the test set, we want every tree to classify the data point, find the most common classification, and compare that prediction to the true label of the data point. This is very similar to what you did in the previous exercise.

To begin, at the end of the for loop outside the if statement, create an empty list named predictions. Next, loop through every forest_tree in forest. Call classify() using testing_data[i] and forest_tree as parameters and append the result to predictions.

Inside the for loop, you should add code that looks like this. Fill in the correct parameters to the classify() function:

predictions = []
for forest_tree in forest:
  predictions.append(classify(____, ____))
4.
After we loop through every tree in the forest, we now want to find the most common prediction and compare it to the true label. The true label can be found using testing_labels[i]. If they’re equal, we’ve correctly classified a point and should add 1 to forest_correct.

An easy way of finding the most common prediction is by using this line of code:

forest_prediction = max(predictions,key=predictions.count)
Your conditional should look like this:

if forest_prediction == testing_labels[i]:
  forest_correct += 1
5.
Finally, after looping through all of the points in the test set, we want to print out the accuracy of our random forest. Divide forest_correct by the number of items in the test set and print the result.

How did the random forest do compared to the single decision tree?

Finish the line of code:

print(____/len(testing_data))'''

#----------script.py-------------------------------
from tree import training_data, training_labels, testing_data, testing_labels, make_random_forest, make_single_tree, classify
import numpy as np
import random
np.random.seed(1)
random.seed(1)

tree = make_single_tree(training_data, training_labels)
forest = make_random_forest (40, training_data, training_labels)
single_tree_correct = 0
forest_correct = 0

for i in range(len(testing_data)):
  
  prediction = classify(testing_data[i], tree)
  if prediction == testing_labels[i]:
    single_tree_correct += 1
  
  predictions = []

  for forest_tree in forest:    
    predictions.append(classify(testing_data[i], forest_tree))
  forest_prediction = max(predictions,key=predictions.count)
  if forest_prediction == testing_labels[i]:
    forest_correct += 1 





print(single_tree_correct/len(testing_data))
print(forest_correct/len(testing_data))

#-----------------------tree.py-------------------------------

import operator
from collections import Counter
import random
import numpy as np
np.random.seed(1)
random.seed(1)

def split(dataset, labels, column):
    data_subsets = []
    label_subsets = []
    counts = list(set([data[column] for data in dataset]))
    counts.sort()
    for k in counts:
        new_data_subset = []
        new_label_subset = []
        for i in range(len(dataset)):
            if dataset[i][column] == k:
                new_data_subset.append(dataset[i])
                new_label_subset.append(labels[i])
        data_subsets.append(new_data_subset)
        label_subsets.append(new_label_subset)
    return data_subsets, label_subsets

def gini(dataset):
  impurity = 1
  label_counts = Counter(dataset)
  for label in label_counts:
    prob_of_label = label_counts[label] / len(dataset)
    impurity -= prob_of_label ** 2
  return impurity

def information_gain(starting_labels, split_labels):
  info_gain = gini(starting_labels)
  for subset in split_labels:
    info_gain -= gini(subset) * len(subset)/len(starting_labels)
  return info_gain

class Leaf:
    def __init__(self, labels, value):
        self.labels = Counter(labels)
        self.value = value

class Internal_Node:
    def __init__(self,
                 feature,
                 branches,
                 value):
        self.feature = feature
        self.branches = branches
        self.value = value

def find_best_split_subset(dataset, labels, num_features):
    features = np.random.choice(6, 3, replace=False)
    best_gain = 0
    best_feature = 0
    for feature in features:
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain

def find_best_split(dataset, labels):
    best_gain = 0
    best_feature = 0
    for feature in range(len(dataset[0])):
        data_subsets, label_subsets = split(dataset, labels, feature)
        gain = information_gain(labels, label_subsets)
        if gain > best_gain:
            best_gain, best_feature = gain, feature
    return best_feature, best_gain

def make_single_tree(data, labels, value = ""):
  best_feature, best_gain = find_best_split(data, labels)
  if best_gain < 0.00000001:
    return Leaf(Counter(labels), value)
  data_subsets, label_subsets = split(data, labels, best_feature)
  branches = []
  for i in range(len(data_subsets)):
    branch = make_single_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][best_feature])
    branches.append(branch)
  return Internal_Node(best_feature, branches, value)

def build_tree_forest(data,labels, n_features, value=""):
    best_feature, best_gain = find_best_split_subset(data, labels, n_features)
    if best_gain < 0.00000001:
      return Leaf(Counter(labels), value)
    data_subsets, label_subsets = split(data, labels, best_feature)
    branches = []
    for i in range(len(data_subsets)):
      branch = build_tree_forest(data_subsets[i], label_subsets[i], n_features, data_subsets[i][0][best_feature])
      branches.append(branch)
    return Internal_Node(best_feature, branches, value)

def print_tree(node, spacing=""):
    """World's most elegant tree printing function."""
    question_dict = {0: "Buying Price", 1:"Price of maintenance", 2:"Number of doors", 3:"Person Capacity", 4:"Size of luggage boot", 5:"Estimated Saftey"}
    # Base case: we've reached a leaf
    if isinstance(node, Leaf):
        print (spacing + str(node.labels))
        return

    # Print the question at this node
    print (spacing + "Splitting on " + question_dict[node.feature])

    # Call this function recursively on the true branch
    for i in range(len(node.branches)):
        print (spacing + '--> Branch ' + node.branches[i].value+':')
        print_tree(node.branches[i], spacing + "  ")

def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars

def change_data(data):
    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},
    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},
    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},
    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]

    for row in data:
        for i in range(len(dicts)):
            row[i] = dicts[i][row[i]]

    return data


def classify(datapoint, tree):
  if isinstance(tree, Leaf):
    items = list(tree.labels.items()) 
    items.sort()
    return max(items, key=operator.itemgetter(1))[0]

  value = datapoint[tree.feature]
  for branch in tree.branches:
    if branch.value == value:
      return classify(datapoint, branch)
  #return classify(datapoint, tree.branches[random.randint(0, len(tree.branches)-1)])


cars = make_cars()
random.shuffle(cars)
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]
# car_data = car_data[:500]
# car_labels = car_labels[:500]


training_data = car_data[:int(len(car_data)*0.8)]
training_labels = car_labels[:int(len(car_data)*0.8)]

testing_data = car_data[int(len(car_data)*0.8):]
testing_labels = car_labels[int(len(car_data)*0.8):]

def make_random_forest(n, training_data, training_labels):
    trees = []
    for i in range(n):
        indices = [random.randint(0, len(training_data)-1) for x in range(len(training_data))]

        training_data_subset = [training_data[index] for index in indices]
        training_labels_subset = [training_labels[index] for index in indices]

        tree = build_tree_forest(training_data_subset, training_labels_subset, 2)
        trees.append(tree)
    return trees
    
'''
RANDOM FORESTS
Random Forest in Scikit-learn
You now have the ability to make a random forest using your own decision trees. However, scikit-learn has a RandomForestClassifier class that will do all of this work for you! RandomForestClassifier is in the sklearn.ensemble module.

RandomForestClassifier works almost identically to DecisionTreeClassifier — the .fit(), .predict(), and .score() methods work in the exact same way.

When creating a RandomForestClassifier, you can choose how many trees to include in the random forest by using the n_estimators parameter like this:

classifier = RandomForestClassifier(n_estimators = 100)
We now have a very powerful machine learning model that is fairly resistant to overfitting!'''
'''
Instructions
1.
Create a RandomForestClassifier named classifier. When you create it, pass two parameters to the constructor:

n_estimators should be 2000. Our forest will be pretty big!
random_state should be 0. There’s an element of randomness when creating random forests thanks to bagging. Setting the random_state to 0 will help us test your code.
classifier = RandomForestClassifier(random_state = ___, n_estimators = ___)
2.
Train the forest using the training data by calling the .fit() method. .fit() takes two parameters — training_points and training_labels.

Fill in the correct parameters:

classifier.fit(____, ____)
3.
Test the random forest on the testing set and print the results. How accurate was the model?

Call .score() using testing_points and testing_labels as parameters.'''

#------------------script.py-------------------------------

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
from cars import training_points, training_labels, testing_points, testing_labels
import warnings
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier (random_state = 0, n_estimators = 2000)

classifier.fit(training_points, training_labels)

score = classifier.score(testing_points, testing_labels )
print(score)

#----------------cars.py-------------------------------

import random
random.seed(1)

def make_cars():
    f = open("car.data", "r")
    cars = []
    for line in f:
        cars.append(line.rstrip().split(","))
    return cars
  
def change_data(data):
    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},
    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},
    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},
    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},
    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]

    for row in data:
        for i in range(len(dicts)):
            row[i] = dicts[i][row[i]]

    return data
  
cars = change_data(make_cars())
random.shuffle(cars)
car_data = [x[:-1] for x in cars]
car_labels = [x[-1] for x in cars]

training_points = car_data[:int(len(car_data)*0.9)]
training_labels = car_labels[:int(len(car_labels)*0.9)]

testing_points = car_data[int(len(car_data)*0.9):]
testing_labels = car_labels[int(len(car_labels)*0.9):]

'''
RANDOM FORESTS
Review
Nice work! Here are some of the major takeaways about random forests:

A random forest is an ensemble machine learning model. It makes a classification by aggregating the classifications of many decision trees.
Random forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.
Every decision tree in a random forest is created by using a different subset of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.
When creating a tree in a random forest, a randomly selected subset of features are considered as candidates for the best splitting feature. If your dataset has n features, it is common practice to randomly select the square root of n features.'''
'''
K-MEANS CLUSTERING
Introduction to Clustering
Often, the data you encounter in the real world won’t have flags attached and won’t provide labeled answers to your question. Finding patterns in this type of data, unlabeled data, is a common theme in many machine learning applications. Unsupervised Learning is how we find patterns and structure in these data.

Clustering is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or clusters. Examples of clustering applications are:

Recommendation engines: group products to personalize the user experience
Search engines: group news topics and search results
Market segmentation: group customers based on geography, demography, and behaviors
Image segmentation: medical imaging or road scene segmentation on self-driving cars
Let’s get started!'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np 

from os.path import join, dirname, abspath
from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()

x = iris.data
y = iris.target

fignum = 1

# Plot the ground truthd

fig = plt.figure(fignum, figsize=(4, 3))

ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

for name, label in [('Zombies', 0),
                    ('Programmers', 1),
                    ('Vampires', 2)]:
    ax.text3D(x[y == label, 3].mean(),
              x[y == label, 0].mean(),
              x[y == label, 2].mean() + 2, name,
              horizontalalignment='center',
              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))

# Reorder the labels to have colors matching the cluster results

y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(x[:, 3], x[:, 0], x[:, 2], c=y, edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

ax.set_xlabel('hates sunlight')
ax.set_ylabel('likes garlic')
ax.set_zlabel('canine teeth (in)')

ax.set_title('')
ax.dist = 12

# Add code here:
plt.show()

'''
K-MEANS CLUSTERING
K-Means Clustering
The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. So two questions arise:

How many groups do we choose?
How do we define similarity?
*K-Means * is the most popular and well-known clustering algorithm, and it tries to address these two questions.

The “K” refers to the number of clusters (groups) we expect to find in a dataset.
The “Means” refers to the average distance of data to each cluster center, also known as the centroid, which we are trying to minimize.
It is an iterative approach:

1.Place k random centroids for the initial clusters.
2.Assign data samples to the nearest centroid.
3.Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence (when points don’t move between clusters and centroids stabilize).

Once we are happy with our clusters, we can take a new unlabeled datapoint and quickly assign it to the appropriate cluster.

In this lesson, we will first implement K-Means the hard way (to help you understand the algorithm) and then the easy way using the sklearn library!

'''
'''
K-MEANS CLUSTERING
Iris Dataset
Before we implement the K-means algorithm, let’s find a dataset. The sklearn package embeds some datasets and sample images. One of them is the Iris dataset.

The Iris dataset consists of measurements of sepals and petals of 3 different plant species:

Iris setosa
Iris versicolor
Iris virginica
Iris
The sepal is the part that encases and protects the flower when it is in the bud stage. A petal is a leaflike part that is often colorful.

From sklearn library, import the datasets module:

from sklearn import datasets
To load the Iris dataset:

iris = datasets.load_iris()
The Iris dataset looks like:

[[ 5.1  3.5  1.4  0.2 ]
 [ 4.9  3.   1.4  0.2 ]
 [ 4.7  3.2  1.3  0.2 ]
 [ 4.6  3.1  1.5  0.2 ]
   . . .
 [ 5.9  3.   5.1  1.8 ]]
We call each piece of data a sample. For example, each flower is one sample.

Each characteristic we are interested in is a feature. For example, petal length is a feature of this dataset.

The features of the dataset are:

Column 0: Sepal length
Column 1: Sepal width
Column 2: Petal length
Column 3: Petal width
The 3 species of Iris plants are what we are going to cluster later in this lesson.'''

'''
Instructions
1.
Import the datasets module and load the Iris data.

From sklearn library, import the datasets module, and load the Iris dataset:

from sklearn import datasets

iris = datasets.load_iris()
2.
Every dataset from sklearn comes with a bunch of different information (not just the data) and is stored in a similar fashion.

First, let’s take a look at the most important thing, the sample data:

print(iris.data)
Each row is a plant!

The Iris dataset looks like:

[[ 5.1  3.5  1.4  0.2 ]
 [ 4.9  3.   1.4  0.2 ]
 [ 4.7  3.2  1.3  0.2 ]
 [ 4.6  3.1  1.5  0.2 ]
   . . .
 [ 5.9  3.   5.1  1.8 ]]
3.
Since the datasets in sklearn datasets are used for practice, they come with the answers (target values) in the target key:

Take a look at the target values:

print(iris.target)
The iris.target values give the ground truth for the Iris dataset. Ground truth, in this case, is the number corresponding to the flower that we are trying to learn.

The ground truth is what’s measured for the target variable for the training and testing examples.

It should look like:

[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
  2 2 ]
Each one is the cluster label for an Iris plant.

4.
It is always a good idea to read the descriptions of the data:

print(iris.DESCR)
Expand the terminal (right panel):

When was the Iris dataset published?
What is the unit of measurement?
DESCR needs to be capitalized.

This dataset was published in 1936, over eighty years ago:

Fisher, R.A. "The use of multiple measurements in taxonomic problems" Annual Eugenics, 7, Part II, 179-188 (1936)
The unit of measurement is cm (centimeter):

    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        
        '''
        
import codecademylib3_seaborn
import matplotlib.pyplot as plt

from sklearn import datasets

iris = datasets.load_iris()

print(iris.data)

print(iris.target)

print(iris.DESCR)

'''
K-MEANS CLUSTERING
Visualize Before K-Means
To get a better sense of the data in the iris.data matrix, let’s visualize it!

With Matplotlib, we can create a 2D scatter plot of the Iris dataset using two of its features (sepal length vs. petal length). The sepal length measurements are stored in column 0 of the matrix, and the petal length measurements are stored in column 2 of the matrix.

But how do we get these values?

Suppose we only want to retrieve the values that are in column 0 of a matrix, we can use the NumPy/Pandas notation [:,0] like so:

matrix[:,0]
[:,0] can be translated to [all_rows , column_0]

Once you have the measurements we need, we can make a scatter plot by:

plt.scatter(x, y)
To show the plot:

plt.show()
Let’s try this! But this time, plot the sepal length (column 0) vs. sepal width (column 1) instead.'''
'''

Instructions
1.
Store iris.data in a variable named samples.

samples = iris.data
2.
Create a list named x that contains the column 0 values of samples.

Create a list named y that contains the column 1 values of samples.

x = samples[:, 0]
y = samples[:, 1]
So now, x contains all the sepal length measurements and y contains all the sepal width measurements.

3.
Use the .scatter() function to create a scatter plot of x and y.

Because some of the data samples have the exact same features, let’s add alpha=0.5:

plt.scatter(x, y, alpha=0.5)
Adding alpha=0.5 makes some points look darker than others. The darker spots are where there is overlap.

4.
Call the .show() function to display the graph.

If you didn’t know there are three species of the Iris plant, would you have known just by looking at the visualization?

Answer:

import codecademylib3_seaborn
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

samples = iris.data

x = samples[:,0]
y = samples[:,1]

plt.scatter(x, y, alpha=0.5)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()
We’ve also x-axis label and y-axis label (for good practice!)

Adding alpha=0.5 makes some points look darker than others. The darker spots are where there is overlap.'''


import codecademylib3_seaborn
import matplotlib.pyplot as plt

from sklearn import datasets

iris = datasets.load_iris()

# Store iris.data

samples = iris.data

x = []
x = samples[:,0]

y = []
y = samples[:,1]

plt.scatter(x,y, alpha=0.2)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')


plt.show()

'''
K-MEANS CLUSTERING
Implementing K-Means: Step 1
The K-Means algorithm:

Place k random centroids for the initial clusters.
Assign data samples to the nearest centroid.
Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence.

After looking at the scatter plot and having a better understanding of the Iris data, let’s start implementing the K-Means algorithm.

In this exercise, we will implement Step 1.

Because we expect there to be three clusters (for the three species of flowers), let’s implement K-Means where the k is 3.

Using the NumPy library, we will create 3 random initial centroids and plot them along with our samples.'''
'''
Instructions
1.
First, create a variable named k and set it to 3.

2.
Then, use NumPy’s random.uniform() function to generate random values in two lists:

a centroids_x list that will have k random values between min(x) and max(x)
a centroids_y list that will have k random values between min(y) and max(y)
The random.uniform() function looks like:

np.random.uniform(low, high, size)
The centroids_x will have the x-values for our initial random centroids and the centroids_y will have the y-values for our initial random centroids.

3.
Create an array named centroids and use the zip() function to add centroids_x and centroids_y to it.

The zip() function looks like:

np.array(list(zip(array1, array2)))
Then, print centroids.

The centroids list should now have all the initial centroids.

centroids = np.array(list(zip(centroids_x, centroids_y)))

print(centroids)
The output should look like:

[[5.49815832 3.5073056 ]
 [7.72370927 4.2138989 ]
 [6.64764806 4.10084725]]
Your centroids array will have slightly different values since we are randomly initializing the centroids!

zip() takes two (or more) lists as inputs and returns an object that contains a list of pairs. Each pair contains one element from each of the inputs.

4.
Make a scatter plot of y vs x.

Make a scatter plot of centroids_y vs centroids_x.

Show the plots to see your centroids!

Answer:'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets

iris = datasets.load_iris()

samples = iris.data

x = samples[:,0]
y = samples[:,1]

combined = np.array(list(zip(x, y)))

# Step 1: Place K random centroids

k = 3

centroids_x = np.random.uniform(min(x), max(x), size=k)
centroids_y = np.random.uniform(min(y), max(y), size=k)

centroids = np.array(list(zip(centroids_x, centroids_y)))

plt.scatter(x, y, alpha=0.5)
plt.scatter(centroids_x, centroids_y)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()'''
Adding alpha=0.5 makes the points look darker than others. This is because some of the points might have the exact the same values. The dots are darker because they are stacked!
'''
'''
K-MEANS CLUSTERING
Implementing K-Means: Step 2
The K-Means algorithm:

Place k random centroids for the initial clusters.
Assign data samples to the nearest centroid.
Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence.

In this exercise, we will implement Step 2.

Now we have the 3 random centroids. Let’s assign data points to their nearest centroids.

To do this we’re going to use the Distance Formula to write a distance() function. Then, we are going to iterate through our data samples and compute the distance from each data point to each of the 3 centroids.

Suppose we have a point and a list of three distances in distances and it looks like [15, 20, 5], then we would want to assign the data point to the 3rd centroid. The argmin(distances) would return the index of the lowest corresponding distance, 2, because the index 2 contains the minimum value.'''
'''
Instructions
1.
Write a distance() function.

It should be able to take in a and b and return the distance between the two points.

For 2D:

def distance(a, b):
  one = (a[0] - b[0]) ** 2
  two = (a[1] - b[1]) ** 2
  distance = (one+two) ** 0.5
  return distance
2.
Create an array called labels that will hold the cluster labels for each data point. Its size should be the length of the data sample.

It should look something like:

[ 0.  0.  0.  0.  0.  0.  ...  0.]
Create an array called distances that will hold the distances for each centroid. It should have the size of k.

It should look something like:

[ 0.  0.  0.]
# Cluster labels for each point (either 0, 1, or 2)
labels = np.zeros(len(samples))

# Distances to each centroid
distances = np.zeros(k)
3.
To assign each data point to the closest centroid, we need to iterate through the whole data sample and calculate each data point’s distance to each centroid.

We can get the index of the smallest distance of distances by doing:

cluster = np.argmin(distances)
Then, assign the cluster to each index of the labels array.

The code should look something like:

for i in range(len(samples)):
  distances[0] = distance(sepal_length_width[i], centroids[0])
  # same as above for distance to centroids[1]
  # same as above for distance to centroids[2]
  cluster = np.argmin(distances)
  labels[i] = cluster
4.
Then, print labels (outside of the for loop).

Awesome! You have just finished Step 2 of the K-means algorithm.

print(labels)
The result labels should look like:

[ 0.  0.  0.  1.  0.  2. 0.  1.  1.  ... ]'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets

iris = datasets.load_iris()

samples = iris.data

x = samples[:,0]
y = samples[:,1]

sepal_length_width = np.array(list(zip(x, y)))

# Step 1: Place K random centroids

k = 3

centroids_x = np.random.uniform(min(x), max(x), size=k)
centroids_y = np.random.uniform(min(y), max(y), size=k)

centroids = np.array(list(zip(centroids_x, centroids_y)))

# Step 2: Assign samples to nearest centroid




# Distance formula
def distance(a,b):
  distance = ((a[0]-b[0])**2 + (a[1]-b[1])**2)**0.5
  return distance

# Cluster labels for each point (either 0, 1, or 2)

#labels = [0 for i in range(len(x))]
labels = np.zeros(len(samples))

# Distances to each centroid

#distances = [0 for i in range(len(centroids_x))]
distances = np.zeros(k)

# Assign to the closest centroid

for i in range(len(x)):
  distances[0] = distance(sepal_length_width[i],centroids[0])
  distances[1] = distance(sepal_length_width[i],centroids[1])
  distances[2] = distance(sepal_length_width[i],centroids[2])
  cluster = np.argmin(distances)
  labels[i]= cluster


# Print labels
print(labels)
'''
K-MEANS CLUSTERING
Implementing K-Means: Step 3
The K-Means algorithm:

Place k random centroids for the initial clusters.
Assign data samples to the nearest centroid.
Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence.

In this exercise, we will implement Step 3.

Find new cluster centers by taking the average of the assigned points. To find the average of the assigned points, we can use the .mean() function.'''

'''
Instructions
1.
Save the old centroids value before updating.

We have already imported deepcopy for you:

from copy import deepcopy
Store centroids into centroids_old using deepcopy():

centroids_old = deepcopy(centroids)
To understand more about the deepcopy() method, read the Python documentation.

2.
Then, create a for loop that iterates k times.

Since k = 3, as we are iterating through the forloop each time, we can calculate mean of the points that have the same cluster label.

Inside the for loop, create an array named points where we get all the data points that have the cluster label i.

There are two ways to do this, check the hints to see both!

One way to do this is:

for i in range(k):
  points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
Another way is to use nested for loop:

for i in range(k):
  points = []
  for j in range(len(sepal_length_width)):
    if labels[j] == i:
      points.append(sepal_length_width[j])
Here, we create an empty list named points first, and use .append() to add values into the list.

3.
Then (still inside the for loop), calculate the mean of those points using .mean() to get the new centroid.

Store the new centroid in centroids[i].

The .mean() fucntion looks like:

np.mean(input, axis=0)
for i in range(k):
  ...
  centroids[i] = np.mean(points, axis=0)
If you don’t have axis=0 parameter, the default is to compute the mean of the flattened array. We need the axis=0 here to specify that we want to compute the means along the rows.

4.
Oustide of the for loop, print centroids_old and centroids to see how centroids changed.

print(centroids_old)
print("- - - - - - - - - - - - - -")
print(centroids)'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from copy import deepcopy

iris = datasets.load_iris()

samples = iris.data
samples = iris.data

x = samples[:,0]
y = samples[:,1]

sepal_length_width = np.array(list(zip(x, y)))

# Step 1: Place K random centroids

k = 3

centroids_x = np.random.uniform(min(x), max(x), size=k)
centroids_y = np.random.uniform(min(y), max(y), size=k)

centroids = np.array(list(zip(centroids_x, centroids_y)))

# Step 2: Assign samples to nearest centroid

def distance(a, b):
  one = (a[0] - b[0]) **2
  two = (a[1] - b[1]) **2
  distance = (one+two) ** 0.5
  return distance

# Cluster labels for each point (either 0, 1, or 2)
labels = np.zeros(len(samples))

# Distances to each centroid
distances = np.zeros(k)

for i in range(len(samples)):
  distances[0] = distance(sepal_length_width[i], centroids[0])
  distances[1] = distance(sepal_length_width[i], centroids[1])
  distances[2] = distance(sepal_length_width[i], centroids[2])
  cluster = np.argmin(distances)
  labels[i] = cluster

# Step 3: Update centroids
centroids_old = deepcopy(centroids)

for i in range(k):
  points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
  centroids[i] = np.mean(points, axis=0)

print(centroids_old)

print(centroids)

'''
K-MEANS CLUSTERING
Implementing K-Means: Step 4
The K-Means algorithm:

Place k random centroids for the initial clusters.
Assign data samples to the nearest centroid.
Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence.

In this exercise, we will implement Step 4.

This is the part of the algorithm where we repeatedly execute Step 2 and 3 until the centroids stabilize (convergence).

We can do this using a while loop. And everything from Step 2 and 3 goes inside the loop.

For the condition of the while loop, we need to create an array named errors. In each error index, we calculate the difference between the updated centroid (centroids) and the old centroid (centroids_old).

The loop ends when all three values in errors are 0.'''
'''

Instructions
1.
On line 40 of script.py, initialize error:

error = np.zeros(3)
Then, use the distance() function to calculate the distance between the updated centroid and the old centroid and put them in error:

error[0] = distance(centroids[0], centroids_old[0])
# do the same for error[1]
# do the same for error[2]
error = np.zeros(3)

error[0] = distance(centroids[0], centroids_old[0])
error[1] = distance(centroids[1], centroids_old[1])
error[2] = distance(centroids[2], centroids_old[2])
2.
After that, add a while loop:

while error.all() != 0:
And move everything below (from Step 2 and 3) inside.

And recalculate error again at the end of each iteration of the while loop:

error[0] = distance(centroids[0], centroids_old[0])
# do the same for error[1]
# do the same for error[2]

#hints

while error.all() != 0:

  # Step 2: Assign samples to nearest centroid

  for i in range(len(samples)):
    distances[0] = distance(sepal_length_width[i], centroids[0])
    distances[1] = distance(sepal_length_width[i], centroids[1])
    distances[2] = distance(sepal_length_width[i], centroids[2])
    cluster = np.argmin(distances)
    labels[i] = cluster

  # Step 3: Update centroids

  centroids_old = deepcopy(centroids)

  for i in range(3):
    points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
    centroids[i] = np.mean(points, axis=0)

  # Add this again:

  error[0] = distance(centroids[0], centroids_old[0])
  error[1] = distance(centroids[1], centroids_old[1])
  error[2] = distance(centroids[2], centroids_old[2])
  
3.
Awesome, now you have everything, let’s visualize it.

After the while loop finishes, let’s create an array of colors:

colors = ['r', 'g', 'b']
Then, create a for loop that iterates k times.

Inside the for loop (similar to what we did in the last exercise), create an array named points where we get all the data points that have the cluster label i.

Then we are going to make a scatter plot of points[:, 0] vs points[:, 1] using the scatter() function:

plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.5)

hints


colors = ['r', 'g', 'b']

for i in range(k):
  points = np.array([sepal_length_width[j] for j in range(len(samples)) if labels[j] == i])
  plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.5)
  
  
4.
Then, paste the following code at the very end. Here, we are visualizing all the points in each of the labels a different color.

plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=150)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from copy import deepcopy

iris = datasets.load_iris()

samples = iris.data

x = samples[:,0]
y = samples[:,1]

sepal_length_width = np.array(list(zip(x, y)))

# Step 1: Place K random centroids

k = 3

centroids_x = np.random.uniform(min(x), max(x), size=k)
centroids_y = np.random.uniform(min(y), max(y), size=k)

centroids = np.array(list(zip(centroids_x, centroids_y)))

def distance(a, b):
  one = (a[0] - b[0]) ** 2
  two = (a[1] - b[1]) ** 2
  distance = (one + two) ** 0.5
  return distance

# To store the value of centroids when it updates
centroids_old = np.zeros(centroids.shape)

# Cluster labeles (either 0, 1, or 2)
labels = np.zeros(len(samples))

distances = np.zeros(3)

# Initialize error:
error = np.zeros(3)
error[0] = distance(centroids[0], centroids_old[0])
error[1] = distance(centroids[1], centroids_old[1])
error[2] = distance(centroids[2], centroids_old[2])

while error.all() != 0:


  # Repeat Steps 2 and 3 until convergence:


  # Step 2: Assign samples to nearest centroid

  for i in range(len(samples)):
    distances[0] = distance(sepal_length_width[i], centroids[0])
    distances[1] = distance(sepal_length_width[i], centroids[1])
    distances[2] = distance(sepal_length_width[i], centroids[2])
    cluster = np.argmin(distances)
    labels[i] = cluster

  # Step 3: Update centroids

  centroids_old = deepcopy(centroids)

  for i in range(3):
    points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
    centroids[i] = np.mean(points, axis=0)

  error[0] = distance(centroids[0], centroids_old[0])
  error[1] = distance(centroids[1], centroids_old[1])
  error[2] = distance(centroids[2], centroids_old[2])

colors = ['r', 'g', 'b']

for i in range(k):
  points = np.array([sepal_length_width[j] for j in range(len(samples)) if labels[j] == i])
  plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.5)

  plt.scatter(centroids[:, 0], centroids[:, 1], c=colors[i],marker='D', s=150)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()
 
'''
K-MEANS CLUSTERING
Implementing K-Means: Scikit-Learn
Awesome, you have implemented K-Means clustering from scratch!

Writing an algorithm whenever you need it can be very time-consuming and you might make mistakes and typos along the way. We will now show you how to implement K-Means more efficiently – using the scikit-learn library.

Instead of implementing K-Means from scratch, the sklearn.cluster module has many methods that can do this for you.

To import KMeans from sklearn.cluster:

from sklearn.cluster import KMeans
For Step 1, use the KMeans() method to build a model that finds k clusters. To specify the number of clusters (k), use the n_clusters keyword argument:

model = KMeans(n_clusters = k)
For Steps 2 and 3, use the .fit() method to compute K-Means clustering:

model.fit(X)
After K-Means, we can now predict the closest cluster each sample in X belongs to. Use the .predict() method to compute cluster centers and predict cluster index for each sample:

model.predict(X)

'''
'''
1.
First, import KMeans from sklearn.cluster.

Answer:

import codecademylib3_seaborn
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data
2.
Somewhere after samples = iris.data, use KMeans() to create an instance called model to find 3 clusters.

To specify the number of clusters, use the n_clusters keyword argument.

model = KMeans(n_clusters=3)
3.
Next, use the .fit() method of model to fit the model to the array of points samples:

model.fit(samples)
4.
After you have the “fitted” model, determine the cluster labels of samples.

Then, print the labels.

labels = model.predict(samples)

print(labels)'''


import codecademylib3_seaborn
import matplotlib.pyplot as plt
from sklearn import datasets

# From sklearn.cluster, import KMeans class
from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data

# Use KMeans() to create a model that finds 3 clusters
model = KMeans(n_clusters = 3)

# Use .fit() to fit the model to samples
model.fit(samples)

# Use .predict() to determine the labels of samples 
labels = model.predict(samples)

# Print the labels
print(labels)

'''
K-MEANS CLUSTERING
New Data?
You used K-Means and found three clusters of the samples data. But it gets cooler!

Since you have created a model that computed K-Means clustering, you can now feed new data samples into it and obtain the cluster labels using the .predict() method.

So, suppose we went to the florist and bought 3 more Irises with the measurements:

[[ 5.1  3.5  1.4  0.2 ]
 [ 3.4  3.1  1.6  0.3 ]
 [ 4.9  3.   1.4  0.2 ]]
We can feed this new data into the model and obtain the labels for them.'''
'''
Instructions
1.
First, store the 2D matrix:

new_samples = np.array([[5.7, 4.4, 1.5, 0.4],
   [6.5, 3. , 5.5, 0.4],
   [5.8, 2.7, 5.1, 1.9]])
To test if it worked, print the new_samples.

2.
Use the model to predict labels for the new_samples, and print the predictions.

The output might look like:

[0 2 2]
Those are the predicted labels for our three new flowers. If you are seeing different labels, don’t worry! Since the cluster centroids are randomly initialized, running the model repeatedly can produce different clusters with the same input data.
'''
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data

model = KMeans(n_clusters=3)

model.fit(samples)

# Store the new Iris measurements
new_samples = np.array([
  [5.7, 4.4, 1.5, 0.4],
  [6.5, 3. , 5.5, 0.4],
  [5.8, 2.7, 5.1, 1.9]])

# Predict labels for the new_samples
labels = model.predict(new_samples)
print(labels)

'''
Question
In the context of this exercise, does feeding new data when running predict() update the predictor?

Answer
No, when we feed in new data samples using the predict() method of our KMeans model, the predictor does not get changed or updated.

When running the predict() method, it will return its best guess, based on what it learned in the previous steps. When we pass in the new data samples and obtain its predictions, we do not explicitly tell the predictor whether its guess was correct or not, so there is no way for it to update itself based on its guesses. Once it has been fitted to its sample test data, it will not be able to update itself to new data, unless we redo the fitting process.
'''


'''
K-MEANS CLUSTERING
Visualize After K-Means
We have done the following using sklearn library:

Load the embedded dataset
Compute K-Means on the dataset (where k is 3)
Predict the labels of the data samples
And the labels resulted in either 0, 1, or 2.

Let’s finish it by making a scatter plot of the data again!

This time, however, use the labels numbers as the colors.

To edit colors of the scatter plot, we can set c = labels:

plt.scatter(x, y, c=labels, alpha=0.5)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')
'''
'''
Instructions
1.
Create an array called x that contains the Column 0 of samples.

Create an array called y that contains the Column 1 of samples.

x = samples[:,0]
y = samples[:,1]
2.
Make a scatter plot of x and y, using labels to define the colors.

plt.scatter(x, y, c=labels, alpha=0.5)

plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()
'''
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data

model = KMeans(n_clusters=3)

model.fit(samples)

# Store the new Iris measurements
new_samples = np.array([
  [5.7, 4.4, 1.5, 0.4],
  [6.5, 3. , 5.5, 0.4],
  [5.8, 2.7, 5.1, 1.9]])

# Predict labels for the new_samples
labels = model.predict(new_samples)
print(labels)

x = samples[:,0]
y = samples[:,1]

plt.scatter(x,y,c=labels, alpha = 0.5)


plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')

plt.show()

'''
K-MEANS CLUSTERING
Evaluation
At this point, we have clustered the Iris data into 3 different groups (implemented using Python and using scikit-learn). But do the clusters correspond to the actual species? Let’s find out!

First, remember that the Iris dataset comes with target values:

target = iris.target
It looks like:

[ 0 0 0 0 0 ... 2 2 2]
According to the metadata:

All the 0‘s are Iris-setosa
All the 1‘s are Iris-versicolor
All the 2‘s are Iris-virginica
Let’s change these values into the corresponding species using the following code:

species = np.chararray(target.shape, itemsize=150)

for i in range(len(samples)):
  if target[i] == 0:
    species[i] = 'setosa'
  elif target[i] == 1:
    species[i] = 'versicolor'
  elif target[i] == 2: 
    species[i] = 'virginica'
Then we are going to use the Pandas library to perform a cross-tabulation.

Cross-tabulations enable you to examine relationships within the data that might not be readily apparent when analyzing total survey responses.

The result should look something like:

labels    setosa    versicolor    virginica
0             50             0            0
1              0             2           36
2              0            48           14
(You might need to expand this narrative panel in order to the read the table better.)

The first column has the cluster labels. The second to fourth columns have the Iris species that are clustered into each of the labels.

By looking at this, you can conclude that:

Iris-setosa was clustered with 100% accuracy.
Iris-versicolor was clustered with 96% accuracy.
Iris-virginica didn’t do so well.
Follow the instructions below to learn how to do a cross-tabulation.'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans
import pandas as pd

iris = datasets.load_iris()

samples = iris.data

target = iris.target

model = KMeans(n_clusters=3)

model.fit(samples)

labels = model.predict(samples)

# Code starts here:
species = np.chararray(target.shape, itemsize=150)

for i in range(len(samples)):
  if target[i] == 0:
    species[i] = 'setosa'
  elif target[i] == 1:
    species[i] = 'versicolor'
  elif target[i] == 2: 
    species[i] = 'virginica'

df = pd.DataFrame({'labels': labels, 'species': species})

print(df)

ct = pd.crosstab(df['labels'], df['species'])
print(ct)

'''
K-MEANS CLUSTERING
The Number of Clusters
At this point, we have grouped the Iris plants into 3 clusters. But suppose we didn’t know there are three species of Iris in the dataset, what is the best number of clusters? And how do we determine that?

Before we answer that, we need to define what is a good cluster?

Good clustering results in tight clusters, meaning that the samples in each cluster are bunched together. How spread out the clusters are is measured by inertia. Inertia is the distance from each sample to the centroid of its cluster. The lower the inertia is, the better our model has done.

You can check the inertia of a model by:

print(model.inertia_)
For the Iris dataset, if we graph all the ks (number of clusters) with their inertias:

Optimal Number of Clusters
Notice how the graph keeps decreasing.

Ultimately, this will always be a trade-off. The goal is to have low inertia and the least number of clusters.

One of the ways to interpret this graph is to use the elbow method: choose an “elbow” in the inertia plot - when inertia begins to decrease more slowly.

In the graph above, 3 is the optimal number of clusters.'''
'''
Instructions
1.
First, create two lists:

num_clusters that has values from 1, 2, 3, … 8
inertias that is empty
Answer:

num_clusters = list(range(1, 9))
inertias = []
2.
Then, iterate through num_clusters and calculate K-means for each number of clusters.

Add each of their inertias into the inertias list.

for k in num_clusters:
  model = KMeans(n_clusters=k)
  model.fit(samples)
  inertias.append(model.inertia_)
3.
Plot the inertias vs num_clusters:

plt.plot(num_clusters, inertias, '-o')

plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')

plt.show()'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data

# Code Start here:

#num_clusters = [i+1 for i in list(range(8))]
num_clusters = list(range(1, 9))

print(num_clusters)

inertias = []

for k in num_clusters:
  model = KMeans(n_clusters=k)
  moodel.fit(samples)
  inertias.append(model.inertia_)

plt.plot(num_clusters, inertias, '-o')

plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')

plt.show()

'''
Question
In the context of this exercise, how is inertia calculated?

Answer
According to the documentation for the KMeans method, the inertia_ attribute is the sum of squared distances of the samples to their nearest centroids.

So, to obtain the value of the inertia, we would obtain each data points’ distance to its nearest centroid, square this distance, and then sum them all together, which gives us the inertia. We will utilize the Euclidean, or geometric, distance formula to calculate this.

The following is a general overview of how we might calculate the inertia.
'''
inertia = 0

for datapoint in dataset:
  # Obtain the nearest centroid of the point.
  centroid = get_centroid(datapoint)

  # Calculate the distance from each datapoint to its centroid
  # using the Euclidean distance formula.
  delta_x = datapoint.x - centroid.x
  delta_y = datapoint.y - centroid.y
  distance = (delta_x ** 2 + delta_y ** 2) ** 0.5

  # Square the distance, and add to the inertia.
  squared_distance = distance ** 2
  inertia += squared_distance
  '''
  '''
'''
K-MEANS CLUSTERING
Try It On Your Own
Now it is your turn!

In this review section, find another dataset from one of the following:

The scikit-learn library (http://scikit-learn.org/stable/datasets/index.html)
UCI Machine Learning Repo (https://archive.ics.uci.edu/ml/index.php)
Codecademy GitHub Repo (coming soon!)
Import the pandas library as pd:

import pandas as pd
Load in the data with read_csv():

digits = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)
Note that if you download the data like this, the data is already split up into a training and a test set, indicated by the extensions .tra and .tes. You’ll need to load in both files.

With the command above, you only load in the training set.

Happy Coding!
'''

import pandas as pd
digits = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)

labels = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes", header=None)


import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans

#print(digits)

#find the right clusters
num_clusters = list(range(1,20))

inertias = []

for k in num_clusters:
  model = KMeans(n_clusters=k)
  model.fit(digits)
  inertias.append(model.inertia_)

plt.plot(num_clusters, inertias, '-o')

plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')

plt.show()

print(num_clusters, inertias)

'''
Question
In the context of this lesson 2, what are some other examples of clustering applications?

Answer
Generally, clustering applies when we want to separate and group data based on similar features.

Clustering can also apply to applications that match people together, such as dating sites, or services to connect professionals, which can utilize clustering to determine recommended connections.

Clustering can also apply to personality tests, like the Myers Briggs personality test, which take certain responses and group you into a certain category based on those responses.

In addition, clustering can also apply when grouping organisms based on their physical traits, such that if you provide information, say for a dog, it should categorize it as that kind of animal.'''



''' Project
MACHINE LEARNING: UNSUPERVISED LEARNING 🤖
Handwriting Recognition using K-Means
The U.S. Postal Service has been using machine learning and scanning technologies since 1999. Because its postal offices have to look at roughly half a billion pieces of mail every day, they have done extensive research and developed very efficient algorithms for reading and understanding addresses. And not only the post office:

ATMs can recognize handwritten bank checks
Evernote can recognize handwritten task lists
Expensify can recognize handwritten receipts
But how do they do it?

In this project, you will be using K-means clustering (the algorithm behind this magic) and scikit-learn to cluster images of handwritten digits.

Let’s get started!

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.


1.
The sklearn library comes with a digits dataset for practice.

In script.py, we have already added three lines of code:

import codecademylib3_seaborn
import numpy as np
from matplotlib import pyplot as plt
From sklearn library, import the datasets module.

Then, load in the digits data using .load_digits() and print digits.


Hint
At this point, your code should look like:

import codecademylib3_seaborn
import numpy as np
from matplotlib import pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
print(digits)
The terminal output should display all the information that comes with the dataset.

2.
When first starting out with a dataset, it’s always a good idea to go through the data description and see what you can already learn.

Instead of printing the digits, print digits.DESCR.

What is the size of an image (in pixel)?
Where is this dataset from?

Hint
Print out the description of the digits data:

print(digits.DESCR)
The result should look like:

Recognition of Handwritten Digits Data Set
==========================================

Notes
-----

Data Set Characteristics:

  :Number of Instances: 5620
  :Number of Attributes: 64
  :Attribute Information: 8x8 image of integer pixels in the range 0-16
  :Missing Attribute Values: None
  :Creator: E. Alpaydin
  :Date: July; 1998
The digit images are 8 x 8. And the dataset is from Bogazici University (Istanbul, Turkey).

3.
Let’s see what the data looks like!

Print digits.data.


Hint
Print out the data:

print(digits.data)
[[ 0.  0.  5. ...,  0.  0.  0. ]
 [ 0.  0.  0. ..., 10. 0.  0. ]
 [ 0.  0.  0. ..., 16.  9.  0. ]
... 
Each list contains 64 values which respent the pixel colors of an image (0-16):

0 is white
16 is black
4.
Next, print out the target values in digits.target.


Hint
Print out the target values:

print(digits.target)
The result should look like:

[ 0 1 2 ..., 8 9 8]
This shows us that the first data point in the set was tagged as a 0 and the last one was tagged as an 8.

5.
To visualize the data images, we need to use Matplotlib. Let’s visualize the image at index 100:

plt.gray() 

plt.matshow(digits.images[100])

plt.show()
The image should look like:

4

Is it a 4? Let’s print out the target label at index 100 to find out!

print(digits.target[100])
Open the hint to see how you can visualize more than one image.


Hint
To take a look at 64 sample images. Copy and paste the code below:

# Figure size (width, height)

fig = plt.figure(figsize=(6, 6))

# Adjust the subplots 

fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# For each of the 64 images

for i in range(64):

    # Initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-th position

    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])

    # Display an image at the i-th position

    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')

    # Label the image with the target value

    ax.text(0, 7, str(digits.target[i]))

plt.show()
K-Means Clustering:
6.
Now we understand what we are working with. Let’s cluster the 1797 different digit images into groups.

Import KMeans from sklearn.cluster.


Hint
from sklearn.cluster import KMeans
7.
What should be the k, the number of clusters, here?

Use the KMeans() method to build a model that finds k clusters.


Hint
Because there are 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, and 9), there should be 10 clusters.

So k, the number of clusters, is 10:

model = KMeans(n_clusters=10, random_state=42)
The random_state will ensure that every time you run your code, the model is built in the same way. This can be any number. We used random_state = 42.

8.
Use the .fit() method to fit the digits.data to the model.


Hint
model.fit(digits.data)
Visualizing after K-Means:
9.
Let’s visualize all the centroids! Because data samples live in a 64-dimensional space, the centroids have values so they can be images!

First, add a figure of size 8x3 using .figure().

Then, add a title using .suptitle().


Hint
fig = plt.figure(figsize=(8, 3))

fig.suptitle('Cluser Center Images', fontsize=14, fontweight='bold')
10.
Scikit-learn sometimes calls centroids “cluster centers”.

Write a for loop to displays each of the cluster_centers_ like so:

for i in range(10):

  # Initialize subplots in a grid of 2X5, at i+1th position
  ax = fig.add_subplot(2, 5, 1 + i)

  # Display images
  ax.imshow(model.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)
The cluster centers should be a list with 64 values (0-16). Here, we are making each of the cluster centers into an 8x8 2D array.

11.
Outside of the for loop, use .show() to display the visualization.

It should look like:

8

These are the centroids of handwriting from thirty different people collected by Bogazici University (Istanbul, Turkey):

Index 0 looks like 0
Index 1 looks like 9
Index 2 looks like 2
Index 3 looks like 1
Index 4 looks like 6
Index 5 looks like 8
Index 6 looks like 4
Index 7 looks like 5
Index 8 looks like 7
Index 9 looks like 3
Notice how the centroids that look like 1 and 8 look very similar and 1 and 4 also look very similar.


Hint
plt.show()
12.
Optional:

If you want to see another example that visualizes the data clusters and their centers using K-means, check out the sklearn‘s own example.

K-means clustering example


Hint
In this code, they use k-means++ to place the initial centroids.

Testing Your Model:
13.
Instead of feeding new arrays into the model, let’s do something cooler!

Inside the right panel, go to test.html.


Hint
https://localhost/test.html

14.
What year will robots take over the world?

Use your mouse to write a digit in each of the boxes and click Get Array.


Hint
2020?

15.
Back in script.py, create a new variable named new_samples and copy and paste the 2D array into it.

new_samples = np.array(      )

Hint
Copy and paste the entire code into the parentheses:

new_samples = np.array(      )
Make sure to even copy paste the outer square brackets.

16.
Use the .predict() function to predict new labels for these four new digits. Store those predictions in a variable named new_labels.


Hint
new_labels = model.predict(new_samples)

'''

print(new_labels)

 '''
17.
But wait, because this is a clustering algorithm, we don’t know which label is which.

By looking at the cluster centers, let’s map out each of the labels with the digits we think it represents:
'''
for i in range(len(new_labels)):
  if new_labels[i] == 0:
    print(0, end='')
  elif new_labels[i] == 1:
    print(9, end='')
  elif new_labels[i] == 2:
    print(2, end='')
  elif new_labels[i] == 3:
    print(1, end='')
  elif new_labels[i] == 4:
    print(6, end='')
  elif new_labels[i] == 5:
    print(8, end='')
  elif new_labels[i] == 6:
    print(4, end='')
  elif new_labels[i] == 7:
    print(5, end='')
  elif new_labels[i] == 8:
    print(7, end='')
  elif new_labels[i] == 9:
    print(3, end='')
    
    '''

Hint
We did print(x, end='') so that all the digits are printed on the same line.

Index 0 looks like 0
Index 1 looks like 9
Index 2 looks like 2
Index 3 looks like 1
Index 4 looks like 6
Index 5 looks like 8
Index 6 looks like 4
Index 7 looks like 5
Index 8 looks like 7
Index 9 looks like 3
18.
Is the model recognizing your handwriting?

Remember, this model is trained on handwritten digits of 30 Turkish people (from the 1990’s).

Try writing your digits similar to these cluster centers:'''

#script.py---------------

import codecademylib3_seaborn
import numpy as np
from matplotlib import pyplot as plt

from sklearn import datasets 

from sklearn.cluster import KMeans 

digits = datasets.load_digits()

#print(digits.DESCR)
#print(digits.data)
print(digits.target)

#plt.gray()
#plt.matshow(digits.images[100])
#plt.show()

#print(digits.target[100])

model = KMeans(n_clusters = 10, random_state = 42)
model.fit(digits.data)

fig = plt.figure(figsize = (8,3))
fig.suptitle('Cluser Center Images', fontsize=14, fontweight='bold')

for i in range(10):

  # Initialize subplots in a grid of 2X5, at i+1th position
  ax = fig.add_subplot(2, 5, 1 + i)

  # Display images
  ax.imshow(model.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)

plt.show()


new_samples = np.array([
[0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.15,2.82,5.65,6.10,3.82,0.84,4.50,5.19,7.32,7.63,6.57,6.87,5.34,1.30,6.03,6.11,4.43,2.07,0.30,7.02,4.81,0.00,0.00,0.00,0.00,0.00,4.20,7.63,1.60,0.00,0.00,0.00,0.00,1.60,7.48,5.12,0.00,0.00,0.00,0.00,0.00,1.52,4.96,0.77,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00],
[0.00,0.00,5.04,7.55,6.03,1.45,0.00,0.00,0.00,0.00,7.47,5.87,6.72,7.10,0.46,0.00,0.00,0.15,7.63,3.66,1.75,7.63,3.13,0.00,0.00,1.07,7.62,2.75,0.08,5.96,6.71,0.08,0.00,1.52,7.63,2.21,0.00,2.82,7.63,1.45,0.00,0.08,2.75,0.23,0.00,2.90,7.63,1.22,0.00,0.00,0.00,0.00,0.00,4.58,7.17,0.08,0.00,0.00,0.00,0.00,0.00,4.19,7.40,0.00],
[0.00,0.00,0.00,1.15,6.41,6.41,2.98,0.00,0.00,0.00,0.61,6.87,6.87,5.88,7.63,2.60,0.00,0.00,4.66,7.48,1.37,0.08,6.87,5.04,0.00,1.53,7.62,4.27,0.00,0.00,6.72,5.12,0.00,1.52,5.80,0.38,0.00,1.76,7.63,3.14,0.00,0.00,0.00,0.00,0.00,4.28,7.33,0.31,0.00,0.00,0.00,0.00,0.00,5.65,6.11,0.00,0.00,0.00,0.00,0.00,0.15,7.10,4.89,0.00],
[0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.46,5.34,7.63,7.55,5.34,1.22,0.00,0.00,5.34,7.40,3.97,4.04,7.17,7.55,0.00,1.60,7.63,3.66,0.00,0.07,2.67,7.40,0.00,0.91,4.27,0.30,0.69,7.02,7.63,6.94,0.00,0.00,0.00,0.00,1.53,7.63,3.82,0.38,0.00,0.00,0.00,0.00,0.61,6.94,5.19,0.00]
])


new_labels = model.predict(new_samples)

for i in range(len(new_labels)):
  if new_labels[i] == 0:
    print(0, end='')
  elif new_labels[i] == 1:
    print(9, end='')
  elif new_labels[i] == 2:
    print(2, end='')
  elif new_labels[i] == 3:
    print(1, end='')
  elif new_labels[i] == 4:
    print(6, end='')
  elif new_labels[i] == 5:
    print(8, end='')
  elif new_labels[i] == 6:
    print(4, end='')
  elif new_labels[i] == 7:
    print(5, end='')
  elif new_labels[i] == 8:
    print(7, end='')
  elif new_labels[i] == 9:
    print(3, end='')


# test.html--------------------------

<html>
<head>
</head>
  
<body onload="InitThis();">
  
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
    
  <script type="text/javascript" src="JsCode.js"></script>
    <div align="center">
      
        <canvas id="myCanvas" width="80" height="80" style="border:2px solid black"></canvas>
        
      <canvas id="myCanvas2" width="80" height="80" style="border:2px solid black"></canvas>
      
      <canvas id="myCanvas3" width="80" height="80" style="border:2px solid black"></canvas>
      
      <canvas id="myCanvas4" width="80" height="80" style="border:2px solid black"></canvas>
      
        <br /><br />
        <button onclick="javascript:clearArea();return false;">Clear Area</button>
        Line width : <select id="selWidth">
            <option value="9">9</option>
            <option value="10">10</option>
            <option value="11">11</option>
            <option value="12">12</option>
            <option value="14" selected="selected">14</option>
            <option value="18">18</option>
        </select>
        Color : <select id="selColor">
            <option value="#141c3a">black</option>
            <option value="#6400e4">purple</option>
            <option value="#4b35ef" selected="selected">royal-blue</option>
            <option value="#fa4359">red</option>
            <option value="#37c3be">mint</option>
            <option value="#ffc107">yellow</option>
            <option value="#cccccc">gray</option>
        </select>
         
      <button onclick="javascript:array();return false;">Get Array</button>

      
    </div>
 
    <pre id="opening_bracket">
  </pre>
  
   <pre id="display">
   </pre>
  
  <pre id="display2">
  </pre>
  
   <pre id="display3">
  </pre>
  
   <pre id="display4">
  </pre>
  
  <pre id="closing_bracket">
  </pre>
  
  
  

</body>
</html>

#JsCode.js------------------------

var mousePressed = false;
var lastX, lastY;
var ctx;

function InitThis() {
  
  // ========= 1
  
    ctx = document.getElementById('myCanvas').getContext("2d");

    $('#myCanvas').mousedown(function (e) {
        mousePressed = true;
        Draw(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, false);
    });

    $('#myCanvas').mousemove(function (e) {
        if (mousePressed) {
            Draw(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, true);
        }
    });

    $('#myCanvas').mouseup(function (e) {
        mousePressed = false;
    });
	    $('#myCanvas').mouseleave(function (e) {
        mousePressed = false;
    });
 
   // =========== 2
  
   ctx2 = document.getElementById('myCanvas2').getContext("2d");

    $('#myCanvas2').mousedown(function (e) {
        mousePressed = true;
        Draw2(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, false);
    });

    $('#myCanvas2').mousemove(function (e) {
        if (mousePressed) {
            Draw2(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, true);
        }
    });

    $('#myCanvas2').mouseup(function (e) {
        mousePressed = false;
    });
	    $('#myCanvas2').mouseleave(function (e) {
        mousePressed = false;
    });
  
  
  // 3==========
  
   ctx3 = document.getElementById('myCanvas3').getContext("2d");

    $('#myCanvas3').mousedown(function (e) {
        mousePressed = true;
        Draw3(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, false);
    });

    $('#myCanvas3').mousemove(function (e) {
        if (mousePressed) {
            Draw3(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, true);
        }
    });

    $('#myCanvas3').mouseup(function (e) {
        mousePressed = false;
    });
	    $('#myCanvas3').mouseleave(function (e) {
        mousePressed = false;
    });
  
  
  // 4 =================
  
   ctx4 = document.getElementById('myCanvas4').getContext("2d");

    $('#myCanvas4').mousedown(function (e) {
        mousePressed = true;
        Draw4(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, false);
    });

    $('#myCanvas4').mousemove(function (e) {
        if (mousePressed) {
            Draw4(e.pageX - $(this).offset().left, e.pageY - $(this).offset().top, true);
        }
    });

    $('#myCanvas4').mouseup(function (e) {
        mousePressed = false;
    });
	    $('#myCanvas4').mouseleave(function (e) {
        mousePressed = false;
    });
  
  
  
}




function Draw(x, y, isDown) {
    if (isDown) {
        ctx.beginPath();
        ctx.strokeStyle = $('#selColor').val();
        ctx.lineWidth = $('#selWidth').val();
        ctx.lineJoin = "round";
        ctx.moveTo(lastX, lastY);
        ctx.lineTo(x, y);
        ctx.closePath();
        ctx.stroke();
    }
    lastX = x; lastY = y;
}

function Draw2(x, y, isDown) {
    if (isDown) {
        ctx2.beginPath();
        ctx2.strokeStyle = $('#selColor').val();
        ctx2.lineWidth = $('#selWidth').val();
        ctx2.lineJoin = "round";
        ctx2.moveTo(lastX, lastY);
        ctx2.lineTo(x, y);
        ctx2.closePath();
        ctx2.stroke();
    }
    lastX = x; lastY = y;
}

function Draw3(x, y, isDown) {
    if (isDown) {
        ctx3.beginPath();
        ctx3.strokeStyle = $('#selColor').val();
        ctx3.lineWidth = $('#selWidth').val();
        ctx3.lineJoin = "round";
        ctx3.moveTo(lastX, lastY);
        ctx3.lineTo(x, y);
        ctx3.closePath();
        ctx3.stroke();
    }
    lastX = x; lastY = y;
}


function Draw4(x, y, isDown) {
    if (isDown) {
        ctx4.beginPath();
        ctx4.strokeStyle = $('#selColor').val();
        ctx4.lineWidth = $('#selWidth').val();
        ctx4.lineJoin = "round";
        ctx4.moveTo(lastX, lastY);
        ctx4.lineTo(x, y);
        ctx4.closePath();
        ctx4.stroke();
    }
    lastX = x; lastY = y;
}
	
function clearArea() {
    // Use the identity matrix while clearing the canvas
    ctx.setTransform(1, 0, 0, 1, 0, 0);
    ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
  
    // clear ctx2
   ctx2.setTransform(1, 0, 0, 1, 0, 0);
    ctx2.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
  
      // clear ctx3
   ctx3.setTransform(1, 0, 0, 1, 0, 0);
    ctx3.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);

  // clear ctx4
  
   ctx4.setTransform(1, 0, 0, 1, 0, 0);
    ctx4.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);


}



function array() {
 
  
   document.getElementById('opening_bracket').innerHTML = "["
  
   var imageData = ctx.getImageData(0, 0, 80, 80);
  
   var data = imageData.data;
  
    for (var i = 0; i < data.length; i += 4) {
      
      var avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
      data[i]     = avg; // red
      data[i + 1] = avg; // green
      data[i + 2] = avg; // blue
      // ctx.putImageData(imageData, 0, 0);
  };
    
   // logs an array of 10 x 10 x 4 (400 items)
   //document.write(data);
   
  var gray = [];
  
  for (var x = 0; x < data.length; x +=4 ) {
      gray.push(data[x]);
  };
  
  // document.write(gray)
  // 122, 122, 124, 0,   121, 122, 122, 122,   
  //document.write(gray.length)
  // 6400
  
  
  var first_digit = [];
  for (var y = 0; y < data.length; y+=4) {
    first_digit.push(data[y])
  }
 
 
 //document.write(first_digit)
 //document.write(first_digit.length)
 // 6400
  
 var compress = []
 var counter = 0;
 var sum = 0;
 var ten = 0;
 
 for (var z = 0; z < first_digit.length; z++) {
   
   sum = sum + first_digit[z];
   
   if (z % 100 === 0) {
     compress.push(sum/100);
     sum = 0;   
   }
       
 };

  function average(list){
 averageVal = 0
 for(var i = 0; i < list.length; i++){
   averageVal = averageVal + list[i]/list.length
 }
 return averageVal
}

squares = []
for(var i = 0; i < 64; i++){
 squares.push([]);

}

for(var y = 0; y < 80; y++) {

  for(var x = 0; x < 80; x++) {
    
    squares[parseInt(y/10) * 8 + parseInt(x/10)].push(first_digit[x + y * 80])
 
  }
  
}

compressed = []

squares.forEach(function(square){
 compressed.push(average(square)/16)
})

//document.write(compressed)
  
  // round
for (var k = 0; k < compress.length; k++) {
  compressed[k] = compressed[k].toFixed(2);
}
  
  
  document.getElementById('display').innerHTML = "[" + compressed + "]" + ","
  
  
  
  
  
  
  
  // part 2
   var imageData = ctx2.getImageData(0, 0, 80, 80);
  
   var data = imageData.data;
  
    for (var i = 0; i < data.length; i += 4) {
      
      var avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
      data[i]     = avg; // red
      data[i + 1] = avg; // green
      data[i + 2] = avg; // blue
      // ctx.putImageData(imageData, 0, 0);
  };
    
   // logs an array of 10 x 10 x 4 (400 items)
   //document.write(data);
   
  var gray = [];
  
  for (var x = 0; x < data.length; x +=4 ) {
      gray.push(data[x]);
  };
  
  // document.write(gray)
  // 122, 122, 124, 0,   121, 122, 122, 122,   
  //document.write(gray.length)
  // 6400
  
  
  var first_digit = [];
  for (var y = 0; y < data.length; y+=4) {
    first_digit.push(data[y])
  }
 
 
 //document.write(first_digit)
 //document.write(first_digit.length)
 // 6400
  
 var compress = []
 var counter = 0;
 var sum = 0;
 var ten = 0;
 
 for (var z = 0; z < first_digit.length; z++) {
   
   sum = sum + first_digit[z];
   
   if (z % 100 === 0) {
     compress.push(sum/100);
     sum = 0;   
   }
       
 };

  function average(list){
 averageVal = 0
 for(var i = 0; i < list.length; i++){
   averageVal = averageVal + list[i]/list.length
 }
 return averageVal
}

squares = []
for(var i = 0; i < 64; i++){
 squares.push([]);

}

for(var y = 0; y < 80; y++) {

  for(var x = 0; x < 80; x++) {
    
    squares[parseInt(y/10) * 8 + parseInt(x/10)].push(first_digit[x + y * 80])
 
  }
  
}

compressed = []

squares.forEach(function(square){
 compressed.push(average(square)/16)
})

//document.write(compressed)
  
// round
for (var k = 0; k < compress.length; k++) {
  compressed[k] = compressed[k].toFixed(2);
}
  


  document.getElementById('display2').innerHTML = "[" + compressed + "]" + ","
  
  
  
  
  
  // =============== part 3
  
  var imageData = ctx3.getImageData(0, 0, 80, 80);
  
   var data = imageData.data;
  
    for (var i = 0; i < data.length; i += 4) {
      
      var avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
      data[i]     = avg; // red
      data[i + 1] = avg; // green
      data[i + 2] = avg; // blue
      // ctx.putImageData(imageData, 0, 0);
  };
    
   // logs an array of 10 x 10 x 4 (400 items)
   //document.write(data);
   
  var gray = [];
  
  for (var x = 0; x < data.length; x +=4 ) {
      gray.push(data[x]);
  };
  
  // document.write(gray)
  // 122, 122, 124, 0,   121, 122, 122, 122,   
  //document.write(gray.length)
  // 6400
  
  
  var first_digit = [];
  for (var y = 0; y < data.length; y+=4) {
    first_digit.push(data[y])
  }
 
 
 //document.write(first_digit)
 //document.write(first_digit.length)
 // 6400
  
 var compress = []
 var counter = 0;
 var sum = 0;
 var ten = 0;
 
 for (var z = 0; z < first_digit.length; z++) {
   
   sum = sum + first_digit[z];
   
   if (z % 100 === 0) {
     compress.push(sum/100);
     sum = 0;   
   }
       
 };

  function average(list){
 averageVal = 0
 for(var i = 0; i < list.length; i++){
   averageVal = averageVal + list[i]/list.length
 }
 return averageVal
}

squares = []
for(var i = 0; i < 64; i++){
 squares.push([]);

}

for(var y = 0; y < 80; y++) {

  for(var x = 0; x < 80; x++) {
    
    squares[parseInt(y/10) * 8 + parseInt(x/10)].push(first_digit[x + y * 80])
 
  }
  
}

compressed = []

squares.forEach(function(square){
 compressed.push(average(square)/16)
})

//document.write(compressed)
  
// round
for (var k = 0; k < compress.length; k++) {
  compressed[k] = compressed[k].toFixed(2);
}
  
  document.getElementById('display3').innerHTML = "[" + compressed + "]" + ","
  
  
  
  
  
  
  
  // =========== 4
  var imageData = ctx4.getImageData(0, 0, 80, 80);
  
   var data = imageData.data;
  
    for (var i = 0; i < data.length; i += 4) {
      
      var avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
      data[i]     = avg; // red
      data[i + 1] = avg; // green
      data[i + 2] = avg; // blue
      // ctx.putImageData(imageData, 0, 0);
  };
    
   // logs an array of 10 x 10 x 4 (400 items)
   //document.write(data);
   
  var gray = [];
  
  for (var x = 0; x < data.length; x +=4 ) {
      gray.push(data[x]);
  };
  
  // document.write(gray)
  // 122, 122, 124, 0,   121, 122, 122, 122,   
  //document.write(gray.length)
  // 6400
  
  
  var first_digit = [];
  for (var y = 0; y < data.length; y+=4) {
    first_digit.push(data[y])
  }
 
 
 //document.write(first_digit)
 //document.write(first_digit.length)
 // 6400
  
 var compress = []
 var counter = 0;
 var sum = 0;
 var ten = 0;
 
 for (var z = 0; z < first_digit.length; z++) {
   
   sum = sum + first_digit[z];
   
   if (z % 100 === 0) {
     compress.push(sum/100);
     sum = 0;   
   }
       
 };

  function average(list){
 averageVal = 0
 for(var i = 0; i < list.length; i++){
   averageVal = averageVal + list[i]/list.length
 }
 return averageVal
}

squares = []
for(var i = 0; i < 64; i++){
 squares.push([]);

}

for(var y = 0; y < 80; y++) {

  for(var x = 0; x < 80; x++) {
    
    squares[parseInt(y/10) * 8 + parseInt(x/10)].push(first_digit[x + y * 80])
 
  }
  
}

compressed = []

squares.forEach(function(square){
 compressed.push(average(square)/16)
})

// round
for (var k = 0; k < compress.length; k++) {
  compressed[k] = compressed[k].toFixed(2);
}
  
//document.write(compressed)
  
  
  document.getElementById('display4').innerHTML = "[" + compressed + "]"
  
  document.getElementById('closing_bracket').innerHTML = "]"

}










#-----------------------------------------------

'''
K-MEANS++ CLUSTERING
Introduction to K-Means++
The K-Means clustering algorithm is more than half a century old, but it is not falling out of fashion; it is still the most popular clustering algorithm for Machine Learning.

However, there can be some problems with its first step. In the traditional K-Means algorithms, the starting postitions of the centroids are intialized completely randomly. This can result in suboptimal clusters.

In this lesson, we will go over another version of K-Means, known as the K-Means++ algorithm. K-Means++ changes the way centroids are initalized to try to fix this problem.
'''
'''
1.
Run the program in script.py to cluster Codecademy learners into two groups using K-Means and K-Means++.

The only difference between each algorithm is how the cluster centroids are initialized.

It’s hard to see, but the clusters are different. Look at the point at x=0.2 and y=1. On the top graph you should see a purple point, but on the bottom graph a yellow point.

Which one of these clusters is better? We have printed the model of each inertia in the workspace. The model with the lower inertia has more coherent clusters. You can think of the model with the lower inertia as being “better”.

Which model performs better clustering?

Continue to the next exercise to see why random initialization of centroids can result in poorer clusters.

For a recap, the K-Means algorithm is the following:

Place k random centroids for the initial clusters.
Assign data samples to the nearest centroid.
Update centroids based on the above-assigned data samples.
Repeat Steps 2 and 3 until convergence.

Convergence happens when centroids stabilize.

On a Codecademy HQ computer, the result looks something like:

Average Runtime: 0.04122559293
Worst Runtime: 0.14529825281
So why can the random initial placement of centroids result in slow convergence?

When you place initial centroids randomly, there are just way too many possibilities.

For example, what if when all the initial centroids are randomized to be located in the top left corner, but the clusters are around the bottom right corner of the dataset?

Well, the K-Means algorithm would take a little longer than if the initial centroids are placed in a more spaced out way.
'''
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans

import random
import timeit

mu = 1
std = 0.5

np.random.seed(100)

xs = np.append(np.append(np.append(np.random.normal(0.25,std,100), np.random.normal(0.75,std,100)), np.random.normal(0.25,std,100)), np.random.normal(0.75,std,100))

ys = np.append(np.append(np.append(np.random.normal(0.25,std,100), np.random.normal(0.25,std,100)), np.random.normal(0.75,std,100)), np.random.normal(0.75,std,100))

values = list(zip(xs, ys))

model = KMeans(init='random', n_clusters=2)
results = model.fit_predict(values)
print("The inertia of model that randomly initialized centroids is " + str(model.inertia_))



colors = ['#6400e4', '#ffc740']
plt.subplot(211)
for i in range(2):
  points = np.array([values[j] for j in range(len(values)) if results[j] == i])
  plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.6)

plt.title('Codecademy Mobile Feedback - Centroids Initialized Randomly')

plt.xlabel('Learn Python')
plt.ylabel('Learn SQL')

plt.subplot(212)
model = KMeans( n_clusters=2)
results = model.fit_predict(values)
print("The inertia of model that initialized the centroids using KMeans++ is " + str(model.inertia_))



colors = ['#6400e4', '#ffc740']

for i in range(2):
  points = np.array([values[j] for j in range(len(values)) if results[j] == i])
  plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.6)

plt.title('Codecademy Mobile Feedback - Centroids Initialized Using KMeans++')

plt.xlabel('Learn Python')
plt.ylabel('Learn SQL')

plt.tight_layout()
plt.show()


'''
K-MEANS++ CLUSTERING
Poor Clustering
Suppose we have four data samples that form a rectangle whose width is greater than its height:

Data Points in Rectangle Shape
If you wanted to find two clusters (k = 2) in the data, which points would you cluster together? You might guess the points that align vertically cluster together, since the height of the rectangle is smaller than its width. We end up with a left cluster (purple points) and a right cluster (yellow points).

Optimal Cluster
Let’s say we use the regular K-Means algorithm to cluster the points, where the cluster centroids are initialized randomly. We get unlucky and those randomly initialized cluster centroids happen to be the midpoints of the top and bottom line segments of the rectangle formed by the four data points.

Random Centroid Placement
The algorithm would converge immediately, without moving the cluster centroids. Consequently, the two top data points are clustered together (yellow points) and the two bottom data points are clustered together (purple points).

Suboptimal Clusters
This is a suboptimal clustering because the width of the rectangle is greater than its height. The optimal clusters would be the two left points as one cluster and the two right points as one cluster, as we thought earlier.'''

'''
1.
Suppose we have four data samples with these values:

(1, 1)
(1, 3)
(4, 1)
(4, 3)
And suppose we perform K-means on this data where the k is 2 and the randomized 2 initial centroids are located at the following positions:

(2.5, 1)
(2.5, 3)
What do you think the result clusters would look like?

Run script.py to find out the answer.

The K-Means converged immediately without moving clusters in this example because the centroids are already stabilized (samples won’t move from one cluster to another).'''

#---------------------script.py-------------------------------

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from copy import deepcopy
from sklearn.cluster import KMeans 

x = [1, 1, 4, 4]
y = [1, 3, 1, 3]

values = np.array(list(zip(x, y)))

centroids_x = [2.5, 2.5]
centroids_y = [1, 3]

centroids = np.array(list(zip(centroids_x, centroids_y)))

model = KMeans(init=centroids, n_clusters=2)

# Initial centroids
# plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=100)

results = model.fit_predict(values)

plt.scatter(x, y, c=results, alpha=1)

# Cluster centers
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], marker='v', s=100)

ax = plt.subplot()
ax.set_xticks([0, 1, 2, 3, 4, 5])
ax.set_yticks([0, 1, 2, 3, 4])

plt.title('Unlucky Initialization')
plt.show()

'''
K-MEANS++ CLUSTERING
What is K-Means++?
To recap, the Step 1 of the K-Means algorithm is “Place k random centroids for the initial clusters”.

The K-Means++ algorithm replaces Step 1 of the K-Means algorithm and adds the following:

1.1 The first cluster centroid is randomly picked from the data points.
1.2 For each remaining data point, the distance from the point to its nearest cluster centroid is calculated.
1.3 The next cluster centroid is picked according to a probability proportional to the distance of each point to its nearest cluster centroid. This makes it likely for the next cluster centroid to be far away from the already initialized centroids.
Repeat 1.2 - 1.3 until k centroids are chosen.'''

'''
Instructions
In the web browser you can see the initialization of centroids by regular K-Means (randomly) and by K-Means++.

Notice that the centroids created by K-Means++ are more spaced out.

Make sure to scroll down to see the second graph!'''

''''
K-MEANS++ CLUSTERING
K-Means++ using Scikit-Learn
Using the scikit-learn library and its cluster module , you can use the KMeans() method to build an original K-Means model that finds 6 clusters like so:

model = KMeans(n_clusters=6, init='random')
The init parameter is used to specify the initialization and init='random' specifies that initial centroids are chosen as random (the original K-Means).

But how do you implement K-Means++?

There are two ways and they both require little change to the syntax:

Option 1: You can adjust the parameter to init='k-means++'.

test = KMeans(n_clusters=6, init='k-means++')
Option 2: Simply drop the parameter.

test = KMeans(n_clusters=6)
This is because that init=k-means++ is actually default in scikit-learn.'''

'''
1.
We’ve brought back our small example where we intentionally selected unlucky initial positions for the cluser centroids.

On line 22 where we create the model, change the init parameter to "k-means++" and see how the clusters change. Were we able to find optimal clusters?

Make sure to put "k-means++" in quotes!
'''

import codecademylib3_seaborn
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from copy import deepcopy
from sklearn.cluster import KMeans 

x = [1, 1, 4, 4]
y = [1, 3, 1, 3]

values = np.array(list(zip(x, y)))

centroids_x = [2.5, 2.5]
centroids_y = [1, 3]

centroids = np.array(list(zip(centroids_x, centroids_y)))

model = KMeans(init="k-means++", n_clusters=2)
# Initial centroids
# plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=100)

results = model.fit_predict(values)

plt.scatter(x, y, c=results, alpha=1)

# Cluster centers
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], marker='v', s=100)

ax = plt.subplot()
ax.set_xticks([0, 1, 2, 3, 4, 5])
ax.set_yticks([0, 1, 2, 3, 4])

plt.title('K-Means++ Initialization')
plt.show()
print("The model's inertia is " + str(model.inertia_))

'''
K-MEANS++ CLUSTERING
Review
Congratulations, now your K-Means model is improved and ready to go!

K-Means++ improves K-Means by placing initial centroids more strategically. As a result, it can result in more optimal clusterings than K-Means.

It can also outperform K-Means in speed. If you get very unlucky initial centroids using K-Means, the algorithm can take a long time to converge. K-Means++ will often converge quicker!

You can implement K-Means++ with the scikit-learn library similar to how you implement K-Means.

The KMeans() function has an init parameter, which specifies the method for initialization:

'random'
'k-means++'
Note: scikit-learn’s KMeans() uses 'k-means++' by default, but it is a good idea to be explicit!
'''
'''
Instructions
The code in the workspace performs two clusterings on Codecademy learner data using K-Means. The first algorithm initializes the centroids at the x positions given on line 12 and the y positions given on line 13. The second algorithm initializes the centroids according to the K-Means++ algorithm.

Try changing the positions at which the centroids are initialized on lines 12 and 13. How does changing the initialization position affect the final clustering? And how does the first clustering compare to the K-Means++ clustering?

Make sure to scroll down to see the second graph!'''

import codecademylib3_seaborn
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from copy import deepcopy
from sklearn.cluster import KMeans

std = 0.5

x = np.append(np.append(np.append(np.random.normal(0.25,std,100), np.random.normal(0.75,std,100)), np.random.normal(0.25,std,100)), np.random.normal(0.75,std,100))

y = np.append(np.append(np.append(np.random.normal(0.25,std,100), np.random.normal(0.25,std,100)), np.random.normal(0.75,std,100)), np.random.normal(0.75,std,100))

values = np.array(list(zip(x, y)))

centroids_x = [2.5, 2.5]
centroids_y = [1, 3]

centroids = np.array(list(zip(centroids_x, centroids_y)))

model_custom = KMeans(init=centroids, n_clusters=2)
results_custom = model_custom.fit_predict(values)

model = KMeans(init='k-means++', n_clusters=2)
results = model.fit_predict(values)

plt.scatter(x, y, c=results_custom, alpha=1)
plt.scatter(model_custom.cluster_centers_[:, 0], model_custom.cluster_centers_[:, 1], marker='v', s=100)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=100)
plt.title('Custom Initialization')
plt.show()
plt.cla()

plt.scatter(x, y, c=results, alpha=1)
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], marker='v', s=100)
plt.title('K-Means++ Initialization')
plt.show()

print("The custom model's inertia is " + str(model_custom.inertia_))
print("The K-means++ model's inertia is " + str(model.inertia_))

'''
PERCEPTRON
What is a Perceptron?
Similar to how atoms are the building blocks of matter and how microprocessors are the building blocks of a computer, perceptrons are the building blocks of Neural Networks.

If you look closely, you might notice that the word “perceptron” is a combination of two words:

Perception (noun) the ability to sense something
Neuron (noun) a nerve cell in the human brain that turns sensory input into meaningful information
Therefore, the perceptron is an artificial neuron that simulates the task of a biological neuron to solve problems through its own “sense” of the world.

Although the perceptron comes with its own artificial design and set of parameters, at its core, a single perceptron is trying to make a simple decision.

Let’s take the example a simple self-driving car that is based on a perceptron. If there’s an obstacle on the left, the car would have to steer right. Similarly, if there’s an obstacle on the right, the car would have to steer left.

For this example, a perceptron could take the position of the obstacle as inputs, and produce a decision — left turn or right turn — based on those inputs.

And here’s the cool part — the perceptron can correct itself based on the result of its decision to make better decisions in the future!

Of course, the real world scenario isn’t that simple. But if you combine a bunch of such perceptrons, you will get a neural network that can even make better decisions on your behalf!

'''

'''
What are Neural Networks?
An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain.
Understanding Neural Networks

As you are reading this article, the very same brain that sometimes forgets why you walked into a room is magically translating these pixels into letters, words, and sentences — a feat that puts the world’s fastest supercomputers to shame. Within the brain, thousands of neurons are firing at incredible speed and accuracy to help us recognize text, images, and the world at large.

Neural Net


A neural network is a programming model that simulates the human brain. Let’s explore how it came into existence.

The Birth of an Artificial Neuron

Computers have been designed to excel at number-crunching tasks, something that most humans find terrifying. On the other hand, humans are naturally wired to effortlessly recognize objects and patterns, something that computers find difficult.

This juxtaposition brought up two important questions in the 1950s:

“How can computers be better at solving problems that humans find effortless?”
“How can computers solve such problems in the way a human brain does?”
In 1957, Frank Rosenblatt explored the second question and invented the Perceptron algorithm that allowed an artificial neuron to simulate a biological neuron! The artificial neuron could take in an input, process it based on some rules, and fire a result. But computers had been doing this for years — what was so remarkable?

Perceptron


There was a final step in the Perceptron algorithm that would give rise to the incredibly mysterious world of Neural Networks — the artificial neuron could train itself based on its own results, and fire better results in the future. In other words, it could learn by trial and error, just like a biological neuron.

More Neurons

The Perceptron Algorithm used multiple artificial neurons, or perceptrons, for image recognition tasks and opened up a whole new way to solve computational problems. However, as it turns out, this wasn’t enough to solve a wide range of problems, and interest in the Perceptron Algorithm along with Neural Networks waned for many years.

But many years later, the neurons fired back.

Multilayer perceptron


It was found out that creating multiple layers of neurons — with one layer feeding its output to the next layer as input — could process a wide range of inputs, make complex decisions, and still produce meaningful results. With some tweaks, the algorithm became known as the Multilayer Perceptron, which led to the rise of Feedforward Neural Networks.

60 Years Later…

With Feedforward Networks, the results improved. But it was only recently, with the development of high-speed processors, that neural networks finally got the necessary computing power to seamlessly integrate into daily human life.

Today, the applications of neural networks have become widespread — from simple tasks like speech recognition to more complicated tasks like self-driving vehicles.

In 2012, Alex Krizhevsky and his team at University of Toronto entered the ImageNet competition (the annual Olympics of computer vision) and trained a deep convolutional neural network [pdf]. No one truly understood how it made the decisions it did, but it worked better than any other traditional classifier, by a huge 10.8% margin.

Summary

The neurons have come a long way. They have braved the AI winter and remained patient amidst the lack of computing power in the 20th century. They have now taken the world by storm and deservedly so.

Neural networks are ridiculously good at generating results but also mysteriously complex; the apparent complexity of the decision-making process makes it difficult to say exactly how neural networks arrive at their superhuman level of accuracy.

Let’s dive into the world of Neural Networks and relish in all its mystery!
'''

'''
PERCEPTRON
Representing a Perceptron
So the perceptron is an artificial neuron that can make a simple decision. Let’s implement one from scratch in Python!

The perceptron has three main components:

Inputs: Each input corresponds to a feature. For example, in the case of a person, features could be age, height, weight, college degree, etc.

Weights: Each input also has a weight which assigns a certain amount of importance to the input. If an input’s weight is large, it means this input plays a bigger role in determining the output. For example, a team’s skill level will have a bigger weight than the average age of players in determining the outcome of a match.

Output: Finally, the perceptron uses the inputs and weights to produce an output. The type of the output varies depending on the nature of the problem. For example, to predict whether or not it’s going to rain, the output has to be binary — 1 for Yes and 0 for No. However, to predict the temperature for the next day, the range of the output has to be larger — say a number from 70 to 90.
'''
'''
PERCEPTRON
Representing a Perceptron
So the perceptron is an artificial neuron that can make a simple decision. Let’s implement one from scratch in Python!

The perceptron has three main components:

Inputs: Each input corresponds to a feature. For example, in the case of a person, features could be age, height, weight, college degree, etc.

Weights: Each input also has a weight which assigns a certain amount of importance to the input. If an input’s weight is large, it means this input plays a bigger role in determining the output. For example, a team’s skill level will have a bigger weight than the average age of players in determining the outcome of a match.

Output: Finally, the perceptron uses the inputs and weights to produce an output. The type of the output varies depending on the nature of the problem. For example, to predict whether or not it’s going to rain, the output has to be binary — 1 for Yes and 0 for No. However, to predict the temperature for the next day, the range of the output has to be larger — say a number from 70 to 90.

Instructions
1.
Our Perceptron class by default takes two inputs and a pre-defined weight for each input.

Complete the __init__() method inside the Perceptron class by creating instance variables self.num_inputs and self.weights that represent the attributes of a Perceptron object.

Assign the parameters num_inputs and weights to self.num_inputs and self.weights respectively.

It should look something like:

class Perceptron:
  def __init__(self, num_inputs=2, weights=[1,1]):
    self.__________ = __________
    self._______ = _______
2.
Create a Perceptron object called cool_perceptron (without any arguments) and print it out to see what it looks like.

It should look something like:

my_object = Class()

print(my_object)
These two lines should be outside of the class creation (so no indents).'''



class Perceptron:
  def __init__(self, num_inputs=2, weights=[1,1]):
    # complete the default constructor method
    self.num_inputs = num_inputs
    self.weights = weights


cool_perceptron = Perceptron()

print(cool_perceptron)

'''
PERCEPTRON
Step 1: Weighted Sum
Great! Now that you understand the structure of the perceptron, here’s an important question — how are the inputs and weights magically turned into an output? This is a two-step process, and the first step is finding the weighted sum of the inputs.

What is the weighted sum? This is just a number that gives a reasonable representation of the inputs:

weighted\ sum = x_1w_1 + x_2w_2 + ... + x_nw_nweighted sum=x 
1
​	 w 
1
​	 +x 
2
​	 w 
2
​	 +...+x 
n
​	 w 
n
​	 
The x‘s are the inputs and the w‘s are the weights.

Here’s how we can implement it:

Start with a weighted sum of 0. Let’s call it weighted_sum.
Start with the first input and multiply it by its corresponding weight. Add this result to weighted_sum.
Go to the next input and multiply it by its corresponding weight. Add this result to weighted_sum.
Repeat this process for all inputs.'''

'''
Instructions
1.
Create a variable called weighted_sum to hold the value of the weighted sum and set its starting value to 0.

Return weighted_sum outside the for loop.

2.
Let’s go through each input-weight pair and find the weighted sum using indexing.

Delete the pass statement inside the for loop. For each iteration in the loop, find the product of the input and weight at index i, add the result to weighted_sum, and store it back in weighted_sum to update the value of weighted_sum.

Instead of doing something like w = w + p, you can use the += operator to simplify the expression to w += p.

weighted_sum += self.weights[i] * inputs[i]
3.
Outside the Perceptron class, after the Perceptron object cool_perceptron has been created, print out the weighted sum for the inputs [24, 55].

What is the weighted sum?

Pass the inputs into the .weighted_sum() method and call it on cool_perceptron.

print(cool_perceptron.weighted_sum([24, 55]))
The weighted sum should be 103 when the weights are [2, 1]:

weighted\ sum = x_1w_1 + x_2w_2 = 24 \cdot 2 + 55 \cdot 1 = 103weighted sum=x 
1
​	 w 
1
​	 +x 
2
​	 w 
2
​	 =24⋅2+55⋅1=103'''

class Perceptron:
  def __init__(self, num_inputs=2, weights=[2,1]):
    self.num_inputs = num_inputs
    self.weights = weights
    
  def weighted_sum(self, inputs):
    # create variable to store weighted sum
    weighted_sum  = 0
    for i in range(self.num_inputs):
      weighted_sum += inputs[i]*self.weights[i]
      # complete this loop
    return weighted_sum
      
cool_perceptron = Perceptron()

print(cool_perceptron.weighted_sum([24,55]))

'''
PERCEPTRON
Step 2: Activation Function
After finding the weighted sum, the second step is to constrain the weighted sum to produce a desired output.

Why is that important? Imagine if a perceptron had inputs in the range of 100-1000 but the goal was to simply predict whether or not something would occur — 1 for “Yes” and 0 for “No”. This would result in a very large weighted sum.

How can the perceptron produce a meaningful output in this case? This is exactly where activation functions come in! These are special functions that transform the weighted sum into a desired and constrained output.

For example, if you want to train a perceptron to detect whether a point is above or below a line (which we will be doing in this lesson!), you might want the output to be a +1 or -1 label. For this task, you can use the “sign activation function” to help the perceptron make the decision:

If weighted sum is positive, return +1
If weighted sum is negative, return -1
In this lesson, we will focus on using the sign activation function because it is the simplest way to get started with perceptrons and eventually visualize one in action.'''

'''
Instructions
1.
Inside the .activation() method, return 1 if the weighted_sum is greater than or equal to 0.

2.
Inside the .activation() method, return -1 if the weighted_sum is less than 0.

3.
Try it out for yourself!

Print out the result of the method .activation() called on cool_perceptron if the weighted sum is 52.'''

class Perceptron:
  def __init__(self, num_inputs=2, weights=[1,1]):
    self.num_inputs = num_inputs
    self.weights = weights
    
  def weighted_sum(self, inputs):
    weighted_sum = 0
    for i in range(self.num_inputs):
      weighted_sum += self.weights[i]*inputs[i]
    return weighted_sum
  
  def activation(self, weighted_sum):
    if weighted_sum >= 0:
      return 1
    elif weighted_sum < 0:
      return -1
    
    #Complete this method

cool_perceptron = Perceptron()
print(cool_perceptron.weighted_sum([24, 55]))

print(cool_perceptron.activation(52))

'''
PERCEPTRON
Training the Perceptron
Our perceptron can now make a prediction given inputs, but how do we know if it gets those predictions right?

Right now we expect the perceptron to be very bad because it has random weights. We haven’t taught it anything yet, so we can’t expect it to get classifications correct! The good news is that we can train the perceptron to produce better and better results! In order to do this, we provide the perceptron a training set — a collection of random inputs with correctly predicted outputs.

On the right, you can see a plot of scattered points with positive and negative labels. This is a simple training set.

In the code, the training set has been represented as a dictionary with coordinates as keys and labels as values. For example:

training_set = {(18, 49): -1, (2, 17): 1, (24, 35): -1, (14, 26): 1, (17, 34): -1}
We can measure the perceptron’s actual performance against this training set. By doing so, we get a sense of “how bad” the perceptron is. The goal is to gradually nudge the perceptron — by slightly changing its weights — towards a better version of itself that correctly matches all the input-output pairs in the training set.

We will use these points to train the perceptron to correctly separate the positive labels from the negative labels by visualizing the perceptron as a line. Stay tuned!
'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import random

def generate_training_set(num_points):
	x_coordinates = [random.randint(0, 50) for i in range(num_points)]
	y_coordinates = [random.randint(0, 50) for i in range(num_points)]
	training_set = dict()
	for x, y in zip(x_coordinates, y_coordinates):
		if x <= 45-y:
			training_set[(x,y)] = 1
		elif x > 45-y:
			training_set[(x,y)] = -1
	return training_set

training_set = generate_training_set(30)

x_plus = []
y_plus = []
x_minus = []
y_minus = []

for data in training_set:
	if training_set[data] == 1:
		x_plus.append(data[0])
		y_plus.append(data[1])
	elif training_set[data] == -1:
		x_minus.append(data[0])
		y_minus.append(data[1])
    
fig = plt.figure()
ax = plt.axes(xlim=(-25, 75), ylim=(-25, 75))

plt.scatter(x_plus, y_plus, marker = '+', c = 'green', s = 128, linewidth = 2)
plt.scatter(x_minus, y_minus, marker = '_', c = 'red', s = 128, linewidth = 2)

plt.title("Training Set")

plt.show()
'''
PERCEPTRON
Training Error
Now that we have our training set, we can start feeding inputs into the perceptron and comparing the actual outputs against the expected labels!

Every time the output mismatches the expected label, we say that the perceptron has made a training error — a quantity that measures “how bad” the perceptron is performing.

As mentioned in the last exercise, the goal is to nudge the perceptron towards zero training error. The training error is calculated by subtracting the predicted label value from the actual label value.

training\ error = actual\ label - predicted\ labeltraining error=actual label−predicted label
For each point in the training set, the perceptron either produces a +1 or a -1 (as we are using the Sign Activation Function). Since the labels are also a +1 or a -1, there are four different possibilities for the error the perceptron makes:

| Actual | Predicted | Training Error | | --- | --- | --- | | +1 | +1 | 0 | | +1 | -1 | 2 | | -1 | -1 | 0 | | -1 | +1 | -2 |

These training error values will be crucial in improving the perceptron’s performance as we will see in the upcoming exercises.'''

'''
Instructions
1.
In the .training() method, let’s find the perceptron’s error on each inputs in training_set.

First, we need the perceptron’s predicted output for a point. Inside the for loop, create a variable called prediction and assign it the correct label value using .activation(), .weighted_sum(), and inputs in a single statement.

To find the predicted label, first pass inputs into the .weighted_sum() method to find the weighted sum. Then pass the weighted sum into the .activation() method to get the predicted label.

2.
Create a variable named actual and assign it the actual label for each inputs in training_set.

You can access the actual label by indexing into training_set using inputs.

3.
Create a variable called error and assign it the value of actual - prediction.

At the end of this exercise, the .training() method should look like:

  def training(self, training_set):
    for inputs in training_set:
      prediction = self.activation(self.weighted_sum(inputs))
      actual = training_set[inputs]
      error = actual - prediction'''
      
class Perceptron:
  def __init__(self, num_inputs=2, weights=[1,1]):
    self.num_inputs = num_inputs
    self.weights = weights
    
  def weighted_sum(self, inputs):
    weighted_sum = 0
    for i in range(self.num_inputs):
      weighted_sum += self.weights[i]*inputs[i]
    return weighted_sum
  
  def activation(self, weighted_sum):
    if weighted_sum >= 0:
      return 1
    if weighted_sum < 0:
      return -1
    
  def training(self, training_set):
    for inputs in training_set:
      prediction = self.activation(self.weighted_sum(inputs))
      actual = training_set[inputs]
      error = actual - prediction


      
cool_perceptron = Perceptron()
print(cool_perceptron.weighted_sum([24, 55]))
print(cool_perceptron.activation(52))

a = dict()
a[(1,2)] = "a"
print(a)
for i in a:
  print(a[i])

'''
Learn
PERCEPTRON
Tweaking the Weights
What do we do once we have the errors for the perceptron? We slowly nudge the perceptron towards a better version of itself that eventually has zero error.

The only way to do that is to change the parameters that define the perceptron. We can’t change the inputs so the only thing that can be tweaked are the weights. As we change the weights, the outputs change as well.

The goal is to find the optimal combination of weights that will produce the correct output for as many points as possible in the dataset.'''

'''

PERCEPTRON
The Perceptron Algorithm
But one question still remains — how do we tweak the weights optimally? We can’t just play around randomly with the weights until the correct combination magically pops up. There needs to be a way to guarantee that the perceptron improves its performance over time.

This is where the Perceptron Algorithm comes in. The math behind why this works is outside the scope of this lesson, so we’ll directly apply the algorithm to optimally tweak the weights and nudge the perceptron towards zero error.

The most important part of the algorithm is the update rule where the weights get updated:

weight = weight + (error * input)weight=weight+(error∗input)
We keep on tweaking the weights until all possible labels are correctly predicted by the perceptron. This means that multiple passes might need to be made through the training_set before the Perceptron Algorithm comes to a halt.

In this exercise, you will continue to work on the .training() method. We have made the following changes to this method from the last exercise:

foundLine = False (a boolean that indicates whether the perceptron has found a line to separate the positive and negative labels)
while not foundLine: (a while loop that continues to train the perceptron until the line is found)
total_error = 0 (to count the total error the perceptron makes in each round)
total_error += abs(error) (to update the total error the perceptron makes in each round)'''
'''
Instructions
1.
If the algorithm doesn’t find an error, the perceptron must have correctly predicted the labels for all points.

Outside the for loop (but inside the while loop), change the value of foundLine to True if total_error equals 0.

2.
In order to update the weight for each inputs, create another for loop (inside the existing for loop) that iterates a loop variable i through a range of self.num_inputs.

3.
Inside the second for loop, update each weight self.weights[i] by applying the update rule:

weight = weight + (error * inputs)weight=weight+(error∗inputs)
4.
Great job! Now give it a try for yourself.

Train cool_perceptron using small_training_set.

You can also print out the optimal weights to see for yourself!

Pass small_training_set as an argument to the .training() method and call the method on cool_perceptron.
'''
class Perceptron:
  def __init__(self, num_inputs=2, weights=[1,1]):
    self.num_inputs = num_inputs
    self.weights = weights
    
  def weighted_sum(self, inputs):
    weighted_sum = 0
    for i in range(self.num_inputs):
      weighted_sum += self.weights[i]*inputs[i]
    return weighted_sum
  
  def activation(self, weighted_sum):
    if weighted_sum >= 0:
      return 1
    if weighted_sum < 0:
      return -1
    
  def training(self, training_set):
    foundLine = False
    while not foundLine:
      total_error = 0
      for inputs in training_set:
        prediction = self.activation(self.weighted_sum(inputs))
        actual = training_set[inputs]
        error = actual - prediction
        total_error += abs(error)
        for i in range(self.num_inputs):
          self.weights[i] += error*inputs[i]
      if total_error == 0:
        foundLine = True

cool_perceptron = Perceptron()
small_training_set = {(0,3):1, (3,0):-1, (0,-3):-1, (-3,0):1}
print(cool_perceptron.weights)
cool_perceptron.training(small_training_set)
print(cool_perceptron.weights)


'''
PERCEPTRON
The Bias Weight
You have understood that the perceptron can be trained to produce correct outputs by tweaking the regular weights.

However, there are times when a minor adjustment is needed for the perceptron to be more accurate. This supporting role is played by the bias weight. It takes a default input value of 1 and some random weight value.

So now the weighted sum equation should look like:

weighted\ sum = x_1w_1 + x_2w_2 + ... + x_nw_n + 1w_bweighted sum=x 
1
​	 w 
1
​	 +x 
2
​	 w 
2
​	 +...+x 
n
​	 w 
n
​	 +1w 
b
​	 
How does this change the code so far? You only have to consider two small changes:

Add a 1 to the set of inputs (now there are 3 inputs instead of 2)
Add a bias weight to the list of weights (now there are 3 weights instead of 2)
We’ll automatically make these replacements in the code so you should be good to go!

'''
class Perceptron:
  def __init__(self, num_inputs=3, weights=[1,1,1]):
    self.num_inputs = num_inputs
    self.weights = weights
    
  def weighted_sum(self, inputs):
    weighted_sum = 0
    for i in range(self.num_inputs):
      weighted_sum += self.weights[i]*inputs[i]
    return weighted_sum
  
  def activation(self, weighted_sum):
    if weighted_sum >= 0:
      return 1
    if weighted_sum < 0:
      return -1
    
  def training(self, training_set):
    foundLine = False
    while not foundLine:
      total_error = 0
      for inputs in training_set:
        prediction = self.activation(self.weighted_sum(inputs))
        actual = training_set[inputs]
        error = actual - prediction
        total_error += abs(error)
        for i in range(self.num_inputs):
          self.weights[i] += error*inputs[i]
      if total_error == 0:
        foundLine = True
      
cool_perceptron = Perceptron()
'''
PERCEPTRON
Representing a Line
So far so good! The perceptron works as expected, but everything seems to be taking place behind the scenes. What if we could visualize the perceptron’s training process to gain a better understanding of what’s going on?

The weights change throughout the training process so if only we could meaningfully visualize those weights …

Turns out we can! In fact, it gets better. The weights can actually be used to represent a line! This greatly simplifies our visualization.

You might know that a line can be represented using the slope-intercept form. A perceptron’s weights can be used to find the slope and intercept of the line that the perceptron represents.

slope = -self.weights[0]/self.weights[1]
intercept = -self.weights[2]/self.weights[1]
The explanation for these equations is beyond the scope of this lesson, so we’ll just use them to visualize the perceptron for now.

In the plot on your right, you should be able to see a line that represents the perceptron in its first iteration of the training process.'''

#------------------training_set.py-------------------------------
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import random
from perceptron import Perceptron, lines

def generate_training_set(num_points):
	x_coordinates = [random.randint(0, 50) for i in range(num_points)]
	y_coordinates = [random.randint(0, 50) for i in range(num_points)]
	training_set = dict()
	for x, y in zip(x_coordinates, y_coordinates):
		if x <= 45-y:
			training_set[(x,y,1)] = 1
		elif x > 45-y:
			training_set[(x,y,1)] = -1
	return training_set

training_set = generate_training_set(30)

x_plus = []
y_plus = []
x_minus = []
y_minus = []

for data in training_set:
	if training_set[data] == 1:
		x_plus.append(data[0])
		y_plus.append(data[1])
	elif training_set[data] == -1:
		x_minus.append(data[0])
		y_minus.append(data[1])

perceptron = Perceptron()
perceptron.training(training_set)

fig = plt.figure()
ax = plt.axes(xlim=(-25, 75), ylim=(-25, 75))
line, = ax.plot([], [], lw=2)

fig.patch.set_facecolor('#ffc107')

plt.scatter(x_plus, y_plus, marker = '+', c = 'green', s = 128, linewidth = 2)
plt.scatter(x_minus, y_minus, marker = '_', c = 'red', s = 128, linewidth = 2)

plt.title('Iteration: 0')


def animate(i):
    print(i)
    line.set_xdata(lines[i][0])  # update the data
    line.set_ydata(lines[i][1])  # update the data
    return line,

def init():
    line.set_data([], [])
    return line,

ani = animation.FuncAnimation(fig, animate, frames=1, init_func=init, interval=50, blit=True, repeat=False)

plt.show()

#--------------------------perceptron.py-------------------------------
lines = []

class Perceptron:
  def __init__(self, num_inputs=3, weights=[1,1,1]):
    self.num_inputs = num_inputs
    self.weights = weights

  def weighted_sum(self, inputs):
    weighted_sum = 0
    for i in range(self.num_inputs):
      weighted_sum += self.weights[i]*inputs[i]
    return weighted_sum

  def activation(self, weighted_sum):
    if weighted_sum >= 0:
      return 1
    if weighted_sum < 0:
      return -1

  def training(self, training_set):
    foundLine = False
    while not foundLine:
      total_error = 0
      for inputs in training_set:
        prediction = self.activation(self.weighted_sum(inputs))
        actual = training_set[inputs]
        error = actual - prediction
        total_error += abs(error)
        for i in range(self.num_inputs):
          self.weights[i] += error*inputs[i]

      slope = -self.weights[0]/self.weights[1]
      intercept = -self.weights[2]/self.weights[1]
      y1 = (slope * 0) + intercept
      y2 = (slope * 50) + intercept
      lines.append([[0,50], [y1, y2]])

      if total_error == 0:
        foundLine = True
'''
PERCEPTRON
Finding a Linear Classifier
Let’s recap what you just learned!

The perceptron has inputs, weights, and an output. The weights are parameters that define the perceptron and they can be used to represent a line. In other words, the perceptron can be visualized as a line.

What does it mean for the perceptron to correctly classify every point in the training set?

Theoretically, it means that the perceptron predicted every label correctly.

Visually, it means that the perceptron found a linear classifier, or a decision boundary, that separates the two distinct set of points in the training set.

In the plot on the right, you should be able to see the linear classifier that was found by the perceptron in the last iteration of the training process.'''
'''
PERCEPTRON
What's Next? Neural Networks
Congratulations! You have now built your own perceptron from scratch.

Let’s step back and think about what you just accomplished and see if there are any limits to a single perceptron.

Earlier, the data points in the training set were linearly separable i.e. a single line could easily separate the two dissimilar sets of points.

What would happen if the data points were scattered in such a way that a line could no longer classify the points? A single perceptron with only two inputs wouldn’t work for such a scenario because it cannot represent a non-linear decision boundary.

That’s when more perceptrons and features come into play!

By increasing the number of features and perceptrons, we can give rise to the Multilayer Perceptrons, also known as Neural Networks, which can solve much more complicated problems.

With a solid understanding of perceptrons, you are now ready to dive into the incredible world of Neural Networks!
'''




























#--------------------------------------- SVM SUPPORT VECTOR MACHINE ------------------------------------------------------#
'''
SUPPORT VECTOR MACHINES
Support Vector Machines
A Support Vector Machine (SVM) is a powerful supervised machine learning model used for classification. An SVM makes classifications by defining a decision boundary and then seeing what side of the boundary an unclassified point falls on. In the next few exercises, we’ll learn how these decision boundaries get defined, but for now, know that they’re defined by using a training set of classified points. That’s why SVMs are supervised machine learning models.

Decision boundaries are easiest to wrap your head around when the data has two features. In this case, the decision boundary is a line. Take a look at the example below.

Two clusters of points separated by a line
Note that if the labels on the figures in this lesson are too small to read, you can resize this pane to increase the size of the images.

This SVM is using data about fictional games of Quidditch from the Harry Potter universe! The classifier is trying to predict whether a team will make the playoffs or not. Every point in the training set represents a “historical” Quidditch team. Each point has two features — the average number of goals the team scores and the average number of minutes it takes the team to catch the Golden Snitch.

After finding a decision boundary using the training set, you could give the SVM an unlabeled data point, and it will predict whether or not that team will make the playoffs.

Decision boundaries exist even when your data has more than two features. If there are three features, the decision boundary is now a plane rather than a line.

Two clusters of points in three dimensions separated by a plane.
As the number of dimensions grows past 3, it becomes very difficult to visualize these points in space. Nonetheless, SVMs can still find a decision boundary. However, rather than being a separating line, or a separating plane, the decision boundary is called a separating hyperplane.

Instructions
1.
Run the code to see two graphs appear. Right now they should be identical. We’re going to fix the bottom graph so it has a good decision boundary. Why is this decision boundary bad?


The decision boundary doesn’t separate the two classes from each other!

2.
Let’s shift the line on the bottom graph to make it separate the two clusters. The slope of the line looks pretty good, so let’s keep that at -2.

We want to move the boundary up, so change intercept_two so the line separates the two clusters.


intercept_two = 15 works pretty well!'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from graph import ax, x_1, y_1, x_2, y_2

#Top graph intercept and slope
intercept_one = 8
slope_one = -2

x_vals = np.array(ax.get_xlim())
y_vals = intercept_one + slope_one * x_vals
plt.plot(x_vals, y_vals, '-')

#Bottom Graph
ax = plt.subplot(2, 1, 2)
plt.title('Good Decision Boundary')
ax.set_xlim(0, 10)
ax.set_ylim(0, 10)

plt.scatter(x_1, y_1, color = "b")
plt.scatter(x_2, y_2, color = "r")

#Change the intercept to separate the clusters
intercept_two = 14
slope_two = -2

x_vals = np.array(ax.get_xlim())
y_vals = intercept_two + slope_two * x_vals
plt.plot(x_vals, y_vals, '-')

plt.tight_layout()
plt.show()

'''
SUPPORT VECTOR MACHINES
Optimal Decision Boundaries
One problem that SVMs need to solve is figuring out what decision boundary to use. After all, there could be an infinite number of decision boundaries that correctly separate the two classes. Take a look at the image below:

6 different valid decision boundaries
There are so many valid decision boundaries, but which one is best? In general, we want our decision boundary to be as far away from training points as possible.

Maximizing the distance between the decision boundary and points in each class will decrease the chance of false classification. Take graph C for example.

An SVM with a decision boundary very close to the blue points.
The decision boundary is close to the blue class, so it is possible that a new point close to the blue cluster would fall on the red side of the line.

Out of all the graphs shown here, graph F has the best decision boundary.

Instructions
1.
Run the code. Both graphs have suboptimal decision boundaries. Why? We’re going to fix the bottom graph.


2.
We’re going to have to make the decision boundary much flatter, which means we first need to lower its y-intercept. Change intercept_two to be 8.

3.
Next, we want the slope to be pretty flat. Change the value of slope_two. The resulting line should split the two clusters.


slope_two = -0.5 works well!
'''
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from graph import ax, x_1, y_1, x_2, y_2

#Top graph intercept and slope
intercept_one = 98
slope_one = -20

x_vals = np.array(ax.get_xlim())
y_vals = intercept_one + slope_one * x_vals
plt.plot(x_vals, y_vals, '-')

#Bottom graph
ax = plt.subplot(2, 1, 2)
plt.title('Good Decision Boundary')
ax.set_xlim(0, 10)
ax.set_ylim(0, 10)

plt.scatter(x_1, y_1, color = "b")
plt.scatter(x_2, y_2, color = "r")

#Bottom graph intercept and slope
intercept_two = 8
slope_two = -0.5

x_vals = np.array(ax.get_xlim())
y_vals = intercept_two + slope_two * x_vals
plt.plot(x_vals, y_vals, '-')

plt.tight_layout()
plt.show()


'''
SUPPORT VECTOR MACHINES
Support Vectors and Margins
We now know that we want our decision boundary to be as far away from our training points as possible. Let’s introduce some new terms that can help explain this idea.

The support vectors are the points in the training set closest to the decision boundary. In fact, these vectors are what define the decision boundary. But why are they called vectors? Instead of thinking about the training data as points, we can think of them as vectors coming from the origin.

Points represented as vectors.
These vectors are crucial in defining the decision boundary — that’s where the “support” comes from. If you are using n features, there are at least n+1 support vectors.

The distance between a support vector and the decision boundary is called the margin. We want to make the margin as large as possible. The support vectors are highlighted in the image below:

decision boundary with margin highlighted
Because the support vectors are so critical in defining the decision boundary, many of the other training points can be ignored. This is one of the advantages of SVMs. Many supervised machine learning algorithms use every training point in order to make a prediction, even though many of those training points aren’t relevant. SVMs are fast because they only use the support vectors!'''

'''
SUPPORT VECTOR MACHINES
scikit-learn
Now that we know the concepts behind SVMs we need to write the code that will find the decision boundary that maximizes the margin. All of the code that we’ve written so far has been guessing and checking — we don’t actually know if we’ve found the best line. Unfortunately, calculating the parameters of the best decision boundary is a fairly complex optimization problem. Luckily, Python’s scikit-learn library has implemented an SVM that will do this for us.

Note that while it is not important to understand how the optimal parameters are found, you should have a strong conceptual understanding of what the model is optimizing.

To use scikit-learn’s SVM we first need to create an SVC object. It is called an SVC because scikit-learn is calling the model a Support Vector Classifier rather than a Support Vector Machine.
'''
classifier = SVC(kernel = 'linear')
'''
We’ll soon go into what the kernel parameter is doing, but for now, let’s use a 'linear' kernel.

Next, the model needs to be trained on a list of data points and a list of labels associated with those data points. The labels are analogous to the color of the point — you can think of a 1 as a red point and a 0 as a blue point. The training is done using the .fit() method:
'''
training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
labels = [1, 1, 1, 0, 0, 0]
classifier.fit(training_points, labels) 
'''
The graph of this dataset would look like this:

An SVM with a decision boundary very close to the blue points.
Calling .fit() creates the line between the points.

Finally, the classifier predicts the label of new points using the .predict() method. The .predict() method takes a list of points you want to classify. Even if you only want to classify one point, make sure it is in a list:
'''
print(classifier.predict([[3, 2]]))
'''
In the image below, you can see the unclassified point [3, 2] as a black dot. It falls on the red side of the line, so the SVM would predict it is red.

An SVM with a decision boundary very close to the blue points.
In addition to using the SVM to make predictions, you can inspect some of its attributes. For example, if you can print classifier.support_vectors_ to see which points from the training set are the support vectors.

In this case, the support vectors look like this:
'''
[[7, 5],
 [8, 2],
 [2, 2]]
'''
 Instructions
1.
Let’s start by making a SVC object with kernel = 'linear'. Name the object classifier.


classifier = SVC(kernel = 'linear')
2.
We’ve imported the training set and labels for you. Call classifier‘s .fit() method using points and labels as parameters.


classifier.fit(points, ____)
Fill in the second parameter in the code above.

3.
We can now classify new points. Try classifying both [3, 4] and [6, 7]. Remember, the .predict() function expects a list of points to predict.

Print the results.


Use [[3, 4], [6, 7]] as the parameter of .predict().'''


from sklearn.svm import SVC
from graph import points, labels

classifier = SVC (kernel = 'linear')

classifier.fit(points, labels)

print(classifier.predict([[3,4],[6,7]]))

'''
SUPPORT VECTOR MACHINES
Outliers
SVMs try to maximize the size of the margin while still correctly separating the points of each class. As a result, outliers can be a problem. Consider the image below.

One graph with a hard margin and one graph with a soft margin
The size of the margin decreases when a single outlier is present, and as a result, the decision boundary changes as well. However, if we allowed the decision boundary to have some error, we could still use the original line.

SVMs have a parameter C that determines how much error the SVM will allow for. If C is large, then the SVM has a hard margin — it won’t allow for many misclassifications, and as a result, the margin could be fairly small. If C is too large, the model runs the risk of overfitting. It relies too heavily on the training data, including the outliers.

On the other hand, if C is small, the SVM has a soft margin. Some points might fall on the wrong side of the line, but the margin will be large. This is resistant to outliers, but if C gets too small, you run the risk of underfitting. The SVM will allow for so much error that the training data won’t be represented.

When using scikit-learn’s SVM, you can set the value of C when you create the object:
'''
classifier = SVC(C = 0.01)

'''
The optimal value of C will depend on your data. Don’t always maximize margin size at the expense of error. Don’t always minimize error at the expense of margin size. The best strategy is to validate your model by testing many different values for C.

Instructions
1.
Run the code to see the SVM’s current boundary line. Note that we’ve imported some helper functions we wrote named draw_points and draw_margins to help visualize the SVM.

2.
Let’s add an outlier! Before calling .fit(), append [3, 3] to points and append 0 to labels. This will add a blue point at [3, 3]


3.
Right now, our classifier has hard margins because C = 1. Change the value of C to 0.01 to see what the SVM looks like with soft margins.


When you create classifier, change the value of C to 0.01.

4.
append at least two more points to points. If you want the points to appear on the graph, make sure their x and y values are between 0 and 12.

Make sure to also append a label to labels for every point you add. A 0 will make the point blue and a 1 will make the point red.

Make sure to add the points before training the SVM.


If you wanted to add a red point at [10, 8], do the following:

points.append([10,8])
labels.append(1)
5.
Play around with the C variable to see how the decision boundary changes with your new points added. Change C to be a value between 0.01 and 1.'''


import codecademylib3_seaborn
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from graph import points, labels, draw_points, draw_margin

classifier = SVC(kernel='linear', C = 1)
points.append([8,4])
points.append([2,8])
points.append([8,8])
labels.append(1)
labels.append(0)
labels.append(1)
classifier.fit(points, labels)

draw_points(points, labels)
draw_margin(classifier)

plt.show()

'''
SUPPORT VECTOR MACHINES
Kernels
Up to this point, we have been using data sets that are linearly separable. This means that it’s possible to draw a straight decision boundary between the two classes. However, what would happen if an SVM came along a dataset that wasn’t linearly separable?

data points clustered in concentric circles
It’s impossible to draw a straight line to separate the red points from the blue points!

Luckily, SVMs have a way of handling these data sets. Remember when we set kernel = 'linear' when creating our SVM? Kernels are the key to creating a decision boundary between data points that are not linearly separable.

Note that most machine learning models should allow for some error. For example, the image below shows data that isn’t linearly separable. However, it is not linearly separable due to a few outliers. We can still draw a straight line that, for the most part, separates the two classes. You shouldn’t need to create a non-linear decision boundary just to fit some outliers. Drawing a line that correctly separates every point would be drastically overfitting the model to the data.

A straight line separating red and blue clusters with some outliers.

Instructions
1.
Let’s take a look at the power of kernels. We’ve created a dataset that isn’t linearly separable and split it into a training set and a validation set.

Create an SVC named classifier with a 'linear' kernel.


2.
Call the .fit() method using training_data and training_labels as parameters.


classifier.fit(___, ___)
3.
Let’s see how accurate our classifier is using a linear kernel.

Call classifier‘s .score() function using validation_data and validation_labels as parameters. Print the results.

This will print the average accuracy of the model.


classifier.score(___, ___)
4.
That’s pretty bad! The classifier is getting it right less than 50% of the time! Change 'linear' to 'poly' and add the parameter degree = 2. Run the program again and see what happens to the score.


Wow! It’s now getting every single point in the validation set correct!

Let’s go learn what kernels are really doing!'''

import codecademylib3_seaborn
from sklearn.svm import SVC
from graph import points, labels
from sklearn.model_selection import train_test_split

training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

classifier = SVC(kernel = 'linear')
#cclassifier = SVC(kernel = 'poly', degree = 2)

classifier.fit(training_data, training_labels)

print(classifier.score(validation_data, validation_labels))

'''
SUPPORT VECTOR MACHINES
Polynomial Kernel
That kernel seems pretty magical. It is able to correctly classify every point! Let’s take a deeper look at what it was really doing.

We start with a group of non-linearly separable points that looked like this:

A circle of red dots surrounding a cluster of blue dots.
The kernel transforms the data in a clever way to make it linearly separable. We used a polynomial kernel which transforms every point in the following way:

(x,\ y) \rightarrow (\sqrt{2}\cdot x \cdot y,\ x^2,\ y^2)(x, y)→( 
2
​	 ⋅x⋅y, x 
2
 , y 
2
 )
The kernel has added a new dimension to each point! For example, the kernel transforms the point [1, 2] like this:

(1,\ 2) \rightarrow (2\sqrt{2},\ 1,\ 4)(1, 2)→(2 
2
​	 , 1, 4)
If we plot these new three dimensional points, we get the following graph:

A cluster of red points and blue points in three dimensions separated by a plane.
Look at that! All of the blue points have scooted away from the red ones. By projecting the data into a higher dimension, the two classes are now linearly separable by a plane. We could visualize what this plane would look like in two dimensions to get the following decision boundary.

The decision boundary is a circle around the inner points.
Instructions
1.
In this exercise, we will be using a non-linearly separable dataset similar to the concentric circles above.

Rather than using a polynomial kernel, we’re going to stick with a linear kernel and do the transformation ourselves. The SVM running a linear kernel on the transformed points should perform identically to the SVM running a polynomial kernel on the original points.

To begin, at the bottom of your code, print training_data[0] to see the first data point. You will also see the accuracy of the SVM when the data is not projected into 3 dimensions.


The SVM is pretty bad! Because it is using a linear kernel, it is trying to draw a straight decision boundary.

2.
Let’s transform the data into three dimensions! Begin by creating two empty lists called new_training and new_validation.


3.
Loop through every point in training_data. For every point, append a list to new_training. The list should contain three numbers:

The square root of 2 times point[0] times point[1].
point[0] squared.
point[1] squared.
Remember, to square a number in Python do number ** 2. To take the square root, do number ** 0.5.


4.
Do the same for every point in validation_data. For every point in validation_data, add the new list to new_validation.


5.
Retrain classifier by calling the .fit() method using new_training and training_labels as parameters.


6.
Finally, run classifier‘s .score() method using new_validation and validation_labels as parameters. Print the results. How did the SVM do when the data was projected to three dimensions?'''

from sklearn.datasets import make_circles
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

#Makes concentric circles
points, labels = make_circles(n_samples=300, factor=.2, noise=.05, random_state = 1)

#Makes training set and validation set.
training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

classifier = SVC(kernel = "linear", random_state = 1)
classifier.fit(training_data, training_labels)
print(classifier.score(validation_data, validation_labels))

print(training_data[0])

new_training = []
new_validation = []

for point in training_data:  
  new_training.append([(2**0.5)*point[0]*point[1], point[0]**2, point[1]**2 ])
for point in validation_data:  
  new_validation.append([(2**0.5)*point[0]*point[1], point[0]**2, point[1]**2 ])
  
print(new_training[0])

classifier.fit(new_training, training_labels)
print(classifier.score(new_validation, validation_labels))

'''
SUPPORT VECTOR MACHINES
Radial Bias Function Kernel
The most commonly used kernel in SVMs is a radial basis function (rbf) kernel. This is the default kernel used in scikit-learn’s SVC object. If you don’t specifically set the kernel to "linear", "poly" the SVC object will use an rbf kernel. If you want to be explicit, you can set kernel = "rbf", although that is redundant.

It is very tricky to visualize how an rbf kernel “transforms” the data. The polynomial kernel we used transformed two-dimensional points into three-dimensional points. An rbf kernel transforms two-dimensional points into points with an infinite number of dimensions!

We won’t get into how the kernel does this — it involves some fairly complicated linear algebra. However, it is important to know about the rbf kernel’s gamma parameter.

classifier = SVC(kernel = "rbf", gamma = 0.5, C = 2)
gamma is similar to the C parameter. You can essentially tune the model to be more or less sensitive to the training data. A higher gamma, say 100, will put more importance on the training data and could result in overfitting. Conversely, A lower gamma like 0.01 makes the points in the training data less relevant and can result in underfitting.

Instructions
1.
We’re going to be using a rbf kernel to draw a decision boundary for the following points:

A cluster of blue points in the middle surrounded by red points.
We’ve imported the data for you and split it into training_data, validation_data, training_labels, and validation_labels.

Begin by creating an SVC named classifier with an "rbf" kernel. Set the kernel’s gamma equal to 1.


The following code would create an SVC with gamma = 100. Your gamma should be 1.

classifier = SVC(kernel = "rbf", gamma = 1)
2.
Next, train the model using the .fit() method using training_data and training_labels as parameters.


classifier.fit(___, ___)
3.
Let’s test the classifier’s accuracy when its gamma is 1. Print the result of the .score() function using validation_data and validation_labels as parameters.


The decision boundary when gamma = 1 looks like this:

The decision boundary fits the blue points nicely.

4.
Let’s see what happens if we increase gamma. Change gamma to 10. What happens to the accuracy of our model?


The decision boundary when gamma = 10 looks like this:

The decision boundary fits to blue points too closely.

5.
The accuracy went down. We overfit our model. Change gamma to 0.1. What happens to the accuracy of our model this time?


Now we’re underfitting. The decision boundary looks like this:'''

#-------script.py---------------
from data import points, labels
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

classifier = SVC(kernel = 'rbf', gamma = 0.4)
classifier.fit(training_data, training_labels)
print(classifier.score(validation_data, validation_labels))

#-------data.py---------------
from random import uniform, seed
import numpy as np

seed(1)

blue_x = []
blue_y = []

red_x = []
red_y = []

#First Blue Column
for i in range(50):
    blue_x.append(uniform(2, 4))
    blue_y.append(uniform(0, 8))

#Horizontal Blue
for i in range(25):
    blue_x.append(uniform(4, 8))
    blue_y.append(uniform(5, 6))

#Left Red
for i in range(30):
    red_x.append(uniform(0,1.9))
    red_y.append(uniform(0, 10))

#Red above blue column
for i in range(15):
    red_x.append(uniform(2, 4))
    red_y.append(uniform(8.1, 10))

#Red below blue horizontal
for i in range(25):
    red_x.append(uniform(4.1,10))
    red_y.append(uniform(0, 4.9))

#Red above blue horizontal
for i in range(25):
    red_x.append(uniform(4.1,10))
    red_y.append(uniform(6.1, 10))

#Smaller blue column
for i in range(10):
    blue_x.append(uniform(6.3, 6.8))
    blue_y.append(uniform(0, 6))


all_x = blue_x + red_x
all_y = blue_y + red_y

points = np.array(list(zip(all_x, all_y)))

labels = np.array([0] * len(blue_x) + [1] * len(red_x))

'''
SUPPORT VECTOR MACHINES
Review
Great work! Here are some of the major takeaways from this lesson on SVMs:

SVMs are supervised machine learning models used for classification.
An SVM uses support vectors to define a decision boundary. Classifications are made by comparing unlabeled points to that decision boundary.
Support vectors are the points of each class closest to the decision boundary. The distance between the support vectors and the decision boundary is called the margin.
SVMs attempt to create the largest margin possible while staying within an acceptable amount of error.
The C parameter controls how much error is allowed. A large C allows for little error and creates a hard margin. A small C allows for more error and creates a soft margin.
SVMs use kernels to classify points that aren’t linearly separable.
Kernels transform points into higher dimensional space. A polynomial kernel transforms points into three dimensions while an rbf kernel transforms points into infinite dimensions.
An rbf kernel has a gamma parameter. If gamma is large, the training data is more relevant, and as a result overfitting can occur.'''

#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- SVC PROJECT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
'''
MACHINE LEARNING: SUPERVISED LEARNING 🤖
Sports Vector Machine
Support Vector Machines are powerful machine learning models that can make complex decision boundaries. An SVM’s decision boundary can twist and curve to accommodate the training data.

In this project, we will use an SVM trained using a baseball dataset to find the decision boundary of the strike zone.

A batter standing in front of the plate with the strike zone outlined.
The strike zone can be thought of as a decision boundary that determines whether or not a pitch is a strike or a ball. There is a strict definition of the strike zone: however, in practice, it will vary depending on the umpire or the player at bat.

Let’s use our knowledge of SVMs to find the real strike zone of several baseball players.

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
17/17Complete
Mark the tasks as complete by checking them off
Create the labels
1.
We’ve imported several DataFrames related to some of baseball’s biggest stars. We have data on Aaron Judge and Jose Altuve. Judge is one of the tallest players in the league and Altuve is one of the shortest. Their strike zones should be pretty different!

Each row in these DataFrames corresponds to a single pitch that the batter saw in the 2017 season. To begin, let’s take a look at all of the features of a pitch. Print aaron_judge.columns.

In this project, we’ll ask you to print out a lot of information. To avoid clutter, feel free to delete the print statements once you understand the data.

2.
Some of these features have obscure names. Let’s learn what the feature description means.

Print aaron_judge.description.unique() to see the different values the description feature could have.

3.
We’re interested in looking at whether a pitch was a ball or a strike. That information is stored in the type feature. Look at the unique values stored in the type feature to get a sense of how balls and strikes are recorded.

'''
print(aaron_judge.type.unique())
'''
You should see that every pitch is either a 'S', a 'B', or an 'X'.

4.
Great! We know every row’s type feature is either an 'S' for a strike, a 'B' for a ball, or an 'X' for neither (for example, an 'X' could be a hit or an out).

We’ll want to use this feature as the label of our data points. However, instead of using strings, it will be easier if we change every 'S' to a 1 and every 'B' to a 0.

You can change the values of a DataFrame column using the map() functions. For example, in the code below, every 'A' in example_column is changed to a 1, and every 'B' is changed to a 2.
'''
df['example_column'] = df['example_column'].map({'A':1, 'B':2})
'''
Finish the following code:
'''
aaron_judge['type'] = aaron_judge['type'].map({'S': ____, 'B': ____})
'''
5.
Let’s make sure that worked. Print the type column from the aaron_judge DataFrame.

'''
print(aaron_judge[____])
'''
Plotting the pitches
6.
There were some NaNs in there. We’ll take care of those in a second. For now, let’s look at the other features we’re interested in.

We want to predict whether a pitch is a ball or a strike based on its location over the plate. You can find the ball’s location in the columns plate_x and plate_z.

Print aaron_judge['plate_x'] to see what that column looks like.

plate_x measures how far left or right the pitch is from the center of home plate. If plate_x = 0, that means the pitch was directly in the middle of the home plate.

7.
We now have the three columns we want to work with: 'plate_x', 'plate_z', and 'type'.

Let’s remove every row that has a NaN in any of those columns.

You can do this by calling the dropna function. This function can take a parameter named subset which should be a list of the columns you’re interested in.

For example, the following code drops all of the NaN values from the columns 'A', 'B', and 'C'.
'''
data_frame = data_frame.dropna(subset = ['A', 'B', 'C'])
'''
Fill in the names of the columns that you don’t want NaN values in:
'''
aaron_judge = aaron_judge.dropna(subset = [____, ____, ____])
'''
8.
We now have points to plot using Matplotlib. Call plt.scatter() using five parameters:

The parameter x should be the plate_x column.
The parameter y should be the plate_z column.
To color the points correctly, the parameter c should be the type column.
To make the strikes red and the balls blue, set the cmap parameter to plt.cm.coolwarm.
To make the points slightly transparent, set the alpha parameter to 0.25.
Call plt.show() to see your graph.

plate_z measures how high off the ground the pitch was. If plate_z = 0, that means the pitch was at ground level when it got to the home plate.

'''
plt.scatter(x = aaron_judge['plate_x'], y = ____, c = ____, cmap = plt.cm.coolwarm, alpha = 0.5)
'''
Building the SVM
9.
Now that we’ve seen the location of every pitch, let’s create an SVM to create a decision boundary. This decision boundary will be the real strike zone for that player. For this section, make sure to write all of your code below the call to the scatter function but above the show function.

To begin, we want to validate our model, so we need to split the data into a training set and a validation set.

Call the train_test_split function using aaron_judge as a parameter.

Set the parameter random_state equal to 1 to ensure your data is split in the same way as our solution code.

This function returns two objects. Store the return values in variables named training_set and validation_set.


Finish the code block below:
'''
training_set, ____ = train_test_split(____, random_state = 1)
'''
10.
Next, create an SVC named classifier with kernel = 'rbf'. For right now, don’t worry about setting the C or gamma parameters.


The SVC should have kernel = 'rbf'.

11.
Call classifier‘s .fit() method. This method should take two parameters:

The training data. This is the plate_x column and the plate_z column in training_set.
The labels. This is the type column in training_set.
The code below shows and example of selecting two columns from a DataFrame:
'''
two_columns = data_frame[['A', 'B']]
'''
The first parameter should be training_set[['plate_x', 'plate_z']].

The second parameter should be training_set['type'].

12.
To visualize the SVM, call the draw_boundary function. This is a function that we wrote ourselves - you won’t find it in scikit-learn.

This function takes two parameters:

The axes of your graph. For us, this is the ax variable that we defined at the top of your code.
The trained SVM. For us, this is classifier. Make sure you’ve called .fit() before trying to visualize the decision boundary.
Run your code to see the predicted strike zone!

Note that the decision boundary will be drawn based on the size of the current axes. So if you call draw_boundary before calling scatter function, you will only see the boundary as a small square.

To get around this, you could manually set the size of the axes by using something likeax.set_ylim(-2, 2) before calling draw_boundary.


Call the following line of code after training the model but before calling plt.show().

draw_boundary(ax, classifier)
Optimizing the SVM
13.
Nice work! We’re now able to see the strike zone. But we don’t know how accurate our classifier is yet. Let’s find its accuracy by calling the .score() method and printing the results.

.score() takes two parameters — the points in the validation set and the labels associated with those points.

These two parameters should be very similar to the parameters used in .fit().


Finish the line of code below:

print(classifier.score(validation_set[['plate_x', 'plate_z']], ______))
14.
Let’s change some of the SVM’s parameters to see if we can get better accuracy.

Set the parameters of the SVM to be gamma = 100 and C = 100.

This will overfit the data, but it will be a good place to start. Run the code to see the overfitted decision boundary. What’s the new accuracy?


When you create SVC, set gamma = 100 and C = 100.

15.
Try to find a configuration of gamma and C that greatly improves the accuracy. You may want to use nested for loops.

Loop through different values of gamma and C and print the accuracy using those parameters. Our best SVM had an accuracy of 83.41%. Can you beat ours?


We used gamma = 3 and C = 1 to get 83.41%.

Explore Other Players
16.
Finally, let’s see how different players’ strike zones change. Aaron Judge is the tallest player in the MLB. Jose Altuve is the shortest player. Instead of using the aaron_judge variable, use jose_altuve.

To make this easier, you might want to consider putting all of your code inside a function and using the dataset as a parameter.

We’ve also imported david_ortiz.

Note that the range of the axes will change for these players. To really compare the strike zones, you may want to force the axes to be the same.

Try putting ax.set_ylim(-2, 6) and ax.set_xlim(-3, 3) right before calling plt.show()

17.
See if you can make an SVM that is more accurate by using more features. Perhaps the location of the ball isn’t the only important feature!

You can see the columns available to you by printing aaron_judge.columns.

For example, try adding the strikes column to your SVM — the number of strikes the batter already has might have an impact on whether the next pitch is a strike or a ball.

Note that our draw_boundary function won’t work if you have more than two features. If you add more features, make sure to comment that out!

Try to make the best SVM possible and share your results with us!
'''

#------------------Script.py---------------
import codecademylib3_seaborn
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from svm_visualization import draw_boundary
from players import aaron_judge, jose_altuve, david_ortiz

fig, ax = plt.subplots()

#print(type(aaron_judge))
#print(aaron_judge.columns)
#print(aaron_judge.columns.unique())
#print(aaron_judge.type)
#print(aaron_judge.type.unique())

aaron_judge['type'] = aaron_judge['type'].map({'S':1, 'B':0})
#print(aaron_judge.type.unique())

#print(aaron_judge.plate_x)
#print(len(aaron_judge))
aaron_judge= aaron_judge.dropna(subset = ['type', 'plate_x', 'plate_z'])
#print(len(aaron_judge))

y = aaron_judge['type']
plt.scatter(x = aaron_judge['plate_x'], y = aaron_judge['plate_z'], c = y, alpha = 0.25, cmap = plt.cm.coolwarm )

training_set, validation_set = train_test_split(aaron_judge, random_state = 1)
#print(len(aaron_judge))
#print(len(training_set))

classifier = SVC(kernel = 'rbf', gamma = 3, C = 1 )
#training_data = [training_set['plate_x'],training_set['plate_z']]
training_data = training_set[['plate_x', 'plate_z']]
training_labels= training_set['type']

validation_data = validation_set[['plate_x', 'plate_x']]
validation_labels = validation_set['type']

#print(training_data)
#print(training_labels.unique())#
#print(training_data['plate_z'].max())
#print(training_data['plate_x'].unique())
classifier.fit(training_data, training_labels)

ax.set_ylim(-2,6)
ax.set_xlim(-3, 3)
draw_boundary(ax, classifier)
plt.show()

#find optimized gamma and C
'''
for j in range(1,100, 10):
  for i in range(10):
    gamma = j * 0.01 + 0.001
    C = i * 0.09 + 0.01
    classifier = SVC(kernel = 'rbf', gamma = gamma, C = C )
    classifier.fit(training_data, training_labels)
    print('current gamma = {}, C = {}'.format(gamma,C))
    print(classifier.score(validation_data, validation_labels))
'''

#---------------player.py---------------
import pickle

aaron_judge = pickle.load( open( "aaron_judge.p", "rb" ) )
jose_altuve = pickle.load( open( "jose_altuve.p", "rb" ) )
david_ortiz = pickle.load( open( "david_ortiz.p", "rb" ) )

#---------------svm_visualization.py---------------
import numpy as np
import matplotlib.pyplot as plt


def make_meshgrid(ax, h=.02):
    # x_min, x_max = x.min() - 1, x.max() + 1
    # y_min, y_max = y.min() - 1, y.max() + 1
    x_min, x_max = ax.get_xlim()
    y_min, y_max = ax.get_ylim()

    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy


def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out


def draw_boundary(ax, clf):

    xx, yy = make_meshgrid(ax)
    return plot_contours(ax, clf, xx, yy,cmap=plt.cm.coolwarm, alpha=0.5)


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- BAYES' THEOREM #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-	
'''
BAYES' THEOREM
Introduction to Bayes' Theorem
In this lesson, we’ll learn about Bayes’ Theorem. Bayes’ Theorem is the basis of a branch of statistics called Bayesian Statistics, where we take prior knowledge into account before calculating new probabilities.

This allows us to find narrow solutions from a huge universe of possibilities. British mathematician Alan Turing used it to crack the German Enigma code during WWII. And now it is used in:

Machine Learning
Statistical Modeling
A/B Testing
Robotics
By the end of this lesson, you’ll be able to solve simple problems involving prior knowledge.'''

'''
BAYES' THEOREM
Independent Events
The ability to determine whether two events are independent is an important skill for statistics.

If two events are independent, then the occurrence of one event does not affect the probability of the other event. Here are some examples of independent events:

I wear a blue shirt; my coworker wears a blue shirt
I take the subway to work; I eat sushi for lunch
The NY Giants win their football game; the NY Rangers win their hockey game
If two events are dependent, then when one event occurs, the probability of the other event occurring changes in a predictable way.

Here are some examples of dependent events:

It rains on Tuesday; I carry an umbrella on Tuesday
I eat spaghetti; I have a red stain on my shirt
I wear sunglasses; I go to the beach'''

'''
A certain family plans to have three children. Is the event that the couple’s third child is a girl independent of the event that the couple’s first two children are girls?

Save your answer ('independent' or 'not independent') to the variable third_child.


The third child is a girl is independent of the event that the first two children are girls. So:'''

third_child = 'independent'

'''
BAYES' THEOREM
Conditional Probability
Conditional probability is the probability that two events happen. It’s easiest to calculate conditional probability when the two events are independent.

Note: For the rest of this lesson, we’ll be using the statistical convention that the probability of an event is written as P(event).

If the probability of event A is P(A) and the probability of event B is P(B) and the two events are independent, then the probability of both events occurring is the product of the probabilities:

P(A ∩ B) = P(A) \times P(B)P(A∩B)=P(A)×P(B)
The symbol ∩ just means “and”, so P(A ∩ B) means the probability that both A and B happen.

For instance, suppose we are rolling a pair of dice, and want to know the probability of rolling two sixes.

BoxcarsMidnight

Each die has six sides, so the probability of rolling a six is 1/6. Each die is independent (i.e., rolling one six does not increase or decrease our chance of rolling a second six), so:

P(6 \cap 6) = P(6) \times P(6) = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36}P(6∩6)=P(6)×P(6)= 
6
1
​	 × 
6
1
​	 = 
36
1



1.
This week, there is a 30% probability that it will rain on any given day. At a certain high school, gym class is held on three days out of the five day school week.

On a school day, what is the probability that it is raining and the students have gym class?

Save your answer to the variable p_rain_and_gym.'''

import numpy as np

p_rain = 0.3
p_gym = 3.0/5.0

p_rain_and_gym = p_rain * p_gym

'''
BAYES' THEOREM
Testing for a Rare Disease
Suppose you are a doctor and you need to test if a patient has a certain rare disease. The test is very accurate: it’s correct 99% of the time. The disease is very rare: only 1 in 100,000 patients have it.

You administer the test and it comes back positive, so your patient must have the disease, right?

Not necessarily. If we just consider the test, there is only a 1% chance that it is wrong, but we actually have more information: we know how rare the disease is.

Given that the test came back positive, there are two possibilities:

The patient had the disease, and the test correctly diagnosed the disease.
The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease.'''
'''

Instructions
1.
What is the probability that the patient had the disease and the test correctly diagnosed the disease?

Save your answer to the variable p_disease_and_correct.


The disease is rare, so the probability that the patient had the disease is 1 out of 100,000:

P(disease) = \frac{1}{100000}P(disease)= 
100000
1
​	 
The test is only wrong 1% of the time, so it is correct 99% of the time:

P(test\ is\ correct) = 0.99P(test is correct)=0.99
So the answer should look like:

p_disease_and_correct = (1.0 / 100000) * 0.99
2.
What is the probability that the patient does not have the disease and the test incorrectly diagnosed the disease?

Save your answer to the variable p_no_disease_and_incorrect.


The disease is rare, so the probability that the patient does not have the disease the disease is 99,999 out of 100,000:

P(disease) = \frac{99999}{100000}P(disease)= 
100000
99999
​	 
The test is only wrong 1% of the time:

P(test\ is\ correct) = 0.01P(test is correct)=0.01
So the answer should look like:

p_no_disease_and_incorrect = (99999.0 / 100000) * 0.01
3.
Print both p_disease_and_correct and p_no_disease_and_incorrect.


print p_disease_and_correct
print p_no_disease_and_incorrect
'''

import numpy as np

p_correct = 0.99
p_disease = 1.0/100000

p_disease_and_correct = p_correct * p_disease
print(p_disease_and_correct)

p_no_disease_and_incorrect = (1-p_disease) * (1-p_correct)
print(p_no_disease_and_incorrect)

9.9e-06
0.0099999

'''
BAYES' THEOREM
Bayes' Theorem
In the previous exercise, we determined two probabilities:

The patient had the disease, and the test correctly diagnosed the disease ≈ 0.00001
The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease ≈ 0.01
Both events are rare, but we can see that it was about 1,000 times more likely that the test was incorrect than that the patient had this rare disease.

We’re able to come to this conclusion because we had more information than just the accuracy of the test; we also knew the prevalence of this disease. That extra information about how we expect the world to work is called a prior.

When we only use the first piece of information (the result of the test), it’s called a Frequentist Approach to statistics. When we incorporate our prior, it’s called a Bayesian Approach.

In statistics, if we have two events (A and B), we write the probability that event A will happen, given that event B already happened as P(A|B). In our example, we want to find P(rare disease | positive result). In other words, we want to find the probability that the patient has the disease given the test came back positive.

We can calculate P(A|B) using Bayes’ Theorem, which states:

P(A|B) = P(B|A)*P(A)/ P(B)

​	 
So in this case, we’d say:

P(rare disease | positive result) = P(positive result | rare disease) * P(rare disease) / P(positive result)
​	 
It is important to note that on the right side of the equation, we have the term P(B|A). This is the probability that event B will happen given that event A has already happened. This is very different from P(A|B), which is the probability we are trying to solve for. The order matters!'''


'''
Instructions
1.
Calculate P(positive result | rare disease), or the probability of a positive test result, given that a patient really has this rare disease.

Save your answer (as a decimal) to p_positive_given_disease.


The test is 99% accurate; given the fact that the patient has the disease, we know that there is a 99% probability that the test will return a positive result.

This is exactly P(positive result | rare disease).

So the answer should look like:

p_positive_given_disease = 0.99
2.
What is P(rare disease), the probability that a randomly selected patient has the rare disease?

Save your answer to p_disease.


The disease is very rare. Only 1 in 100,000 people have it.

3.
We now need to compute the denominator; we need to find P(positive result).

As we discussed previously, there are two ways to get a positive result:

The patient had the disease, and the test correctly diagnosed the disease.
The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease.
Using these two probabilities, calculate the total probability that a randomly selected patient receives a positive test result, P(positive result).

Save your answer to the variable p_positive.


The probability that the patient had the disease, and the test correctly diagnosed the disease is:

1.0 / 100000.0 * 0.99
The probability that the patient didn’t have the disease and the test incorrectly diagnosed that they had the disease is:

99999.0 / 100000.0 * 0.01
The probability of either event A or event B happening is given by:

P(A\ or\ B) = P(A) + P(B)P(A or B)=P(A)+P(B)
4.
Substitute all three of these values into Bayes’ Theorem and calculate P(rare disease | positive result).

Save your result as p_disease_given_positive.


The numerator should be (p_positive_given_disease) * (p_disease).

The denominator should be p_positive.

5.
Print p_disease_given_positive.

Is it likely that your patient has this disease?


print p_disease_given_positive
The result should look something like:

0.000989010989011
Not very likely!'''

import numpy as np

p_positive_given_disease = 0.99

p_disease = 1.0/100000

#patient had Disease and correct
p1 = p_disease * 0.99

#patient doesn't have Disease and in-correct
p2 = (1-p_disease)*(1-0.99)

p_positive = p1 + p2

p_disease_given_positive = p_positive_given_disease *  p_disease / p_positive

print(p_disease_given_positive)

'''
BAYES' THEOREM
Spam Filters
Let’s explore a different example. Email spam filters use Bayes’ Theorem to determine if certain words indicate that an email is spam.

Let’s a take word that often appears in spam: “enhancement”.

With just 3 facts, we can make some preliminary steps towards a good spam filter:

“enhancement” appears in just 0.1% of non-spam emails
“enhancement” appears in 5% of spam emails
Spam emails make up about 20% of total emails
Given that an email contains “enhancement”, what is the probability that the email is spam?

Instructions
1.
In this example, we are dealing with two probabilities:

P(enhancement) - the probability that the word “enhancement” appears in an email.
P(spam) - the probability that an email is spam.
Using Bayes’ Theorem to answer our question means that we want to calculate P(A|B).

But what are A and B referring to in this case?

Save the string 'enhancement' or 'spam' to the variable a.

Save the string 'enhancement' or 'spam' to the variable b.


Recall that P(A|B) means “The probability of A given that B is true”.

The correct answer is:
'''
a = 'spam'
b = 'enhancement'

'''
2.
What is P(spam)?

Save your answer to p_spam.


We’ve given you this exact probability above. What percentage of total emails are spam?

3.
What is P(enhancement | spam)?

Save your answer to p_enhancement_given_spam.


If the email is spam, what is the probability that “enhancement” is in it?

4.
We want to know the overall probability that any email (spam or non-spam) contains “enhancement”.

Because we know the probability of “enhancement” occurring in both spam (0.05) and non-spam (0.001) emails, we can use a weighted average to calculate the probability of “enhancement” occurring in an email:

P(enhancement) = P(enhancement|spam) * P(spam) + P(enhacement| not spam) * P(not spam)

Save your answer to p_enhancement.


5.
Now that we know:

P(spam)
P(enhancement | spam)
P(enhancement)
We can plug this into Bayes’ Theorem:

P(A|B) = P(B|A) * P(A) / P(B)
​	 
Save your answer as p_spam_enhancement.


P(spam | enhancement) = P(enhancement | spam) * P(spam) / P(enhancement)
​	 
6.
Print p_spam_enhancement. This is the probability that an email is spam given that it contains the word “enhancement”.

Should we block all emails that contain “enhancement”?

How much non-spam email would we block?


print p_spam_enhancement
Woah, that’s ≈ 92.59%'''

import numpy as np

a = 'spam'

b = 'enhancement'

P_ench_non_spam = 0.001

p_ench_spam = 0.05

p_spam = 0.2

p_enhancement_given_spam = p_ench_spam

p_enhancement = P_ench_non_spam * (1 - p_spam)  + p_ench_spam * p_spam

p_spam_enhancement = p_enhancement_given_spam * p_spam / p_enhancement

print(p_spam_enhancement)

'''
BAYES' THEOREM
Review
In this course, we learned several new definitions:

Two events are independent if the occurrence of one event does not affect the probability of the second event
If two events are independent then:
P(A ∩ B) = P(A) \times P(B)P(A∩B)=P(A)×P(B)
A prior is an additional piece of information that tells us how likely an event is
A frequentist approach to statistics does not incorporate a prior
A Bayesian approach to statistics incorporates prior knowledge
Bayes’ Theorem is the following:
P(A|B) = P(B|A) * P(A) / P(B) '''


'''
MACHINE LEARNING: SUPERVISED LEARNING 🤖
Dr. Dirac's Statistics Midterm
Grading a multiple choice exam is easy. But how much do multiple choice exams tell us about what a student really knows? Dr. Dirac is administering a statistics midterm exam and wants to use Bayes’ Theorem to help him understand the following:

Given that a student answered a question correctly, what is the probability that she really knows the material?
Dr. Dirac knows the following probabilities based on many years of teaching:

There is a question on the exam that 60% of students know the correct answer to.
Given that a student knows the correct answer, there is still a 15% chance that the student picked the wrong answer.
Given that a student does not know the answer, there is still a 20% chance that the student picks the correct answer by guessing.
Using these probabilities, we can answer the question.

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
6/6Complete
Mark the tasks as complete by checking them off
Bayes' Theorem on a Statistics Exam
1.
In order to use Bayes Theorem, we need to phrase our question as P(A|B).

What is A and B in this case?


P(knows the material | answers correctly)

2.
What is the probability that the student knows the material?


P(knows the material) = 0.60

3.
Given that the student knows the material, what is the probability that she answers correctly?


P(answers correctly | knows material) = 1 - 0.15

4.
What is the probability of any student answering correctly?


The probability of answering correctly is equal to the weighted average of P(answers correctly | knows material) and P(answers correctly| does not know material).

You were given both of these probabilities (1 - 0.15 and 0.20, respectively).

How can you calculate the weights?

5.
Using the three probabilities and Bayes’ Theorem, calculate P(knows material | answers correctly).


Your final answer should be around 86%.'''


import numpy as np

p_know = 0.6

p_wrong_know = 0.15

p_correct_unknow = 0.2

'''
P(A|B)
A REALLY KNOW THE MATERIAL
B GIVEN ANSWER CORRECTLY
'''

p_correct_given_know = 1 - 0.15

'''
p_correct = P_know * P_correct_given_know + P_unknow * P_correct_unkonw
'''
p_correct = p_know * p_correct_given_know + (1-p_know) * p_correct_unknow

p_konw_given_correct = p_correct_given_know * p_know / p_correct

print(p_konw_given_correct)

'''
NAIVE BAYES CLASSIFIER
The Naive Bayes Classifier
A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes’ Theorem to make predictions and classifications. Recall Bayes’ Theorem:

P(A | B) = P(B | A) * P(A) / P(B)
​	 
This equation is finding the probability of A given B. This can be turned into a classifier if we replace B with a data point and A with a class. For example, let’s say we’re trying to classify an email as either spam or not spam. We could calculate P(spam | email) and P(not spam | email). Whichever probability is higher will be the classifier’s prediction. Naive Bayes classifiers are often used for text classification.

So why is this a supervised machine learning algorithm? In order to compute the probabilities used in Bayes’ theorem, we need previous data points. For example, in the spam example, we’ll need to compute P(spam). This can be found by looking at a tagged dataset of emails and finding the ratio of spam to non-spam emails.'''

'''
NAIVE BAYES CLASSIFIER
The Naive Bayes Classifier
A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes’ Theorem to make predictions and classifications. Recall Bayes’ Theorem:

P(A |  B) = P(B | A) * P(A) / P(B)

This equation is finding the probability of A given B. This can be turned into a classifier if we replace B with a data point and A with a class. For example, let’s say we’re trying to classify an email as either spam or not spam. We could calculate P(spam | email) and P(not spam | email). Whichever probability is higher will be the classifier’s prediction. Naive Bayes classifiers are often used for text classification.

So why is this a supervised machine learning algorithm? In order to compute the probabilities used in Bayes’ theorem, we need previous data points. For example, in the spam example, we’ll need to compute P(spam). This can be found by looking at a tagged dataset of emails and finding the ratio of spam to non-spam emails.'''

'''
NAIVE BAYES CLASSIFIER
Investigate the Data
In this lesson, we are going to create a Naive Bayes classifier that can predict whether a review for a product is positive or negative. This type of classifier could be extremely helpful for a company that is curious about the public reaction to a new product. Rather than reading thousands of reviews or tweets about the product, you could feed those documents into the Naive Bayes classifier and instantly find out how many are positive and how many are negative.

The dataset we will be using for this lesson contains Amazon product reviews for baby products. The original dataset contained many different features including the reviewer’s name, the date the review was made, and the overall score. We’ve removed many of those features; the only features that we’re interested in are the text of the review and whether the review was “positive” or “negative”. We labeled all reviews with a score less than 4 as a negative review.

Note that in the next two lessons, we’ve only imported a small percentage of the data to help the code run faster. We’ll import the full dataset later when we put everything together!

NAIVE BAYES CLASSIFIER
Bayes Theorem I
For the rest of this lesson, we’re going to write a classifier that can predict whether the review “This crib was amazing” is a positive or negative review. We want to compute both P(positive | review) and P(negative | review) and find which probability is larger. To do this, we’ll be using Bayes’ Theorem. Let’s look at Bayes’ Theorem for P(positive | review).

P(\text{positive}\ | \ \text{review}) = \frac{P(\text{review\ |\ positive}) \cdot P(\text{positive})}{P(\text{review})}P(positive ∣ review)= 
P(review)
P(review | positive)⋅P(positive)
​	 
The first part of Bayes’ Theorem that we are going to tackle is P(positive). This is the probability that any review is positive. To find this, we need to look at all of our reviews in our dataset - both positive and negative - and find the percentage of reviews that are positive.

We’ve bolded the part of Bayes’ Theorem we’re working on.

P(\text{positive}\ | \ \text{review}) = \frac{P(\text{review\ |\ positive}) \cdot \textbf{P(positive})}{P(\text{review})}P(positive ∣ review)= 
P(review)
P(review | positive)⋅P(positive)'''

'''
NAIVE BAYES CLASSIFIER
Bayes Theorem I
For the rest of this lesson, we’re going to write a classifier that can predict whether the review “This crib was amazing” is a positive or negative review. We want to compute both P(positive | review) and P(negative | review) and find which probability is larger. To do this, we’ll be using Bayes’ Theorem. Let’s look at Bayes’ Theorem for P(positive | review).

P(positive | review) = P(review | positive) * P(positive) / P(review) 

The first part of Bayes’ Theorem that we are going to tackle is P(positive). This is the probability that any review is positive. To find this, we need to look at all of our reviews in our dataset - both positive and negative - and find the percentage of reviews that are positive.

We’ve bolded the part of Bayes’ Theorem we’re working on.

P(\text{positive}\ | \ \text{review}) = \frac{P(\text{review\ |\ positive}) \cdot \textbf{P(positive})}{P(\text{review})}P(positive ∣ review)= 
P(review)
P(review | positive)⋅P(positive)'''

'''
NAIVE BAYES CLASSIFIER
Bayes Theorem II
Let’s continue to try to classify the review “This crib was amazing”.

The second part of Bayes’ Theorem is a bit more extensive. We now want to compute P(review | positive).

P( positive | review ) = P(review | positive) * P(positive) / P(review)
​	 
In other words, if we assume that the review is positive, what is the probability that the words “This”, “crib”, “was”, and “amazing” are the only words in the review?

To find this, we have to assume that each word is conditionally independent. This means that one word appearing doesn’t affect the probability of another word from showing up. This is a pretty big assumption!

We now have this equation. You can scroll to the right to see the full equation.

P("This crib was amazing" | positive) = P("This" | positive) * P("crib" |positive) * P("was" | positive) * P("amazing" | positive) 

Let’s break this down even further by looking at one of these terms. P("crib"|positive) is the probability that the word “crib” appears in a positive review. To find this, we need to count up the total number of times “crib” appeared in our dataset of positive reviews. If we take that number and divide it by the total number of words in our positive review dataset, we will end up with the probability of “crib” appearing in a positive review.

P("crib" | positive ) = # of "crib" in positive / # of words in positive

​	 
If we do this for every word in our review and multiply the results together, we have P(review | positive).

Instructions
1.
Let’s first find the total number of words in all positive reviews and store that number in a variable named total_pos.

To do this, we can use the built-in Python sum() function. sum() takes a list as a parameter. The list that you want to sum is the values of the dictionary pos_counter, which you can get by using pos_counter.values().

Do the same for total_neg.


Use pos_counter.values() as the only parameter of sum():

total_pos = sum(pos_counter.values())
2.
Create two variables named pos_probability and neg_probability. Each of these variables should start at 1. These are the variables we are going to use to keep track of the probabilities.


pos_probability = 1
neg_probability = 1
3.
Create a list of the words in review and store it in a variable named review_words. You can do this by using Python’s .split() function.

For example if the string test contained "Hello there", then test.split() would return ["Hello", "there"].


review_words = review.split()
4.
Loop through every word in review_words. Find the number of times word appears in pos_counter and neg_counter. Store those values in variables named word_in_pos and word_in_neg.

In the next steps, we’ll use this variable inside the for loop to do a series of multiplications.


The beginning of your for loop should look like this:

for word in review_words:
The number of times word appears in pos_counter can be found like this:

word_in_pos = pos_counter[word]
5.
Inside the for loop, set pos_probability to be pos_probability multiplied by word_in_pos / total_pos.

This step is finding each term to be multiplied together. For example, when word is "crib", you’re calculating the following:

P( "crib" | positive) = # of "crib" in positive / # of words in positive​	 

You can use *= to do the multiplication:

pos_probability *= word_in_pos / total_pos
Another way to do this is

pos_probability = pos_probability * word_in_pos / total_pos
6.
Do the same multiplication for neg_probability.

Outside the for loop, print both pos_probability and neg_probability. Those values are P(“This crib was amazing”|positive) and P(“This crib was amazing”|negative).


neg_probability should look very similar:

py neg_probability *= word_in_neg / total_neg

These should both be pretty small numbers. In fact, they will most likely be written in scientific notation. For example, 1.5e-5 is the same as 0.000015.'''

from reviews import neg_counter, pos_counter

review = "This crib was amazing"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

print(total_pos)
print(total_neg)

pos_probability = 1
neg_probability = 1 

review_words = review.split()

word_in_pos = []
word_in_neg = []


for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  pos_probability *= word_in_pos / total_pos
  neg_probability *= word_in_neg / total_neg
  
  
print(pos_probability)
print(pos_probability)
 
'''
NAIVE BAYES CLASSIFIER
Smoothing
In the last exercise, one of the probabilities that we computed was the following:

P("crib" | positive) = # of ``crib" in positive / # of words in positive

But what happens if “crib” was never in any of the positive reviews in our dataset? This fraction would then be 0, and since everything is multiplied together, the entire probability P(review | positive) would become 0.

This is especially problematic if there are typos in the review we are trying to classify. If the unclassified review has a typo in it, it is very unlikely that that same exact typo will be in the dataset, and the entire probability will be 0. To solve this problem, we will use a technique called smoothing.

In this case, we smooth by adding 1 to the numerator of each probability and N to the denominator of each probability. N is the number of unique words in our review dataset.

For example, P("crib" | positive) goes from this:

P("crib" | positive) = # of ``crib" in positive / # of words in positive
​	 
To this:

P("crib" | positive) = # of ``crib" in positive + 1 / # of words in positive + N
​	

Instructions
1.
Let’s demonstrate how these probabilities break if there’s a word that never appears in the given datasets.

Change review to "This cribb was amazing". Notice the second b in cribb.

2.
Inside your for loop, when you multiply pos_probability and neg_probability by a fraction, add 1 to the numerator.

Make sure to include parentheses around the numerator!


The fraction that you multiply pos_probability by should now look like this:

(word_in_pos + 1) / total_pos
3.
In the denominator of those fractions, add the number of unique words in the appropriate dataset.

For the positive probability, this should be the length of pos_counter which can be found using len().

Again, make sure to put parentheses around your denominator so the division happens after the addition!

Did smoothing fix the problem?


pos_probability should now be multiplied by:

(word_in_pos + 1) / (total_pos + len(pos_counter))
Make sure to do the same for neg_probability.'''

from reviews import neg_counter, pos_counter

review = "This cribb was amazing"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()

for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  
  pos_probability *= (word_in_pos + 1) / (total_pos +len(pos_counter))
  neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))
  
print(pos_probability)
print(neg_probability)

'''
NAIVE BAYES CLASSIFIER
Classify
If we look back to Bayes’ Theorem, we’ve now completed both parts of the numerator. We now need to multiply them together.

P(positive | review) = P(review | positive) * P(positive) / P(review)

Let’s now consider the denominator P(review). In our small example, this is the probability that “This”, “crib”, “was”, and “amazing” are the only words in the review. Notice that this is extremely similar to P(review | positive). The only difference is that we don’t assume that the review is positive.

However, before we start to compute the denominator, let’s think about what our ultimate question is. We want to predict whether the review “This crib was amazing” is a positive or negative review. In other words, we’re asking whether P(positive | review) is greater than P(negative | review). If we expand those two probabilities, we end up with the following equations.

P( positive | review ) = P( review | positive) * P(positive) / P(review) 

​	 
P( negative | review ) = P(review | negative) * P(negative) / P(negative )

​	 
Notice that P(review) is in the denominator of each. That value will be the same in both cases! Since we’re only interested in comparing these two probabilities, there’s no reason why we need to divide them by the same value. We can completely ignore the denominator!

Let’s see if our review was more likely to be positive or negative!

Instructions
1.
After the for loop, multiply pos_probability by percent_pos and neg_probability by percent_neg. Store the two values in final_pos and final_neg and print both.


For the final positive probability, do the following:

final_pos = pos_probability * percent_pos
2.
Compare final_pos to final_neg:

If final_pos was greater than final_neg, print "The review is positive".
Otherwise print "The review is negative".
Did our Naive Bayes Classifier get it right for the review "This crib was amazing"?


Use an if statement like this:

if final_pos > final_neg:
3.
Replace the review "This crib was amazing" with one that you think should be classified as negative. Run your program again.

Did your classifier correctly classify the new review?'''

from reviews import neg_counter, pos_counter

#review = "This crib was amazing"
#review = "bad silly regret crap waste nonsense"
review = "great wounderful nice perfect beautiful"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()

for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  
  pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))
  neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))

final_pos = pos_probability * percent_pos 
final_neg = neg_probability * percent_neg

if final_pos > final_neg:
  print("The review is positive")
else:
  print('The review is negative')

'''NAIVE BAYES CLASSIFIER
Formatting the Data for scikit-learn
Congratulations! You’ve made your own Naive Bayes text classifier. If you have a dataset of text that has been tagged with different classes, you can give your classifier a brand new document and it will predict what class it belongs to.

We’re now going to look at how Python’s scikit-learn library can do all of that work for us!

In order to use scikit-learn’s Naive Bayes classifier, we need to first transform our data into a format that scikit-learn can use. To do so, we’re going to use scikit-learn’s CountVectorizer object.

To begin, we need to create a CountVectorizer and teach it the vocabulary of the training set. This is done by calling the .fit() method.

For example, in the code below, we’ve created a CountVectorizer that has been trained on the vocabulary "Training", "review", "one", and "Second".

vectorizer = CountVectorizer()

vectorizer.fit(["Training review one", "Second review"])
After fitting the vectorizer, we can now call its .transform() method. The .transform() method takes a list of strings and will transform those strings into counts of the trained words. Take a look at the code below.

counts = vectorizer.transform(["one review two review"])
counts now stores the array [2, 1, 0, 0]. The word "review" appeared twice, the word "one" appeared once, and neither "Training" nor "Second" appeared at all.

But how did we know that the 2 corresponded to review? You can print vectorizer.vocabulary_ to see the index that each word corresponds to. It might look something like this:

{'one': 1, 'Training': 2, 'review': 0, 'Second': 3}
Finally, notice that even though the word "two" was in our new review, there wasn’t an index for it in the vocabulary. This is because "two" wasn’t in any of the strings used in the .fit() method.

We can now usecounts as input to our Naive Bayes Classifier.

Note that in the code in the editor, we’ve imported only a small percentage of our review dataset to make load times faster. We’ll import the full dataset later when we put all of the pieces together!

Instructions
1.
Create a CountVectorizer and name it counter.


counter = CountVectorizer()
2.
Call counter‘s .fit() method. .fit() takes a list of strings and it will learn the vocabulary of those strings. We want our counter to learn the vocabulary from both neg_list and pos_list.

Call .fit() using neg_list + pos_list as a parameter.


Fill in the appropriate parameter in the code below:

counter.fit(____)
3.
Print counter.vocabulary_. This is the vocabulary that your counter just learned. The numbers associated with each word are the indices of each word when you transform a review.


Make sure to include the underscore in the print statement!

4.
Let’s transform our brand new review. Create a variable named review_counts and set it equal to counter‘s .transform() function. Remember, .transform() takes a list of strings to transform. So call .transform() using [review] as a parameter.

Print review_counts.toarray(). If you don’t include the toarray(), review_counts won’t print in a readable format.

It looks like this is an array of all 0s, but the indices that correspond to the words "this", "crib", "was", and "amazing" should all be 1.


Fill in the correct parameter. Remember, it must be a list containing review.

review_counts = counter.transform(____)
5.
We’ll use review_counts as the test point for our Naive Bayes Classifier, but we also need to transform our training set.

Our training set is neg_list + pos_list. Call .transform() using that as a parameter. Store the results in a variable named training_counts. We’ll use these variables in the next exercise.


neg_list + pos_list is already a list, so unlike last time, we don’t need to wrap a list around it.'''

from reviews import neg_list, pos_list
from sklearn.feature_extraction.text import CountVectorizer

review = "This crib crib was a amazing"

counter = CountVectorizer()

counter.fit(neg_list + pos_list)

print(counter.vocabulary_)

review_counts = counter.transform([review])

print(review_counts)

training_counts = counter.transform(neg_list + pos_list)


{'wanted': 1521, 'to': 1429, 'love': 805, 'this': 1408, 'but': 182, 'it': 712, 'was': 1525, 'pretty': 1056, 'expensive': 467, 'for': 525, 'only': 951, 'few': 495, 'months': 871, 'worth': 1584, 'of': 937, 'calendar': 187, 'pages': 981, 'ended': 434, 'up': 1486, 'buying': 185, 'regular': 1130, 'weekly': 1541, 'planner': 1024, '55': 11, 'off': 938, 'the': 1393, 'that': 1392, 'is': 709, '11': 2, 'and': 63, 'has': 618, 'all': 47, 'seven': 1219, 'days': 339, 'on': 947, 'right': 1163, 'page': 980, 'left': 765, 'room': 1166, 'write': 1588, 'do': 380, 'list': 785, 'goals': 577, 'found': 539, 'be': 120, 'more': 873, 'helpful': 633, 'because': 123, 'could': 306, 'mark': 823, 'each': 409, 'day': 337, 'eating': 417, 'sleeping': 1252, 'blocks': 149, 'then': 1397, 'also': 55, 'see': 1207, 'them': 1395, 'side': 1235, 'by': 186, 'her': 636, 'patterns': 993, 'easily': 413, 'with': 1568, 'view': 1511, 'cute': 328, 'just': 724, 'not': 919, 'what': 1550, 'like': 778, 'log': 792, 'think': 1405, 'would': 1585, 'work': 1576, 'better': 137, 'clearer': 243, 'am': 59, 'pm': 1034, 'sections': 1205, '12': 3, 'hours': 661, 'so': 1270, 'you': 1598, 'really': 1113, 'need': 903, 'two': 1474, 'if': 673, 'your': 1600, 'baby': 104, 'feeds': 490, 'or': 959, 'wets': 1549, 'lot': 803, 'in': 681, 'early': 411, 'morning': 874, 'between': 138, 'midnight': 852, '7am': 14, 'we': 1539, 're': 1104, 'cramming': 315, 'those': 1409, 'blank': 146, 'spaces': 1289, 'above': 19, 'now': 926, 'my': 888, 'wife': 1561, 'have': 623, 'six': 1243, 'month': 870, 'old': 945, 'boy': 164, 'around': 81, 'decided': 342, 'she': 1224, 'return': 1155, 'instead': 696, 'being': 132, 'stay': 1314, 'at': 92, 'home': 652, 'mom': 866, 'hired': 644, 'an': 61, 'nanny': 891, 'care': 194, 'our': 966, 'little': 787, 'arrangement': 82, 'worked': 1577, 'quite': 1097, 'well': 1544, 'ever': 452, 'since': 1240, 'shortly': 1230, 'after': 40, 'starting': 1308, 'realized': 1111, 'some': 1276, 'sort': 1285, 'journal': 723, 'track': 1446, 'activities': 28, 'while': 1556, 'he': 627, 'were': 1546, 'working': 1579, 'used': 1492, 'plain': 1022, 'notebook': 921, 'period': 1005, 'weeks': 1542, 'until': 1485, 'stumbled': 1336, 'tracker': 1447, 'daily': 332, 'childcare': 227, 'layout': 755, 'use': 1491, 'excellent': 460, 'idea': 671, 'are': 78, 'clearly': 244, 'divided': 379, 'into': 703, 'columns': 253, 'tracking': 1449, 'feedings': 489, 'nap': 892, 'time': 1425, 'diaper': 360, 'changes': 217, 'play': 1026, 'as': 86, 'general': 559, 'areas': 80, 'notes': 922, 'milestones': 855, 'legibility': 766, 'huge': 667, 'improvement': 679, 'over': 971, 'standard': 1303, 'entries': 445, 'becoming': 125, 'small': 1260, 'paragraphs': 987, 'short': 1229, 'moments': 867, 'can': 191, 'summarize': 1347, 'data': 335, 'totals': 1442, 'section': 1204, 'determine': 355, 'key': 730, 'information': 693, 'how': 665, 'much': 885, 'did': 364, 'eat': 415, 'bowel': 161, 'movement': 883, 'sleep': 1250, 'they': 1401, 'get': 564, 'etc': 449, 'there': 1399, 'however': 666, 'frustrating': 549, 'limitations': 781, 'first': 510, 'entire': 443, 'about': 18, 'half': 602, 'sheet': 1225, 'down': 392, 'middle': 851, 'portrait': 1044, 'constrains': 285, 'very': 1505, 'column': 252, 'row': 1172, 'okay': 944, 'summarized': 1348, 'ounces': 965, '34': 9, 'once': 948, 'becomes': 124, 'active': 27, 'know': 737, 'than': 1390, 'tummy': 1469, 'under': 1479, 'things': 1404, 'start': 1306, 'tight': 1423, 'another': 65, 'problem': 1067, 'covers': 313, 'out': 968, 'fine': 505, 'intention': 699, 'child': 225, 'providers': 1079, 'which': 1555, 'often': 941, 'starts': 1309, 'earlier': 410, '6am': 13, 'using': 1498, 'require': 1142, 'second': 1201, 'good': 581, 'easy': 414, 'read': 1106, 'instantly': 695, 'gather': 557, 'matter': 832, 'high': 639, 'quality': 1092, 'paper': 986, 'consistent': 282, 'don': 388, 'gets': 565, 'most': 875, 'less': 768, 'one': 949, 'babies': 103, 'cover': 312, 'thick': 1402, 'hardback': 616, 'bends': 135, 'bag': 110, 'should': 1231, 'bigger': 141, 'understand': 1480, 'portability': 1043, 'concern': 271, 'current': 326, 'size': 1244, 'entirely': 444, 'too': 1434, 'conclusion': 273, 'making': 817, 'own': 975, 'format': 533, 'spreadsheet': 1296, 'includes': 685, '24': 7, 'hour': 660, 'space': 1288, 'comments': 259, 'along': 53, 'other': 964, 'had': 601, 'bound': 160, 'cheaply': 222, 'local': 791, 'shop': 1228, 'adequate': 34, 'thought': 1411, 'keeping': 727, 'simple': 1238, 'handwritten': 608, 'nice': 913, 'haven': 624, 'thing': 1403, 'here': 637, 'why': 1560, 'when': 1551, 'breastfeeding': 171, 'phone': 1009, 'close': 245, 'keep': 726, 'yourself': 1601, 'entertained': 442, 'able': 17, 'grab': 583, 'both': 155, 'pen': 999, 'consistently': 283, 'skilled': 1247, 'me': 836, 'nurse': 928, 'same': 1179, 'place': 1019, 'every': 453, 'deprived': 351, 'least': 762, 'forget': 528, 'look': 796, 'started': 1307, 'perfect': 1002, 'app': 73, 'either': 425, 'mindlessly': 858, 'hit': 647, 'button': 183, 'connect': 276, 'gives': 571, 'example': 459, 'tell': 1385, 'long': 794, 'average': 98, 'been': 127, 'taking': 1370, 'training': 1452, 'nursed': 929, 'him': 641, '177': 5, 'times': 1427, 'last': 749, 'granted': 588, 'serves': 1215, 'no': 917, 'useful': 1493, 'purpose': 1086, 'feeling': 492, 'perverse': 1008, 'satisfaction': 1181, 'adorable': 36, 'book': 151, 'pieces': 1016, 'attached': 95, 'activity': 29, 'several': 1220, 'though': 1410, 'sew': 1221, 'make': 815, 'directed': 369, 'will': 1563, 'realize': 1110, 'priced': 1061, 'hard': 615, 'age': 42, 'group': 595, 'playing': 1027, 'teether': 1381, 'ridiculous': 1162, 'clamp': 237, 'daughter': 336, 'going': 579, 'vibrating': 1507, 'mouth': 878, 'big': 140, 'awkward': 101, 'push': 1088, 'money': 869, 'opinion': 957, 'product': 1070, 'case': 202, 'made': 812, 'bite': 144, 'vibration': 1508, 'does': 385, 'toy': 1444, 'bottom': 158, '5mo': 12, 'who': 1557, 'loves': 807, 'tap': 1373, 'toys': 1445, 'his': 645, 'yes': 1595, 'sounds': 1287, 'weird': 1543, 'husband': 668, 'drummer': 401, 'got': 582, 'obsessed': 933, 'tapping': 1376, 'even': 450, 'drinking': 397, 'bottle': 156, 'try': 1467, 'hands': 607, 'feel': 491, 'mini': 860, 'dodge': 384, 'ball': 114, 'sticks': 1320, 'face': 476, 'happy': 614, 'bought': 159, 'infant': 690, 'gum': 599, 'massager': 829, 'electric': 427, 'toothbrush': 1437, 'always': 58, 'interested': 700, 'brush': 179, 'teeth': 1380, 'supervised': 1351, 'doesn': 386, 'choke': 231, 'skinny': 1248, 'objects': 932, 'guard': 597, 'block': 148, 'prevent': 1057, 'any': 67, 'type': 1475, 'choking': 232, 'enough': 439, 'turn': 1470, 'again': 41, 'hold': 648, 'trying': 1468, 'find': 503, 'vibrations': 1509, 'himself': 642, 'sucks': 1339, 'chew': 223, 'teething': 1384, 'bites': 145, 'never': 909, 'dumb': 405, 'vibrate': 1506, 'disappointing': 372, 'liked': 779, 'massaging': 830, 'action': 26, 'took': 1435, 'finally': 502, 'battery': 119, 'gave': 558, 'nearly': 897, 'spurts': 1298, 'rest': 1149, 'wouldn': 1586, 'waste': 1531, 'great': 590, 'freaks': 541, 'wants': 1523, 'nothing': 923, 'might': 853, 'wonderful': 1574, 'appears': 75, 'hated': 621, 'cannot': 192, 'comment': 258, 'effectiveness': 421, 'bit': 143, 'seemed': 1208, 'cool': 299, 'passed': 991, 'friend': 544, 'recommended': 1118, 'throwing': 1417, 'floor': 520, 'hopefully': 656, 'squeeze': 1299, 'help': 631, 'heavy': 629, 'seems': 1209, 'enjoy': 438, 'way': 1537, 'feels': 493, 'actually': 31, 'tired': 1428, 'minute': 861, 'maybe': 835, 'older': 946, 'teethers': 1382, 'twin': 1472, 'boys': 165, 'years': 1594, 'ago': 43, 'absolutely': 21, 'loved': 806, 'new': 910, 'sought': 1286, 'surprised': 1357, 'arrived': 84, 'packaging': 978, 'stated': 1311, 'safety': 1176, 'tested': 1389, 'bpa': 166, 'lead': 757, 'phthalates': 1011, 'state': 1310, 'anywhere': 71, 'package': 977, 'free': 542, 'former': 534, 'regulator': 1131, 'medical': 841, 'devices': 359, 'sensitive': 1212, 'company': 264, 'labeling': 741, 'statements': 1312, 'perhaps': 1004, 'rather': 1102, 'err': 446, 'caution': 208, 'where': 1553, 'concerned': 272, 'lack': 742, 'clear': 242, 'pause': 994, 'specifically': 1292, 'test': 1388, 'acceptable': 22, 'amount': 60, 'allowed': 50, 'simply': 1239, 'label': 740, 'phthalate': 1010, 'eight': 424, 'sons': 1282, 'heard': 628, 'put': 1089, 'manner': 819, 'plastic': 1025, 'their': 1394, 'mouths': 879, 'sure': 1356, 'riddled': 1161, 'strongly': 1332, 'suspected': 1358, 'dangerous': 333, 'conscience': 279, 'allow': 49, 'anything': 70, 'may': 834, 'contain': 288, 'someone': 1277, 'site': 1241, 'indicated': 686, 'representative': 1141, 'from': 547, 'told': 1433, 'still': 1321, 'skeptical': 1246, 'such': 1337, 'companies': 263, 'proudly': 1077, 'products': 1071, 'language': 746, 'confused': 275, 'son': 1281, 'suck': 1338, 'fingers': 506, 'messaging': 848, 'corn': 301, 'squeezed': 1300, 'order': 960, 'young': 1599, 'already': 54, 'isn': 710, 'reason': 1114, 'pick': 1012, 'car': 193, 'stuffed': 1335, 'different': 367, 'complaint': 266, 'star': 1304, 'bead': 121, 'end': 433, 'pointy': 1038, 'catches': 205, 'sore': 1284, 'coming': 257, 'wrong': 1591, 'cry': 324, 'colors': 251, 'definitely': 345, 'catch': 204, 'eye': 473, 'overall': 972, 'ok': 943, 'price': 1060, 'necklace': 902, 'weren': 1547, 'reviewer': 1157, 'pointed': 1037, 'chews': 224, 'edge': 420, 'painful': 984, 'thrown': 1418, 'having': 626, 'pull': 1080, 'road': 1164, 'week': 1540, 'screamed': 1194, 'suddenly': 1340, 'back': 106, 'crying': 325, 'hysterically': 669, 'sling': 1255, 'seen': 1210, 'mothers': 877, 'these': 1400, 'before': 128, 'childbirth': 226, 'class': 238, 'raved': 1103, 'excited': 461, 'received': 1115, 'gift': 567, 'cradle': 314, 'position': 1045, 'afraid': 39, 'sufficate': 1341, 'fabric': 474, 'hand': 603, 'next': 912, 'easier': 412, 'grocery': 594, 'store': 1325, 'pulled': 1081, 'shoulder': 1232, 'crawling': 316, 'pulling': 1082, 'restrained': 1151, 've': 1503, 'tried': 1462, 'kangaroo': 725, 'scared': 1188, 'fall': 479, 'wiggle': 1562, 'worm': 1582, 'strap': 1328, 'neck': 901, 'interfere': 701, 'circulation': 236, 'glad': 573, 'rethink': 1153, 'kind': 733, 'carrier': 198, 'slings': 1256, 'practiced': 1050, 'cat': 203, 'comfortable': 256, 'must': 887, 'admit': 35, 'front': 548, 'packs': 979, 'cut': 327, 'across': 25, 'go': 576, 'carry': 199, 'screams': 1195, 'hardly': 617, 'cries': 320, 'say': 1187, 'mean': 838, 'turns': 1471, 'red': 1123, 'soon': 1283, 'take': 1366, 'pack': 976, 'honestly': 653, 'lay': 754, 'cooperates': 300, 'carried': 197, 'brought': 178, 'hospital': 659, 'walking': 1519, 'plan': 1023, 'hip': 643, 'watched': 1534, 'video': 1510, 'support': 1354, 'hope': 654, 'stick': 1318, 'traditional': 1451, 'larger': 748, 'women': 1572, 'endowed': 435, 'purchased': 1085, 'total': 1441, 'thinking': 1406, 'needed': 904, 'change': 215, 'bedding': 126, 'constantly': 284, 'due': 404, 'spit': 1295, 'instructions': 697, 'said': 1177, 'hang': 610, 'dry': 402, 'through': 1414, 'toss': 1440, 'dryer': 403, 'mattress': 833, 'known': 739, 'didn': 365, 'extra': 471, 'went': 1545, 'cosleeper': 304, 'material': 831, 'special': 1290, 'holds': 650, 'lots': 804, 'washes': 1527, 'uses': 1497, 'shipping': 1227, 'quick': 1095, 'sheets': 1226, 'themselves': 1396, 'soft': 1271, 'course': 311, 'inexpensive': 689, 'expected': 464, 'five': 513, 'co': 246, 'sleeper': 1251, 'assembled': 90, 'yet': 1596, 'box': 163, 'torn': 1439, 'dented': 348, 'looking': 798, 'contents': 292, 'looks': 799, 'law': 753, 'puts': 1090, 'together': 1432, 'stroller': 1331, 'won': 1573, 'fit': 511, 'graco': 585, 'straps': 1329, 'tie': 1421, 'trays': 1457, 'came': 190, 'un': 1478, 'sewed': 1222, 'buy': 184, 'happened': 612, 'unforutnately': 1481, 'travel': 1456, 'system': 1361, 'carseat': 201, 'grace': 584, 'marathon': 822, 'useless': 1495, 'quattro': 1093, 'metrolite': 850, 'complete': 268, 'primarily': 1062, 'takes': 1369, 'trash': 1455, 'bags': 111, 'supposed': 1355, 'couple': 309, 'quickly': 1096, 'discovered': 374, 'pail': 982, 'full': 550, 'dropping': 399, 'mechanism': 840, 'stuck': 1334, 'forcing': 527, 'clean': 240, 'whenever': 1552, 'wipe': 1564, 'messy': 849, 'load': 790, 'top': 1438, 'routinely': 1170, 'flip': 518, 'dump': 406, 'dirty': 370, 'lift': 774, 'whole': 1559, 'lid': 770, 'allows': 51, 'tabs': 1364, 'getting': 566, 'trap': 1454, 'smell': 1262, 'negates': 906, 'makes': 816, 'something': 1278, 'cheaper': 221, 'bad': 108, 'proof': 1074, 'fact': 477, 'empty': 432, 'leaks': 758, 'pleasant': 1029, 'truly': 1466, 'believe': 133, 'best': 136, 'market': 824, 'garbage': 555, 'fantastic': 481, 'job': 719, 'containing': 290, 'odors': 936, 'couldn': 307, 'biggest': 142, 'kitchen': 734, 'sized': 1245, 'champ': 214, 'requires': 1144, 'us': 1490, 'remove': 1138, 'wasteful': 1532, 'solution': 1275, 'smaller': 1261, 'accomodate': 23, 'common': 260, 'household': 663, 'tall': 1372, 'efficiently': 422, 'point': 1036, 'separate': 1213, 'disguise': 376, 'doubt': 391, 'expend': 466, 'effort': 423, 'works': 1580, 'piston': 1018, 'drop': 398, 'joint': 720, 'meet': 844, 'thus': 1419, 'caught': 207, 'its': 716, 'replacement': 1140, 'moved': 882, 'drum': 400, 'foot': 524, 'securing': 1206, 'handed': 604, 'operation': 956, 'states': 1313, 'third': 1407, 'trip': 1463, 'ask': 87, 'china': 230, 'contains': 291, 'category': 206, 'throw': 1416, 'soiled': 1272, 'ours': 967, 'tends': 1387, 'lazy': 756, 'fill': 499, 'yuck': 1602, 'save': 1182, 'diapers': 362, 'reading': 1107, 'reviews': 1159, 'line': 782, 'chose': 234, 'register': 1127, 'disappointed': 371, 'sense': 1211, 'super': 1350, 'hero': 638, 'smelly': 1265, 'enter': 440, 'corner': 302, 'guess': 598, 'grateful': 589, 'house': 662, 'poopie': 1040, 'wet': 1548, 'scented': 1189, 'away': 99, 'garabage': 554, 'taken': 1367, 'opening': 954, 'difficult': 368, 'blue': 150, 'round': 1168, 'part': 990, 'broke': 176, 'masking': 827, 'tape': 1374, 'thrilled': 1413, 'specific': 1291, 'cheap': 220, 'smells': 1264, 'large': 747, 'far': 482, 'gab': 553, 'nature': 895, 'awesome': 100, 'stinks': 1322, 'neat': 898, 'disposal': 377, 'shower': 1233, 'handle': 606, 'purchase': 1084, 'liners': 784, 'mistake': 864, 'stars': 1305, 'parents': 989, 'children': 228, '14': 4, 'come': 254, 'odor': 934, 'literally': 786, 'smelled': 1263, 'poop': 1039, 'stand': 1302, 'changed': 216, 'saving': 1184, 'plus': 1033, 'infants': 691, 'want': 1520, 'consider': 281, 'else': 428, 'decor': 343, 'gifts': 568, 'favorite': 483, 'positive': 1046, 'open': 952, 'without': 1570, 'breaking': 168, 'nail': 890, 'changing': 218, 'depending': 349, 'waited': 1515, 'door': 390, 'knot': 736, 'brim': 173, 'dispose': 378, 'squirmy': 1301, 'table': 1363, 'registered': 1128, 'item': 714, 'impressed': 677, 'jammed': 718, 'leaves': 764, 'faint': 478, 'live': 788, 'hassle': 620, 'beware': 139, 'value': 1501, 'manicures': 818, 'refills': 1126, 'pleased': 1031, 'diet': 366, 'bowl': 162, 'movements': 884, 'longer': 795, 'contained': 289, 'tightly': 1424, 'wrap': 1587, 'washing': 1528, 'frequently': 543, 'stench': 1317, 'comes': 255, 'maintain': 813, 'serious': 1214, 'design': 352, 'flaw': 516, 'tapered': 1375, 'noticeably': 925, 'probably': 1066, 'balancing': 113, 'purposes': 1087, 'pros': 1075, 'alternatives': 57, 'normal': 918, 'bagscons': 112, 'glowing': 575, 'receiving': 1116, 'replace': 1139, 'broken': 177, 'genie': 560, 'disappointment': 373, 'complaints': 267, 'unsanitary': 1483, 'putting': 1091, 'wipes': 1565, 'slot': 1257, 'dumping': 407, 'bucket': 180, 'germy': 563, 'mess': 847, 'unrealistic': 1482, 'fear': 484, 'toddler': 1431, 'near': 896, 'germs': 562, 'let': 769, 'frightening': 546, 'experience': 468, 'yikes': 1597, 'prevention': 1058, 'prepared': 1055, 'opened': 953, 'seals': 1198, 'individual': 688, 'sea': 1196, 'ones': 950, 'certainly': 211, 'arrives': 85, 'year': 1593, 'll': 789, 'cleaner': 241, 'involved': 707, 'version': 1504, 'operate': 955, 'model': 865, 'cross': 321, 'stays': 1316, 'friends': 545, 'houses': 664, 'fought': 538, 'genies': 561, 'warm': 1524, 'appeared': 74, 'lysol': 811, 'various': 1502, 'tricks': 1460, 'mask': 826, 'emanating': 429, 'luck': 809, 'breast': 169, 'fed': 486, 'imagine': 674, 'solids': 1274, 'ick': 670, 'wondering': 1575, 'fighting': 496, 'option': 958, 'air': 45, 'conditioning': 274, 'summer': 1349, 'reviewers': 1158, 'complained': 265, 'completely': 269, 'convenient': 298, 'drag': 395, 'extremely': 472, 'horrible': 658, 'please': 1030, 'figured': 498, 'protection': 1076, 'desired': 353, 'badly': 109, 'multiple': 886, 'nasty': 894, 'spew': 1294, 'forth': 536, 'sealed': 1197, 'somewhat': 1280, 'correctly': 303, 'concept': 270, 'fabulous': 475, 'cons': 278, 'area': 79, 'addition': 33, 'odorless': 935, 'notice': 924, 'worse': 1583, 'cost': 305, 'savings': 1185, 'began': 129, 'cylinder': 329, 'continually': 293, 'fun': 551, 'apart': 72, '00': 0, 'placed': 1020, 'hole': 651, 'sometimes': 1279, 'dealing': 341, 'flipped': 519, 'fell': 494, 'task': 1377, 'retrieve': 1154, 'figure': 497, 'fix': 514, '1st': 6, 'hauled': 622, 'pickup': 1014, 'lasted': 750, 'recommend': 1117, 'newborn': 911, 'within': 1569, 'liner': 783, 'assume': 91, 'stickies': 1319, 'anymore': 68, 'groceries': 593, 'perfectly': 1003, 'deal': 340, 'babi': 102, 'italia': 713, 'pinehurst': 1017, 'classic': 239, 'crib': 318, 'ultimate': 1476, 'backup': 107, 'wash': 1526, 'problems': 1068, 'fitting': 512, 'rails': 1099, 'forced': 526, 'attach': 94, 'snaps': 1269, 'stretch': 1330, 'flat': 515, 'today': 1430, 'marks': 825, 'paint': 985, 'ah': 44, 'improvised': 680, 'rail': 1098, 'length': 767, 'actual': 30, 'usually': 1499, 'teethes': 1383, 'based': 116, 'bargains': 115, 'expectations': 463, 'night': 914, 'unsnap': 1484, '10': 1, 'places': 1021, 'engage': 437, 'bumpers': 181, 'elastic': 426, 'slats': 1249, 'attractive': 96, 'miracle': 863, 'hoped': 655, 'trouble': 1464, 'lifting': 775, 'lightweight': 777, 'foam': 521, 'result': 1152, 'seconds': 1203, 'snapping': 1268, 'ties': 1422, 'bending': 134, 'solid': 1273, 'minutes': 862, 'manufacturer': 820, 'recommends': 1119, 'means': 839, 'snap': 1267, 'suggest': 1343, 'coil': 249, 'center': 209, 'feeding': 488, 'schedule': 1190, 'life': 771, 'doctor': 382, 'questions': 1094, 'habits': 600, 'saver': 1183, 'trends': 1459, 'answer': 66, 'pediatrician': 996, 'communicate': 261, 'everyone': 455, 'required': 1143, 'leave': 763, 'finish': 507, 'haves': 625, 'helps': 635, 'exactly': 458, 'gone': 580, 'mother': 876, 'watching': 1535, 'happier': 613, 'routine': 1169, 'sitter': 1242, 'helped': 632, 'prepare': 1054, 'evening': 451, 'likely': 780, 'sick': 1234, 'many': 821, 'producing': 1069, 'dehydrated': 346, 'note': 920, 'writes': 1589, 'whether': 1554, 'lunch': 810, 'playtime': 1028, 'included': 684, 'walk': 1517, 'moms': 868, 'wanting': 1522, 'kids': 731, 'dads': 331, 'lol': 793, 'alternative': 56, 'printing': 1064, 'searching': 1199, 'crumpled': 323, 'piece': 1015, 'previous': 1059, 'preferred': 1053, 'held': 630, 'basics': 117, 'wish': 1566, 'struggle': 1333, 'caretaker': 196, 'wrote': 1592, 'spend': 1293, 'neighbor': 907, 'loosely': 801, 'developing': 356, 'milk': 856, 'cohesion': 248, 'visits': 1514, 'brand': 167, 'books': 152, 'absolute': 20, 'trackers': 1448, 'available': 97, 'naps': 893, 'tracks': 1450, 'nights': 915, 'important': 676, 'during': 408, 'caregiver': 195, 'postpartum': 1047, 'nurses': 930, 'urination': 1489, 'remind': 1137, 'overwhelmed': 973, 'remember': 1135, 'cried': 319, 'major': 814, 'contact': 287, 'call': 188, 'trend': 1458, 'indication': 687, 'suggested': 1344, 'pumping': 1083, 'supplement': 1352, 'formula': 535, 'sufficient': 1342, 'water': 1536, 'drink': 396, 'supply': 1353, 'breastmilk': 172, 'wasn': 1530, 'giving': 572, 'gas': 556, 'ate': 93, 'food': 522, 'allergy': 48, 'peanut': 995, 'family': 480, 'smile': 1266, 'visit': 1512, 'ect': 419, 'memory': 846, 'babysitter': 105, 'grandma': 586, 'goes': 578, 'recorded': 1121, 'diary': 363, 'certain': 210, 'suit': 1345, 'rough': 1167, 'refer': 1124, 'forgot': 529, 'woke': 1571, 'emergency': 431, 'consent': 280, 'form': 531, 'needs': 905, 'immunizations': 675, 'info': 692, 'glance': 574, 'developmental': 358, 'organized': 963, 'create': 317, 'spreadsheets': 1297, 'people': 1000, 'practical': 1049, 'pees': 998, 'poops': 1041, 'breastfeed': 170, 'especially': 448, 'adults': 37, 'sharing': 1223, 'responsibilities': 1148, 'wake': 1516, 'eaten': 416, 'written': 1590, 'record': 1120, 'dr': 394, 'appts': 77, 'analyze': 62, 'asks': 89, 'urinating': 1488, 'realizing': 1112, 'oh': 942, 'hasn': 619, 'slept': 1254, 'number': 927, 'done': 389, 'looked': 797, 'pattern': 992, 'routines': 1171, 'jot': 721, 'knew': 735, 'beginning': 130, 'transfer': 1453, 'everything': 456, 'funny': 552, 'forgotten': 530, 'born': 154, 'pee': 997, 'poopy': 1042, 'asking': 88, 'per': 1001, 'bring': 174, 'color': 250, 'code': 247, 'pottied': 1048, 'ordering': 961, 'ummmm': 1477, 'pain': 983, 'killers': 732, 'mass': 828, 'hormones': 657, 'slowly': 1259, 'lst': 808, 'minds': 859, 'handy': 609, 'eats': 418, 'discuss': 375, 'progress': 1072, 'grandparents': 587, 'watch': 1533, 'doing': 387, 'highly': 640, 'continue': 294, 'delivery': 347, 'saw': 1186, 'ends': 436, 'swear': 1360, 'throughout': 1415, 'mile': 854, 'stone': 1323, '8230': 16, 'outings': 969, 'temperature': 1386, 'readings': 1108, 'calls': 189, 'stopped': 1324, 'swaddling': 1359, 'resource': 1146, 'later': 751, 'formally': 532, 'album': 46, 'valuable': 1500, 'tidbits': 1420, 'memories': 845, 'rummage': 1173, 'sale': 1178, 'helping': 634, 'ex': 457, 'visiting': 1513, 'almost': 52, 'filled': 500, 'development': 357, 'story': 1327, 'dad': 330, 'connected': 277, 'everyday': 454, 'medicine': 842, 'give': 569, 'rundown': 1175, 'three': 1412, 'issues': 711, 'necessary': 899, 'explain': 469, 'behavior': 131, 'lactation': 743, 'consultants': 286, 'contributed': 296, 'deprivation': 350, 'dark': 334, 'respond': 1147, 'referring': 1125, 'tasks': 1378, 'include': 683, 'finished': 508, 'knowing': 738, 'meal': 837, 'predicting': 1051, 'communication': 262, 'childs': 229, 'learn': 760, 'growth': 596, 'laid': 745, 'set': 1217, 'nitpick': 916, 'user': 1496, 'identical': 672, 'holding': 649, 'lean': 759, 'forward': 537, 'happend': 611, 'prior': 1065, 'remembering': 1136, 'wasi': 1529, 'kept': 729, 'moods': 872, 'became': 122, 'tool': 1436, 'documenting': 383, 'recording': 1122, 'diapering': 361, 'session': 1216, 'wayside': 1538, 'finding': 504, 'flexible': 517, 'lifestyle': 773, 'nursing': 931, 'move': 881, 'peruse': 1007, 'secondary': 1202, 'account': 24, 'talking': 1371, 'personas': 1006, 'christmas': 235, 'february': 485, 'restless': 1150, 'blending': 147, '8217': 15, 'religiously': 1132, 'stored': 1326, 'defiantly': 344, 'tricky': 1461, 'print': 1063, 'light': 776, 'grey': 592, 'bottles': 157, 'meds': 843, 'challenge': 213, 'detailed': 354, 'slots': 1258, 'prefer': 1052, 'chooses': 233, 'sleeps': 1253, 'bath': 118, 'doc': 381, 'appt': 76, 'thank': 1391, 'stayed': 1315, 'suitcase': 1346, 'loose': 800, 'output': 970, 'crucial': 322, 'iphone': 708, 'proved': 1078, 'annoying': 64, 'walked': 1518, 'scrap': 1193, 'inaccurate': 682, 'update': 1487, 'continued': 295, 'control': 297, 'add': 32, 'rely': 1133, '3mo': 10, 'chair': 212, 'jotting': 722, 'scheduling': 1192, 'twins': 1473, 'four': 540, 'givers': 570, 'inside': 694, 'feed': 487, 'intervals': 702, '30': 8, 'ribbon': 1160, 'movable': 880, 'tab': 1362, 'advertised': 38, 'keeps': 728, 'sane': 1180, 'esp': 447, 'realistic': 1109, 'filling': 501, 'download': 393, 'embrace': 430, 'world': 1581, 'entering': 441, 'tabulate': 1365, 'charts': 219, 'review': 1156, 'myself': 889, 'daycare': 338, 'workers': 1578, 'offended': 939, 'explaining': 470, 'usefulness': 1494, 'greatest': 591, 'inventions': 706, 'relying': 1134, 'learning': 761, 'overwhelming': 974, 'history': 646, 'reactions': 1105, 'coupled': 310, 'itzbeen': 717, 'pocket': 1035, 'timer': 1426, 'necessity': 900, 'lost': 802, 'mind': 857, 'takers': 1368, 'picking': 1013, 'run': 1174, 'anyone': 69, 'expecting': 465, 'items': 715, 'intake': 698, 'organize': 962, 'lifesaver': 772, 'exhausted': 462, 'sides': 1236, 'plenty': 1032, 'handing': 605, 'parent': 988, 'whoever': 1558, 'ran': 1100, 'wishing': 1567, 'researching': 1145, 'settled': 1218, 'regret': 1129, 'laugh': 752, 'progression': 1073, 'toward': 1443, 'schedules': 1191, 'count': 308, 'introduced': 705, 'foods': 523, 'offered': 940, 'signs': 1237, 'intolerance': 704, 'arrival': 83, 'seasoned': 1200, 'therapist': 1398, 'brings': 175, 'ladder': 744, 'fire': 509, 'truck': 1465, 'bored': 153, 'roll': 1165, 'nesting': 908, 'improved': 678, 'cars': 200, 'random': 1101, 'teach': 1379}
  (0, 318)	2
  (0, 1408)	1
  (0, 1525)	1
  
'''
  
NAIVE BAYES CLASSIFIER
Using scikit-learn
Now that we’ve formatted our data correctly, we can use it using scikit-learn’s MultinomialNB classifier.

This classifier can be trained using the .fit() method. .fit() takes two parameters: The array of data points (which we just made) and an array of labels corresponding to each data point.

Finally, once the model has been trained, we can use the .predict() method to predict the labels of new points. .predict() takes a list of points that you want to classify and it returns the predicted labels of those points.

Finally, .predict_proba() will return the probability of each label given a point. Instead of just returning whether the review was good or bad, it will return the likelihood of a good or bad review.

Note that in the code editor, we’ve imported some of the variables you created last time. Specifically, we’ve imported the counter object, training_counts and then make review_counts. This means the program won’t have to re-create those variables and should help the runtime of your program.

Instructions
1.
Begin by making a MultinomialNB object called classifier.


This should be similar to how you created the CountVectorizer in the previous exercise.

2.
We now want to fit the classifier. We have the transformed points (found in training_counts), but we don’t have the labels associated with those points.

We made the training points by combining neg_list and pos_list. So the first half of the labels should be 0 (for negative) and the second half should be 1 (for positive).

Create a list named training_labels that has 1000 0s followed by 1000 1s.

Note that there are 1000 negative and 1000 positive reviews. Normally you could find this out by asking for the length of your dataset — in this example, we haven’t included the dataset because it takes so long to load!


You can use the * operator to quickly make this list. We’ve shown you how to add the 0s. Add the 1s.

training_labels = [0] * 1000 + ____
3.
Call classifier‘s .fit() function. Fit takes two parameters: the training set and the training labels.


Your training set is found in the variable training_counts and the training labels are found in training_labels.

4.
Call classifier‘s .predict() method and print the results. This method takes a list of the points that you want to test.

Was your review classified as a positive or negative review?


Your test points are found in the variable review_counts.

From the way we constructed our labels, 0 is bad and 1 is good.

5.
After printing predict, print a call to the predict_proba method. The parameter to predict_proba should be the same as predict.

The first number printed is the probability that the review was a 0 (bad) and the second number is the probability the review was a 1 (good).

6.
Change the text review to see the probabilities change.

Can you create a review that the algorithm is really confident about being positive?

The review "This crib was great amazing and wonderful" had the following probabilities:

[[ 0.04977729 0.95022271]]

Can you create a review that is even more positive?

Another interesting challenge is to create a clearly negative review that our classifier thinks is positive.


Think of words that would only be in positive or negative reviews and and them to review.'''

from reviews import counter, training_counts
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

#review = "This crib was amazing"
review = 'This crib was great amazing and wonderful'
review_counts = counter.transform([review])

classifier = MultinomialNB()
neg_label = [0 for i in range(1000)]
pos_label = [1 for i in range(1000)]
training_labels = neg_label + pos_label

classifier.fit(training_counts, training_labels)

print(classifier.predict(review_counts))

print(classifier.predict_proba(review_counts))


'''
NAIVE BAYES CLASSIFIER
Review
In this lesson, you’ve learned how to leverage Bayes’ Theorem to create a supervised machine learning algorithm. Here are some of the major takeaways from the lesson:

A tagged dataset is necessary to calculate the probabilities used in Bayes’ Theorem.
In this example, the features of our dataset are the words used in a product review. In order to apply Bayes’ Theorem, we assume that these features are independent.
Using Bayes’ Theorem, we can find P(class|data point) for every possible class. In this example, there were two classes — positive and negative. The class with the highest probability will be the algorithm’s prediction.
Even though our algorithm is running smoothly, there’s always more that we can add to try to improve performance. The following techniques are focused on ways in which we process data before feeding it into the Naive Bayes classifier:

Remove punctuation from the training set. Right now in our dataset, there are 702 instances of "great!" and 2322 instances of "great.". We should probably combine those into 3024 instances of "great".
Lowercase every word in the training set. We do this for the same reason why we remove punctuation. We want "Great" and "great" to be the same.
Use a bigram or trigram model. Right now, the features of a review are individual words. For example, the features of the point “This crib is great” are “This”, “crib”, “is”, and “great”. If we used a bigram model, the features would be “This crib”, “crib is”, and “is great”. Using a bigram model makes the assumption of independence more reasonable.
These three improvements would all be considered part of the field Natural Language Processing.

You can find the baby product review dataset, along with many others, on Dr. Julian McAuley’s website.
http://jmcauley.ucsd.edu/data/amazon/
'''


from reviews import baby_counter, baby_training, instant_video_counter, instant_video_training, video_game_counter, video_game_training
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

review = "this game was violent"

baby_review_counts = baby_counter.transform([review])
instant_video_review_counts = instant_video_counter.transform([review])
video_game_review_counts = video_game_counter.transform([review])

baby_classifier = MultinomialNB()
instant_video_classifier = MultinomialNB()
video_game_classifier = MultinomialNB()

baby_labels = [0] * 1000 + [1] * 1000
instant_video_labels = [0] * 1000 + [1] * 1000
video_game_labels = [0] * 1000 + [1] * 1000


baby_classifier.fit(baby_training, baby_labels)
instant_video_classifier.fit(instant_video_training, instant_video_labels)
video_game_classifier.fit(video_game_training, video_game_labels)

print("Baby training set: " +str(baby_classifier.predict_proba(baby_review_counts)))
print("Amazon Instant Video training set: " + str(instant_video_classifier.predict_proba(instant_video_review_counts)))
print("Video Games training set: " + str(video_game_classifier.predict_proba(video_game_review_counts)))

'''
MACHINE LEARNING: SUPERVISED LEARNING 🤖
Email Similarity
In this project, you will use scikit-learn’s Naive Bayes implementation on several different datasets. By reporting the accuracy of the classifier, we can find which datasets are harder to distinguish. For example, how difficult do you think it is to distinguish the difference between emails about hockey and emails about soccer? How hard is it to tell the difference between emails about hockey and emails about tech? In this project, we’ll find out exactly how difficult those two tasks are.

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
15/15Complete
Mark the tasks as complete by checking them off
Exploring the Data
1.
We’ve imported a dataset of emails from scikit-learn’s datasets. All of these emails are tagged based on their content.

Print emails.target_names to see the different categories.

2.
We’re interested in seeing how effective our Naive Bayes classifier is at telling the difference between a baseball email and a hockey email. We can select the categories of articles we want from fetch_20newsgroups by adding the parameter categories.

In the function call, set categories equal to the list ['rec.sport.baseball', 'rec.sport.hockey']


The function should now look like this:
'''
emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'])
'''
3.
Let’s take a look at one of these emails.

All of the emails are stored in a list called emails.data. Print the email at index 5 in the list.

'''
print(emails.data[5])
'''
4.
All of the labels can be found in the list emails.target. Print the label of the email at index 5.

The labels themselves are numbers, but those numbers correspond to the label names found at emails.target_names.

Is this a baseball email or a hockey email?


Instead of printing index 5 from emails.data, print it from emails.target.

Print emails.target_names to see what that number corresponds to.

The target of email 5 is 1, which corresponds to rec.sport.hockey.

Making the Training and Test Sets
5.
We now want to split our data into training and test sets. Change the name of your variable from emails to train_emails. Add these three parameters to the function call:

subset='train'
shuffle = True
random_state = 108
Adding the random_state parameter will make sure that every time you run the code, your dataset is split in the same way.


Adding the subset parameter will make your function call look like this:
'''
train_emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'], subset = 'train')
'''
Add the other two parameters in addition to subset.

6.
Create another variable named test_emails and set it equal to fetch_20newsgroups. The parameters of the function should be the same as before except subset should now be 'test'.

Counting Words
7.
We want to transform these emails into lists of word counts. The CountVectorizer class makes this easy for us.

Create a CountVectorizer object and name it counter.

'''
counter = CountVectorizer()
'''
8.
We need to tell counter what possible words can exist in our emails. counter has a .fit() a function that takes a list of all your data.

Call .fit() with test_emails.data + train_emails.data as a parameter.


Fill test_emails.data + train_emails.data into the blank
'''
counter.fit(_____)
'''
9.
We can now make a list of the counts of our words in our training set.

Create a variable named train_counts. Set it equal to counter‘s transform function using train_emails.data as a parameter.


Use the apporpriate parameter in the function below:
'''
train_counts = counter.transform(_____)
'''
10.
Let’s also make a variable named test_counts. This should be the same function call as before, but use test_emails.data as the parameter of transform.

Making a Naive Bayes Classifier
11.
Let’s now make a Naive Bayes classifier that we can train and test on. Create a MultinomialNB object named classifier.


This is similar to how you created counter. Instead of making a CountVectorizer, create a MultinomialNB

12.
Call classifier‘s .fit() function. .fit() takes two parameters. The first should be our training set, which for us is train_counts. The second should be the labels associated with the training emails. Those are found in train_emails.target.


Fill train_counts and train_emails.target into the two blanks:
'''
classifier.fit(_____, _____)
'''
13.
Test the Naive Bayes Classifier by printing classifier‘s .score() function. .score() takes the test set and the test labels as parameters.

.score() returns the accuracy of the classifier on the test data. Accuracy measures the percentage of classifications a classifier correctly made.


The two parameters to .score() should be test_counts and test_emails.target.

Make sure to print this function call:
'''
print(classifier.score(____, ____))
'''
.score() will classify all the emails in the test set and compare the classification of each email to its actual label. After completing these comparisons, it will calculate and return the accuracy.

Testing Other Datasets
14.
Our classifier does a pretty good job distinguishing between soccer emails and hockey emails. But let’s see how it does with emails about really different topics.

Find where you create train_emails and test_emails. Change the categories to be ['comp.sys.ibm.pc.hardware','rec.sport.hockey'].

Did your classifier do a better or worse job on these two datasets?


The classifier was 99% accurate when trying to classify hockey and tech emails.

This is better than when it was trying to classify hockey and soccer emails. This makes sense — emails about sports probably share more words in common.

15.
Play around with different sets of data. Can you find a set that’s incredibly accurate or incredibly inaccurate?

The possible categories are listed below.

'alt.atheism'
'comp.graphics'
'comp.os.ms-windows.misc'
'comp.sys.ibm.pc.hardware'
'comp.sys.mac.hardware'
'comp.windows.x'
'misc.forsale'
'rec.autos'
'rec.motorcycles'
'rec.sport.baseball'
'rec.sport.hockey'
'sci.crypt'
'sci.electronics'
'sci.med'
'sci.space'
'soc.religion.christian'
'talk.politics.guns'
'talk.politics.mideast'
'talk.politics.misc'
'talk.religion.misc'

Your classifier can work even when there are more than two labels. Try setting categories equal to a list of three or four of the categories.'''

from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'])

#print(emails.target_names)
#print(type(emails))
#print(emails)

#print(len(emails.data))
#print(emails.data[10])

#print(emails.target_names)

#test between baseball and hockey
#train_emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'], subset = 'train', shuffle = True, random_state = 108)
#test_emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'], subset = 'test', shuffle = True, random_state = 108)

#test between Technology and hockey
#train_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.hockey'], subset = 'train', shuffle = True, random_state = 108)
#test_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.hockey'], subset = 'test', shuffle = True, random_state = 108)

#play around with different topic
train_emails = fetch_20newsgroups(categories = ['comp.graphics', 'talk.politics.mideast'], subset = 'train', shuffle = True, random_state = 108)
test_emails = fetch_20newsgroups(categories = ['comp.graphics', 'talk.politics.mideast'], subset = 'test', shuffle = True, random_state = 108)


counter = CountVectorizer()
counter.fit(train_emails.data + test_emails.data)
train_counts = counter.transform(train_emails.data)
test_counts = counter.transform(test_emails.data)

classifier = MultinomialNB()
classifier.fit(train_counts, train_emails.target)
print(classifier.score(test_counts, test_emails.target))




#-----------------------------MINMAX--------------------------------------
'''
MINIMAX
Games as Trees
Have you ever played a game against someone and felt like they were always two steps ahead? No matter what clever move you tried, they had somehow envisioned it and had the perfect counterattack. This concept of thinking ahead is the central idea behind the minimax algorithm.

The minimax algorithm is a decision-making algorithm that is used for finding the best move in a two player game. It’s a recursive algorithm — it calls itself. In order for us to determine if making move A is a good idea, we need to think about what our opponent would do if we made that move.

We’d guess what our opponent would do by running the minimax algorithm from our opponent’s point of view. In the hypothetical world where we made move A, what would they do? Surely they want to win as badly as we do, so they’d evaluate the strength of their move by thinking about what we would do if they made move B.

As this process repeats, we can start to make a tree of these hypothetical game states. We’ll eventually reach a point where the game is over — we’ll reach a leaf of the tree. Either we won, our opponent won, or it was a tie. At this point, the recursion can stop. Because the game is over, we no longer need to think about how our opponent would react if we reached this point of the game.

Instructions
On this page, you’ll see the game tree of a Tic-Tac-Toe game that is almost complete. At the root of the node, it is "X"‘s turn.

Some of the leaves of the tree still have squares that can be filled in. Why are those boards leaves?

MINIMAX
Tic-Tac-Toe
For the rest of this exercise, we’re going to be writing the minimax algorithm to be used on a game of Tic-Tac-Toe. We’ve imported a Tic-Tac-Toe game engine in the file tic_tac_toe.py. Before starting to write the minimax function, let’s play around with some of the Tic-Tac-Toe functions we’ve defined for you in tic_tac_toe.py.

To begin, a board is represented as a list of lists. In script.py we’ve created a board named my_board where the X player has already made the first move. They’ve chosen the top right corner. To nicely print this board, use the print_board() function using my_board as a parameter.

Next, we want to be able to take a turn. The select_space() function lets us do this. Select space takes three parameters:

The board that you want to take the turn on.
The space that you want to fill in. This should be a number between 1 and 9.
The symbol that you want to put in that space. This should be a string — either an "X" or an "O".
We can also get a list of the available spaces using available_moves() and passing the board as a parameter.

Finally, we can check to see if someone has won the game. The has_won() function takes the board and a symbol (either "X" or "O"). It returns True if that symbol has won the game, and False otherwise.

Let’s test these functions! Write your code in script.py, but feel free to take a look at tic_tac_toe.py if you want to look at how the game engine works.'''

#------------script.py---------------
from tic_tac_toe import *

my_board = [
	["1", "2", "X"],
	["4", "5", "6"],
	["7", "8", "9"]
]

print_board(my_board)

select_space(my_board, 4, "O")
select_space(my_board, 5, "X")
select_space(my_board, 6, "O")
select_space(my_board, 7, "X")

print_board(my_board)

print(has_won(my_board, "X"))
print(has_won(my_board, "O"))

#---------tic_tac_toe.py---------------

def print_board(board):
    print("|-------------|")
    print("| Tic Tac Toe |")
    print("|-------------|")
    print("|             |")
    print("|    " + board[0][0] + " " + board[0][1] + " " + board[0][2] + "    |")
    print("|    " + board[1][0] + " " + board[1][1] + " " + board[1][2] + "    |")
    print("|    " + board[2][0] + " " + board[2][1] + " " + board[2][2] + "    |")
    print("|             |")
    print("|-------------|")
    print()


def select_space(board, move, turn):
    if move not in range(1,10):
        return False
    row = int((move-1)/3)
    col = (move-1)%3
    if board[row][col] != "X" and board[row][col] != "O":
        board[row][col] = turn
        return True
    else:
        return False

def available_moves(board):
    moves = []
    for row in board:
        for col in row:
            if col != "X" and col != "O":
                moves.append(int(col))
    return moves

def has_won(board, player):
    for row in board:
        if row.count(player) == 3:
            return True
    for i in range(3):
        if board[0][i] == player and board[1][i] == player and board[2][i] == player:
            return True
    if board[0][0] == player and board[1][1] == player and board[2][2] == player:
        return True
    if board[0][2] == player and board[1][1] == player and board[2][0] == player:
        return True
    return False
    
'''

MINIMAX
Detecting Tic-Tac-Toe Leaves
An essential step in the minimax function is evaluating the strength of a leaf. If the game gets to a certain leaf, we want to know if that was a better outcome for player "X" or for player "O".

Here’s one potential evaluation function: a leaf where player "X" wins evaluates to a 1, a leaf where player "O" wins evaluates to a -1, and a leaf that is a tie evaluates to 0.

Let’s write this evaluation function for our game of Tic-Tac-Toe.

First, we need to detect whether a board is a leaf — we need know if the game is over. A game of Tic-Tac-Toe is over if either player has won, or if there are no more open spaces. We can write a function that uses has_won() and available_moves() to check to see if the game is over.

If the game is over, we now want to evaluate the state of the board. If "X" won, the board should have a value of 1. If "O" won, the board should have a value of -1. If neither player won, it was a tie, and the board should have a value of 0.

Instructions
1.
At the bottom of script.py, create a function called game_is_over() that takes a board as a parameter. The function should return True if the game is over and False otherwise.


Return True if has_won(board, "X") or has_won(board, "O") or if the length of available_moves(board) is 0.

2.
We’ve given you four different boards to test your function. Call game_is_over() on the boards start_board, x_won, o_won, and tie. Print the result of each.


One of these function calls should look like this:
'''
print(game_is_over(start_board))

'''
Your results from these four print statements should be False, True, True, True.

3.
Let’s write another function called evaluate_board() that takes board as a parameter. This function will only ever be called if we’ve detected the game is over. The function should return a 1 if "X" won, a -1 if "O" won, and a 0 otherwise.


Inside this function return 1 if has_won(board, "X") is True. Make a similar function call to see if "O" has won. If neither player has won, it’s a tie so return 0.

4.
Test your function on the four different boards! For each board, write an if statement checking if the game is over. If it is, evaluate the board and print the result. You just wrote the base case of the minimax algorithm!


Your code for one of the boards should look like this:
'''
if game_is_over(tie):
  print(evaluate_board(tie))

#--------script.py---------------
from tic_tac_toe import *

start_board = [
	["1", "2", "3"],
	["4", "5", "6"],
	["7", "8", "9"]
]

x_won = [
	["X", "O", "3"],
	["4", "X", "O"],
	["7", "8", "X"]
]

o_won = [
	["O", "X", "3"],
	["O", "X", "X"],
	["O", "8", "9"]
]

tie = [
	["X", "X", "O"],
	["O", "O", "X"],
	["X", "O", "X"]
]

test1 = [
	["1", "2", "O"],
	["O", "5", "X"],
	["X", "O", "X"]
]

test2 = [
	["X", "2", "O"],
	["O", "5", "X"],
	["X", "8", "X"]
]

test3 = [
	["1", "X", "O"],
	["3", "O", "X"],
	["7", "O", "X"]
]

test4 = [
	["X", "X", "3"],
	["O", "5", "6"],
	["X", "8", "9"]
]


def game_is_over(board):
  if not available_moves(board):
    return True
  if has_won(board, "O") or has_won(board, "X"):
    return True
  return False

print(game_is_over(start_board))
print(game_is_over(x_won))
print(game_is_over(o_won))
print(game_is_over(tie))

def evaluate_board(board):
  if game_is_over(board):
    if has_won(board, "O"):
      return -1
    elif has_won(board, "X"):
      return 1
  else:
    return 0
  
print(evaluate_board(start_board))
print(evaluate_board(x_won))
print(evaluate_board(o_won))
print(evaluate_board(tie))

#---------------tic_tac_toe.py---------------

def print_board(board):
    print("|-------------|")
    print("| Tic Tac Toe |")
    print("|-------------|")
    print("|             |")
    print("|    " + board[0][0] + " " + board[0][1] + " " + board[0][2] + "    |")
    print("|    " + board[1][0] + " " + board[1][1] + " " + board[1][2] + "    |")
    print("|    " + board[2][0] + " " + board[2][1] + " " + board[2][2] + "    |")
    print("|             |")
    print("|-------------|")
    print()


def select_space(board, move, turn):
    if move not in range(1,10):
        return False
    row = int((move-1)/3)
    col = (move-1)%3
    if board[row][col] != "X" and board[row][col] != "O":
        board[row][col] = turn
        return True
    else:
        return False

def available_moves(board):
    moves = []
    for row in board:
        for col in row:
            if col != "X" and col != "O":
                moves.append(int(col))
    return moves

def has_won(board, player):
    for row in board:
        if row.count(player) == 3:
            return True
    for i in range(3):
        if board[0][i] == player and board[1][i] == player and board[2][i] == player:
            return True
    if board[0][0] == player and board[1][1] == player and board[2][2] == player:
        return True
    if board[0][2] == player and board[1][1] == player and board[2][0] == player:
        return True
    return False
    
'''MINIMAX
Copying Boards
One of the central ideas behind the minimax algorithm is the idea of exploring future hypothetical board states. Essentially, we’re saying if we were to make this move, what would happen. As a result, as we’re implementing this algorithm in our code, we don’t want to actually make our move on the board. We want to make a copy of the board and make the move on that one.

Let’s look at how copying works in Python. Let’s say we have a board that looks like this
'''
my_board = [
    ["X", "2", "3"],
    ["O", "O", "6"],
    ["X", "8", "9"]
]'''
If we want to create a copy of our board our first instinct might be to do something like this
'''
new_board = my_board'''
This won’t work the way we want it to! Python objects are saved in memory, and variables point to a location in memory. In this case, new_board, and my_board are two variables that point to the same object in memory. If you change a value in one, it will change in the other because they’re both pointing to the same object.

One way to solve this problem is to use the deepcopy() function from Python’s copy library.
'''
new_board = deepcopy(my_board)'''
new_board is now a copy of my_board in a different place in memory. When we change a value in new_board, the values in my_board will stay the same!'''



from tic_tac_toe import *
from copy import deepcopy

my_board = [
	["1", "2", "X"],
	["4", "5", "6"],
	["7", "8", "9"]
]

#copy the board in a wrong way
new_board = my_board

print_board(my_board)

print_board(new_board)

select_space(new_board, 5,"O")

print_board(my_board)

print_board(new_board)

#copy the board in a right way
new_board = deepcopy(my_board)

select_space(new_board, 7,"X")

print_board(my_board)

print_board(new_board)

   
'''
MINIMAX
The Minimax Function
We’re now ready to dive in and write our minimax() function. The result of this function will be the “value” of the best possible move. In other words, if the function returns a 1, that means a move exists that guarantees that "X" will win. If the function returns a -1, that means that there’s nothing that "X" can do to prevent "O" from winning. If the function returns a 0, then the best "X" can do is force a tie (assuming "O" doesn’t make a mistake).

Our minimax() function has two parameters. The first is the game state that we’re interested in finding the best move. When the minimax() function first gets called, this parameter is the current state of the game. We’re asking “what is the best move for the current player right now?”

The second parameter is a boolean named is_maximizing representing whose turn it is. If is_maximizing is True, then we know we’re working with the maximizing player. This means when we’re picking the “best” move from the list of moves, we’ll pick the move with the highest value. If is_maximizing is False, then we’re the minimizing player and want to pick the minimum value.

Let’s start writing our minimax() function.

Instructions
1.
We’ve started the minimax() function for you and included the base case we wrote a few exercises ago.

We now need to define what should happen if the node isn’t a leaf.

We’ll want to set up some variables that are different depending on whether is_maximizing is True or False.

Below the base case, write an if statement to check if is_maximizing is True.

Inside the if statement, create a variable named best_value. Since we’re working with the maximizing player right now, this is the variable that will keep track of the highest possible value from all of the potential moves.

Right now, we haven’t looked at any moves, so we should start best_value at something lower than the lowest possible value — -float("Inf").

Write an else statement. Inside this else statement we’ll be setting up variables for the minimizing player. In this case, best_value should start at float("Inf").

Return best_value after the else statement.


Your if statement should look like this:

if is_maximizing:
  # Set the initial value of best_value for the maximizing player.
else:
  # Set the initial value of best_value for the minimizing player.
2.
We now want to loop through all of the possible moves of input_board before the return statement. We’re looking to find the best possible move.

Remember, you can get all of the possible moves by calling available_moves() using input_board as a parameter.

After the else statement, but before you return best_value, loop through all of the possible moves and print each move.

Let’s call our function to see these print statements. Outside of your function definition, call minimax() using the parameters x_winning (the board we’re using) and True (we’re calling it as the maximizing player).


Your for loop might look something like this:
'''
for move in available_moves(input_board):'''
You’d then want to print(move) inside the for loop.

3.
Delete the print statements for move. Rather than just printing the move in this for loop, let’s create a copy of the game board and make the move!

Inside the for loop, create a deepcopy of input_board named new_board.

Apply the move to new_board by calling the select_space() function. select_space() takes three parameters.

The board you want to use (new_board)
The space you’re selecting (the move from the for loop)
The symbol you’re filling the space in with. This is different depending on whether we’re the maximizing or minimizing player. In your if and else statements, create a variable named symbol. symbol should be "X" if we’re the maximizing player and "O" if we’re the minimizing player. Use symbol as the third parameter of select_space().
To help us check if you accurately made the move, return new_board outside the for loop (instead of returning best_move). We’ll fix that return statement soon!


Inside the loop, make a copy of the board:
'''
new_board = deepcopy(input_board)'''
Finally, make the move on the new board. Make sure to fill in the third parameter with the correct symbol.
'''
select_space(new_board, move, ___)'''
Outside the for loop, return new_board.'''

from tic_tac_toe import *
from copy import deepcopy

x_winning = [
	["X", "2", "O"],
	["4", "O", "6"],
	["7", "8", "X"]
]

def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
  if has_won(board, "X"):
    return 1
  elif has_won(board, "O"):
    return -1
  else:
    return 0

print(available_moves(x_winning))
  
def minimax(input_board, is_maximizing):
  # Base case - the game is over, so we return the value of the board
  if game_is_over(input_board):
    return evaluate_board(input_board)
  
  if is_maximizing:
    best_value = -float('inf')
    symbol = "X"
  else:
    best_value = float('inf')
    symbol = "O"
  
  for move in available_moves(input_board):
    new_board = deepcopy(input_board)
    select_space(new_board, move, symbol)
      
  return new_board

minimax(x_winning, True)

'''
MINIMAX
Recursion In Minimax
Nice work! We’re halfway through writing our minimax() function — it’s time to make the recursive call.

We have our variable called best_value . We’ve made a hypothetical board where we’ve made one of our potential moves. We now want to know whether the value of that board is better than our current best_value.

In order to find the value of the hypothetical board, we’ll call minimax(). But this time our parameters are different! The first parameter isn’t the starting board. Instead, it’s new_board, the hypothetical board that we just made.

The second parameter is dependent on whether we’re the maximizing or minimizing player. If is_maximizing is True, then the new parameter should be false False. If is_maximizing is False, then we should give the recursive call True.

It’s like we’re taking the new board, passing it to the other player, and asking “what would the value of this board be if we gave it to you?”

To give the recursive call the opposite of is_maximizing, we can give it not is_maximizing.

That call to minimax() will return the value of the hypothetical board. We can then compare the value to our best_value. If the value of the hypothetical board was better than best_value, then we should make that value the new best_value.

Instructions
1.
Let’s make that recursive call!

Inside the for loop after calling select_space(), create a variable named hypothetical_value and set it equal to minimax() using the parameters new_board and not is_maximizing.

To help us check if you did this correctly, return hypothetical_value instead of best_value. We’ll change that return statement soon!


Fill in the correct parameters.`
'''
hypothetical_value = minimax(____, ____)'''
2.
Now that we have hypothetical_value we want to see if it is better than best_value.

Inside the for loop, write another set of if/else statements checking to see if is_maximizing is True or False

If is_maximizing is True, then best_value should become the value of hypothetical_value if hypothetical_value is greater than best_value.

If is_maximizing is False, then best_value should become the value of hypothetical_value if hypothetical_value is less than best_value.

Switch your return statements back to returning best_value.


Your code for the maximizing case should look like this
'''
if is_maximizing == True:
  if ____ > ____:
    best_value = hypothetical_value'''
3.
Wow! Great work, our minimax function is done. We’ve set up a couple of boards for you. Call minimax() three different times on the boards x_winning, and o_winning. In each of those boards, it’s "X"‘s turn, so set is_maximizing to True.

Print the results of each. What does it mean if the result is a -1, 0 or 1?

You can also try running minimax() on new_game. This might take a few seconds — the algorithm needs to traverse the entire game tree to reach the leaves!


One of these function calls should be the following:

print(minimax(x_winning, True))'''

from tic_tac_toe import *
from copy import deepcopy

def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
  if has_won(board, "X"):
    return 1
  elif has_won(board, "O"):
    return -1
  else:
    return 0

new_game = [
	["1", "2", "3"],
	["4", "5", "6"],
	["7", "8", "9"]
]

x_winning = [
	["X", "2", "O"],
	["4", "O", "6"],
	["7", "8", "X"]
]

o_winning = [
	["X", "X", "O"],
	["4", "X", "6"],
	["7", "O", "O"]
]

def minimax(input_board, is_maximizing):
  # Base case - the game is over, so we return the value of the board
  if game_is_over(input_board):
    return evaluate_board(input_board)
  if is_maximizing == True:
    best_value = -float("Inf")
    symbol = "X"
  else:
    best_value = float("Inf")
    symbol = "O"
  for move in available_moves(input_board):
    new_board = deepcopy(input_board)
    select_space(new_board, move, symbol)
    hypothetical_value = minimax(new_board, not is_maximizing)
    if is_maximizing:
      if hypothetical_value > best_value:
        best_value = hypothetical_value
    else:
      if hypothetical_value < best_value:
        best_value = hypothetical_value
    
  return best_value


print(minimax(new_game, True))
print(minimax(x_winning, True))
print(minimax(o_winning, True))


'''
MINIMAX
Which Move?
Right now our minimax() function is returning the value of the best possible move. So if our final answer is a 1, we know that "X" should be able to win the game. But that doesn’t really help us — we know that "X" should win, but we aren’t keeping track of what move will cause that!

To do this, we need to make two changes to our algorithm. We first need to set up a variable to keep track of the best move (let’s call it best_move). Whenever the algorithm finds a new best_value, best_move variable should be updated to be whatever move resulted in that value.

Second, we want the algorithm to return best_move at the very end. But in order for the recursion to work, the algorithm is dependent on returning best_value. To fix this, we’ll now return a list of two numbers — [best_value, best_move].

Let’s edit our minimax function to keep track of which move leads to the best possible value!

Instructions
1.
Instead of returning just the value, we’re going to return a list that looks like [best_value, best_move].

We haven’t kept track of the best move yet, so for now, let’s change both of our return statements to be [best_value, ""]. This includes the base case! The base case should return [evaluate_board(input_board), ""]

We also need to make sure when we’re setting hypothetical_value, we’re setting it equal to the value — not the entire list. The recursive call should now look like this.
'''
minimax(new_board, not is_maximizing)[0]'''

After changing the return statements to be a list, add [0] to the end of your lines of code where you define hypothetical_value. It should look like this:
'''
hypothetical_value = solution(new_board, False)[0]'''
2.
Let’s now keep track of which move was best.

Right after the base case, create a variable named best_move. Set it equal to the empty string ("")

For both the maximizing case and the minimizing case, if we’ve found a new best_value, we should also update best_move. Inside those two if statements, set best_move equal to your variable from your for loop (ours is named move). We’re now remembering which move goes with the best possible value.

Change your last return statement. Instead of returning [best_value, ""], return [best_value, best_move].


One of your if statements should look like this:
'''
if is_maximizing == True and hypothetical_value > best_value:
  best_value = hypothetical_value
  best_move = move'''
3.
Call your function on x_winning, and o_winning as the maximizing player. Print the results. What does the return value tell you now?

You can also try it on new_game. This might take a few seconds.


One of the function calls should look like this:

print(minimax(x_winning, True))
This will return a list of two items. The second item in the list is the optimal move!'''


from tic_tac_toe import *
from copy import deepcopy

def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
  if has_won(board, "X"):
    return 1
  elif has_won(board, "O"):
    return -1
  else:
    return 0

new_game = [
	["1", "2", "3"],
	["4", "5", "6"],
	["7", "8", "9"]
]

x_winning = [
	["X", "2", "O"],
	["4", "O", "6"],
	["7", "8", "X"]
]

o_winning = [
	["X", "X", "O"],
	["4", "X", "6"],
	["7", "O", "O"]
]

def minimax(input_board, is_maximizing):
  # Base case - the game is over, so we return the value of the board
  if game_is_over(input_board):
    return [evaluate_board(input_board),'']
  best_move = ""
  if is_maximizing == True:
    best_value = -float("Inf")
    symbol = "X"
  else:
    best_value = float("Inf")
    symbol = "O"
  for move in available_moves(input_board):
    new_board = deepcopy(input_board)
    select_space(new_board, move, symbol)
    hypothetical_value = minimax(new_board, not is_maximizing)[0]
    if is_maximizing == True and hypothetical_value > best_value:
      best_value = hypothetical_value
      best_move = move
    if is_maximizing == False and hypothetical_value < best_value:
      best_value = hypothetical_value
      best_move = move
  return [best_value, best_move]

print(minimax(x_winning,True))
print(minimax(o_winning,True))
print(minimax(new_game,True))

'''
MINIMAX
Play a Game
Amazing! Our minimax() function is now returning a list of [value, move]. move gives you the number you should pick to play an optimal game of Tic-Tac-Toe for any given game state.

This line of code instructs the AI to make a move as the "X" player:

select_space(my_board, minimax(my_board, True)[1], "X")
Take some time to really understand all of the parameters. Why do we pass True to minimax()? Why do we use [1] at the end of minimax()?'''
'''
Take some time to play a game against the computer. If you’re playing with "X"s, make your move as "X", and then call minimax() on the board using is_maximizing = False. The second item in that list will tell you the AI’s move. You can then enter the move for the AI as "O", make your next move as "X", and call the minimax() function again to get the AI’s next move.

You can also try having two AIs play each other. If you uncomment the code at the bottom of the file, two AI will play each other until the game is over. What do you think the result will be? The file will run for about 15 seconds before showing you the outcome of the game.'''

from tic_tac_toe import *

my_board = [
	["1", "2", "3"],
	["4", "5", "6"],
	["7", "8", "9"]
]

while not game_is_over(my_board):
  select_space(my_board, minimax(my_board, True)[1], "X")
  print_board(my_board)
  if not game_is_over(my_board):
    select_space(my_board, minimax(my_board, False)[1], "O")
    print_board(my_board)  
    
#------------tic_tac_toe.py---------------

def print_board(board):
    print("|-------------|")
    print("| Tic Tac Toe |")
    print("|-------------|")
    print("|             |")
    print("|    " + board[0][0] + " " + board[0][1] + " " + board[0][2] + "    |")
    print("|    " + board[1][0] + " " + board[1][1] + " " + board[1][2] + "    |")
    print("|    " + board[2][0] + " " + board[2][1] + " " + board[2][2] + "    |")
    print("|             |")
    print("|-------------|")
    print()


def select_space(board, move, turn):
    if move not in range(1,10):
        return False
    row = int((move-1)/3)
    col = (move-1)%3
    if board[row][col] != "X" and board[row][col] != "O":
        board[row][col] = turn
        return True
    else:
        return False

def available_moves(board):
    moves = []
    for row in board:
        for col in row:
            if col != "X" and col != "O":
                moves.append(int(col))
    return moves

def has_won(board, player):
    for row in board:
        if row.count(player) == 3:
            return True
    for i in range(3):
        if board[0][i] == player and board[1][i] == player and board[2][i] == player:
            return True
    if board[0][0] == player and board[1][1] == player and board[2][2] == player:
        return True
    if board[0][2] == player and board[1][1] == player and board[2][0] == player:
        return True
    return False
  
  
from tic_tac_toe import *
from copy import deepcopy

def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
  if has_won(board, "X"):
    return 1
  elif has_won(board, "O"):
    return -1
  else:
    return 0

def minimax(input_board, is_maximizing):
  # Base case - the game is over, so we return the value of the board
  if game_is_over(input_board):
    return [evaluate_board(input_board), ""]
  # The maximizing player
  if is_maximizing:
    # The best value starts at the lowest possible value
    best_value = -float("Inf")
    best_move = ""
    # Loop through all the available moves
    for move in available_moves(input_board):
      # Make a copy of the board and apply the move to it
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      # Recursively find your opponent's best move
      hypothetical_value = minimax(new_board, False)[0]
      # Update best value if you found a better hypothetical value
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
    return [best_value, best_move]
  # The minimizing player
  else:
    # The best value starts at the highest possible value
    best_value = float("Inf")
    best_move = ""
    # Testing all potential moves
    for move in available_moves(input_board):
      # Copying the board and making the move
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      # Passing the new board back to the maximizing player
      hypothetical_value = minimax(new_board, True)[0]
      # Keeping track of the best value seen so far
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
    return [best_value, best_move]
    
'''
MINIMAX
Review
Nice work! You implemented the minimax algorithm to create an unbeatable Tic Tac Toe AI! Here are some major takeaways from this lesson.

A game can be represented as a tree. The current state of the game is the root of the tree, and each potential move is a child of that node. The leaves of the tree are game states where the game has ended (either in a win or a tie).
The minimax algorithm returns the best possible move for a given game state. It assumes that your opponent will also be using the minimax algorithm to determine their best move.
Game states can be evaluated and given a specific score. This is relatively easy when the game is over — the score is usually a 1, -1 depending on who won. If the game is a tie, the score is usually a 0.
In our next lesson on the minimax algorithm, we’ll look at games that are more complex than Tic Tac Toe. How does the algorithm change if it’s too computationally intensive to reach the leaves of the game tree? What strategies can we use to traverse the tree in a smarter way? We’ll tackle these questions in our next lesson!'''
'''
Take a look at our Connect Four AI for a sneak preview of our next minimax lesson. In the terminal type python3 minimax.py to play against the AI.

You can make your move by typing the number of the column that you want to put your piece in.

In the code, you can change the “intelligence” of the AI by changing the parameter of play_game(). The parameter should be a number greater than 0. If you make it greater than 6 or 7, it will take the computer a long time to find their best move.

Make sure to click the Run button to save your code before running your file in the terminal!

You can also set up an AI vs AI game by commenting out play_game() and calling two_ai_game(). This function takes two parameters — the “intelligence” of each AI players. Try starting a game with a bad X player and a smart O player by calling two_ai_game(2, 6) and see what happens.

Feel free to test out more games with different AIs.'''


#-------------------minimax.py---------------

from connect_four import *

play_game(4)


#-------------------connect_four.py---------------

   from copy import deepcopy
import random
random.seed(108)

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      x_streaks = count_streaks(board, "X")
      o_streaks = count_streaks(board, "O")
      return x_streaks - o_streaks

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def minimax(input_board, is_maximizing, depth, alpha, beta):
  if game_is_over(input_board) or depth == 0:
        return [evaluate_board(input_board), ""]
  if is_maximizing:
    best_value = -float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      hypothetical_value = minimax(new_board, False, depth - 1, alpha, beta)[0]
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
      alpha = max(alpha, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]
  else:
    best_value = float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      hypothetical_value = minimax(new_board, True, depth - 1, alpha, beta)[0]
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
      beta = min(beta, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]


def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")
    print_board(board)
    if has_won(board, "X"):
        print("X won!")
    elif has_won(board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

def two_ai_game(ai1, ai2):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    my_board = []
    for x in range(BOARDWIDTH):
      my_board.append([' '] * BOARDHEIGHT)
    while not game_is_over(my_board):
      result = minimax(my_board, True, ai1, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        result = minimax(my_board, False, ai2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")
 

'''  
ADVANCED MINIMAX
Connect Four
In our first lesson on the minimax algorithm, we wrote a program that could play the perfect game of Tic-Tac-Toe. Our AI looked at all possible future moves and chose the one that would be most beneficial. This was a viable strategy because Tic Tac Toe is a small enough game that it wouldn’t take too long to reach the leaves of the game tree.

However, there are games, like Chess, that have much larger trees. There are 20 different options for the first move in Chess, compared to 9 in Tic-Tac-Toe. On top of that, the number of possible moves often increases as a chess game progresses. Traversing to the leaves of a Chess tree simply takes too much computational power.

In this lesson, we’ll investigate two techniques to solve this problem. The first technique puts a hard limit on how far down the tree you allow the algorithm to go. The second technique uses a clever trick called pruning to avoid exploring parts of the tree that we know will be useless.

Before we dive in, let’s look at the tree of a more complicated game — Connect Four!

If you’ve never played Connect Four before, the goal is to get a streak of four of your pieces in any direction — horizontally, vertically, or diagonally. You can place a piece by picking a column. The piece will fall to the lowest available row in that column.

Instructions
1.
We’ve imported a Connect Four game engine along with a board that’s in the middle of a game.

To start, let’s call the print_board() function using half_done as a parameter.


Your function call should look like this:

print_board(half_done)
These players aren’t playing very well!

2.
Call the tree_size() function using half_done and "X" as parameters. Print the result. This will show you the number of game states in the tree, assuming half_done is the root of the tree and it is "X"‘s turn.


Finish this function call :

print(tree_size(half_done, _____))
3.
Let’s make a move and see how the size of the tree changes. Let’s place an "X" in column 6. Before calling the tree_size() function, call the select_space() function with the following three parameters:

half_done — The board that you’re making the move on.
6 — The column you’re selecting.
"X" — The type of piece you’re putting in column 6.
Since "X" has taken their turn, it is now "O"‘s turn. Change the second parameter in the tree_size() function to be "O".


Complete this function to make "X" take their turn:

select_space(half_done, ____, "X")
Then make sure to change the second parameter of tree_size().'''


#--------script.py---------------

from connect_four import *

print_board(half_done)

select_space(half_done,6,"X")

print(tree_size(half_done, 'O'))


#-------------Connect_four.py---------------
from copy import deepcopy
import random
random.seed(108)

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      x_streaks = count_streaks(board, "X")
      o_streaks = count_streaks(board, "O")
      return x_streaks - o_streaks

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def minimax(input_board, is_maximizing, depth, alpha, beta):
  if game_is_over(input_board) or depth == 0:
        return [evaluate_board(input_board), ""]
  if is_maximizing:
    best_value = -float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      hypothetical_value = minimax(new_board, False, depth - 1, alpha, beta)[0]
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
      alpha = max(alpha, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]
  else:
    best_value = float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      hypothetical_value = minimax(new_board, True, depth - 1, alpha, beta)[0]
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
      beta = min(beta, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]


def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")

def two_ai_game(ai1, ai2):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    my_board = []
    for x in range(BOARDWIDTH):
      my_board.append([' '] * BOARDHEIGHT)
    while not game_is_over(my_board):
      result = minimax(my_board, True, ai1, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        result = minimax(my_board, False, ai2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

#two_ai_game(3, 4)
#play_game(5)


def tree_size(board, turn):
    if game_is_over(board):
        return 1
    count = 1
    for move in available_moves(board):
        new_board = deepcopy(board)
        if turn == "X":
            select_space(new_board, move, "X")
            count += tree_size(new_board, "O")
        else:
            select_space(new_board, move, "O")
            count += tree_size(new_board, "X")
    return count


def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game


half_done = []
for x in range(7):
  half_done.append([' '] * 6)

for i in range(6):
    if i % 2 == 0:
        select_space(half_done, 1, "X")
    else:
        select_space(half_done, 1, "O")

for i in range(6):
    if i % 2 == 0:
        select_space(half_done, 7, "X")
    else:
        select_space(half_done, 7, "O")

for i in range(6):
    if i % 2 == 0:
        select_space(half_done, 3, "X")
    else:
        select_space(half_done, 3, "O")

for i in range(6):
    if i % 2 == 0:
        select_space(half_done, 2, "X")
    else:
        select_space(half_done, 2, "O")

for i in range(5):
    if i % 2 == 0:
        select_space(half_done, 6, "X")
    else:
        select_space(half_done, 6, "O")
  

'''
ADVANCED MINIMAX
Depth and Base Case
The first strategy we’ll use to handle these enormous trees is stopping the recursion early. There’s no need to go all the way to the leaves! We’ll just look a few moves ahead.

Being able to stop before reaching the leaves is critically important for the efficiency of this algorithm. It could take literal days to reach the leaves of a game of chess. Stopping after only a few levels limits the algorithm’s understanding of the game, but it makes the runtime realistic.

In order to implement this, we’ll add another parameter to our function called depth. Every time we make a recursive call, we’ll decrease depth by 1 like so:

def minimax(input_board, minimizing_player, depth):
  # Base Case
  if game_is over(input_bopard):
    return ...
  else:
    # …
    # Recursive Call
    hypothetical_value = minimax(new_board, True, depth - 1)[0]

We’ll also have to edit our base case condition. We now want to stop if the game is over (we’ve hit a leaf), or if depth is 0.

Instructions
1.
We’ve given you a minimax() function that recurses to the leaves. Edit it so it has a third parameter named depth.

Change the recursive call to decrease depth by 1 each time.

Change your base case — the recursion should stop when the game is over or when depth is 0.


The recursive call should have a third parameter — depth - 1.

2.
Outside the function, call minimax() on new_board as the maximizing player with a depth of 3 and print the results. Remember, minimax() returns a list of two numbers. The first is the value of the best possible move, and the second is the move itself.


The minimax() function now has three parameters. Print the results like this:

print(minimax(new_board, True, 3))'''

#-----------------scripy.py-------------------
from connect_four import *
import random
random.seed(108)

new_board = make_board()

# Add a third parameter named depth
def minimax(input_board, is_maximizing, depth):
  # Change this if statement to also check to see if depth = 0
  
  if game_is_over(input_board) or depth ==0:
    return [evaluate_board(input_board), ""]
  best_move = ""
  if is_maximizing == True:
    best_value = -float("Inf")
    symbol = "X"
  else:
    best_value = float("Inf")
    symbol = "O"
  for move in available_moves(input_board):
    new_board = deepcopy(input_board)
    select_space(new_board, move, symbol)
    #Add a third parameter to this recursive call
    hypothetical_value = minimax(new_board, not is_maximizing, depth -1)[0]
    if is_maximizing == True and hypothetical_value > best_value:
      best_value = hypothetical_value
      best_move = move
    if is_maximizing == False and hypothetical_value < best_value:
      best_value = hypothetical_value
      best_move = move
  return [best_value, best_move]

print(minimax(new_board, True, 3))

#----------------------connect_four.py---------------
from copy import deepcopy

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      return 0

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")

def two_ai_game(ai1, ai2):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    my_board = []
    for x in range(BOARDWIDTH):
      my_board.append([' '] * BOARDHEIGHT)
    while not game_is_over(my_board):
      result = minimax(my_board, True, ai1, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        result = minimax(my_board, False, ai2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")
        
def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game      
    

'''
ADVANCED MINIMAX
Evaluation Function
By adding the depth parameter to our function, we’ve prevented it from spending days trying to reach the end of the tree. But we still have a problem: our evaluation function doesn’t know what to do if we’re not at a leaf. Right now, we’re returning positive infinity if Player 1 has won, negative infinity if Player 2 has won, and 0 otherwise. Unfortunately, since we’re not making it to the leaves of the board, neither player has won and we’re always returning 0. We need to rewrite our evaluation function.

Writing an evaluation function takes knowledge about the game you’re playing. It is the part of the code where a programmer can infuse their creativity into their AI. If you’re playing Chess, your evaluation function could count the difference between the number of pieces each player owns. For example, if white had 15 pieces, and black had 10 pieces, the evaluation function would return 5. This evaluation function would make an AI that prioritizes capturing pieces above all else.

You could go even further — some pieces might be more valuable than others. Your evaluation function could keep track of the value of each piece to see who is ahead. This evaluation function would create an AI that might really try to protect their queen. You could even start finding more abstract ways to value a game state. Maybe the position of each piece on a Chess board tells you something about who is winning.

It’s up to you to define what you value in a game. These evaluation functions could be incredibly complex. If the maximizing player is winning (by your definition of what it means to be winning), then the evaluation function should return something greater than 0. If the minimizing player is winning, then the evaluation function should return something less than 0.

Instructions
1.
Let’s rewrite our evaluation function for Connect Four. We’ll be editing the part of the evaluation function under the else statement. We need to define how to evaluate a board when nobody has won.

Let’s write a slightly silly evaluation function that prioritizes having the top piece of a column. If the board looks like the image below, we want our evaluation function to return 2 since the maximizing player ("X") has two more “top pieces” than "O".

A connect four board with four Xs on the top of columns and two Os on the top

For now, inside the else statement, delete the current return statement. Create two variables named num_top_x and num_top_o and start them both at 0. Return num_top_x - num_top_o.


2.
Before this new return statement, loop through every column in board. Inside that loop, loop through every square in column. You’re now looking at every square in every column going from top to bottom.

If square equals "X" add one to num_top_x and then break the inner for loop to go to the next column.


Your for loops should look like this:

for column in board:
  for square in column:
    # Checking square here
You can now check to see if square is an "X" or an "O". If it is an "X", add 1 to the proper variable and break.

if square == _____:
  num_top_x += 1
  break
3.
If square equals "O" add one to num_top_o and then break the inner for loop to go to the next column.


4.
We’ve imported three boards for you to test this function. We should first get an understanding of what these three boards look like.

Note that these boards aren’t game states you’d find in real games of Connect Four — "X" has been taking some extra turns. Nevertheless, we can still evaluate them!

Call print_board once per board — board_one, board_two, and board_three. What should the evaluation function return for those three boards?


5.
Call evaluate_board once on each board and print the results. Did we trick you with board_three?'''

#------------------script.py---------------
from connect_four import *
import random
random.seed(108)

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      num_top_x = 0
      num_top_o = 0
      for column in board:
        for square in column:
          if square == "X":
            num_top_x += 1
            break
          if square == "O":
            num_top_o += 1
            break
             
    return num_top_x - num_top_o

#print_board(board_one)
#print(board_one)

print_board(board_one)
print_board(board_two)
print_board(board_three)

print(evaluate_board(board_one))
print(evaluate_board(board_two))
print(evaluate_board(board_three))
#----------------------Connect_four.py---------------
from copy import deepcopy

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count
  
def minimax(input_board, is_maximizing, depth, alpha, beta):
  if game_is_over(input_board) or depth == 0:
        return [evaluate_board(input_board), ""]
  if is_maximizing:
    best_value = -float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      hypothetical_value = minimax(new_board, False, depth - 1, alpha, beta)[0]
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
      alpha = max(alpha, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]
  else:
    best_value = float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      hypothetical_value = minimax(new_board, True, depth - 1, alpha, beta)[0]
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
      beta = min(beta, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]

def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")

def two_ai_game(ai1, ai2):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    my_board = []
    for x in range(BOARDWIDTH):
      my_board.append([' '] * BOARDHEIGHT)
    while not game_is_over(my_board):
      result = minimax(my_board, True, ai1, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        result = minimax(my_board, False, ai2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game        
        
board_one = make_board()
select_space(board_one, 3, "X")
select_space(board_one, 2, "X")
select_space(board_one, 3, "X")
select_space(board_one, 3, "O")
select_space(board_one, 1, "O")

board_two = make_board()
select_space(board_two, 4, "O")
select_space(board_two, 4, "X")
select_space(board_two, 4, "X")
select_space(board_two, 4, "O")
select_space(board_two, 4, "X")
select_space(board_two, 4, "O")
select_space(board_two, 1, "X")
select_space(board_two, 2, "X")
select_space(board_two, 3, "X")
select_space(board_two, 5, "X")
select_space(board_two, 6, "X")
select_space(board_two, 7, "X")

board_three = make_board()
select_space(board_three, 4, "X")
select_space(board_three, 5, "X")
select_space(board_three, 6, "X")
select_space(board_three, 7, "X")
select_space(board_three, 4, "O")
select_space(board_three, 5, "O")
select_space(board_three, 6, "O")
select_space(board_three, 7, "X")
select_space(board_three, 1, "O")
select_space(board_three, 2, "O")
select_space(board_three, 3, "O")




'''
Alpha-Beta Pruning

While examining the children of a maximizer, if v of maximizer > beta, prune the rest of the children.
While examining the children of a minimizer, if v of minimizer < alpha, prune the rest of the children.

from https://www.youtube.com/watch?v=xBXHtz4Gbdo
'''

'''
ADVANCED MINIMAX
Implement Alpha-Beta Pruning
Alpha-beta pruning is accomplished by keeping track of two variables for each node — alpha and beta. alpha keeps track of the minimum score the maximizing player can possibly get. It starts at negative infinity and gets updated as that minimum score increases.

On the other hand, beta represents the maximum score the minimizing player can possibly get. It starts at positive infinity and will decrease as that maximum possible score decreases.

For any node, if alpha is greater than or equal to beta, that means that we can stop looking through that node’s children.

To implement this in our code, we’ll have to include two new parameters in our function — alpha and beta. When we first call minimax() we’ll set alpha to negative infinity and beta to positive infinity.

We also want to make sure we pass alpha and beta into our recursive calls. We’re passing these two values down the tree.

Next, we want to check to see if we should reset alpha and beta. In the maximizing case, we want to reset alpha if the newly found best_value is greater than alpha. In the minimizing case, we want to reset beta if best_value is less than beta.

Finally, after resetting alpha and beta, we want to check to see if we can prune. If alpha is greater than or equal to beta, we can break and stop looking through the other potential moves.

Instructions
1.
Add two new parameters named alpha and beta to your minimax() function as the final two parameters. Inside your minimax() function, when you the recursive call, add alpha and beta as the final two parameters.


You’ll need to change two lines of code:

The definition of the function
The recursive call
2.
After resetting the value of best_value if is_maximizing is True, we want to check to see if we should reset alpha. Set alpha equal to the maximum of alpha and best_value. You can do this quickly by using the max() function. For example, the following line of code would set a equal to the maximum of b and c.

a = max(b, c)
Change both returns statements to include alpha as the last item in the list. For example, the base case return statement should be [evaluate_board(input_board), "", alpha].

Note that this third value in the return statement is not necessary for the algorithm — we need the value of alpha so we can check to see if you did this step correctly!


Use alpha = max(alpha, best_value) after the if statement comparing hypothetical_value and best_value.

3.
If we reset the value of best_value and is_maximizing is False, we want to set beta to be the minimum of beta and best_value. You can use the min() function this time.

In both return statements, add beta as the last item in the list. This is once again unnecessary for the algorithm, but we need it to check your code!


You can use the min() function:

beta = min(best, best_value)
4.
At the very end of the for loop, check to see if alpha is greater than or equal to beta. If that is true, break which will cause your program to stop looking through the remaining possible moves of the current game state.


Finish this block of code. It should be at the very end of the for loop

if ____ >= ____:
  break
5.
We’re going to call minimax() on board, but before we do let’s see what board looks like. Call print_board using board as a parameter.

6.
Call minimax() on board as the maximizing player and print the result. Set depth equal to 6. alpha should be -float("Inf") and beta should be float("Inf").


Finish this block of code:

print(minimax(_____, True, 6, -float("Inf"), _____))'''

#-----------------script.py---------------
from connect_four import *
import random
random.seed(108)

def minimax(input_board, is_maximizing, depth, alpha, beta):
  # Base case - the game is over, so we return the value of the board
  
  if game_is_over(input_board) or depth == 0:
    return [evaluate_board(input_board), "", alpha,beta]
  best_move = ""
  if is_maximizing == True:
    best_value = -float("Inf")
    symbol = "X"
  else:
    best_value = float("Inf")
    symbol = "O"
  for move in available_moves(input_board):
    new_board = deepcopy(input_board)
    select_space(new_board, move, symbol)
    hypothetical_value = minimax(new_board, not is_maximizing, depth - 1, alpha, beta)[0]
    if is_maximizing == True and hypothetical_value > best_value:
      best_value = hypothetical_value
      alpha = max( best_value, alpha)
      best_move = move
    if is_maximizing == False and hypothetical_value < best_value:
      best_value = hypothetical_value
      beta = min (best_value, beta)
      best_move = move
    if alpha > beta:
      break
    
  return [best_value, best_move, alpha, beta]
  
print_board(board)

print(minimax(board, True, 6, -float('Inf'), float("Inf")))


#----------------connect_four.py---------------
from copy import deepcopy

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      num_top_x = 0
      num_top_o = 0

      for col in board:
        for square in col:
          if square == "X":
            num_top_x += 1
            break
          elif square == "O":
            num_top_o += 1
            break

      return num_top_x - num_top_o


def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")

def two_ai_game(ai1, ai2):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    my_board = []
    for x in range(BOARDWIDTH):
      my_board.append([' '] * BOARDHEIGHT)
    while not game_is_over(my_board):
      result = minimax(my_board, True, ai1, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        result = minimax(my_board, False, ai2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game

board = make_board()
select_space(board, 3, "X")
select_space(board, 2, "X")
select_space(board, 3, "X")
select_space(board, 3, "O")
select_space(board, 1, "O")
select_space(board, 4, "O")

'''
ADVANCED MINIMAX
Review
Great work! We’ve now edited our minimax() function to work with games that are more complicated than Tic Tac Toe. The core of the algorithm is identical, but we’ve added two major improvements:

We wrote an evaluation function specific to our understanding of the game (in this case, Connect Four). This evaluation function allows us to stop the recursion before reaching the leaves of the game tree.
We implemented alpha-beta pruning. By cleverly detecting useless sections of the game tree, we’re able to ignore those sections and therefore look farther down the tree.
Now’s our chance to put it all together. We’ve written most of the function two_ai_game() which sets up a game of Connect Four played by two AIs. For each player, you need to call fill in the third parameter of their minimax() call.

Remember, right now our evaluation function is using a pretty bad strategy. An AI using the evaluation function we wrote will prioritize making sure its pieces are the top pieces of each column.

Do you think you could write an evaluation function that uses a better strategy? In the project for this course, you can try to write an evaluation function that can beat our AI!

Instructions
1.
Fill in the third parameter of both minimax() function calls. This parameter is the depth of the recursive call. The higher the number, the “smarter” the AI will be.

What happens if they have equal intelligence? What happens if one is significantly smarter than the other?

We suggest keeping these parameters under 7. Anything higher and the program will take a while to complete!'''

#scripty.py---------------
from connect_four import *

def two_ai_game():
    my_board = make_board()
    while not game_is_over(my_board):
      # Fill in the third parameter for the first player's "intelligence"
      result = minimax(my_board, True, 9, -float("Inf"), float("Inf"))
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        #Fill in the third parameter for the second player's "intelligence"
        result = minimax(my_board, False, 2, -float("Inf"), float("Inf"))
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

two_ai_game()

#connect_four.py---------------

from copy import deepcopy
import random
random.seed(108)

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      num_top_x = 0
      num_top_o = 0

      for col in board:
        for square in col:
          if square == "X":
            num_top_x += 1
            break
          elif square == "O":
            num_top_o += 1
            break

      return num_top_x - num_top_o

def minimax(input_board, is_maximizing, depth, alpha, beta):
  if game_is_over(input_board) or depth == 0:
        return [evaluate_board(input_board), ""]
  if is_maximizing:
    best_value = -float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      hypothetical_value = minimax(new_board, False, depth - 1, alpha, beta)[0]
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
      alpha = max(alpha, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]
  else:
    best_value = float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      hypothetical_value = minimax(new_board, True, depth - 1, alpha, beta)[0]
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
      beta = min(beta, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]

def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")



def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game



'''
MACHINE LEARNING: SUPERVISED LEARNING 🤖
Build Your Own Connect Four AI
Now that you’ve gotten a sense of how the minimax algorithm works, it’s time to try to create an unstoppable Connect Four AI. To create the smartest AI possible, you’ll write your own evaluation function. Challenge the AI that we’ve written to see if you can become the Codecademy Connect Four champion!

If you get stuck during this project, check out the project walkthrough video which can be found in the help menu.

Tasks
12/13Complete
Mark the tasks as complete by checking them off
Changing Strategies
1.
We’ve imported the Connect Four game engine along with the completed minimax() function that we wrote during the lesson — including alpha-beta pruning.

However, notice that we added a new parameter to the minimax() function. The last parameter now represents the name of the evaluation function that you want to run. This essentially lets you swap out the “strategy” of your AI.

Right now in two_ai_game() player "X" is using the evaluation function codecademy_evaluate_board. This is our secret evaluation function that you’re going to try to beat.

For right now, let’s finish the two_ai_game() function. The second player needs to know which evaluation function to use. Right now, we only have one — codecademy_evaluate_board.

Fill that in as the last parameter of the second minimax call and run your code to see two AIs with the same strategy play each other. Remember, changing the third parameter will change how far down the game tree the algorithm looks. Changing this parameter will change the “intelligence” of the AI.


2.
Let’s prove to ourselves we can easily change an AI’s strategy. Write a function called random_eval() that takes board as a parameter.

random_eval() isn’t going to use any strategy. It’s just going to return a random number between -100 and 100. You can get a random number between -100 and 100 by using random.randint(-100, 100).

Remember, a good evaluation function should return a large positive number if it looks like "X" is winning and a large negative number if "O" is winning. This evaluation function will be pretty useless!


3.
Let’s replace the "X" player’s strategy with this new random one. Change the last parameter in "X"‘s minimax call to random_eval.

Which player do you expect to win now?


Make Your Own Strategy
4.
It’s now time to write your own strategy for your AI. We’ll help you get started, but we want to see what you can come up with. To begin, create a function named my_evaluate_board() that takes board as a parameter.


5.
Let’s first see if either player has won the game. You can use the has_won() function to check to see if a player has won.

has_won() takes two parameters — board and "X" or "O".

If "X" has won the game, return float("Inf"). If "O" has won the game, return -float("Inf").


6.
Now we need to figure out how to evaluate a board if neither player has won. This is where things get a little trickier.

If the game isn’t over, a good strategy would be to have more streaks of two or streaks of three than your opponent. Having these streaks means that you’re closer to getting a streak of four and winning the game!

One of the trickiest part of counting streaks is that they can happen in eight different directions — up, up-right, right, down-right, down, down-left, left, and up-left.

For now, let’s just keep track of streaks to the right.

Inside your function after the if statements, create two variables named x_two_streak and o_two_streak and start them at 0.

7.
Now we want to loop through every space on the board and see if there’s the same symbol to the right. First, let’s set up a loop that goes through every piece:
'''
for col in range(len(board)):
  for row in range(len(board[0])):
    # Check the value of board[col][row]
    '''
As this loop runs, we’ll look through each piece of the board starting at the top of the left-most column. The loop will go down that column until it reaches the bottom of the board and then jumps to the top of the second column.

If board[col][row] and board[col + 1][row] are both "X", then we’ve found a streak of two going to the right. You should increase x_two_streak by one.

Do the same for o_two_streak.

One thing to note is that we don’t want to check the final column, because that column doesn’t have a column to the right. So the outer for loop should actually look like this: for col in range(len(board) -1).


8.
Finally, after finding the "X" streaks and the "O" streaks, what should we do with them? Well, if "X" has more streaks, that means they are winning and we should return a positive number. If "O" has more streaks, we should return a negative number.

Returning x_two_streak - o_two_streak will do exactly this!

Testing Our Evaluation Function
9.
Now that we’ve written this evaluation function, let’s test it to make sure it’s working correctly. To begin, comment out the function call of two_ai_game() at the bottom of your code. We don’t want to play a full game until we know our evaluation function is working correctly.

Next, create a new board named new_board by calling the make_board() function. You can see we do this at the top of two_ai_game().


10.
Make a few moves on the board. You can do this using the select_space() function. The following code would put an "X" in column 6:

'''
 select_space(new_board, 6, "X")
 '''
You can print out the board using print_board(new_board). Make enough moves so there are a couple of "X" two streaks and a couple "O" two streaks.

11.
Print the result of my_evaluate_board(new_board). Is it what you expected?

You might want to also set up the board so "X" or "O" wins to make sure those conditions are working correctly too.

12.
Assuming your function is working correctly, let’s plug this strategy into our two_ai_game() function.

Find the call of minimax for the "X" player and make the last parameter my_evaluate_board.

Uncomment your call to two_ai_game() and watch your AI play ours! Feel free to adjust that third parameter to make either AI more or less “intelligent”.

Expand Your Evaluation Function
13.
You have a good foundation for your evaluation function, but there’s so much more you can do! Here are some ideas on how to expand your function:

Check for streaks in all eight directions.
Weight streaks of three higher than streaks of two.
Only count streaks if your opponent hasn’t blocked the possibility of a Connect Four. For example, if there’s an "X" streak of two to the right, but the next column over has an "O", then that streak is useless.
Only count streaks if there’s enough board space to make a Connect Four. For example, there’s no need to check for left streaks of two in the second column from the left. Even if there is a streak of two, you can’t make a Connect Four going to the left, so the streak is useless.
Testing your evaluation function on test boards is critically important before plugging it into the two_ai_game() function. Make sure you know that your function is working how you expect it to before challenging our AI.

In the hint, we’ll show you the code for our evaluation function. We strongly recommend trying to create your own function before looking at ours!


Here’s the code for our evaluate_board() function. Most of the work is done in the helper function count_streaks().

count_streaks() loops through every square in the board. For every square, it looks 4 squares out in all eight directions. If none of those squares contain the opposite player’s symbol, then we know it’s possible to make a Connect Four in that direction.

We then add the number of times our symbol appears in those 4 squares. Essentially we’re saying “it’s possible to make a Connect Four in this direction and we have 1, 2, or 3 out of the four necessary pieces already placed.
'''
def evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      x_streaks = count_streaks(board, "X")
      o_streaks = count_streaks(board, "O")
      return x_streaks - o_streaks

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

#------------------script.py---------------

from connect_four import *

def two_ai_game():
    my_board = make_board()
    while not game_is_over(my_board):
      #The "X" player finds their best move.
      result = minimax(my_board, True, 3, -float("Inf"), float("Inf"), codecademy_evaluate_board)
      print( "X Turn\nX selected ", result[1])
      print(result[1])
      select_space(my_board, result[1], "X")
      print_board(my_board)

      if not game_is_over(my_board):
        #The "O" player finds their best move
        result = minimax(my_board, False, 6,-float("Inf"), float("Inf"), my_evaluate_board)
        print( "O Turn\nO selected ", result[1])
        print(result[1])
        select_space(my_board, result[1], "O")
        print_board(my_board)
    if has_won(my_board, "X"):
        print("X won!")
    elif has_won(my_board, "O"):
        print("O won!")
    else:
        print("It's a tie!")

def random_eval(board):
  return random.randint(-100,100)

def my_evaluate_board(board):
  x_two_streak = 0
  o_two_streak = 0  
  if has_won(board, "X"):
    return float('Inf')
  if has_won(board, 'O'):
    return -float('Inf')
  for col in range(len(board)-1):
    for row in range(len(board[0])):
      if board[col][row] == "X":
        if board[col + 1 ][row] == "X":
          x_two_streak += 1
      if board[col][row] == "O":
        if board[col + 1 ][row] == "O":
          o_two_streak += 1
  return x_two_streak - o_two_streak

new_board = make_board()
select_space(new_board, 6, "X")
select_space(new_board, 5, "X")
select_space(new_board, 1, "O")
select_space(new_board, 2, "O")
select_space(new_board, 1, "X")
select_space(new_board, 2, "X")
select_space(new_board, 5, "O")
select_space(new_board, 6, "O")
select_space(new_board, 1, "O")
select_space(new_board, 2, "O")

print_board(new_board)

print(my_evaluate_board(new_board))        
    
two_ai_game()

#----------------connect_four.py---------------
from copy import deepcopy
import random
random.seed(108)

def print_board(board):
    print()
    print(' ', end='')
    for x in range(1, len(board) + 1):
        print(' %s  ' % x, end='')
    print()

    print('+---+' + ('---+' * (len(board) - 1)))

    for y in range(len(board[0])):
        print('|   |' + ('   |' * (len(board) - 1)))

        print('|', end='')
        for x in range(len(board)):
            print(' %s |' % board[x][y], end='')
        print()

        print('|   |' + ('   |' * (len(board) - 1)))

        print('+---+' + ('---+' * (len(board) - 1)))

def select_space(board, column, player):
    if not move_is_valid(board, column):
        return False
    if player != "X" and player != "O":
        return False
    for y in range(len(board[0])-1, -1, -1):
        if board[column-1][y] == ' ':
            board[column-1][y] = player
            return True
    return False

def board_is_full(board):
    for x in range(len(board)):
        for y in range(len(board[0])):
            if board[x][y] == ' ':
                return False
    return True

def move_is_valid(board, move):
    if move < 1 or move > (len(board)):
        return False

    if board[move-1][0] != ' ':
        return False

    return True

def available_moves(board):
    moves = []
    for i in range(1, len(board)+1):
        if move_is_valid(board, i):
            moves.append(i)
    return moves

def has_won(board, symbol):
    # check horizontal spaces
    for y in range(len(board[0])):
        for x in range(len(board) - 3):
            if board[x][y] == symbol and board[x+1][y] == symbol and board[x+2][y] == symbol and board[x+3][y] == symbol:
                return True

    # check vertical spaces
    for x in range(len(board)):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x][y+1] == symbol and board[x][y+2] == symbol and board[x][y+3] == symbol:
                return True

    # check / diagonal spaces
    for x in range(len(board) - 3):
        for y in range(3, len(board[0])):
            if board[x][y] == symbol and board[x+1][y-1] == symbol and board[x+2][y-2] == symbol and board[x+3][y-3] == symbol:
                return True

    # check \ diagonal spaces
    for x in range(len(board) - 3):
        for y in range(len(board[0]) - 3):
            if board[x][y] == symbol and board[x+1][y+1] == symbol and board[x+2][y+2] == symbol and board[x+3][y+3] == symbol:
                return True

    return False


def game_is_over(board):
  return has_won(board, "X") or has_won(board, "O") or len(available_moves(board)) == 0

def codecademy_evaluate_board(board):
    if has_won(board, "X"):
      return float("Inf")
    elif has_won(board, "O"):
      return -float("Inf")
    else:
      x_streaks = count_streaks(board, "X")
      o_streaks = count_streaks(board, "O")
      return x_streaks - o_streaks

def count_streaks(board, symbol):
    count = 0
    for col in range(len(board)):
        for row in range(len(board[0])):
            if board[col][row] != symbol:
                continue
            # right
            if col < len(board) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #left
            if col > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-right
            if col < len(board) - 3 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-right
            if col < len(board) - 3 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col + i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col + i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #up-left
            if col > 2 and row > 2:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row - i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down-left
            if col > 2 and row < len(board[0]) - 3:
                num_in_streak = 0
                for i in range(4):
                    if board[col - i][row + i] == symbol:
                        num_in_streak += 1
                    elif board[col - i][row + i] != " ":
                        num_in_streak = 0
                        break
                count += num_in_streak
            #down
            num_in_streak = 0
            if row < len(board[0]) - 3:
                for i in range(4):
                    if row + i < len(board[0]):
                        if board[col][row + i] == symbol:
                            num_in_streak += 1
                        else:
                            break
            for i in range(4):
                if row - i > 0:
                    if board[col][row - i] == symbol:
                        num_in_streak += 1
                    elif board[col][row - i] == " ":
                        break
                    else:
                        num_in_streak == 0
            if row < 3:
                if num_in_streak + row < 4:
                    num_in_streak = 0
            count += num_in_streak
    return count

def minimax(input_board, is_maximizing, depth, alpha, beta, eval_function):
  if game_is_over(input_board) or depth == 0:
        return [eval_function(input_board), ""]
  if is_maximizing:
    best_value = -float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "X")
      hypothetical_value = minimax(new_board, False, depth - 1, alpha, beta, eval_function)[0]
      if hypothetical_value > best_value:
        best_value = hypothetical_value
        best_move = move
      alpha = max(alpha, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]
  else:
    best_value = float("Inf")
    moves = available_moves(input_board)
    random.shuffle(moves)
    best_move = moves[0]
    for move in moves:
      new_board = deepcopy(input_board)
      select_space(new_board, move, "O")
      hypothetical_value = minimax(new_board, True, depth - 1, alpha, beta, eval_function)[0]
      if hypothetical_value < best_value:
        best_value = hypothetical_value
        best_move = move
      beta = min(beta, best_value)
      if alpha >= beta:
        break
    return [best_value, best_move]

def play_game(ai):
    BOARDWIDTH = 7
    BOARDHEIGHT = 6
    board = []
    for x in range(BOARDWIDTH):
      board.append([' '] * BOARDHEIGHT)
    while not game_is_over(board):
        print_board(board)
        moves = available_moves(board)
        print("Available moves: " , moves)
        choice = 100
        good_move = False
        while not good_move:
            choice = input("Select a move:\n")
            try:
                move = int(choice)
            except ValueError:
                continue
            if move in moves:
                good_move = True
        select_space(board, int(choice), "X")
        if not game_is_over(board):
          result = minimax(board, False, ai, -float("Inf"), float("Inf"))
          print("Computer chose: ", result[1])
          select_space(board, result[1], "O")

def make_board():
    new_game = []
    for x in range(7):
        new_game.append([' '] * 6)
    return new_game
    
    




#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- MACHINE LEARN NAIVE BAYES MACHINE LEARN NAIVE BAYES #-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- PROJECT PROJECT PROJECT PROJECT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

'''
Off-Platform Project: Classifying Tweets

In this off-platform project, you will use a Naive Bayes Classifier to find patterns in real tweets. We've given you three files: new_york.json, london.json, and paris.json. These three files contain tweets that we gathered from those locations.

The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris.



In this off-platform project, you will use a Naive Bayes Classifier to find patterns in real tweets. We've given you three fileInvestigate the Data¶

To begin, let's take a look at the data. We've imported new_york.json and printed the following information:
•The number of tweets.
•The columns, or features, of a tweet.
•The text of the 12th tweet in the New York dataset.

Run the cell below to see the results
'''


import pandas as pd

new_york_tweets = pd.read_json("new_york.json", lines=True)
print(len(new_york_tweets))
print(new_york_tweets.columns)
print(new_york_tweets.loc[12]["text"])


'''RESUTL
4723

Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',
       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',
       'in_reply_to_user_id', 'in_reply_to_user_id_str',
       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',
       'contributors', 'is_quote_status', 'quote_count', 'reply_count',
       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',
       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',
       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',
       'quoted_status', 'quoted_status_permalink', 'extended_entities',
       'withheld_in_countries'],
      dtype='object')
Be best #ThursdayThoughts'''

'''
In the code block below, load the London and Paris tweets into DataFrames named london_tweets and paris_tweets.

How many London tweets are there? How many Paris ones are there?'''

​london_tweets = pd.read_json('london.json', lines = True)
paris_tweets = pd.read_json('paris.json', lines = True)

print(len(london_tweets))

print(london_tweets.columns)

print(len(paris_tweets))

print(paris_tweets.columns)

'''RESUTL
5341
Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',
       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',
       'in_reply_to_user_id', 'in_reply_to_user_id_str',
       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',
       'contributors', 'is_quote_status', 'extended_tweet', 'quote_count',
       'reply_count', 'retweet_count', 'favorite_count', 'entities',
       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',
       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',
       'quoted_status', 'quoted_status_permalink', 'extended_entities'],
      dtype='object')
2510
Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',
       'in_reply_to_status_id', 'in_reply_to_status_id_str',
       'in_reply_to_user_id', 'in_reply_to_user_id_str',
       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',
       'contributors', 'is_quote_status', 'quote_count', 'reply_count',
       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',
       'filter_level', 'lang', 'timestamp_ms', 'display_text_range',
       'extended_entities', 'possibly_sensitive', 'quoted_status_id',
       'quoted_status_id_str', 'quoted_status', 'quoted_status_permalink',
       'extended_tweet'],
      dtype='object')
      '''
      
'''
Classifying using language: Naive Bayes Classifier

We're going to create a Naive Bayes Classifier! Let's begin by looking at the way language is used differently in these three locations. Let's grab the text of all of the tweets and make it one big list. In the code block below, we've created a list of all the New York tweets. Do the same for london_tweets and paris_tweets.

Then combine all three into a variable named all_tweets by using the + operator. For example, all_tweets = new_york_text + london_text + ...

Let's also make the labels associated with those tweets. 0 represents a New York tweet, 1 represents a London tweet, and 2 represents a Paris tweet. Finish the definition of labels.'''

new_york_text = new_york_tweets["text"].tolist()
london_text = london_tweets['text'].tolist()
paris_text = paris_tweets['text'].tolist()

all_tweets = new_york_text + london_text + paris_text
labels = [0] * len(new_york_text) + [1]*len(london_text) + [2]*len(paris_text)

#print(new_york_text)
#print(labels)

'''Making a Training and Test Set

We can now break our data into a training set and a test set. We'll use scikit-learn's train_test_split function to do this split. This function takes two required parameters: It takes the data, followed by the labels. Set the optional parameter test_size to be 0.2. Finally, set the optional parameter random_state to 1. This will make it so your data is split in the same way as the data in our solution code. 

Remember, this function returns 4 items in this order:
1.The training data
2.The testing data
3.The training labels
4.The testing labels

Store the results in variables named train_data, test_data, train_labels, and test_labels.

Print the length of train_data and the length of test_data.'''


from sklearn.model_selection import train_test_split

train_data, test_data, train_labels, test_labels = train_test_split(all_tweets, labels, train_size = 0.8, test_size = 0.2 , random_state = 1)

print(len(train_data))

print(len(test_data))

#10059
#2515

'''
Making the Count Vectors

To use a Naive Bayes Classifier, we need to transform our lists of words into count vectors. Recall that this changes the sentence "I love New York, New York" into a list that contains:
•Two 1s because the words "I" and "love" each appear once.
•Two 2s because the words "New" and "York" each appear twice.
•Many 0s because every other word in the training set didn't appear at all.

To start, create a CountVectorizer named counter.

Next, call the .fit() method using train_data as a parameter. This teaches the counter our vocabulary.

Finally, let's transform train_data and test_data into Count Vectors. Call counter's .transform() method using train_data as a parameter and store the result in train_counts. Do the same for test_data and store the result in test_counts.

Print train_data[3] and train_counts[3] to see what a tweet looks like as a Count Vector.'''

from sklearn.feature_extraction.text import CountVectorizer

counter = CountVectorizer()

counter.fit(train_data)

train_counts = counter.transform(train_data)
test_counts = counter.transform(test_data)

print(train_data[3])
print(train_counts[3])

'''result
saying bye is hard. Especially when youre saying bye to comfort.
  (0, 5022)	2
  (0, 6371)	1
  (0, 9552)	1
  (0, 12314)	1
  (0, 13903)	1
  (0, 23994)	2
  (0, 27146)	1
  (0, 29397)	1
  (0, 30274)	1'''
  
'''Train and Test the Naive Bayes Classifier

We now have the inputs to our classifier. Let's use the CountVectors to train and test the Naive Bayes Classifier!

First, make a MultinomialNB named classifier.

Next, call classifier's .fit() method. This method takes two parameters — the training data and the training labels. train_counts contains the training data and train_labels containts the labels for that data.

Calling .fit() calculates all of the probabilities used in Bayes Theorem. The model is now ready to quickly predict the location of a new tweet. 

Finally, let's test our model. classifier's .predict() method using test_counts as a parameter. Store the results in a variable named predictions.
'''

from sklearn.naive_bayes import MultinomialNB

classifier = MultinomialNB()

classifier.fit(train_counts, train_labels)
predictions = classifier.predict(test_counts)

'''Evaluating Your Model

Now that the classifier has made its predictions, let's see how well it did. Let's look at two different ways to do this. First, call scikit-learn's accuracy_score function. This function should take two parameters — the test_labels and your predictions. Print the results. This prints the percentage of tweets in the test set that the classifier correctly classified.
'''

from sklearn.metrics import accuracy_score

print(accuracy_score(test_labels, predictions))

#0.6779324055666004

'''The other way you can evaluate your model is by looking at the confusion matrix. A confusion matrix is a table that describes how your classifier made its predictions. For example, if there were two labels, A and B, a confusion matrix might look like this:
9 1
3 5

In this example, the first row shows how the classifier classified the true A's. It guessed that 9 of them were A's and 1 of them was a B. The second row shows how the classifier did on the true B's. It guessed that 3 of them were A's and 5 of them were B's.

For our project using tweets, there were three classes — 0 for New York, 1 for London, and 2 for Paris. You can see the confustion matrix by printing the result of the confusion_matrix function using test_labels and predictions as parameters.
'''

from sklearn.metrics import confusion_matrix

print(confusion_matrix(test_labels, predictions))

'''
[[541 404  28]
 [203 824  34]
 [ 38 103 340]]
'''
'''
Test Your Own Tweet¶

Nice work! The confusion matrix should line up with your intuition. The classifier predicts tweets that were actually from New York as either New York tweets or London tweets, but almost never Paris tweets. Similarly, the classifier rarely misclassifies the tweets that were actually from Paris. Tweets coming from two English speaking countries are harder to distinguish than tweets in different languages.

Now it's your chance to write a tweet and see how the classifier works! Create a string and store it in a variable named tweet. 

Call counter's .transform() method using [tweet] as a parameter. Save the result as tweet_counts. Notice that your variable has to be in an array — .transform() can't take just a string, it must be a list. 

Finally, pass tweet_counts as parameter to classifier's .predict() method. Print the result. This should give you the prediction for the tweet. Remember a 0 represents New York, a 1 represents London, and a 2 represents Paris. Can you write different tweets that the classifier predicts as being from New York, London, and Paris?'''

tweet = "The Statue of Liberty is beautiful"
tweet_counts = counter.transform([tweet])
print(classifier.predict(tweet_counts))

#[0]


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- MACHINE LEARN KNN K-MEANS K-NEAREST MACHINE LEARN   #-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- PROJECT PROJECT PROJECT PROJECT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Off-Platform Project: Viral Tweets

In this project, we are going to use the K-Nearest Neighbor algorithm to predict whether a tweet will go viral. Before jumping into using the classifier, let's first consider the problem we're trying to solve. Which features of a tweet are most closely linked to its popularity? Maybe the number of hashtags or the number of links in the tweet strongly influences its popularity. Maybe its virality is dependent on how many followers the person has. Maybe it's something more subtle like the specific language used in the tweets.

Let's explore these options by looking at the data we have available to us. We've imported the dataset and printed the following information:
•The total number of tweets in the dataset.
•The columns, or features, of the dataset.
•The text of the first tweet in the dataset.

Some of these features are dictionaries. For example, the feature "user" is a dictionary. We might want to use some of the information found in these sub-dictionaries. Print all_tweets.loc[0]["user"] to see what the "user" feature looks like.

After printing that, try printing just the "location" found in that "user" dictionary. For example, all_tweets.loc[0]["user"]["screen_name"] would give you only the screen name associated with the first tweet.'''

import pandas as pd

all_tweets = pd.read_json("random_tweets.json", lines=True)

print(len(all_tweets))
print(all_tweets.columns)
print(all_tweets.loc[0]['text'])
print(all_tweets.loc[0]['text'].count('#'))  #hash tag count
print(len(all_tweets.loc[0]['text'].split()))       #word count
print(all_tweets.loc[0]['user']['location']) #acces to user detail

#Print the user here and the user's location here.

'''result
11099
Index(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities',
       'metadata', 'source', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo',
       'coordinates', 'place', 'contributors', 'retweeted_status',
       'is_quote_status', 'retweet_count', 'favorite_count', 'favorited',
       'retweeted', 'lang', 'possibly_sensitive', 'quoted_status_id',
       'quoted_status_id_str', 'extended_entities', 'quoted_status',
       'withheld_in_countries'],
      dtype='object')
RT @KWWLStormTrack7: We are more than a month into summer but the days are getting shorter. The sunrise is about 25 minutes later on July 3…
0
26
Waterloo, Iowa
'''

'''
Defining Viral Tweets

A K-Nearest Neighbor classifier is a supervised machine learning algorithm, and as a result, we need to have a dataset with tagged labels. For this specific example, we need a dataset where every tweet is marked as viral or not viral. Unfortunately, this isn't a feature of our dataset — we'll need to make it ourselves.

So how do we define a viral tweet? A good place to start is to look at the number of retweets the tweet has. This can be found using the feature "retweet_count". Let's say we wanted to create a column called is_viral that is a 1 if the tweet had more than 5 retweets and 0 otherwise. We could do that like this:
all_tweets['is_viral'] = np.where(all_tweets['retweet_count'] > 5, 1, 0)

Instead of using 5 as the benchmark for a viral tweet, let's use the median number of retweets. You can find that by calling the median() function on all_tweets["retweet_count"]. Print the median number of retweets to understand what this threshold is.

Print the number of viral tweets and non-viral tweets. You can do this using all_tweets['is_viral'].value_counts().

After finishing this project, consider coming back and playing with this threshold number. How do you think your model would work if it was trying to find incredibly viral tweets? For example, how would it work if it were looking for tweets with 1000 or more retweets?'''

import numpy as np

all_tweets['is_viral'] =  np.where(all_tweets['retweet_count'] > 13, 1, 0)

print(np.median(all_tweets['retweet_count']))

print(all_tweets['is_viral'].value_counts())

print(type(all_tweets))


'''result

13.0
0    5562
1    5537
Name: is_viral, dtype: int64
<class 'pandas.core.frame.DataFrame'>

'''

'''
Making Features¶

Now that we've created a label for every tweet in our dataset, we can begin thinking about which features might determine whether a tweet is viral. We can create new columns in our dataset to represent these features. For example, let's say we think the length of a tweet might be a valuable feature. The following line creates a new column containing the length of the tweet.
all_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']), axis=1)

Setting axis = 1 creates a new column rather than a new row.

Create a new column called followers_count that contains the number of followers of each user. You can find this information in tweet['user']['followers_count']. Do the same for friends_count.

For the rest of this project, we will be using these three features, but we encourage you to create your own. Here are some potential ideas for more features.
•The number of hashtags in the tweet. You can find this by looking at the text of the tweet and using the .count() function with # as a parameter.
•The number of links in the tweet. Using a similar strategy to the one above, use .count() to count the number of times http appears in the tweet.
•The number of words in the tweet. Call .split() on the text of a tweet. This will give you a list of the words in the tweet. Find the length of that list.
•The average length of the words in the tweet.

'''

all_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']), axis=1)
all_tweets['followers_count'] = all_tweets.apply(lambda tweet: tweet['user']['followers_count'], axis = 1)
all_tweets['friends_count'] = all_tweets.apply(lambda tweet: tweet['user']['friends_count'], axis = 1)
all_tweets['hashtags_count'] = all_tweets.apply(lambda tweet: tweet['text'].count('#'), axis = 1)
all_tweets['link_count'] = all_tweets.apply(lambda tweet: tweet['text'].count("http"), axis = 1)
all_tweets['word_count'] = all_tweets.apply(lambda tweet: len(tweet['text'].split()), axis = 1)

'''
Normalizing The Data

We've now made the columns that we want to feed into our classifier. Let's get rid of all the data that is no longer relevant. Create a variable named labels and set it equal to the 'is_viral' column of all_tweets.

If we had a dataframe named df we could get a single column named A like this:
one_column = df['A']

Create a variable named data and set it equal to all of the columns that you created in the last step. Those columns are tweet_length, followers_count, and friends_count.

When selecting multiple columns, the names of the columns should be in a list. Check out the example below to see how to select column A and B:
features = df[['A', 'B']]

Now create a new variable named scaled_data. scaled_data should be the result of the scale function with data as a parameter. Also include the parameter axis = 0. This scales the columns as opposed to the rows.

The scale function will normalize the data so all of the features will vary within the same range.

Print scaled_data[0] to get a sense of what our data looks like.'''


from sklearn.preprocessing import scale

labels = all_tweets['is_viral']
data = all_tweets[['tweet_length', 'followers_count', 'friends_count', 'hashtags_count', 'word_count', 'link_count']]

scaled_data = scale(data, axis = 0)

print(data['tweet_length'])
print(type(scaled_data))
print(scaled_data[0])

''' result0        140
1         77
2        140
3        140
4        140
        ... 
11094    140
11095     75
11096    140
11097    140
11098     75
Name: tweet_length, Length: 11099, dtype: int64
<class 'numpy.ndarray'>
[ 0.6164054  -0.02878298 -0.14483305 -0.32045057  1.15105133 -0.78415588]
'''

'''
Creating the Training Set and Test Set

To evaluate the effectiveness of our classifier, we now split scaled_data and labels into a training set and test set using scikit-learn's train_test_split function. This function takes two required parameters: It takes the data, followed by the labels. Set the optional parameter test_size to be 0.2. You can also set the random_state parameter so your code will randomly split the data in the same way as our solution code splits the data. We used random_state = 1. Remember, this function returns 4 items in this order:
1.The training data
2.The testing data
3.The training labels
4.The testing labels

Store the results in variables named train_data, test_data, train_labels, and test_labels.'''

from sklearn.model_selection import train_test_split

train_data, test_data, train_labels, test_labels = train_test_split(scaled_data, labels, train_size = 0.8, test_size = 0.2, random_state =1)

print(len(train_data))

print(len(test_data))

'''result
8879
2220

'''

'''

Using the Classifier

We can finally use the K-Nearest Neighbor classifier. Let's test it using k = 5. Begin by creating a KNeighborsClassifier object named classifier with the parameter n_neighbors equal to 5.

Next, train classifier by calling the .fit() method with train_data and train_labels as parameters.

Finally, let's test the model! Call classifier's .score() method using test_data and test_labels as parameters. Print the results.
'''

from sklearn.neighbors import KNeighborsClassifier

classifier = KNeighborsClassifier(n_neighbors = 5)

classifier.fit(train_data, train_labels)

print(classifier.score(test_data, test_labels))


#0.7175675675675676

'''
Choosing K

We've tested our classifier with k = 5, but maybe there's a k that will work better. Let's test many different values for k and graph the results. 

First, create an empty list called scores. Next, create a for loop that has a variable k that begins at 1 and ends at 200.

Inside the for loop, create a KNeighobrsClassifier object named classifier with the parameter n_neighbors equal to k.

Train classifier by calling the .fit() method with train_data and train_labels as parameters.

Next, let's test the model! Call classifier's .score() method using test_data and test_labels as parameters. append the result to scores.

Finally, let's plot the results. Outside of the loop, use Matplotlib's plot() function. plot() takes two parameters — the data on the x-axis and the data on the y-axis. Data on the x-axis should be the values we used for k. In this case, range(1,200). Data on the y-axis should be scores. Make sure to call the plt.show() function after calling plt.plot(). This should take a couple of seconds to run!
'''

import matplotlib.pyplot as plt

scores = []

for k in range(1,201):
    classifier = KNeighborsClassifier(n_neighbors = 5)
    classifier.fit(train_data, train_labels)
    scores.append(classifier.score(test_data, test_labels))

plt.plot(range(1,201),scores)
plt.show()
    
'''

Explore on your own

Nice work! You can see the classifier gets better as k increases, but as k gets too high, underfitting starts to happen.

By using the features tweet_length, followers_count, and friends_count, we were able to get up to around 63% accuracy. That is better than random, but still not exceptional. Can you find some different features that perform better? Share your graphs with us on Twitter and maybe it will go viral!'''

#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-Ntural Language Processing NLTK nltk #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Text Preprocessing
"You never know what you have... until you clean your data."
~ Unknown (or possibly made up)

Cleaning and preparation are crucial for many tasks, and NLP is no exception. Text preprocessing is usually the first step you’ll take when faced with an NLP task.

Without preprocessing, your computer interprets "the", "The", and "<p>The" as entirely different words. There is a LOT you can do here, depending on the formatting you need. Lucky for you, Regex and NLTK will do most of it for you! Common tasks include:

Noise removal — stripping text of formatting (e.g., HTML tags).

Tokenization — breaking text into individual words.

Normalization — cleaning text data in any other way:

Stemming is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “sing” may become “s” and “sung” would remain “sung.”
Lemmatization is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”
Other common tasks include lowercasing, stopwords removal, spelling correction, etc.'''

'''
1.
We used NLTK’s PorterStemmer to normalize the text — run the code to see how it does. (It may take a few seconds for the code to run.)

Checkpoint 2 Passed
2.
In the output terminal you’ll see our program counts "go" and "went" as different words! Also, what’s up with "mani" and "hardli"? A lemmatizer will fix this. Let’s do it.

Where lemmatizer is defined, replace None with WordNetLemmatizer().

Where we defined lemmatized, replace the empty list with a list comprehension that uses lemmatizer to lemmatize() each token in tokenized.

(Don’t know Python that well? No problem. Just check the hints for help throughout the lesson.)

Checkpoint 3 Passed

Hint
lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]
3.
Why are the lemmatized verbs like "went" still conjugated? By default lemmatize() treats every word as a noun.

Give lemmatize() a second argument: get_part_of_speech(token). This will tell our lemmatizer what part of speech the word is.

Run your code again to see the result!

Checkpoint 4 Passed

Hint
lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]'''


# regex for removing punctuation!
import re
# nltk preprocessing magic
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
# grabbing a part of speech function:
from part_of_speech import get_part_of_speech

text = "So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed."

cleaned = re.sub('\W+', ' ', text)
tokenized = word_tokenize(cleaned)

stemmer = PorterStemmer()
stemmed = [stemmer.stem(token) for token in tokenized]

## -- CHANGE these -- ##
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]

print("Stemmed text:")
print(stemmed)
print("\nLemmatized text:")
print(lemmatized)



#-- result ----------------

Stemmed text:
['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']

Lemmatized text:
['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']




'''

GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Parsing Text
You now have a preprocessed, clean list of words. Now what? It may be helpful to know how the words relate to each other and the underlying syntax (grammar). Parsing is a stage of NLP concerned with segmenting text based on syntax.

You probably do not want to be doing any parsing by hand and NLTK has a few tricks up its sleeve to help you out:

Part-of-speech tagging (POS tagging) identifies parts of speech (verbs, nouns, adjectives, etc.). NLTK can do it faster (and maybe more accurately) than your grammar teacher.

Named entity recognition (NER) helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. This can be a clue as to the topic of the text and NLTK captures many for you.

Dependency grammar trees help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect.

In English we leave a lot of ambiguity, so syntax can be tough, even for a computer program. Take a look at the following sentence:

I saw a cow under a tree with binoculars.
Do I have the binoculars? Does the cow have binoculars? Does the tree have binoculars?

Regex parsing, using Python’s re library, allows for a bit more nuance. When coupled with POS tagging, you can identify specific phrase chunks. On its own, it can find you addresses, emails, and many other common patterns within large chunks of text.

Instructions
1.
Run the code to see the silly squid sentences parsed into dependency trees visually!

Checkpoint 2 Passed
2.
Change my_sentence to a sentence of your choosing and run the code again to see it parsed out as a tree!'''


#------ script.py --------------

import spacy
from nltk import Tree
from squids import squids_text

dependency_parser = spacy.load('en')

parsed_squids = dependency_parser(squids_text)

# Assign my_sentence a new value:
my_sentence = "Your sentence goes here!"
my_parsed_sentence = dependency_parser(my_sentence)

def to_nltk_tree(node):
  if node.n_lefts + node.n_rights > 0:
    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]
    return Tree(node.orth_, parsed_child_nodes)
  else:
    return node.orth_

for sent in parsed_squids.sents:
  to_nltk_tree(sent.root).pretty_print()
  
for sent in my_parsed_sentence.sents:
 to_nltk_tree(sent.root).pretty_print()

 #-----------squids.py -------------

 squids_text = "So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed."


#-------result --------------


          went               
  _________|_________         
 |   |     to        |       
 |   |     |         |        
 |   |  dentist     day      
 |   |     |      ___|____    
 I   .    the   the     other

             saw                                     
  ____________|___________________                    
 |   |   |    |                  jump                
 |   |   |    |          _________|__________         
 |   |   |    |         |                   out      
 |   |   |    |         |                    |        
 |   |   |    |         |                    of      
 |   |   |    |         |                    |        
 |   |   |    |         |                   bag      
 |   |   |    |         |                    |        
 |   |   |  enough     one                dentist    
 |   |   |    |      ___|____           _____|_____   
 ,   I   .   Sure   an     angry       my          's

    noticed         
  _____|__________   
She  hardly even  . 

     goes         
  ____|______      
 |    |   sentence
 |    |      |     
here  !     Your  

'''

GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Language Models - Bag-of-Words Approach
How can we help a machine make sense of a bunch of word tokens? We can help computers make predictions about language by training a language model on a corpus (a bunch of example text).

Language models are probabilistic computer models of language. We build and use these models to figure out the likelihood that a given sound, letter, word, or phrase will be used. Once a model has been trained, it can be tested out on new texts.

One of the most common language models is the unigram model, a statistical language model commonly known as bag-of-words. As its name suggests, bag-of-words does not have much order to its chaos! What it does have is a tally count of each instance for each word. Consider the following text example:

The squids jumped out of the suitcases.
Provided some initial preprocessing, bag-of-words would result in a mapping like:'''

{"the": 2, "squid": 1, "jump": 1, "out": 1, "of": 1, "suitcase": 1}'''
Now look at this sentence and mapping: “Why are your suitcases full of jumping squids?”'''

{"why": 1, "be": 1, "your": 1, "suitcase": 1, "full": 1, "of": 1, "jump": 1, "squid": 1}'''
You can see how even with different word order and sentence structures, “jump,” “squid,” and “suitcase” are shared topics between the two examples. Bag-of-words can be an excellent way of looking at language when you want to make predictions concerning topic or sentiment of a text. When grammar and word order are irrelevant, this is probably a good model to use.

Instructions
1.
We’ve turned a passage from Through the Looking Glass by Lewis Carroll into a list of words (aside from stopwords, which we’ve removed) using nltk preprocessing. Run your code to see the full list.

Checkpoint 2 Passed
2.
Now let’s turn this list into a bag-of-words using Counter()!

Comment out the print statement and set bag_of_looking_glass_words equal to a call of Counter() on normalized. Print bag_of_looking_glass_words. What are the most common words?


Hint
bag_of_looking_glass_words = Counter(normalized)
3.
Try changing text to another string of your choosing and see what happens!

'''

#-------------- script.py -------------------

# importing regex and nltk
import re, nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
# importing Counter to get word counts for bag of words
from collections import Counter
# importing a passage from Through the Looking Glass
from looking_glass import looking_glass_text
# importing part-of-speech function for lemmatization
from part_of_speech import get_part_of_speech

# Change text to another string:
text = looking_glass_text

cleaned = re.sub('\W+', ' ', text).lower()
tokenized = word_tokenize(cleaned)

stop_words = stopwords.words('english')
filtered = [word for word in tokenized if word not in stop_words]

normalizer = WordNetLemmatizer()
normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]
# Comment out the print statement below
#print(normalized)

# Define bag_of_looking_glass_words & print:
bag_of_looking_glass_words = Counter(normalized)
print(bag_of_looking_glass_words)

#--------------------- looking_glass.py
looking_glass_text = """
 However, the egg only got larger and larger, and more and more human: when she had come within a few yards of it, she saw that it had eyes and a nose and mouth; and when she had come close to it, she saw clearly that it was HUMPTY DUMPTY himself. It cant be anybody else! she said to herself. Im as certain of it, as if his name were written all over his face.

It might have been written a hundred times, easily, on that enormous face. Humpty Dumpty was sitting with his legs crossed, like a Turk, on the top of a high wallsuch a narrow one that Alice quite wondered how he could keep his balanceand, as his eyes were steadily fixed in the opposite direction, and he didnt take the least notice of her, she thought he must be a stuffed figure after all.

And how exactly like an egg he is! she said aloud, standing with her hands ready to catch him, for she was every moment expecting him to fall.

Its very provoking, Humpty Dumpty said after a long silence, looking away from Alice as he spoke, to be called an eggVery!

I said you looked like an egg, Sir, Alice gently explained. And some eggs are very pretty, you know she added, hoping to turn her remark into a sort of a compliment.

Some people, said Humpty Dumpty, looking away from her as usual, have no more sense than a baby!

Alice didnt know what to say to this: it wasnt at all like conversation, she thought, as he never said anything to her; in fact, his last remark was evidently addressed to a treeso she stood and softly repeated to herself:

     Humpty Dumpty sat on a wall:
     Humpty Dumpty had a great fall.
     All the Kings horses and all the Kings men
     Couldnt put Humpty Dumpty in his place again.

That last line is much too long for the poetry, she added, almost out loud, forgetting that Humpty Dumpty would hear her.

Dont stand there chattering to yourself like that, Humpty Dumpty said, looking at her for the first time, but tell me your name and your business.

My name is Alice, but

Its a stupid enough name! Humpty Dumpty interrupted impatiently. What does it mean?

Must a name mean something? Alice asked doubtfully.

Of course it must, Humpty Dumpty said with a short laugh: my name means the shape I amand a good handsome shape it is, too. With a name like yours, you might be any shape, almost.

Why do you sit out here all alone? said Alice, not wishing to begin an argument.

Why, because theres nobody with me! cried Humpty Dumpty. Did you think I didnt know the answer to that? Ask another.

Dont you think youd be safer down on the ground? Alice went on, not with any idea of making another riddle, but simply in her good-natured anxiety for the queer creature. That wall is so very narrow!

What tremendously easy riddles you ask! Humpty Dumpty growled out. Of course I dont think so! Why, if ever I did fall offwhich theres no chance ofbut if I did Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. If I did fall, he went on, The King has promised mewith his very own mouthtoto

To send all his horses and all his men, Alice interrupted, rather unwisely.

Now I declare thats too bad! Humpty Dumpty cried, breaking into a sudden passion. Youve been listening at doorsand behind treesand down chimneysor you couldnt have known it!

I havent, indeed! Alice said very gently. Its in a book.

Ah, well! They may write such things in a book, Humpty Dumpty said in a calmer tone. Thats what you call a History of England, that is. Now, take a good look at me! Im one that has spoken to a King, I am: mayhap youll never see such another: and to show you Im not proud, you may shake hands with me! And he grinned almost from ear to ear, as he leant forwards (and as nearly as possible fell off the wall in doing so) and offered Alice his hand. She watched him a little anxiously as she took it. If he smiled much more, the ends of his mouth might meet behind, she thought: and then I dont know what would happen to his head! Im afraid it would come off!

Yes, all his horses and all his men, Humpty Dumpty went on. Theyd pick me up again in a minute, they would! However, this conversation is going on a little too fast: lets go back to the last remark but one.

Im afraid I cant quite remember it, Alice said very politely.

In that case we start fresh, said Humpty Dumpty, and its my turn to choose a subject (He talks about it just as if it was a game! thought Alice.) So heres a question for you. How old did you say you were?

Alice made a short calculation, and said Seven years and six months.

Wrong! Humpty Dumpty exclaimed triumphantly. You never said a word like it!

I though you meant How old are you? Alice explained.

If Id meant that, Id have said it, said Humpty Dumpty. 
"""

#-------------- part_of_speech.py----------
from nltk.corpus import wordnet
from collections import Counter
def get_part_of_speech(word):
  probable_part_of_speech = wordnet.synsets(word)
  pos_counts = Counter()
  pos_counts["n"] = len(  [ item for item in probable_part_of_speech if item.pos()=="n"]  )
  pos_counts["v"] = len(  [ item for item in probable_part_of_speech if item.pos()=="v"]  )
  pos_counts["a"] = len(  [ item for item in probable_part_of_speech if item.pos()=="a"]  )
  pos_counts["r"] = len(  [ item for item in probable_part_of_speech if item.pos()=="r"]  )
  
  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]
  return most_likely_part_of_speech

#------------ result -------------

 Counter({'humpty': 19, 'dumpty': 19, 'say': 19, 'alice': 16, 'name': 7, 'like': 7, 'think': 7, 'look': 6, 'im': 5, 'know': 5, 'mean': 5, 'go': 5, 'egg': 4, 'fall': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'write': 3, 'might': 3, 'sit': 3, 'one': 3, 'didnt': 3, 'take': 3, 'must': 3, 'stand': 3, 'hand': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'ask': 3, 'shape': 3, 'good': 3, 'another': 3, 'however': 2, 'large': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'long': 2, 'away': 2, 'speak': 2, 'call': 2, 'gently': 2, 'explain': 2, 'add': 2, 'turn': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupt': 2, 'course': 2, 'short': 2, 'laugh': 2, 'there': 2, 'cry': 2, 'make': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'id': 2, 'get': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'leg': 1, 'cross': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wonder': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fix': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuff': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expect': 1, 'provoke': 1, 'silence': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hop': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'address': 1, 'treeso': 1, 'softly': 1, 'repeat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forget': 1, 'hear': 1, 'chatter': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'doubtfully': 1, 'amand': 1, 'handsome': 1, 'alone': 1, 'wish': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safe': 1, 'grind': 1, 'idea': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growl': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'purse': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'promise': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'break': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listen': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'thing': 1, 'calm': 1, 'tone': 1, 'history': 1, 'england': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grin': 1, 'lean': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offer': 1, 'watch': 1, 'anxiously': 1, 'smile': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'fast': 1, 'let': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaim': 1, 'triumphantly': 1, 'word': 1, 'though': 1})

'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Language Models - N-Grams and NLM
For parsing entire phrases or conducting language prediction, you will want to use a model that pays attention to each word’s neighbors. Unlike bag-of-words, the n-gram model considers a sequence of some number (n) units and calculates the probability of each unit in a body of language given the preceding sequence of length n. Because of this, n-gram probabilities with larger n values can be impressive at language prediction.

Take a look at our revised squid example: “The squids jumped out of the suitcases. The squids were furious.”

A bigram model (where n is 2) might give us the following count frequencies:

{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}
There are a couple problems with the n gram model:

How can your language model make sense of the sentence “The cat fell asleep in the mailbox” if it’s never seen the word “mailbox” before? During training, your model will probably come across test words that it has never encountered before (this issue also pertains to bag of words). A tactic known as language smoothing can help adjust probabilities for unknown words, but it isn’t always ideal.

For a model that more accurately predicts human language patterns, you want n (your sequence length) to be as large as possible. That way, you will have more natural sounding language, right? Well, as the sequence length grows, the number of examples of each sequence within your training corpus shrinks. With too few examples, you won’t have enough data to make many predictions.

Enter neural language models (NLM)! Much recent work within NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. This deep learning approach allows computers a much more adaptive tack to processing human language.

Instructions
1.
If you run the code, you’ll see the 10 most commonly used words in Through the Looking Glass parsed with NLTK’s ngrams module — if you’re thinking this looks like a bag of words, that’s because it is one!

2.
What do you think the most common phrases in the text are? Let’s find out…

Where looking_glass_bigrams is defined, change the second argument to 2 to see bigrams. Change n to 3 for looking_glass_trigrams to see trigrams.


Hint
The ngrams() function takes two arguments: the text you want to use and the n value.

3.
Change n to a number greater than 3 for looking_glass_ngrams. Try increasing the number.

At what n are you just getting lines from poems repeated in the text? This is where there may be too few examples of each sequence within your training corpus to make any helpful predictions.'''

#--------------script.py

import nltk, re
from nltk.tokenize import word_tokenize
# importing ngrams module from nltk
from nltk.util import ngrams
from collections import Counter
from looking_glass import looking_glass_full_text

cleaned = re.sub('\W+', ' ', looking_glass_full_text).lower()
tokenized = word_tokenize(cleaned)

# Change the n value to 2:
looking_glass_bigrams = ngrams(tokenized, 2)
looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)

# Change the n value to 3:
looking_glass_trigrams = ngrams(tokenized, 3)
looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)

# Change the n value to a number greater than 3:
looking_glass_ngrams = ngrams(tokenized, 5)
looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)

print("Looking Glass Bigrams:")
print(looking_glass_bigrams_frequency.most_common(10))

print("\nLooking Glass Trigrams:")
print(looking_glass_trigrams_frequency.most_common(10))

print("\nLooking Glass n-grams:")
print(looking_glass_ngrams_frequency.most_common(10))

#--------------- result -------------

Looking Glass Bigrams:
[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]

Looking Glass Trigrams:
[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]

Looking Glass n-grams:
[(('one', 'and', 'one', 'and', 'one'), 8), (('and', 'one', 'and', 'one', 'and'), 7), (('for', 'a', 'minute', 'or', 'two'), 6), (('the', 'lion', 'and', 'the', 'unicorn'), 6), (('as', 'well', 'as', 'she', 'could'), 5), (('is', 'worth', 'a', 'thousand', 'pounds'), 4), (('the', 'walrus', 'and', 'the', 'carpenter'), 4), (('said', 'to', 'herself', 'as', 'she'), 4), (('twas', 'brillig', 'and', 'the', 'slithy'), 3), (('brillig', 'and', 'the', 'slithy', 'toves'), 3)]


'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Topic Models
We’ve touched on the idea of finding topics within a body of language. But what if the text is long and the topics aren’t obvious?

Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language. For example, one Codecademy curriculum developer used topic modeling to discover patterns within Taylor Swift songs related to love and heartbreak over time.

A common technique is to deprioritize the most common words and prioritize less frequently used terms as topics in a process known as term frequency-inverse document frequency (tf-idf). Say what?! This may sound counter-intuitive at first. Why would you want to give more priority to less-used words? Well, when you’re working with a lot of text, it makes a bit of sense if you don’t want your topics filled with words like “the” and “is.” The Python libraries gensim and sklearn have modules to handle tf-idf.

Whether you use your plain bag of words (which will give you term frequency) or run it through tf-idf, the next step in your topic modeling journey is often latent Dirichlet allocation (LDA). LDA is a statistical model that takes your documents and determines which words keep popping up together in the same contexts (i.e., documents). We’ll use sklearn to tackle this for us.

If you have any interest in visualizing your newly minted topics, word2vec is a great technique to have up your sleeve. word2vec can map out your topic model results spatially as vectors so that similarly used words are closer together. In the case of a language sample consisting of “The squids jumped out of the suitcases. The squids were furious. Why are your suitcases full of jumping squids?”, we might see that “suitcase”, “jump”, and “squid” were words used within similar contexts. This word-to-vector mapping is known as a word embedding.

Instructions
1.
Check out how the bag of words model and tf-idf models stack up when faced with a new Sherlock Holmes text!

Run the code as is to see what topics they uncover…

Checkpoint 2 Passed
2.
Tf-idf has some interesting findings, but the regular bag of words is full of words that tell us very little about the topic of the texts!

Let’s fix this. Add some words to stop_list that don’t tell you much about the topic and then run your code again. Do this until you have at least 10 words in stop_list so that the bag of words LDA model has some interesting topics.

Checkpoint 3 Passed

Hint
Some words you may want to add to the stop_list:

"say", "see", "holmes", "shall", "say", 
"man", "upon", "know", "quite", "one", 
"well", "could", "would", "take", "may", 
"think", "come", "go", "little", "must", 
"look"'''

#--------------script.py-----------------


import nltk, re
from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3
from preprocessing import preprocess_text
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# preparing the text
corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]
preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]

# Update stop_list:
stop_list = ["say", "see", "holmes", "shall", "say", 
"man", "upon", "know", "quite", "one", 
"well", "could", "would", "take", "may", 
"think", "come", "go", "little", "must", 
"look"]
# filtering topics for stop words
def filter_out_stop_words(corpus):
  no_stops_corpus = []
  for chapter in corpus:
    no_stops_chapter = " ".join([word for word in chapter.split(" ") if word not in stop_list])
    no_stops_corpus.append(no_stops_chapter)
  return no_stops_corpus
filtered_for_stops = filter_out_stop_words(preprocessed_corpus)

# creating the bag of words model
bag_of_words_creator = CountVectorizer()
bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)

# creating the tf-idf model
tfidf_creator = TfidfVectorizer(min_df = 0.2)
tfidf = tfidf_creator.fit_transform(preprocessed_corpus)

# creating the bag of words LDA model
lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)
lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)

# creating the tf-idf LDA model
lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)
lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)

print("~~~ Topics found by bag of words LDA ~~~")
for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):
  message = "Topic #{}: ".format(topic_id + 1)
  message += " ".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])
  print(message)

print("\n\n~~~ Topics found by tf-idf LDA ~~~")
for topic_id, topic in enumerate(lda_tfidf_creator.components_):
  message = "Topic #{}: ".format(topic_id + 1)
  message += " ".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])
  print(message)


#---------preprocessing.py
import nltk, re
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter

stop_words = stopwords.words('english')
normalizer = WordNetLemmatizer()

def get_part_of_speech(word):
  probable_part_of_speech = wordnet.synsets(word)
  pos_counts = Counter()
  pos_counts["n"] = len(  [ item for item in probable_part_of_speech if item.pos()=="n"]  )
  pos_counts["v"] = len(  [ item for item in probable_part_of_speech if item.pos()=="v"]  )
  pos_counts["a"] = len(  [ item for item in probable_part_of_speech if item.pos()=="a"]  )
  pos_counts["r"] = len(  [ item for item in probable_part_of_speech if item.pos()=="r"]  )
  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]
  return most_likely_part_of_speech

def preprocess_text(text):
  cleaned = re.sub(r'\W+', ' ', text).lower()
  tokenized = word_tokenize(cleaned)
  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]
  filtered = [word for word in normalized if word not in stop_words]
  return " ".join(filtered)

#----------result ---------------------
'''
~~~ Topics found by bag of words LDA ~~~
Topic #1: mr mccarthy sherlock find
Topic #2: mccarthy hand find hear
Topic #3: behind remark escape word
Topic #4: lodge woman much worm
Topic #5: mccarthy father case mr
Topic #6: note paper write eye
Topic #7: find mccarthy hand case
Topic #8: hand young find indeed
Topic #9: find mccarthy father room
Topic #10: mccarthy turner young lestrade


~~~ Topics found by tf-idf LDA ~~~
Topic #1: forward paper limb wear
Topic #2: appearance small interest gentleman
Topic #3: say holmes man mccarthy
Topic #4: biography attempt sink little
Topic #5: morning employ slip shall
Topic #6: holmes majesty king photograph
Topic #7: turn wind new old
Topic #8: holmes say upon lead
Topic #9: respectable guinea save anyone
Topic #10: whether lounge towards flush'''

'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Text Similarity
Most of us have a good autocorrect story. Our phone’s messenger quietly swaps one letter for another as we type and suddenly the meaning of our message has changed (to our horror or pleasure). However, addressing text similarity — including spelling correction — is a major challenge within natural language processing.

Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the Levenshtein distance or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.

Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and phonetic similarity (how much two words or phrases sound the same).

It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through lexical similarity (the degree to which texts use the same vocabulary and phrases). Meanwhile, semantic similarity (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished.

Instructions
1.
Assign the variable three_away_from_code a word with a Levenshtein distance of 3 from “code”. Assign two_away_from_chunk a word with a Levenshtein distance of 2 from “chunk”.'''

#-------------script.py
import nltk
# NLTK has a built-in function
# to check Levenshtein distance:
from nltk.metrics import edit_distance

def print_levenshtein(string1, string2):
  print("The Levenshtein distance from '{0}' to '{1}' is {2}!".format(string1, string2, edit_distance(string1, string2)))

# Check the distance between
# any two words here!
print_levenshtein("fart", "target")

# Assign passing strings here:
three_away_from_code = "kudi"

two_away_from_chunk = "chnuk"

print_levenshtein("code", three_away_from_code)
print_levenshtein("chunk", two_away_from_chunk)


#---------result ------------

The Levenshtein distance from 'fart' to 'target' is 3!
The Levenshtein distance from 'code' to 'kudi' is 3!
The Levenshtein distance from 'chunk' to 'chnuk' is 2!


'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Language Prediction & Text Generation
How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? Language prediction is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.

Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.

If you go the n-gram route, you will most likely rely on Markov chains to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current n-gram on hand.

For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where n is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.

A more advanced approach, using a neural language model, is the Long Short Term Memory (LSTM) model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks.

Instructions
1.
Add three short stories by your favorite author or the lyrics to three songs by your favorite artist to document1.py, document2.py, and document3.py. Then run script.py to see a short example of text prediction.

Does it look like something by your favorite author or artist?

If you accidentally close one of the files, just click the file folder in the top left corner of the code editor to find the file and re-open it.'''

#----------- script.py--------------------
import nltk, re, random
from nltk.tokenize import word_tokenize
from collections import defaultdict, deque
from document1 import training_doc1
from document2 import training_doc2
from document3 import training_doc3

class MarkovChain:
  def __init__(self):
    self.lookup_dict = defaultdict(list)
    self._seeded = False
    self.__seed_me()

  def __seed_me(self, rand_seed=None):
    if self._seeded is not True:
      try:
        if rand_seed is not None:
          random.seed(rand_seed)
        else:
          random.seed()
        self._seeded = True
      except NotImplementedError:
        self._seeded = False
    
  def add_document(self, str):
    preprocessed_list = self._preprocess(str)
    pairs = self.__generate_tuple_keys(preprocessed_list)
    for pair in pairs:
      self.lookup_dict[pair[0]].append(pair[1])
  
  def _preprocess(self, str):
    cleaned = re.sub(r'\W+', ' ', str).lower()
    tokenized = word_tokenize(cleaned)
    return tokenized

  def __generate_tuple_keys(self, data):
    if len(data) < 1:
      return

    for i in range(len(data) - 1):
      yield [ data[i], data[i + 1] ]
      
  def generate_text(self, max_length=50):
    context = deque()
    output = []
    if len(self.lookup_dict) > 0:
      self.__seed_me(rand_seed=len(self.lookup_dict))
      chain_head = [list(self.lookup_dict)[0]]
      context.extend(chain_head)
      
      while len(output) < (max_length - 1):
        next_choices = self.lookup_dict[context[-1]]
        if len(next_choices) > 0:
          next_word = random.choice(next_choices)
          context.append(next_word)
          output.append(context.popleft())
        else:
          break
      output.extend(list(context))
    return " ".join(output)

my_markov = MarkovChain()
my_markov.add_document(training_doc1)
my_markov.add_document(training_doc2)
my_markov.add_document(training_doc3)
generated_text = my_markov.generate_text()
print(generated_text)


#------------- document1.py ------------------

training_doc1 = """


"""

#------------- document2.py ------------------

training_doc1 = """


"""

#------------- document3.py ------------------

training_doc1 = """


"""

#------------ result --------------

'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Advanced NLP Topics
Believe it or not, you’ve just scratched the surface of natural language processing. There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.

Naive Bayes classifiers are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.

We’ve made enormous gains in machine translation, but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.

Some of the most life-altering applications of NLP are focused on improving language accessibility for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.

NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue.
'''

'''
.
Assign review a string with a brief review of this lesson so far. Next, run your code. Is the Naive Bayes Classifier accurately classifying your review?'''


#----------------------script.py -----------------------
from reviews import counter, training_counts
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Add your review:
review = "this one is too much general"
review_counts = counter.transform([review])

classifier = MultinomialNB()
training_labels = [0] * 1000 + [1] * 1000

classifier.fit(training_counts, training_labels)
  
neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()
pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()

if pos > 50:
  print("Thank you for your positive review!")
elif neg > 50:
  print("We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.")
else:
  print("Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?")

  
print("\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.".format(neg, pos))

# -------------reviews.py-------------------
import pickle
counter = pickle.load( open( "count_vect.p", "rb" ) )
training_counts =  pickle.load( open( "train.p", "rb" ) )

# count_vect.p and train.p are too big to import

# result 

We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.

According to our trained Naive Bayes classifier, the probability that your review was negative was 74.0% and the probability it was positive was 26.0%.




'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
Challenges and Considerations
As you’ve seen, there are a vast array of applications for NLP. However, as they say, “with great language processing comes great responsibility” (or something along those lines). When working with NLP, we have several important considerations to take into account:

Different NLP tasks may be more or less difficult in different languages. Because so many NLP tools are built by and for English speakers, these tools may lag behind in processing other languages. The tools may also be programmed with cultural and linguistic biases specific to English speakers.
What if your Amazon Alexa could only understand wealthy men from coastal areas of the United States? English itself is not a homogeneous body. English varies by person, by dialect, and by many sociolinguistic factors. When we build and train NLP tools, are we only building them for one type of English speaker?
You can have the best intentions and still inadvertently program a bigoted tool. While NLP can limit bias, it can also propagate bias. As an NLP developer, it’s important to consider biases, both within your code and within the training corpus. A machine will learn the same biases you teach it, whether intentionally or unintentionally.
As you become someone who builds tools with natural language processing, it’s vital to take into account your users’ privacy. There are many powerful NLP tools that come head-to-head with privacy concerns. Who is collecting your data? How much data is being collected and what do those companies plan to do with your data?
Instructions
1.
Test out different slang on the Naive Bayes Classifier! What happens when you use the word “lit” to mean “wonderful” or “fun”?

Is the sentiment prediction accurate? Test out different slang.'''

# code is save above

'''
GETTING STARTED WITH NATURAL LANGUAGE PROCESSING
NLP Review
Check out how much you’ve learned about natural language processing!

Natural language processing combines computer science, linguistics, and artificial intelligence to enable computers to process human languages.
NLTK is a Python library used for NLP.
Text preprocessing is a stage of NLP focused on cleaning and preparing text for other NLP tasks.
Parsing is a stage of NLP concerned with breaking up text based on syntax.
Language models are probabilistic machine models of language use for NLP comprehension tasks. Common models include bag-of-words, n-gram models, and neural language modeling.
Topic modeling is the NLP process by which hidden topics are identified given a body of text.
Text similarity is a facet of NLP concerned with semblance between instances of language.
Language prediction is an application of NLP concerned with predicting language given preceding language.
There are many social and ethical considerations to take into account when designing NLP tools.''

Instructions
You can build a lot of fun tools with NLP knowledge and a bit of Python. This is just the beginning.

Feel free to test out the plagiarism classifier we built in the code editor (does it work?) or use the space to play around with other NLP code you’ve encountered in this lesson!
'''

#----------- script.py -------------------

import nltk
# Levenshtein distance:
from nltk.metrics import edit_distance

# an arbitrary plagiarism classifier:
def is_plagiarized(text1, text2):
  n = 7
  if edit_distance(text1.lower(), text2.lower()) > ((len(text1) + len(text2)) / n):
    return False
  return True

doc1 = "is this plagiarized"
doc2 = "maybe it's plagiarized"

print(is_plagiarized(doc1, doc2))

'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Introduction
Discovering new code words in declassified CIA documents may seem like a mission for a foreign intelligence service, and detecting gender biases in the Harry Potter novels a task for a literature professor. Yet by utilizing natural language parsing with regular expressions, the power to perform such analyses is in your own hands!

While you may not put much explicit thought into the structure of your sentences as you write, the syntax choices you make are critical in ensuring your writing has meaning. Analyzing such sentence structure as well as word choice can not only provide insights into the connotation of a piece text, but can also highlight the biases of its author or uncover additional insights that even a deep, rigorous reading of the text might not reveal.

By using Python’s regular expression modulere and the Natural Language Toolkit, known as NLTK, you can find keywords of interest, discover where and how often they are used, and discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing. Let’s get started!

Instructions
1.
The code in the workspace performs natural language parsing with regular expressions on L. Frank Baum’s classic novel The Wonderful Wizard of Oz! . Run the code to view the output, which gives the frequency of different phrases that appear in the text.

Proceed to the next exercise when you are ready to learn how to perform such parsing yourself!'''

from nltk import RegexpParser
from pos_tagged_oz import pos_tagged_oz
from np_chunk_counter import np_chunk_counter

# define noun-phrase chunk grammar here
chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# create a list to hold noun-phrase chunked sentences
np_chunked_oz = list()

# create a for-loop through each pos-tagged sentence in pos_tagged_oz here
for pos_tagged_sentence in pos_tagged_oz:
  # chunk each sentence and append to np_chunked_oz here
  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

# store and print the most common np-chunks here
most_common_np_chunks = np_chunk_counter(np_chunked_oz)
print(most_common_np_chunks)

#-----------------result -----------------

[((('i', 'NN'),), 326), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]


'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Compiling and Matching
Before you dive into more complex syntax parsing, you’ll begin with basic regular expressions in Python using the re module as a regex refresher.

The first method you will explore is .compile(). This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. The regular expression object below will exactly match 4 upper or lower case characters.
'''
regular_expression_object = re.compile("[A-Za-z]{4}")'''
Regular expression objects have a .match() method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string. To see if your regular expression matches the string "Toto" you can do the following:
'''
result = regular_expression_object.match("Toto")'''
If .match() finds a match that starts at the beginning of the string, it will return a match object. The match object lets you know what piece of text the regular expression matched, and at what index the match begins and ends. If there is no match, .match() will return None.

With the match object stored in result, you can access the matched text by calling result.group(0). If you use a regex containing capture groups, you can access these groups by calling .group() with the appropriately numbered capture group as an argument.

Instead of compiling the regular expression first and then looking for a match in separate lines of code, you can simplify your match to one line:
'''
result = re.match("[A-Za-z]{4}","Toto")'''
With this syntax, re‘s .match() method takes a regular expression pattern as the first argument and a string as the second argument.'''

import re

# characters are defined
character_1 = "Dorothy"
character_2 = "Henry"

# compile your regular expression here
regular_expression = re.compile("[A-Za-z]{7}")

# check for a match to character_1 here
result_1 = regular_expression.match(character_1)

# store and print the matched text here
print(result_1)
match_1 = result_1.group(0)
print(match_1)


# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here
result_2 = re.match('[A-Za-z]{7}',character_2)

#----------------- result --------------------------------

<_sre.SRE_Match object; span=(0, 7), match='Dorothy'>
Dorothy

'''

NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Searching and Finding
You can make your regular expression matches even more dynamic with the help of the .search() method. Unlike .match() which will only find matches at the start of a string, .search() will look left to right through an entire piece of text and return a match object for the first match to the regular expression given. If no match is found, .search() will return None. For example, to search for a sequence of 8 word characters in the string Are you a Munchkin?:
'''
result = re.search("\w{8}","Are you a Munchkin?")'''
Using .search() on the string above will find a match of "Munchkin", while using .match() on the same string would return None!

So far you have used methods that only return one piece of matching text. What if you want to find all the occurrences of a word or keyword in a piece of text to determine a frequency count? Step in the .findall() method!

Given a regular expression as its first argument and a string as its second argument, .findall() will return a list of all non-overlapping matches of the regular expression in the string. Consider the below piece of text:
'''
text = "Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night."'''
To find all non-overlapping sequences of 8 word characters in the sentence you can do the following:
'''
list_of_matches = re.findall("\w{8}",text)'''
.findall() will thus return the list ['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin'].'''

import re

# import L. Frank Baum's The Wonderful Wizard of Oz
oz_text = open("the_wizard_of_oz_text.txt",encoding='utf-8').read().lower()

# search oz_text for an occurrence of 'wizard' here
found_wizard = re.search('wizard',oz_text)
print(found_wizard)

# find all the occurrences of 'lion' in oz_text here
all_lions = re.findall('lion', oz_text)
print(all_lions)

# store and print the length of all_lions here
number_lions = len(all_lions)
print(number_lions)

#----------- result ------------------------

<_sre.SRE_Match object; span=(14, 20), match='wizard'>
['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']
183

'''

NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Part-of-Speech Tagging
While it is useful to match and search for patterns of individual characters in a text, you can often find more meaning by analyzing text on a word-by-word basis, focusing on the part of speech of each word in a sentence. This process of identifying and labeling the part of speech of words is known as part-of-speech tagging!

It may have been a while since you’ve been in English class, so let’s review the nine parts of speech with an example:

Wow! Ramona and her class are happily studying the new textbook she has on NLP.

Noun: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP)
Pronoun: a word used in place of a noun (her,she)
Determiner: a word that introduces, or “determines”, a noun (the)
Verb: expresses action (studying) or being (are,has)
Adjective: modifies or describes a noun or pronoun (new)
Adverb: modifies or describes a verb, an adjective, or another adverb (happily)
Preposition: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on)
Conjunction: a word that joins words, phrases, or clauses (and)
Interjection: a word used to express emotion (Wow)
You can automate the part-of-speech tagging process with nltk‘s pos_tag() function! The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag.

Given the sentence split into a list of words below:

word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']
you can tag the parts of speech as follows:

part_of_speech_tagged_sentence = pos_tag(word_sentence)
The call to pos_tag() will return the following:

[('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'), ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('heart', 'NN'), ('?', '.')]
Abbreviations are given instead of the full part of speech name. Some common abbreviations include: NN for nouns, VB for verbs, RB for adverbs, JJ for adjectives, and DT for determiners. A complete list of part-of-speech tags and their abbreviations can be found here.
'''
'''
Alphabetical list of part-of-speech tags used in the Penn Treebank Project:
Number
Tag
Description
1.	CC	Coordinating conjunction
2.	CD	Cardinal number
3.	DT	Determiner
4.	EX	Existential there
5.	FW	Foreign word
6.	IN	Preposition or subordinating conjunction
7.	JJ	Adjective
8.	JJR	Adjective, comparative
9.	JJS	Adjective, superlative
10.	LS	List item marker
11.	MD	Modal
12.	NN	Noun, singular or mass
13.	NNS	Noun, plural
14.	NNP	Proper noun, singular
15.	NNPS	Proper noun, plural
16.	PDT	Predeterminer
17.	POS	Possessive ending
18.	PRP	Personal pronoun
19.	PRP$	Possessive pronoun
20.	RB	Adverb
21.	RBR	Adverb, comparative
22.	RBS	Adverb, superlative
23.	RP	Particle
24.	SYM	Symbol
25.	TO	to
26.	UH	Interjection
27.	VB	Verb, base form
28.	VBD	Verb, past tense
29.	VBG	Verb, gerund or present participle
30.	VBN	Verb, past participle
31.	VBP	Verb, non-3rd person singular present
32.	VBZ	Verb, 3rd person singular present
33.	WDT	Wh-determiner
34.	WP	Wh-pronoun
35.	WP$	Possessive wh-pronoun
36.	WRB	Wh-adverb

'''
import nltk
from nltk import pos_tag
from word_tokenized_oz import word_tokenized_oz

# save and print the sentence stored at index 100 in word_tokenized_oz here

witches_fate = word_tokenized_oz[100]
print(witches_fate)

# create a list to hold part-of-speech tagged sentences here
pos_tagged_oz = []


# create a for loop through each word tokenized sentence in word_tokenized_oz here

for word_tokenized_sentence in word_tokenized_oz:
  
  # part-of-speech tag each sentence and append to pos_tagged_oz here
  pos_tagged_oz.append(pos_tag(word_tokenized_sentence))
  

# store and print the 101st part-of-speech tagged sentence here
witches_fate_pos = pos_tagged_oz[100]
print(witches_fate_pos)

#----------------result ------------------------

['``', 'the', 'house', 'must', 'have', 'fallen', 'on', 'her', '.']
[('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]

'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Introduction to Chunking
You have made it to the juicy stuff! Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text. This technique of grouping words by their part-of-speech tag is called chunking.

With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions. You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text.

The regular expression you build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:

chunk_grammar = "AN: {<JJ><NN>}"
AN is a user-defined name for the kind of chunk you are searching for. You can use whatever name makes sense given your chunk grammar. In this case AN stands for adjective-noun
A pair of curly braces {} surround the actual chunk grammar
<JJ> operates similarly to a regex character class, matching any adjective
<NN> matches any noun, singular or plural
The chunk grammar above will thus match any adjective that is followed by a noun.

To use the chunk grammar defined, you must create a nltk RegexpParser object and give it a piece of chunk grammar as an argument.

chunk_parser = RegexpParser(chunk_grammar)
You can then use the RegexpParser object’s .parse() method, which takes a list of part-of-speech tagged words as an argument, and identifies where such chunks occur in the sentence!

Consider the part-of-speech tagged sentence below:

pos_tagged_sentence = [('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.')]
You can chunk the sentence to find any adjectives followed by a noun with the following:

chunked = chunk_parser.parse(pos_tagged_sentence)'''


from nltk import RegexpParser, Tree
from pos_tagged_oz import pos_tagged_oz

# define adjective-noun chunk grammar here
chunk_grammar = "AN:{<JJ><NN>}"

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# chunk the pos-tagged sentence at index 282 in pos_tagged_oz here
scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])
print(scaredy_cat)

# pretty_print the chunked sentence here
Tree.fromstring(str(scaredy_cat)).pretty_print()

#--------------- RESULT --------------

(S ``/`` where/WRB is/VBZ the/DT (AN emerald/JJ city/NN) ?/. ''/'')
                         S                                    
   ______________________|__________________________           
  |       |       |      |     |    |               AN        
  |       |       |      |     |    |        _______|_____     
``/`` where/WRB is/VBZ the/DT ?/. ''/'' emerald/JJ     city/NN

'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Chunking Noun Phrases
While you are able to chunk any sequence of parts of speech that you like, there are certain types of chunking that are linguistically helpful for determining meaning and bias in a piece of text. One such type of chunking is NP-chunking, or noun phrase chunking. A noun phrase is a phrase that contains a noun and operates, as a unit, as a noun.

A popular form of noun phrase begins with a determiner DT, which specifies the noun being referenced, followed by any number of adjectives JJ, which describe the noun, and ends with a noun NN.

Consider the part-of-speech tagged sentence below:

[('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP$'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]
Can you spot the three noun phrases of the form described above? They are:

(('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))
(('the', 'DT'), ('east', 'NN'))
(('bondage', 'NN'))
With the help of a regular expression defined chunk grammar, you can easily find all the non-overlapping noun phrases in a piece of text! Just like in normal regular expressions, you can use quantifiers to indicate how many of each part of speech you want to match.

The chunk grammar for a noun phrase can be written as follows:

chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"
NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase
<DT> matches any determiner
? is an optional quantifier, matching either 0 or 1 determiners
<JJ> matches any adjective
* is the Kleene star quantifier, matching 0 or more occurrences of an adjective
<NN> matches any noun, singular or plural
By finding all the NP-chunks in a text, you can perform a frequency analysis and identify important, recurring noun phrases. You can also use these NP-chunks as pseudo-topics and tag articles and documents by their highest count NP-chunks! Or perhaps your analysis has you looking at the adjective choices an author makes for different nouns.

It is ultimately up to you, with your knowledge of the text you are working with, to interpret the meaning and use-case of the NP-chunks and their frequency of occurrence.

'''

from nltk import RegexpParser
from pos_tagged_oz import pos_tagged_oz
from np_chunk_counter import np_chunk_counter

# define noun-phrase chunk grammar here
chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# create a list to hold noun-phrase chunked sentences
np_chunked_oz = list()

# create a for loop through each pos-tagged sentence in pos_tagged_oz here
for pos_tagged_sentence in pos_tagged_oz:
  # chunk each sentence and append to np_chunked_oz here
  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

# store and print the most common np-chunks here
most_common_np_chunks = np_chunk_counter(np_chunked_oz)
print(most_common_np_chunks)

#----------------RESULT --------------------------

[((('i', 'NN'),), 326), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]


'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Chunking Verb Phrases
Another popular type of chunking is VP-chunking, or verb phrase chunking. A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers.

Verb phrases can take a variety of structures, and here you will consider two. The first structure begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb.

Both structures are considered because verb phrases of each form are essentially the same in meaning. For example, consider the part-of-speech tagged verb phrases given below:

(('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'))
('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), (('said', 'VBD'),
The chunk grammar to find the first form of verb phrase is given below:

chunk_grammar = "VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}"
VP is the user-defined name of the chunk you are searching for. In this case VP stands for verb phrase
<VB.*> matches any verb using the . as a wildcard and the * quantifier to match 0 or more occurrences of any character. This ensures matching verbs of any tense (ex. VB for present tense, VBD for past tense, or VBN for past participle)
<DT>?<JJ>*<NN> matches any noun phrase
<RB.?> matches any adverb using the . as a wildcard and the optional quantifier to match 0 or 1 occurrence of any character. This ensures matching any form of adverb (regular RB, comparative RBR, or superlative RBS)
? is an optional quantifier, matching either 0 or 1 adverbs
The chunk grammar for the second form of verb phrase is given below:

chunk_grammar = "VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}"
Just like with NP-chunks, you can find all the VP-chunks in a text and perform a frequency analysis to identify important, recurring verb phrases. These verb phrases can give insight into what kind of action different characters take or how the actions that characters take are described by the author.

Once again, this is the part of the analysis where you get to be creative and use your own knowledge about the text you are working with to find interesting insights!'''


from nltk import RegexpParser
from pos_tagged_oz import pos_tagged_oz
from vp_chunk_counter import vp_chunk_counter

# define verb phrase chunk grammar here
chunk_grammar = "VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}"
#chunk_grammar = "VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}"

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# create a list to hold verb-phrase chunked sentences
vp_chunked_oz = list()

# create for loop through each pos-tagged sentence in pos_tagged_oz here
for pos_tagged_sentence in pos_tagged_oz:
  # chunk each sentence and append to vp_chunked_oz here
  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))
  
# store and print the most common vp-chunks here
most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)
print(most_common_vp_chunks)

#-------------RESULT------------------------

[((('said', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 33), ((('said', 'VBD'), ('dorothy', 'NN')), 31), ((('asked', 'VBN'), ('dorothy', 'NN')), 20), ((('said', 'VBD'), ('the', 'DT'), ('tin', 'NN')), 19), ((('said', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 15), ((('said', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 10), ((('asked', 'VBN'), ('the', 'DT'), ('scarecrow', 'NN')), 10), ((('answered', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 8), ((('said', 'VBD'), ('oz', 'NN')), 8), ((('said', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 7), ((('pass', 'VB'), ('the', 'DT'), ('night', 'NN')), 6), ((('asked', 'VBN'), ('the', 'DT'), ('girl', 'NN')), 6), ((('see', 'VB'), ('the', 'DT'), ('great', 'JJ'), ('oz', 'NN')), 6), ((('answered', 'VBD'), ('oz', 'NN')), 6), ((('replied', 'VBD'), ('oz', 'NN')), 6), ((('cried', 'VBN'), ('dorothy', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('tin', 'NN')), 5), ((('asked', 'VBN'), ('the', 'DT'), ('lion', 'NN')), 5), ((('remarked', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('answered', 'VBD'), ('dorothy', 'NN')), 5), ((('replied', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 5), ((('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('witch', 'NN')), 4), ((('replied', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 4), ((('said', 'VBD'), ('the', 'DT'), ('farmer', 'NN')), 4), ((('thought', 'VBD'), ('i', 'NN')), 4), ((('answered', 'VBD'), ('the', 'DT'), ('woodman', 'NN')), 4), ((('have', 'VBP'), ('no', 'DT'), ('heart', 'NN')), 4)]

'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Chunk Filtering
Another option you have to find chunks in your text is chunk filtering. Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them.

A popular method for performing chunk filtering is to chunk an entire sentence together and then indicate which parts of speech are to be filtered out. If the filtered parts of speech are in the middle of a chunk, it will split the chunk into two separate chunks! The chunk grammar you can use to perform chunk filtering is given below:

chunk_grammar = """NP: {<.*>+}
                       }<VB.?|IN>+{"""
NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase
The brackets {} indicate what parts of speech you are chunking. <.*>+ matches every part of speech in the sentence
The inverted brackets }{ indicate which parts of speech you want to filter from the chunk. <VB.?|IN>+ will filter out any verbs or prepositions
Chunk filtering provides an alternate way for you to search through a text and find the chunks of information useful for your analysis!'''

from nltk import RegexpParser, Tree
from pos_tagged_oz import pos_tagged_oz

# define chunk grammar to chunk an entire sentence together
grammar = "Chunk: {<.*>+}"

# create RegexpParser object
parser = RegexpParser(grammar)

# chunk the pos-tagged sentence at index 230 in pos_tagged_oz
chunked_dancers = parser.parse(pos_tagged_oz[230])
print(chunked_dancers)

# define noun phrase chunk grammar using chunk filtering here
chunk_grammar = """NP: {<.*>+}
                       }<VB.?|IN>+{"""

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here
filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])
print(filtered_dancers)

# pretty_print the chunked and filtered sentence here
Tree.fromstring(str(filtered_dancers)).pretty_print()

#------------RESULT -------------------------

'''
(S
  (Chunk
    then/RB
    she/PRP
    sat/VBD
    upon/IN
    a/DT
    settee/NN
    and/CC
    watched/VBD
    the/DT
    people/NNS
    dance/NN
    ./.))
(S
  (NP then/RB she/PRP)
  sat/VBD
  upon/IN
  (NP a/DT settee/NN and/CC)
  watched/VBD
  (NP the/DT people/NNS dance/NN ./.))
                                                 S                                                  
    _____________________________________________|_______________________________                    
   |       |         |               NP                  NP                      NP                 
   |       |         |          _____|_____       _______|_______        ________|________________   
sat/VBD upon/IN watched/VBD then/RB     she/PRP a/DT settee/NN and/CC the/DT people/NNS dance/NN ./.'''

'''
NATURAL LANGUAGE PARSING WITH REGULAR EXPRESSIONS
Review
And there you go! Now you have the toolkit to dig into any piece of text data and perform natural language parsing with regular expressions. What insights will you gain, or what bias may you uncover? Let’s review what you have learned:

The re module’s .compile() and .match() methods allow you to enter any regex pattern and look for a single match at the beginning of a piece of text
The re module’s .search() method lets you find a single match to a regex pattern anywhere in a string, while the .findall() method finds all the matches of a regex pattern in a string
Part-of-speech tagging identifies and labels the part of speech of words in a sentence, and can be performed in nltk using the pos_tag() function
Chunking groups together patterns of words by their part-of-speech tag. Chunking can be performed in nltk by defining a piece of chunk grammar using regular expression syntax and calling a RegexpParser‘s .parse() method on a word tokenized sentence
NP-chunking chunks together an optional determiner DT, any number of adjectives JJ, and a noun NN to form a noun phrase. The frequency of different NP-chunks can identify important topics in a text or demonstrate how an author describes different subjects
VP-chunking chunks together a verb VB, a noun phrase, and an optional adverb RB to form a verb phrase. The frequency of different VP-chunks can give insight into what kind of action different subjects take or how the actions that different subjects take are described by an author, potentially indicating bias
Chunk filtering provides an alternative means of chunking by specifying what parts of speech you do not want in a chunk and removing them'''


from nltk import RegexpParser
from pos_tagged_oz import pos_tagged_oz
from chunk_counter import chunk_counter

# define your own chunk grammar here
chunk_grammar = '''Chunk: {}
													}{'''

# create RegexpParser object
chunk_parser = RegexpParser(chunk_grammar)

# create a list to hold chunked sentences
chunked_oz = list()

# create a for loop through each pos-tagged sentence in pos_tagged_oz
for pos_tagged_sentence in pos_tagged_oz:
  # chunk each sentence and append to chunked_oz
  chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

# store and print the most common chunks
most_common_chunks = chunk_counter(chunked_oz)
print(most_common_chunks)

'''
BAG-OF-WORDS LANGUAGE MODEL
Intro to Bag-of-Words
“A bag-of-words is all you need,” some NLPers have decreed.

The bag-of-words language model is a simple-yet-powerful tool to have up your sleeve when working on natural language processing (NLP). The model has many, many use cases including:

determining topics in a song
filtering spam from your inbox
finding out if a tweet has positive or negative sentiment
creating word clouds'''

#------------- script.py ------------------------

from spam_data import training_spam_docs, training_doc_tokens, training_labels
from sklearn.naive_bayes import MultinomialNB
from preprocessing import preprocess_text

# Add your email text to test_text between the triple quotes:
test_text = """
Dear recipient,
Avangar Technologies announces the beginning of a new unprecendented global employment campaign.
reviser yeller winers butchery twenties
Due to company's exploding growth Avangar is expanding business to the European region.
During last employment campaign over 1500 people worldwide took part in Avangar's business
and more than half of them are currently employed by the company. And now we are offering you
one more opportunity to earn extra money working with Avangar Technologies.
druggists blame classy gentry Aladdin

We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their
time per day and earn extra 拢300-500 weekly. All offered positions are currently part-time
and give you a chance to work mainly from home.
lovelies hockey Malton meager reordered

Please visit Avangar's corporate web site (http://www.avangar.com/sta/home/0077.htm) for more details regarding these vacancies.


"""
test_tokens = preprocess_text(test_text)

def create_features_dictionary(document_tokens):
  features_dictionary = {}
  index = 0
  for token in document_tokens:
    if token not in features_dictionary:
      features_dictionary[token] = index
      index += 1
  return features_dictionary

def tokens_to_bow_vector(document_tokens, features_dictionary):
  bow_vector = [0] * len(features_dictionary)
  for token in document_tokens:
    if token in features_dictionary:
      feature_index = features_dictionary[token]
      bow_vector[feature_index] += 1
  return bow_vector

bow_sms_dictionary = create_features_dictionary(training_doc_tokens)
training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]
test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]

spam_classifier = MultinomialNB()
spam_classifier.fit(training_vectors, training_labels)

predictions = spam_classifier.predict(test_vectors)

print("Looks like a normal email!" if predictions[0] == 0 else "You've got spam!")

'''
BAG-OF-WORDS LANGUAGE MODEL
Bag-of-What?
Bag-of-words (BoW) is a statistical language model based on word count. Say what?

Let’s start with that first part: a statistical language model is a way for computers to make sense of language based on probability. For example, let’s say we have the text:

“Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”

A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”

Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.

If you’re already familiar with statistical language models, you may also have heard BoW referred to as the unigram model. It’s technically a special case of another statistical model, the n-gram model, with n (the number of words in a sequence) set to 1.

If you have no idea what n-grams are, don’t worry — we’ll dive deeper into them in another lesson.'''

'''
BAG-OF-WORDS LANGUAGE MODEL
BoW Dictionaries
One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. Take the example below:

The squids jumped out of the suitcases.
The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences).

Let’s build a function that converts a given training text into a bag-of-words!'''

from preprocessing import preprocess_text
# Define text_to_bow() below:
def text_to_bow(some_text):
  bow_dictionary = {}
  tokens = preprocess_text(some_text)
  for token in tokens:
    if token in bow_dictionary:
      bow_dictionary[token] += 1
    else:
      bow_dictionary[token] = 1
  return bow_dictionary

print(text_to_bow("I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish..."))

#resut 
{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}


'''
BAG-OF-WORDS LANGUAGE MODEL
Introducing BoW Vectors
Sometimes a dictionary just won’t fit the bill. Topic modelling applications, for example, require an implementation of bag-of-words that is a bit more mathematical: feature vectors.

A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. A few monsters could be represented as vectors like so:

has_fangs	melts_in_water	hates_sunlight	has_fur
vampire	1	0	1	0
werewolf	1	0	0	1
witch	0	1	0	0

For bag-of-words, instead of monsters you would have documents and the features would be different words. And we don’t just care if a word is present in a document; we want to know how many times it occurred! Turning text into a BoW vector is known as feature extraction or vectorization.

But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices.

For example, with “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?” our dictionary might be:

{'five': 0,
'fantastic': 1,
'fish': 2,
'fly': 3,
'off': 4,
'to': 5,
'find': 6,
'faraway': 7,
'function': 8,
'maybe': 9,
'another': 10}
Using this dictionary, we can convert new documents into vectors using a vectorization function. For example, we can take a brand new sentence “Another five fish find another faraway fish.” — test data — and convert it to a vector that looks like:

[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]
The word ‘another’ appeared twice in the test data. If we look at the feature dictionary for ‘another’, we find that its index is 10. So when we go back and look at our vector, we’d expect the number at index 10 to be 2.'''

'''BAG-OF-WORDS LANGUAGE MODEL
Building a Features Dictionary
Now that you know what a bag-of-words vector looks like, you can create a function that builds them!

First, we need a way of generating a features dictionary from a list of training documents. We can build a Python function to do that for us…
'''

from preprocessing import preprocess_text
# Define create_features_dictionary() below:
def create_features_dictionary(documents):
  features_dictionary = {}
  merged = " ".join(documents)
  tokens = preprocess_text(merged)
  index = 0
  for token in tokens:
    if token not in features_dictionary:
      features_dictionary[token] = index
      index += 1
  return features_dictionary, tokens

training_documents = ["Five fantastic fish flew off to find faraway functions.", "Maybe find another five fantastic fish?", "Find my fish with a function please!"]

print(create_features_dictionary(training_documents)[0])

#-----------result------------
{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}


'''
BAG-OF-WORDS LANGUAGE MODEL
Building a BoW Vector
Nice work! Time to put that dictionary of vocabulary to good use and build a bag-of-words vector from a new document.

In Python, we can use a list to represent a vector. Each index in the list will correspond to a word and be set to its count.

'''
from preprocessing import preprocess_text
# Define text_to_bow_vector() below:
def text_to_bow_vector(some_text, features_dictionary):
  bow_vector = [0] * len(features_dictionary)
  tokens = preprocess_text(some_text)
  for token in tokens:
    feature_index = features_dictionary[token]
    bow_vector[feature_index] += 1
  return bow_vector, tokens

features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}

text = "Another five fish find another faraway fish."
print(text_to_bow_vector(text, features_dictionary)[0])

#----------- result --------------------------

[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]

'''

It's All in the Bag
Phew! That was a lot of work.

It’s time to put create_features_dictionary() and tokens_to_bow_vector() together and use them in a spam filter we created that uses a Naive Bayes classifier. We’ve slightly modified the two functions for this use case, but they should still look familiar.

Let’s see create_features_dictionary() and tokens_to_bow_vector() in action with real test data, helping fend off spam!'''

from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs
from sklearn.naive_bayes import MultinomialNB

def create_features_dictionary(document_tokens):
  features_dictionary = {}
  index = 0
  for token in document_tokens:
    if token not in features_dictionary:
      features_dictionary[token] = index
      index += 1
  return features_dictionary

def tokens_to_bow_vector(document_tokens, features_dictionary):
  bow_vector = [0] * len(features_dictionary)
  for token in document_tokens:
    if token in features_dictionary:
      feature_index = features_dictionary[token]
      bow_vector[feature_index] += 1
  return bow_vector

# Define bow_sms_dictionary:
bow_sms_dictionary = create_features_dictionary(training_doc_tokens)

# Define training_vectors:
training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]

# Define test_vectors:
test_vectors = [tokens_to_bow_vector(test_doc, bow_sms_dictionary) for test_doc in test_spam_docs]


spam_classifier = MultinomialNB()

def spam_or_not(label):
  return "spam" if label else "not spam"

# Uncomment the code below when you're done:
spam_classifier.fit(training_vectors, training_labels)

predictions = spam_classifier.score(test_vectors, test_labels)

print("The predictions for the test data were {0}% accurate.\n\nFor example, '{1}' was classified as {2}.\n\nMeanwhile, '{3}' was classified as {4}.".format(predictions * 100, test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))

#------------result-------------
The predictions for the test data were 99.0% accurate.

For example, 'well obviously not because all the people in my cool college life go home _' was classified as not spam.

Meanwhile, 'urgent we be try to contact you last weekend draw show u have win a 1000 prize guarantee call 09064017295 claim code k52 valid 12hrs 150p pm' was classified as spam.


'''BAG-OF-WORDS LANGUAGE MODEL
Spam A Lot No More
Amazing work! As is the case with many tasks in Python, there’s already a library that can do all of that work for you.

For text_to_bow(), you can approximate the functionality with the collections module’s Counter() function:
'''
from collections import Counter

tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']
print(Counter(tokens))

# Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})
'''
For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:
'''
from sklearn.feature_extraction.text import CountVectorizer

training_documents = ["Five fantastic fish flew off to find faraway functions.", "Maybe find another five fantastic fish?", "Find my fish with a function please!"]
test_text = ["Another five fish find another faraway fish."]
bow_vectorizer = CountVectorizer()
bow_vectorizer.fit(training_documents)
bow_vector = bow_vectorizer.transform(test_text)
print(bow_vector.toarray())
# [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]'''


from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs
from sklearn.naive_bayes import MultinomialNB
# Import CountVectorizer from sklearn:
from sklearn.feature_extraction.text import CountVectorizer

# Define bow_vectorizer:
bow_vectorizer = CountVectorizer()

# Define training_vectors:
training_vectors = bow_vectorizer.fit_transform(training_docs)
# Define test_vectors:
test_vectors = bow_vectorizer.transform(test_docs)

spam_classifier = MultinomialNB()

def spam_or_not(label):
  return "spam" if label else "not spam"

# Uncomment the code below when you're done:
spam_classifier.fit(training_vectors, training_labels)

predictions = spam_classifier.score(test_vectors, test_labels)

print("The predictions for the test data were {0}% accurate.\n\nFor example, '{1}' was classified as {2}.\n\nMeanwhile, '{3}' was classified as {4}.".format(predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[15], spam_or_not(test_labels[15])))

#-------------- result ------------------------

The predictions for the test data were 100.0% accurate.

For example, 'really do hope the work doesnt get stressful have a gr8 day' was classified as not spam.

Meanwhile, '2p per min to call germany 08448350055 from your bt line just 2p per min check planettalkinstant com for info t s c s text stop to opt out' was classified as spam.

'''
BAG-OF-WORDS LANGUAGE MODEL
BoW Wow
As you can see, bag-of-words is pretty useful! BoW also has several advantages over other language models. For one, it’s an easier model to get started with and a few Python libraries already have built-in support for it.

Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity (i.e., it has more training knowledge to draw from) than other statistical models.

Imagine you want to make a shirt to sell to people. If you have the shirt exactly tailored to someone’s body, it probably won’t fit that many people. But if you make a shirt that is just a giant bag with arm holes, you know that no one will buy it. What do you do? You loosely fit the shirt to someone’s body, leaving some extra room for different body shapes.

Overfitting (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.

The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models.'''

from preprocessing import preprocess_text
from nltk.util import ngrams
from collections import Counter

text = "It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'"
tokens = preprocess_text(text)

# Bigram approach:
bigrams_prepped = ngrams(tokens, 2)
bigrams = Counter(bigrams_prepped)
print("Three most frequent word sequences and the number of occurrences according to Bigrams:")
print(bigrams.most_common(3))

# Bag of Words approach:
# Define bag_of_words here:
bag_of_words = Counter(tokens)
print("\nThree most frequent words and number of occurrences according to Bag of Words:")
most_common_three = bag_of_words.most_common(3)
print(most_common_three)

#--------------- result ---------------
'''
Three most frequent word sequences and the number of occurrences according to Bigrams:
[(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]

Three most frequent words and number of occurrences according to Bag-of-Words:

BAG-OF-WORDS LANGUAGE MODEL
BoW Ow
Alas, there is a trade-off for all the brilliance BoW brings to the table.

Unless you want sentences that look like “the a but for the”, BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because bag-of-words has high perplexity, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.

If your BoW model finds “good” frequently occurring in a text sample, you might assume there’s a positive sentiment being communicated in that text… but if you look at the original text you may find that in fact every “good” was preceded by a “not.”

Hmm, that would have been helpful to know. The BoW model’s word tokens lack context, which can make a word’s intended meaning unclear.

Perhaps you are wondering, “What happens if the model comes across a new word that wasn’t in the training data?” As mentioned, like all statistical models, BoW suffers from overfitting when it comes to vocabulary.

There are several ways that NLP developers have tackled this issue. A common approach is through language smoothing in which some probability is siphoned from the known words and given to unknown words.
'''
import nltk, re, random
from nltk.tokenize import word_tokenize
from collections import defaultdict, deque, Counter
from document import oscar_wilde_thoughts

# Change sequence_length:
sequence_length = 1

class MarkovChain:
  def __init__(self):
    self.lookup_dict = defaultdict(list)
    self.most_common = []
    self._seeded = False
    self.__seed_me()

  def __seed_me(self, rand_seed=None):
    if self._seeded is not True:
      try:
        if rand_seed is not None:
          random.seed(rand_seed)
        else:
          random.seed()
        self._seeded = True
      except NotImplementedError:
        self._seeded = False
    
  def add_document(self, str):
    preprocessed_list = self._preprocess(str)
    self.most_common = Counter(preprocessed_list).most_common(20)
    pairs = self.__generate_tuple_keys(preprocessed_list)
    for pair in pairs:
      self.lookup_dict[pair[0]].append(pair[1])
  
  def _preprocess(self, str):
    cleaned = re.sub(r'\W+', ' ', str).lower()
    tokenized = word_tokenize(cleaned)
    return tokenized

  def __generate_tuple_keys(self, data):
    if len(data) < sequence_length:
      return

    for i in range(len(data) - 1):
      yield [ data[i], data[i + 1] ]
      
  def generate_text(self, max_length=50):
    context = deque()
    output = []
    if len(self.lookup_dict) > 0:
      self.__seed_me(rand_seed=len(self.lookup_dict))
      chain_head = [list(self.lookup_dict)[0]]
      context.extend(chain_head)
      if sequence_length > 1:
        while len(output) < (max_length - 1):
          next_choices = self.lookup_dict[context[-1]]
          if len(next_choices) > 0:
            next_word = random.choice(next_choices)
            context.append(next_word)
            output.append(context.popleft())
          else:
            break
        output.extend(list(context))
      else:
        while len(output) < (max_length - 1):
          next_choices = [word[0] for word in self.most_common]
          next_word = random.choice(next_choices)
          output.append(next_word)
    return " ".join(output)

my_markov = MarkovChain()
my_markov.add_document(oscar_wilde_thoughts)
random_oscar_wilde = my_markov.generate_text()
print(random_oscar_wilde)

#---------- result -------------------------

in london whereon we build greater hopes for in the country that in cups an honest workman and sees the fancy of your country has his work the art schools of wood carving on one could not see the song of indifference is something which they come only well and

'''
BAG-OF-WORDS LANGUAGE MODEL
Review of Bag-of-Words
You made it! And you’ve learned plenty about the bag-of-words language model along the way:

Bag-of-words (BoW) — also referred to as the unigram model — is a statistical language model based on word count.
There are loads of real-world applications for BoW.
BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.
For BoW, training data is the text that is used to build a BoW model.
BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.
A feature vector is a numeric depiction of an item’s salient features.
Feature extraction (or vectorization) is the process of turning text into a BoW vector.
A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.
BoW has less data sparsity than other statistical models. It also suffers less from overfitting.
BoW has higher perplexity than other models, making it less ideal for language prediction.
One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words.
The spam data for this lesson were taken from the UCI Machine Learning Repository.

Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
'''

from spam_data import training_spam_docs, training_doc_tokens, training_labels, training_docs
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

test_text = """
Play around with the spam classifier!
"""

bow_vectorizer = CountVectorizer()

training_vectors = bow_vectorizer.fit_transform(training_docs)
test_vectors = bow_vectorizer.transform([test_text])

spam_classifier = MultinomialNB()
spam_classifier.fit(training_vectors, training_labels)

predictions = spam_classifier.predict(test_vectors)

print("Looks like a normal email!" if predictions[0] == 0 else "You've got spam!")




#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-PYTHON MACHINE LEARNING CHEATSHEET PYTHON MACHINE LEARNING CHEATSHEET PYTHON MACHINE LEARNING CHEATSHEET#-#-#-#
#-#-#-#-PYTHON SK LEARN CHEATSHEET PYTHON SK LEARN CHEATSHEET PYTHON SK LEARN CHEATSHEET PYTHON SK LEARN CHEATSHEET   #-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms. It’s built upon some of the technology you might already be familiar with, like NumPy, pandas, and Matplotlib!

As you build robust Machine Learning programs, it’s helpful to have all the sklearn commands all in one place in case you forget.

LINEAR REGRESSION
Import and create the model:
'''

from sklearn.linear_model import LinearRegression

your_model = LinearRegression()

#Fit:

your_model.fit(x_training_data, y_training_data)
#.coef_: contains the coefficients
#.intercept_: contains the intercept

#Predict:

predictions = your_model.predict(your_x_data)

#.score(): returns the coefficient of determination R²

'''
NAIVE BAYES
Import and create the model:
'''

from sklearn.naive_bayes import MultinomialNB

your_model = MultinomialNB()
#Fit:

your_model.fit(x_training_data, y_training_data)
#Predict:

# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data)

'''
K-NEAREST NEIGHBORS
Import and create the model:
'''

from sklearn.neigbors import KNeighborsClassifier

your_model = KNeighborsClassifier()

#Fit:

your_model.fit(x_training_data, y_training_data)

#Predict:

# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data)

'''
K-MEANS
Import and create the model:
'''

from sklearn.cluster import KMeans

your_model = KMeans(n_clusters=4, init='random')

#n_clusters: number of clusters to form and number of centroids to generate
#init: method for initialization
    #k-means++: K-Means++ [default]
    #random: K-Means
#random_state: the seed used by the random number generator [optional]

#Fit:

your_model.fit(x_training_data)

#Predict:

predictions = your_model.predict(your_x_data)

'''
VALIDATING THE MODEL
Import and print accuracy, recall, precision, and F1 score:
'''

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

print(accuracy_score(true_labels, guesses))
print(recall_score(true_labels, guesses))
print(precision_score(true_labels, guesses))
print(f1_score(true_labels, guesses))

#Import and print the confusion matrix:

from sklearn.metrics import confusion_matrix

print(confusion_matrix(true_labels, guesses))

#TRAINING SETS AND TEST SETS

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
train_size: the proportion of the dataset to include in the train split

#test_size: the proportion of the dataset to include in the test split
#random_state: the seed used by the random number generator [optional]
#Robot Emoji
#Happy Coding!






#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON PANDAS DATA FRAME PYTHON PANDAS DATA FRAME PYTHON PANDAS DATA FRAME - - - - - -#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

#----------------------------------------------------------------
import codecademylib
import pandas as pd

orders = pd.read_csv('shoefly.csv')

print (orders.head())

#emails = orders["email"]

emails = orders.email

frances_palmer = orders[(orders.first_name == "Frances") & (orders.last_name == "Palmer")]

comfy_shoes = orders[orders.shoe_type.isin(["clogs", "boots","ballet flats"])]

#------------------------------------------------
MODIFYING DATAFRAMES
Applying a Lambda to a Row
We can also operate on multiple columns at once. If we use apply without specifying a single column and add the argument axis=1, the input to our lambda function will be an entire row, not a column. To access particular values of the row, we use the syntax row.column_name or row[‘column_name’].

Suppose we have a table representing a grocery list:

Item	Price	Is taxed?
Apple	1.00	No
Milk	4.20	No
Paper Towels	5.00	Yes
Light Bulbs	3.75	Yes
If we want to add in the price with tax for each line, we’ll need to look at two columns: Price and Is taxed?.

If Is taxed? is Yes, then we’ll want to multiply Price by 1.075 (for 7.5% sales tax).

If Is taxed? is No, we’ll just have Price without multiplying it.

We can create this column using a lambda function and the keyword axis=1:

df['Price with Tax'] = df.apply(lambda row:
     row['Price'] * 1.075
     if row['Is taxed?'] == 'Yes'
     else row['Price'],
     axis=1
)
Instructions
1.
If an employee worked for more than 40 hours, she needs to be paid overtime (1.5 times the normal hourly wage).

For instance, if an employee worked for 43 hours and made $10/hour, she would receive $400 for the first 40 hours that she worked, and an additional $45 for the 3 hours of overtime, for a total for $445.

Create a lambda function total_earned that accepts an input row with keys hours_worked and hourly_wage and uses an if statement to calculate the hourly wage.

If we were writing a regular function, it would look like this:

def total_earned(row):
   if row['hours_worked'] <= 40:
       return row['hours_worked'] * \
           row['hourly_wage']
    else:
        return (40 * row['hourly_wage'])\
            + (row['hours_worked'] - 40) * \
            (row['hourly_wage'] * 1.50)
2.
Use the lambda function total_earned and apply to add a column total_earned to df with the total amount earned by each employee.

#------------------------------------------------------------------
import codecademylib
import pandas as pd

df = pd.read_csv('employees.csv')

total_earned = lambda row: 
	row['hours_worked'] * row['hourly_wage'] 
	if row['hours_worked'] <= 40 
  else 40 * row['hourly_wage'] + (row['hours_worked'] - 40) * 1.5 * row['hourly_wage']

df['total_earned'] = df.apply(total_earned, axis = 1)

print(df)

#-----------------------------------------------------------------

MODIFYING DATAFRAMES
Review
Great job! In this lesson, you learned how to modify an existing DataFrame. Some of the skills you’ve learned include:

Adding columns to a DataFrame
Using lambda functions to calculate complex quantities
Renaming columns
Let’s practice what you just learned!

Instructions
1.
Once more, you’ll be the data analyst for ShoeFly.com, a fictional online shoe store.

More messy order data has been loaded into the variable orders. Examine the first 5 rows of the data using print and head.

2.
Many of our customers want to buy vegan shoes (shoes made from materials that do not come from animals). Add a new column called shoe_source, which is vegan if the materials is not leather and animal otherwise.

The following lambda function might be helpful:

mylambda = lambda x: 'animal' \
           if x == 'leather' else 'vegan'
3.
Our marketing department wants to send out an email to each customer. Using the columns last_name and gender create a column called salutation which contains Dear Mr. <last_name> for men and Dear Ms. <last_name> for women.

Here are some examples:

last_name	gender	salutation
Smith	Male	Dear Mr. Smith
Jones	Female	Dear Ms. Jones
The following lambda function might be helpful:

mylambda = lambda row: \
  'Dear Mr. {}'.format(row.last_name) \
  if row.gender == 'male' \
  else 'Dear Ms. {}'.format(row.last_name)
Make sure your strings are exactly the same! Spacing and punctuation matter!

#----------------------------------------------------------

import codecademylib
import pandas as pd

inventory = pd.read_csv('inventory.csv')
print (inventory.head(10))

staten_island = inventory.iloc[:10]
product_request = staten_island['product_description']

#print(product_request)

seed_request = inventory[(inventory.location == 'Brooklyn') &(inventory.product_type == 'seeds')]
#print(seed_request)

check_stock = lambda row: True if row['quantity'] > 0 else False 
inventory['in_stock'] = inventory.apply(check_stock , axis = 1)
#print(inventory)

inventory['total_value'] = inventory.price * inventory.quantity
print(inventory)

combine_lambda = lambda row:\
	'{}-{}'.format(row.product_type, row.product_description)

inventory['full_description'] = inventory.apply(combine_lambda, axis = 1)
print(inventory)


#-------------------------------------------------------

AGGREGATES IN PANDAS
Review
This lesson introduced you to aggregates in Pandas. You learned:

How to perform aggregate statistics over individual rows with the same value using groupby.
How to rearrange a DataFrame into a pivot table, a great way to compare data across two dimensions.
Instructions
1.
Let’s examine some more data from ShoeFly.com. This time, we’ll be looking at data about user visits to the website (the same dataset that you saw in the introduction to this lesson).

The data is a DataFrame called user_visits. Use print and head() to examine the first few rows of the DataFrame.

2.
The column utm_source contains information about how users got to ShoeFly’s homepage. For instance, if utm_source = Facebook, then the user came to ShoeFly by clicking on an ad on Facebook.com.

Use a groupby statement to calculate how many visits came from each of the different sources. Save your answer to the variable click_source.

Remember to use reset_index()!

3.
Paste the following code into script.py so that you can see the results of your previous groupby:

print(click_source)
4.
Our Marketing department thinks that the traffic to our site has been changing over the past few months. Use groupby to calculate the number of visits to our site from each utm_source for each month. Save your answer to the variable click_source_by_month.

5.
The head of Marketing is complaining that this table is hard to read. Use pivot to create a pivot table where the rows are utm_source and the columns are month. Save your results to the variable click_source_by_month_pivot.

It should look something like this:

utm_source	1 - January	2 - February	3 - March
email	…	…	…
facebook	…	…	…
google	…	…	…
twitter	…	…	…
yahoo	…	…	…
6.
View your pivot table by pasting the following code into script.py:

print(click_source_by_month_pivot)

#-----------------------------------

import codecademylib
import pandas as pd

user_visits = pd.read_csv('page_visits.csv')

print (user_visits.head())

click_source = user_visits.groupby('utm_source').id.count().reset_index()

print(click_source)

click_source_by_month = user_visits.groupby(['utm_source', 'month']).id.count().reset_index()

click_source_by_month_pivot = click_source_by_month.pivot(
	columns= 'month',
	index = 'utm_source',
	values = 'id').reset_index()

print (click_source_by_month_pivot)


#-----------------------------------------------

DATA MANIPULATION WITH PANDAS
A/B Testing for ShoeFly.com
Our favorite online shoe store, ShoeFly.com is performing an A/B Test. They have two different versions of an ad, which they have placed in emails, as well as in banner ads on Facebook, Twitter, and Google. They want to know how the two ads are performing on each of the different platforms on each day of the week. Help them analyze the data using aggregate measures.

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
10/11Complete
Mark the tasks as complete by checking them off
Analyzing Ad Sources
1.
Examine the first few rows of ad_clicks.

Try pasting the following code:

print(ad_clicks.head())
2.
Your manager wants to know which ad platform is getting you the most views.

How many views (i.e., rows of the table) came from each utm_source?

Try using the following code:

ad_clicks.groupby('utm_source')\
    .user_id.count()\
    .reset_index()
3.
If the column ad_click_timestamp is not null, then someone actually clicked on the ad that was displayed.

Create a new column called is_click, which is True if ad_click_timestamp is not null and False otherwise.

Try using the following code:

ad_clicks['is_click'] = ~ad_clicks\
   .ad_click_timestamp.isnull()
The ~ is a NOT operator, and isnull() tests whether or not the value of ad_click_timestamp is null.

4.
We want to know the percent of people who clicked on ads from each utm_source.

Start by grouping by utm_source and is_click and counting the number of user_id‘s in each of those groups. Save your answer to the variable clicks_by_source.

Try using the following code:

clicks_by_source = ad_clicks\
   .groupby(['utm_source',
             'is_click'])\
   .user_id.count()\
   .reset_index()
5.
Now let’s pivot the data so that the columns are is_click (either True or False), the index is utm_source, and the values are user_id.

Save your results to the variable clicks_pivot.

Try using the following code:

clicks_pivot = clicks_by_source\
   .pivot(index='utm_source',
          columns='is_click',
          values='user_id')\
   .reset_index()
6.
Create a new column in clicks_pivot called percent_clicked which is equal to the percent of users who clicked on the ad from each utm_source.

Was there a difference in click rates for each source?

Try the following code:

clicks_pivot['percent_clicked'] = \
   clicks_pivot[True] / \
   (clicks_pivot[True] + 
    clicks_pivot[False])
clicks_pivot[True] is the number of people who clicked (because is_click was True for those users)

clicks_pivot[False] is the number of people who did not click (because is_click was False for those users)

So, the percent of people who clicked would be (Total Who Clicked) / (Total Who Clicked + Total Who Did Not Click)

Analyzing an A/B Test
7.
The column experimental_group tells us whether the user was shown Ad A or Ad B.

Were approximately the same number of people shown both adds?

We can group by experimental_group and count the number of users.

8.
Using the column is_click that we defined earlier, check to see if a greater percentage of users clicked on Ad A or Ad B.

Group by both experimental_group and is_click and count the number of user_id‘s.

You might want to use a pivot table like we did for the utm_source exercises.

9.
The Product Manager for the A/B test thinks that the clicks might have changed by day of the week.

Start by creating two DataFrames: a_clicks and b_clicks, which contain only the results for A group and B group, respectively.

To create a_clicks:

a_clicks = ad_clicks[
   ad_clicks.experimental_group
   == 'A']
10.
For each group (a_clicks and b_clicks), calculate the percent of users who clicked on the ad by day.

First, group by is_click and day. Next, pivot the data so that the columns are based on is_click. Finally, calculate the percent of people who clicked on the ad.

11.
Compare the results for A and B. What happened over the course of the week?

Do you recommend that your company use Ad A or Ad B?

#-----------------------------------------------------

import codecademylib
import pandas as pd

ad_clicks = pd.read_csv('ad_clicks.csv')
print (ad_clicks.head())

utm_source_count = ad_clicks.groupby('utm_source').user_id.count()
#print(utm_source_count).reset_index()

ad_clicks['is_click'] = ~ad_clicks.ad_click_timestamp.isnull()
#print (ad_clicks)

clicks_by_source = ad_clicks.groupby(['utm_source', 'is_click']).user_id.count().reset_index()
#print (clicks_by_source)

clicks_pivot = clicks_by_source.pivot(
	columns= 'is_click',
	index = 'utm_source',
	values = 'user_id')
#print (clicks_pivot)

clicks_pivot['percent_clicked'] = 100 * clicks_pivot[True] / (clicks_pivot[True] + clicks_pivot[False])
#print (clicks_pivot)

ab_total = ad_clicks.groupby('experimental_group').user_id.count()
#print (ab_total)

ab_groups = ad_clicks.groupby(['experimental_group', 'is_click'])['user_id'].count().reset_index()
print(ab_groups)

ab_groups_pivot = ab_groups.pivot(
	columns = 'is_click',
	index = 'experimental_group',
	values = 'user_id')

ab_groups_pivot['ab_percentage'] = ab_groups_pivot[True] / (ab_groups_pivot[True] + ab_groups_pivot[False])

print ("\n")
print (ab_groups_pivot)

a_clicks = ad_clicks[ad_clicks.experimental_group == "A"]
#print (a_clicks)

b_clicks = ad_clicks[ad_clicks.experimental_group =='B']
#print (b_clicks)

a_clicks_by_day = a_clicks.groupby(['day', 'is_click']).user_id.count().reset_index()

a_clicks_by_day_pivot = a_clicks_by_day.pivot(
  columns = 'is_click',
  index = 'day',
	values = 'user_id')

a_clicks_by_day_pivot['percent'] = a_clicks_by_day_pivot[True] / (a_clicks_by_day_pivot[True] + a_clicks_by_day_pivot[False])

print (a_clicks_by_day_pivot)

b_clicks_by_day = b_clicks.groupby(['day', 'is_click']).user_id.count().reset_index()

b_clicks_by_day_pivot = b_clicks_by_day.pivot (
	columns = 'is_click',
	index = 'day',
	values = 'user_id')

b_clicks_by_day_pivot['precent'] = b_clicks_by_day_pivot[True] / (b_clicks_by_day_pivot[True] + b_clicks_by_day_pivot[False])

print (b_clicks_by_day_pivot)

#-------------------------------------------------------------

import codecademylib
import pandas as pd

sales = pd.read_csv('sales.csv')
print(sales)
targets = pd.read_csv('targets.csv')
print(targets)

sales_vs_targets = pd.merge(sales, targets)

print (sales_vs_targets)

crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target]

#-------------------------------------------------------------

import codecademylib
import pandas as pd

sales = pd.read_csv('sales.csv')
print(sales)
targets = pd.read_csv('targets.csv')
print(targets)
men_women = pd.read_csv('men_women_sales.csv')
print (men_women)

all_data = sales.merge(targets).merge(men_women)
print(all_data)

results = all_data[(all_data.revenue > all_data.target)&(all_data.women > all_data.men)]

#-------------------------------------------------------------

WORKING WITH MULTIPLE DATAFRAMES
Merge on Specific Columns
In the previous example, the merge function “knew” how to combine tables based on the columns that were the same between two tables. For instance, products and orders both had a column called product_id. This won’t always be true when we want to perform a merge.

Generally, the products and customers DataFrames would not have the columns product_id or customer_id. Instead, they would both be called id and it would be implied that the id was the product_id for the products table and customer_id for the customers table. They would look like this:

Customers
id	customer_name	address	phone_number
1	John Smith	123 Main St.	212-123-4567
2	Jane Doe	456 Park Ave.	949-867-5309
3	Joe Schmo	798 Broadway	112-358-1321
Products
id	description	price
1	thing-a-ma-jig	5
2	whatcha-ma-call-it	10
3	doo-hickey	7
4	gizmo	3

**How would this affect our merges?**
Because the id columns would mean something different in each table, our default merges would be wrong.

One way that we could address this problem is to use .rename to rename the columns for our merges. In the example below, we will rename the column id to customer_id, so that orders and customers have a common column for the merge.

pd.merge(
    orders,
    customers.rename(columns={'id': 'customer_id'}))
	
#----------------------------------------------------------------------------

import codecademylib
import pandas as pd

orders = pd.read_csv('orders.csv')
print(orders)
products = pd.read_csv('products.csv')
print(products)

orders_products = pd.merge(orders, products.rename(columns = {'id':'product_id'}))

print (orders_products)
#------------------------------------------------------------------

WORKING WITH MULTIPLE DATAFRAMES
Merge on Specific Columns II
In the previous exercise, we learned how to use rename to merge two DataFrames whose columns don’t match.

If we don’t want to do that, we have another option. We could use the keywords left_on and right_on to specify which columns we want to perform the merge on. In the example below, the “left” table is the one that comes first (orders), and the “right” table is the one that comes second (customers). This syntax says that we should match the customer_id from orders to the id in customers.

pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id')
If we use this syntax, we’ll end up with two columns called id, one from the first table and one from the second. Pandas won’t let you have two columns with the same name, so it will change them to id_x and id_y.

It will look like this:

id_x	customer_id	product_id	quantity	timestamp	id_y	customer_name	address	phone_number
1	2	3	1	2017-01-01 00:00:00	2	Jane Doe	456 Park Ave	949-867-5309
2	2	2	3	2017-01-01 00:00:00	2	Jane Doe	456 Park Ave	949-867-5309
3	3	1	1	2017-01-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
4	3	2	2	2016-02-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
5	3	3	3	2017-02-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
6	1	4	2	2017-03-01 00:00:00	1	John Smith	123 Main St.	212-123-4567
7	1	1	1	2017-02-02 00:00:00	1	John Smith	123 Main St.	212-123-4567
8	1	4	1	2017-02-02 00:00:00	1	John Smith	123 Main St.	212-123-4567
The new column names id_x and id_y aren’t very helpful for us when we read the table. We can help make them more useful by using the keyword suffixes. We can provide a list of suffixes to use instead of “_x” and “_y”.

For example, we could use the following code to make the suffixes reflect the table names:

pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id',
    suffixes=['_order', '_customer']
)
The resulting table would look like this:

id_order	customer_id	product_id	quantity	timestamp	id_customer	customer_name	address	phone_number
1	2	3	1	2017-01-01 00:00:00	2	Jane Doe	456 Park Ave	949-867-5309
2	2	2	3	2017-01-01 00:00:00	2	Jane Doe	456 Park Ave	949-867-5309
3	3	1	1	2017-01-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
4	3	2	2	2016-02-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
5	3	3	3	2017-02-01 00:00:00	3	Joe Schmo	789 Broadway	112-358-1321
6	1	4	2	2017-03-01 00:00:00	1	John Smith	123 Main St.	212-123-4567
7	1	1	1	2017-02-02 00:00:00	1	John Smith	123 Main St.	212-123-4567
8	1	4	1	2017-02-02 00:00:00	1	John Smith	123 Main St.	212-123-4567

#-----------------------------------------------------------
import codecademylib
import pandas as pd

orders = pd.read_csv('orders.csv')
print(orders)
products = pd.read_csv('products.csv')
print(products)

orders_products = pd.merge(
  orders,
  products,
	left_on = 'product_id',
	right_on = 'id',
	suffixes = ['_orders', '_products'])

print (orders_products)

#------------------------------------------

WORKING WITH MULTIPLE DATAFRAMES
Outer Merge
In the previous exercise, we saw that when we merge two DataFrames whose rows don’t match perfectly, we lose the unmatched rows.

This type of merge (where we only include matching rows) is called an inner merge. There are other types of merges that we can use when we want to keep information from the unmatched rows.

Suppose that two companies, Company A and Company B have just merged. They each have a list of customers, but they keep slightly different data. Company A has each customer’s name and email. Company B has each customer’s name and phone number. They have some customers in common, but some are different.

company_a

name	email
Sally Sparrow	sally.sparrow@gmail.com
Peter Grant	pgrant@yahoo.com
Leslie May	leslie_may@gmail.com
company_b

name	phone
Peter Grant	212-345-6789
Leslie May	626-987-6543
Aaron Burr	303-456-7891
If we wanted to combine the data from both companies without losing the customers who are missing from one of the tables, we could use an Outer Join. An Outer Join would include all rows from both tables, even if they don’t match. Any missing values are filled in with None or nan (which stands for “Not a Number”).

pd.merge(company_a, company_b, how='outer')
The resulting table would look like this:



#-----------------------------------------------------

WORKING WITH MULTIPLE DATAFRAMES
Left and Right Merge
Let’s return to the merge of Company A and Company B.

Left Merge
Suppose we want to identify which customers are missing phone information. We would want a list of all customers who have email, but don’t have phone.

We could get this by performing a Left Merge. A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.

For this command, the order of the arguments matters. If the first DataFrame is company_a and we do a left join, we’ll only end up with rows that appear in company_a.

By listing company_a first, we get all customers from Company A, and only customers from Company B who are also customers of Company A.

pd.merge(company_a, company_b, how='left')
The result would look like this:

name	email	phone
Sally Sparrow	sally.sparrow@gmail.com	None
Peter Grant	pgrant@yahoo.com	212-345-6789
Leslie May	leslie_may@gmail.com	626-987-6543
Now let’s say we want a list of all customers who have phone but no email. We can do this by performing a Right Merge.

Right Merge
Right merge is the exact opposite of left merge. Here, the merged table will include all rows from the second (right) table, but only rows from the first (left) table that match the second table.

By listing company_a first and company_b second, we get all customers from Company B, and only customers from Company A who are also customers of Company B.

pd.merge(company_a, company_b, how="right")
The result would look like this:

name	email	phone
Peter Grant	pgrant@yahoo.com	212-345-6789
Leslie May	leslie_may@gmail.com	626-987-6543
Aaron Burr	None	303-456-7891

#--------------------------------------------------------------------
WORKING WITH MULTIPLE DATAFRAMES
Concatenate DataFrames
Sometimes, a dataset is broken into multiple tables. For instance, data is often split into multiple CSV files so that each download is smaller.

When we need to reconstruct a single DataFrame from multiple smaller DataFrames, we can use the method pd.concat([df1, df2, df2, ...]). This method only works if all of the columns are the same in all of the DataFrames.

For instance, suppose that we have two DataFrames:

df1
name	email
Katja Obinger	k.obinger@gmail.com
Alison Hendrix	alisonH@yahoo.com
Cosima Niehaus	cosi.niehaus@gmail.com
Rachel Duncan	rachelduncan@hotmail.com
df2
name	email
Jean Gray	jgray@netscape.net
Scott Summers	ssummers@gmail.com
Kitty Pryde	kitkat@gmail.com
Charles Xavier	cxavier@hotmail.com
If we want to combine these two DataFrames, we can use the following command:

pd.concat([df1, df2])
That would result in the following DataFrame:

#--------------------------------------------------------------------

WORKING WITH MULTIPLE DATAFRAMES
Review
This lesson introduced some methods for combining multiple DataFrames:

Creating a DataFrame made by matching the common columns of two DataFrames is called a merge
We can specify which columns should be matches by using the keyword arguments left_on and right_on
We can combine DataFrames whose rows don’t all match using left, right, and outer merges and the how keyword argument
We can stack or concatenate DataFrames with the same columns using pd.concat
Instructions
1.
Cool T-Shirts Inc. just created a website for ordering their products. They want you to analyze two datasets for them:

visits contains information on all visits to their landing page
checkouts contains all users who began to checkout on their website
Use print to inspect each DataFrame.

2.
We want to know the amount of time from a user’s initial visit to the website to when they start to check out.

Use merge to combine visits and checkouts and save it to the variable v_to_c.

3.
In order to calculate the time between visiting and checking out, define a column of v_to_c called time by pasting the following code into script.py:

v_to_c['time'] = v_to_c.checkout_time - \
                 v_to_c.visit_time

print(v_to_c)
4.
To get the average time to checkout, paste the following code into script.py:

print(v_to_c.time.mean())

#-------------------------------------------------------------------------------------
DATA MANIPULATION WITH PANDAS
Page Visits Funnel
Cool T-Shirts Inc. has asked you to analyze data on visits to their website. Your job is to build a funnel, which is a description of how many people continue to the next step of a multi-step process.

In this case, our funnel is going to describe the following process:

A user visits CoolTShirts.com
A user adds a t-shirt to their cart
A user clicks “checkout”
A user actually purchases a t-shirt
If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
11/12Complete
Mark the tasks as complete by checking them off
Funnel for Cool T-Shirts Inc.
1.
Inspect the DataFrames using print and head:

visits lists all of the users who have visited the website
cart lists all of the users who have added a t-shirt to their cart
checkout lists all of the users who have started the checkout
purchase lists all of the users who have purchased a t-shirt
2.
Combine visits and cart using a left merge.

If we want to combine df1 and df2 with a left merge, we use the following code:

pd.merge(df1, df2, how='left')
OR

df1.merge(df2, how='left')
3.
How long is your merged DataFrame?

Use len to find out the number of rows in a DataFrame.

4.
How many of the timestamps are null for the column cart_time?

What do these null rows mean?

You can select null rows from column1 of a DataFrame df using the following code:

df[df.column1.isnull()]
5.
What percent of users who visited Cool T-Shirts Inc. ended up not placing a t-shirt in their cart?

Note: To calculate percentages, it will be helpful to turn either the numerator or the denominator into a float, by using float(), with the number to convert passed in as input. Otherwise, Python will use integer division, which truncates decimal points.

If a row of your merged DataFrame has cart_time equal to null, then that user visited the website, but did not place a t-shirt in their cart.

6.
Repeat the left merge for cart and checkout and count null values. What percentage of users put items in their cart, but did not proceed to checkout?

You can find the percentage of users who put items in their cart but did not proceed to checkout by counting the null values of checkout_time and comparing it to the total number of users who put items in their cart.

7.
Merge all four steps of the funnel, in order, using a series of left merges. Save the results to the variable all_data.

Examine the result using print and head.

Suppose we wanted to merge df1, df2, and df3 using a left merge. We could use the following code:

df1.merge(df2, how='left')\
   .merge(df3, how='left')
8.
What percentage of users proceeded to checkout, but did not purchase a t-shirt?

9.
Which step of the funnel is weakest (i.e., has the highest percentage of users not completing it)?

How might Cool T-Shirts Inc. change their website to fix this problem?

Average Time to Purchase
10.
Using the giant merged DataFrame all_data that you created, let’s calculate the average time from initial visit to final purchase. Start by adding the following column to your DataFrame:

all_data['time_to_purchase'] = \
    all_data.purchase_time - \
    all_data.visit_time
11.
Examine the results using:

print(all_data.time_to_purchase)
12.
Calculate the average time to purchase using the following code:

print(all_data.time_to_purchase.mean())
#-------------------------------------------------------------------------------------

import codecademylib
import pandas as pd

visits = pd.read_csv('visits.csv',
                     parse_dates=[1])
cart = pd.read_csv('cart.csv',
                   parse_dates=[1])
checkout = pd.read_csv('checkout.csv',
                       parse_dates=[1])
purchase = pd.read_csv('purchase.csv',
                       parse_dates=[1])

#print (visits.head())
#print (cart.head())
#print (checkout.head())
#print (purchase.head())

df = pd.merge(visits, cart, how = "left")
df = pd.merge(df, checkout, how = 'left')
df = pd.merge(df, purchase, how = 'left')

user_id_count = df.user_id.count()
cart_count = df.cart_time.count()
checkout_count = df.checkout_time.count()
purchase_count = df.purchase_time.count()

cart_perc = 100 * cart_count / user_id_count
print ("Cart Percentage is " + str(cart_perc) + "%")

checkout_perc = 100 * checkout_count / cart_count
print ("Cart Percentage is " + str(checkout_perc) + "%")

purchase_perc = 100 * purchase_count / checkout_count
print ("Cart Percentage is " + str(purchase_perc) + "%")

df['time_to_purchase'] = df.purchase_time - df.visit_time

print (df.time_to_purchase)

print (df.time_to_purchase.mean())

#print (df[df.cart_time.isnull()].count())

#count_is_null = lambda row: 1 if row.isnull() else 0

#print (df.cart_time.apply(count_is_null).count())

#----------------------------------------------------

import pandas as pd
pd.set_option('display.max_colwidth', -1)

# Loading the data and investigating it
jeopardy_data = pd.read_csv("jeopardy.csv")
#print(jeopardy_data.columns)

# Renaming misformatted columns
jeopardy_data = jeopardy_data.rename(columns = {" Air Date": "Air Date", " Round" : "Round", " Category": "Category", " Value": "Value", " Question":"Question", " Answer": "Answer"})
#print(jeopardy_data.columns)
#print(jeopardy_data["Question"])

# Filtering a dataset by a list of words
def filter_data(data, words):
  # Lowercases all words in the list of words as well as the questions. Returns true is all of the words in the list appear in the question.
  filter = lambda x: all(word.lower() in x.lower() for word in words)
  # Applies the lambda function to the Question column and returns the rows where the function returned True
  return data.loc[data["Question"].apply(filter)]

# Testing the filter function
filtered = filter_data(jeopardy_data, ["King", "England"])
#print(filtered["Question"])

# Adding a new column. If the value of the float column is not "None", then we cut off the first character (which is a dollar sign), and replace all commas with nothing, and then cast that value to a float. If the answer was "None", then we just enter a 0.
jeopardy_data["Float Value"] = jeopardy_data["Value"].apply(lambda x: float(x[1:].replace(',','')) if x != "None" else 0)

# Filtering the dataset and finding the average value of those questions
filtered = filter_data(jeopardy_data, ["King"])
print(filtered["Float Value"].mean())

# A function to find the unique answers of a set of data
def get_answer_counts(data):
    return data["Answer"].value_counts()

# Testing the answer count function
print(get_answer_counts(filtered))


#------------------------ import form hand note -------------------------

df = pd.read_csv('source.csv')
df = pd.to_csv('target.csv')

df.head(10) #print 10 rows
df.head() # print 5 rows

df.tail()

df.info() # basic dataframe info

df = pd.DataFrame({
		'name' : ['John', 'Jame', 'Joe'],
		'address' : ['123 Main St', '456 Maple Ave', '789 Broadway'],
		'age' : [34, 28, 51]
		})

df = pd.DataFrame([
		['John', 'Jame', 'Joe'],
		['123 Main St', '456 Maple Ave', '789 Broadway'],
		[34, 28, 51],
		],
		columns = ['name', 'address', 'column'])
	
	
import pandas as pd
data = [['Alex',10],['Bob',12],['Clarke',13]]
df = pd.DataFrame(data,columns=['Name','Age'])
print df
#Its output is as follows −
'''
      Name      Age
0     Alex      10
1     Bob       12
2     Clarke    13
'''

flag_to_check = pd.DataFrame ([[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], 
columns =  ['Red', 'Green', 'Blue', 'Gold', 'White', 'Black', 'Orange','Circles','Crosses', 'Saltires', 'Quarters', 'Sunstars', 'Crescent', 'Triangle'])


		
#use dictionary to create dataframe
data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}	
df = pd.DataFrame.from_dict(data)

#   col_1 col_2
#0      3     a
#1      2     b
#2      1     c
#3      0     d

data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}
pd.DataFrame.from_dict(data, orient='index')
#       0  1  2  3
#row_1  3  2  1  0
#row_2  a  b  c  d






# select certain column

df.age 		# method 1
df['age']	# method 2

df['name', 'age']

# select certain column sample 2


To select columns from a DataFrame:

name = df[['Column1', 'Column2']]

So it should look something like:

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]



# select certain row

df.iloc[2]
df.iloc[2:]
df.iloc[2:3]
df.iloc[:4]
df.iloc[-2]
df.iloc[[1,3,5]]

#return value of a cell

In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493

#select with logic

df[df.name = 'xxx']
df[df.age > 30]
df[(df.age < 30) & (df.name = 'Jane')] # | for "or"
df[df.name Isin (['James', 'Jane', 'Jack'])

#reset_index 

df.reset_index(drop = True , inplace = True)
#drop = True   delete old index

#add new column
df['new_column'] = ['a', 'b', 'c']
df['is_taxed'] = True # add whole new column with same value
df['proift'] = df['price'] - df['cost'] # or df.price - df.cost
df['l_name'] = df.name.apply(lower)
df['last_name'] = df.name.apply(lambda x: x.split(' ')[-1])

df['price with tax'] = df.apply(lambda row: \n
						row['price']*1.075
						if row['is taxed'] == 'Yes'
						else row['price'],
						axis = 1)

#column raname

df.rename(columns = {'name' : 'new_name',
					'namei' : 'new_namei',
					'name3' : 'new_name3'},
					inplace = True)
					
df = pd.DataFrame([
		['John', 'Jame', 'Joe'],
		['123 Main St', '456 Maple Ave', '789 Broadway'],
		[34, 28, 51],
		],
		columns = ['name', 'address', 'column'])
df.columns = ['First Name', 'Age', 'Address'] #batch change column name

#apply command to column

df.column_name.command()

mean max unique
std min unique
median count

print(shipments.state)
['CA', 'CA', 'NY', 'NY', 'NJ', 'NJ']

print(shipments.state.unique())
['CA', 'NY', 'NJ']

print(shipments.state.unique().count())
3

user_id_count = df.user_id.count()
print(df.time_to_purchase.mean())

#groupby

df.groupby('column1').column2.measurment()

grades = df.groupby('students').grade.mean()

tea_counts = teas.groupby('category').id.count().reset_index()
tea_counts = tea_counts.rename(columns = {'id' : 'counts'})

high_earners = df.groupby('category').wage.apply(lambda x: np.percentile(x, 75)).reset_index()
df.groupby(['location', 'Day of week'])['Total cales'].mean().reset_index()

#pivot table
df.pivot(columns = 'column pivot',
		inex = 'column to be row',
		values = 'column to be values')
		
#merge df
new_df = pd.merge(df1, df2)
new_df = df.merge(df1).merge(df3)

#concate
menu = pd.concate([df1], [df2]) # zhe ge you cuowu

#concat and re index
pd.concat([s1, s2], ignore_index=True)

#merge left / right
how = left
#only left df item will be rept

#merge inner / outter
df_new = pd.merge(df1, df2, how = 'outer')
#merge all lines without losing data 'nan' and 'None' will be filled

#merge and change column name
pd.merge(
		orders,
		customers,
		left_on = 'customer_id',
		right_on = 'id',
		suffixes = ['_order', '_customer'])
		
#rename at merge to match the df name
pd.merge(orders, customers.rename(columns = {'id' : 'customer'}))




						





# rearrange column sequence 

df = df[['x', 'y', 'a', 'b']]



#delete column
df = df.drop(columns = 'column name')

#rename column

df.rename(columns = {'old_name' : 'new_name',
					 'old_name1' : 'new_name1'},
					 inplace = True)
					 
#------------------read excel file---------------------

import io
import requests
import pandas as pd
from zipfile import ZipFile
r = requests.get('http://www.contextures.com/SampleData.zip')
ZipFile(io.BytesIO(r.content)).extractall()

df = pd.read_excel('SampleData.xlsx', sheet_name='SalesOrders')
					 

#----------------- read html file ------------------

dfs = pd.read_html(html_string)

dfs = pd.read_html('http://www.contextures.com/xlSampleData01.html')

df = pd.read_html(html_string, header=0)[0]
#since there could be many tables in one html, dfs stands for many df . and [0] indicate the first one.

#header = 0 only used when header become one row of data 

#--------------- to excel ---------------

df.to_csv('ozito.csv')

df1.to_excel("output.xlsx")

df1.to_excel("output.xlsx", sheet_name='Sheet_name_1')



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- PYTHON DATAFRAME NUMPY AXIS EXPLAINED #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

'''
Axis in Series
Series is a one-dimensional array of values. Under the hood, it uses NumPy ndarray. That is where the term “axis” came from. NumPy uses it quite frequently because ndarray can have a lot of dimensions.

Series object has only “axis 0” because it has only one dimension.'''

import pandas as pd
pd.Series(['red', 'green', 'blue', 'white', 'black'])

|	#0	red
|	#1	green
|	#2	blue
|	#3	white
|	#4	black
V
#axis 0


'''
The arrow on the image displays “axis 0” and its direction for the Series object.

Usually, in Python, one-dimensional structures are displayed as a row of values. On the contrary, here we see that Series is displayed as a column of values.

Each cell in Series is accessible via index value along the “axis 0”. For our Series object indexes are: 0, 1, 2, 3, 4. Here is an example of accessing different values:'''

#>>> import pandas as pd
#>>> srs = pd.Series(['red', 'green', 'blue', 'white', 'black'])
#>>> srs[0]
#'red'
#>>> srs[3]
#'white'


'''
Axes in DataFrame
DataFrame is a two-dimensional data structure akin to SQL table or Excel spreadsheet. It has columns and rows. Its columns are made of separate Series objects. Let’s see an example:'''

import pandas as pd
srs_a = pd.Series([10, 30, 40, 90, 80])
srs_b = pd.Series(['red', 'green', 'blue', 'white', 'black'])

df = pd.DataFrame({'a': srs_a, 'b':srs_b})
df

#	-------------> axis 1
#|		a	b
#|	0	10	red
#|	1	30	green
#|	2	40	blue
#|	3	90	white
#|	4	80	black
#V

#a
#x
#i
#s

#0


'''A DataFrame object has two axes: “axis 0” and “axis 1”. “axis 0” represents rows and “axis 1” represents columns. Now it’s clear that Series and DataFrame share the same direction for “axis 0” – it goes along rows direction.

Our DataFrame object has 0, 1, 2, 3, 4 indexes along the “axis 0”, and additionally, it has “axis 1” indexes which are: ‘a’ and ‘b’.

To access an element within DataFrame we need to provide two indexes (one per each axis). Also, instead of bare brackets, we need to use .loc method:'''

#>>> import pandas as pd
#>>> srs_a = pd.Series([1,3,6,8,9])
#>>> srs_b = pd.Series(['red', 'green', 'blue', 'white', 'black'])
#>>> df = pd.DataFrame({'a': srs_a, 'b': srs_b})
#>>> df.loc[2, 'b']
#'blue'
#>>> df.loc[3, 'a']
#8

'''
Using “axis” parameter in API calls
There are a lot of different API calls for Series and DataFrame objects which accept “axis” parameter. Series object has only one axis, so this parameter always equals 0 for it. Thus, you can omit it, because it does not affect the result:

>>> import pandas as pd
>>> srs = pd.Series([1, 3, pd.np.nan, 4, pd.np.nan])
>>> srs.dropna()
0    1.0
1    3.0
3    4.0
dtype: float64
>>> srs.dropna(axis=0)
0    1.0
1    3.0
3    4.0
dtype: float64
On the contrary, DataFrame has two axes, and “axis” parameter determines along which axis an operation should be performed. For example, .sum can be applied along “axis 0”. That means, .sum operation calculates a sum for each column:

>>> import pandas as pd
>>> srs_a = pd.Series([10,30,60,80,90])
>>> srs_b = pd.Series([22, 44, 55, 77, 101])
>>> df = pd.DataFrame({'a': srs_a, 'b': srs_b})
>>> df
    a    b
0  10   22
1  30   44
2  60   55
3  80   77
4  90  101
>>> df.sum(axis=0)
a    270
b    299
dtype: int64
We see, that having sum with axis=0 smashed all values along the direction of the “axis 0” and left only columns(‘a’ and ‘b’) with appropriate sums.

With axis=1 it produces a sum for each row:

>>> df.sum(axis=1)
0     32
1     74
2    115
3    157
4    191
dtype: int64
If you prefer regular names instead of numbers, each axis has a string alias. “axis 0” has two aliases: ‘index’ and ‘rows’. “axis 1” has only one: ‘columns’. You can use these aliases instead of numbers:

>>> df.sum(axis='index')
a    270
b    299
dtype: int64
>>> df.sum(axis='rows')
a    270
b    299
dtype: int64
>>> df.sum(axis='columns')
0     32
1     74
2    115
3    157
4    191
dtype: int64
Dropping NaN values
Let’s build a simple DataFrame with NaN values and observe how axis affects .dropna method:

>>> import pandas as pd
>>> import numpy as np
>>> df = pd.DataFrame({'a': [2, np.nan, 8, 3], 'b': [np.nan, 32, 15, 7], 'c': [-3, 5, 22, 19]})
>>> df
     a     b   c
0  2.0   NaN  -3
1  NaN  32.0   5
2  8.0  15.0  22
3  3.0   7.0  19
>>> df.dropna(axis=0)
     a     b   c
2  8.0  15.0  22
3  3.0   7.0  19
Here .dropna filters out any row(we are moving along “axis 0”) which contains NaN value.

Let’s use “axis 1” direction:

>>> df.dropna(axis=1)
    c
0  -3
1   5
2  22
3  19
Now .dropna collapsed “axis 1” and removed all columns with NaN values. Columns ‘a’ and ‘b’ contained NaN values, thus only ‘c’ column was left.

Concatenation
Concatenation function with axis=0 stacks the first DataFrame over the second:

>>> import pandas as pd
>>> df1 = pd.DataFrame({'a': [1,3,6,8,9], 'b': ['red', 'green', 'blue', 'white', 'black']})
>>> df2 = pd.DataFrame({'a': [0,2,4,5,7], 'b': ['jun', 'jul', 'aug', 'sep', 'oct']})
>>> pd.concat([df1, df2], axis=0)
   a      b
0  1    red
1  3  green
2  6   blue
3  8  white
4  9  black
0  0    jun
1  2    jul
2  4    aug
3  5    sep
4  7    oct
With axis=1 both DataFrames are put along each other:

>>> pd.concat([df1, df2], axis=1)
   a      b  a    b
0  1    red  0  jun
1  3  green  2  jul
2  6   blue  4  aug
3  8  white  5  sep
4  9  black  7  oct
Summary
Pandas borrowed the “axis” concept from NumPy library. The “axis” parameter does not have any influence on a Series object because it has only one axis. On the contrary, DataFrame API heavily relies on the parameter, because it’s a two-dimensional data structure, and many operations can be performed along different axes producing totally different results.'''


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- SAMPLES #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-


import pandas as pd
from matplotlib import pyplot as plt

restaurants = pd.read_csv('restaurants.csv')
print(restaurants.head())

cuisine_options_count = len(restaurants.cuisine.unique())
#cuisine_options_count = restaurants.cuisine.nunique()      #nunique give you the num of the unique type directly.

cuisine_counts = restaurants.groupby('cuisine').id.count().reset_index() # CALCULATE NO OF RESTAURANT OF EACH CUISINE

print(cuisine_counts)



######BACK######






#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON NUMPY STATISTIC PYTHON NUMPY STATISTIC PYTHON NUMPY STATISTIC PYTHON NUMPY STATISTIC #-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#INTRODUCTION TO STATISTICS WITH NUMPY 3/13

import numpy as np

class_year = np.array([1967, 1949, 2004, 1997, 1953, 1950, 1958, 1974, 1987, 2006, 2013, 1978, 1951, 1998, 1996, 1952, 2005, 2007, 2003, 1955, 1963, 1978, 2001, 2012, 2014, 1948, 1970, 2011, 1962, 1966, 1978, 1988, 2006, 1971, 1994, 1978, 1977, 1960, 2008, 1965, 1990, 2011, 1962, 1995, 2004, 1991, 1952, 2013, 1983, 1955, 1957, 1947, 1994, 1978, 1957, 2016, 1969, 1996, 1958, 1994, 1958, 2008, 1988, 1977, 1991, 1997, 2009, 1976, 1999, 1975, 1949, 1985, 2001, 1952, 1953, 1949, 2015, 2006, 1996, 2015, 2009, 1949, 2004, 2010, 2011, 2001, 1998, 1967, 1994, 1966, 1994, 1986, 1963, 1954, 1963, 1987, 1992, 2008, 1979, 1987])

millennials = np.mean(class_year >= 2005 ) 

print(millennials )

#INTRODUCTION TO STATISTICS WITH NUMPY 4/13

import numpy as np

allergy_trials = np.array([[6, 1, 3, 8, 2], 
                           [2, 6, 3, 9, 8], 
                           [5, 2, 6, 9, 9]])

total_mean = np.mean(allergy_trials)


trial_mean = np.mean(allergy_trials, axis = 1)

patient_mean = np.mean(allergy_trials, axis = 0)

print(total_mean)
print(trial_mean)
print(patient_mean)

#INTRODUCTION TO STATISTICS WITH NUMPY 7/13

import numpy as np

large_set = np.genfromtxt('household_income.csv', delimiter=',')

small_set = [10100, 35500, 105000, 85000, 25500, 40500, 65000]

small_set.sort()
small_set_median = small_set[(len(small_set)-1)/2]

large_set_median = np.median(large_set)

print(small_set_median)
print(large_set_median)

#INTRODUCTION TO STATISTICS WITH NUMPY 8/13

import numpy as np

time_spent = np.genfromtxt('file.csv', delimiter=',')

print(time_spent)

minutes_mean = np.mean(time_spent)

minutes_more_than_2 = np.mean(time_spent > 8)

minutes_median = np.median(time_spent)

print(minutes_mean)
print(minutes_median)
print(minutes_more_than_2)

best_measure = minutes_median

#INTRODUCTION TO STATISTICS WITH NUMPY
#Percentiles, Part II
Some percentiles have specific names:

The 25th percentile is called the first quartile
The 50th percentile is called the median
The 75th percentile is called the third quartile
The minimum, first quartile, median, third quartile, and maximum of a dataset are called a five-number summary. This set of numbers is a great thing to compute when we get a new dataset.

The difference between the first and third quartile is a value called the interquartile range. For example, say we have the following array:

d = [1, 2, 3, 4, 4, 4, 6, 6, 7, 8, 8]
We can calculate the 25th and 75th percentiles using np.percentile:

np.percentile(d, 25)
>>> 3.5
np.percentile(d, 75)
>>> 6.5
Then to find the interquartile range, we subtract the value of the 25th percentile from the value of the 75th:

6.5 - 3.5 = 3
50% of the dataset will lie within the interquartile range. The interquartile range gives us an idea of how spread out our data is. The smaller the interquartile range value, the less variance in our dataset. The greater the value, the larger the variance.

#INTRODUCTION TO STATISTICS WITH NUMPY 13/13

import numpy as np

rainfall = np.array([5.21, 3.76, 3.27, 2.35, 1.89, 1.55, 0.65, 1.06, 1.72, 3.35, 4.82, 5.11])


rain_mean = np.mean(rainfall)

rain_median = np.median(rainfall)

first_quarter = np.percentile(rainfall, 25)

third_quarter = np.percentile(rainfall, 75)

interquartile_range = third_quarter - first_quarter

rain_std = np.std(rainfall)

print(rainfall)
print(rain_mean)
print(rain_median)
print(first_quarter)
print(third_quarter)
print(interquartile_range)
print(rain_std)


#------------------------------------------------------------------------------

import codecademylib

import numpy as np

calorie_stats = np.genfromtxt('cereal.csv',delimiter = ",")

average_calories = np.mean(calorie_stats)

print(average_calories)

calorie_stats_sorted = sorted(calorie_stats)

print (calorie_stats)

print (calorie_stats_sorted)

median_calories = np.median(calorie_stats)

print("\n")
print (median_calories)

nth_percentile = np.percentile(calorie_stats, 3.8961)

print("\n")
print nth_percentile

more_calories = np.mean (calorie_stats > 60)

print (more_calories)

more_calories = np.std(calorie_stats)
print (more_calories)

#------------------------------------------------------------------------------


import codecademylib
import numpy as np
from matplotlib import pyplot as plt

# Brachiosaurus
b_data = np.random.normal(6.7,0.7,size = 1000)

# Fictionosaurus
f_data = np.random.normal(7.7,0.3,size = 1000)

plt.hist(b_data,
         bins=30, range=(5, 8.5), histtype='step',
         label='Brachiosaurus')
plt.hist(f_data,
         bins=30, range=(5, 8.5), histtype='step',
         label='Fictionosaurus')
plt.xlabel('Femur Length (ft)')
plt.legend(loc=2)
plt.show()

#--------------------------------------------
STATISTICAL DISTRIBUTIONS WITH NUMPY
Review
Let’s review! In this lesson, you learned how to use NumPy to analyze different distributions and generate random numbers to produce datasets. Here’s what we covered:

What is a histogram and how to map one using Matplotlib
How to identify different dataset shapes, depending on peaks or distribution of data
The definition of a normal distribution and how to use NumPy to generate one using NumPy’s random number functions
The relationships between normal distributions and standard deviations
The definition of a binomial distribution
Now you can use NumPy to analyze and graph your own datasets! You should practice building your intuition about not only what the data says, but what conclusions can be drawn from your observations.

Instructions
1.
Practice what you’ve just learned with a dataset on sunflower heights! Imagine that you work for a botanical garden and you want to see how the sunflowers you planted last year did to see if you want to plant more of them.

Calculate the mean and standard deviation of this dataset. Save the mean to sunflowers_mean and the standard deviation to sunflowers_std.

2.
We can see from the histogram that our data isn’t normally distributed. Let’s create a normally distributed sample to compare against what we observed.

Generate 5,000 random samples with the same mean and standard deviation as sunflowers. Save these to sunflowers_normal.

3.
Now that you generated sunflowers_normal, uncomment (remove all of the #) the second plt.hist statement. Press run to see your normal distribution and your observed distribution.

4.
Generally, 10% of sunflowers that are planted fail to bloom. We planted 200, and want to know the probability that fewer than 20 will fail to bloom.

First, generate 5,000 binomial random numbers that represent our situation. Save them to experiments.

5.
What percent of experiments had fewer than 20 sunflowers fail to bloom?

Save your answer to the variable prob. This is the approximate probability that fewer than 20 of our sunflowers will fail to bloom.

6.
Print prob. Is it likely that fewer than 20 of our sunflowers will fail to bloom?
#--------------------------------------------

import codecademylib
import numpy as np
from matplotlib import pyplot as plt

sunflowers = np.genfromtxt('sunflower_heights.csv',
                           delimiter=',')

# Calculate mean and std of sunflowers here:
sunflowers_mean = np.mean(sunflowers)
sunflowers_std = np.std(sunflowers)

print (sunflowers_mean)
print (sunflowers_std)

# Calculate sunflowers_normal here:
sunflowers_normal = np.random.normal(sunflowers_mean,sunflowers_std,5000)

plt.hist(sunflowers,
         range=(11, 15), histtype='step', linewidth=2,
        label='observed', normed=True)
plt.hist(sunflowers_normal,
         range=(11, 15), histtype='step', linewidth=2,
        label='normal', normed=True)
plt.legend()
plt.show()

# Calculate probabilities here:
experiments = np.random.binomial(200,0.1,5000)

prob = np.mean(experiments<20)

print("\n")
print (prob)

#-----------------------------------------------------------------
NUMPY: A PYTHON LIBRARY FOR STATISTICS
Election Results
You’re part of an impartial research group that conducts phone surveys prior to local elections. During this election season, the group conducted a survey to determine how many people would vote for Cynthia Ceballos vs. Justin Kerrigan in the mayoral election.

Now that the election has occurred, your group wants to compare the survey responses to the actual results.

Was your survey a good indicator? Let’s find out!

If you get stuck during this project or would like to see an experienced developer work through it, click “Get Help“ to see a project walkthrough video.

Tasks
7/9Complete
Mark the tasks as complete by checking them off
PROJECT STEPS
1.
First, import numpy and matplotlib.

2.
At the top of script.py is a list of the different survey responses.

Calculate the number of people who answered ‘Ceballos’ and save the answer to the variable total_ceballos.

Print the variable to the terminal to see its value.

total = sum([1 for n in list if n == foo])
3.
Calculate the percentage of people in the survey who voted for Ceballos and save it to the variable percentage_ceballos.

Print the variable to the terminal to see its value.

percentage = 100 * total/len(list)
4.
In the real election, 54% of the 10,000 town population voted for Cynthia Ceballos. Your supervisors are concerned because this is a very different outcome than what the poll predicted. They want you to determine if there is something wrong with the poll or if given the sample size, it was an entirely reasonable result.

Generate a binomial distribution that takes the number of total survey responses, the actual success rate, and the size of the town’s population as its parameters. Then divide the distribution by the number of survey responses. Save your calculation to the variable possible_surveys.

a = np.random.binomial(N, P, size=10000) / N.
Where N is the number of trials, P is the probability of success, and size is the total population. Also notice the period at the end of the line of code above. The period ensures that you are dividing each element in a by a float.

If you do not include the period, Python will assume you want integer division (an integer divided by an integer). Naturally, integer division returns an integer. For quotients less than 1 (for example, 7 divided by 18), this is problematic. Python will return a 0 (as opposed to a decimal, like 0.388), which can result in erroneous calculations.

5.
Plot a histogram of possible_surveys with a range of 0-1 and 20 bins.

plt.hist(array, range=(a, b), bins=n)
6.
As we saw, 47% of people we surveyed said they would vote for Ceballos, but 54% of people voted for Ceballos in the actual election.

Calculate the percentage of surveys that could have an outcome of Ceballos receiving less than 50% of the vote and save it to the variable ceballos_loss_surveys.

Print the variable to the terminal.

np.mean(array < 0.5)
7.
With this current poll, about 20% of the time a survey output would predict Kerrigan winning, even if Ceballos won the actual election.

Your co-worker points out that your poll would be more accurate if it had more responders.

Generate another binomial distribution, but this time, see what would happen if you had instead surveyed 7,000 people. Divide the distribution by the size of the survey and save your findings to large_survey.

8.
Now, recalculate the percentage of surveys that would have an outcome of Ceballos losing and save it to the variable ceballos_loss_new, and print the value to the terminal.

What do we notice about this new value?

What advice would you give to your supervisors about predicting results from surveys?

9.
Click here for a video walkthrough from our experts to help you check your work!
#--------------------------------------------------------------------


import codecademylib
import numpy as np
from matplotlib import pyplot as plt


survey_responses = ['Ceballos', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos','Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos',
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos',
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos']

#total_ceballos = survey_responses.count("Ceballos")
total_ceballos = sum([1 for response in survey_responses if response == "Ceballos"])
servey_length = float(len(survey_responses))
#percentage_ceballos = total_ceballos / 70.
percentage_ceballos = total_ceballos / servey_length
print total_ceballos
print percentage_ceballos

possible_surveys = np.random.binomial(servey_length,.54,size = 10000) / servey_length

plt.hist(possible_surveys, bins = 20, range = (0,1))
plt.show()

#Codecademy answer
possible_serveys_length = float(len(possible_surveys))
incorrect_predicitons = len(possible_surveys[possible_surveys < 0.5 ])
ceballos_loss_surveys = incorrect_predicitons / possible_serveys_length
print ceballos_loss_surveys

#Xu's answer
ceballos_loss_surveys = np.mean(possible_surveys < 0.5)
print (ceballos_loss_surveys)

#Codecademy answer
large_survey_length = float (7000)
large_survey = np.random.binomial(large_survey_length, 0.54, size = 10000) / large_survey_length

incorrect_predictions = len(large_survey[large_survey < 0.5])
ceballos_loss_new = incorrect_predictions / large_survey_length
#Xu's answer
large_survey = np.random.binomial(7000, 0.54, size = 10000) / 7000.

ceballos_loss_new = np.mean(large_survey < 0.5)

print (ceballos_loss_new)


#Learn Statistics With Python

#get average / mean of a list / series
np.average(list)

'''Median
The formal definition for the median of a dataset is:

The value that, assuming the dataset is ordered from smallest to largest, falls in the middle. If there are an even number of values in a dataset, you either report both of the middle two values or their average.'''

#np.sort()----------------

import numpy as np

# Array of the first five author ages
five_author_ages = np.array([29, 49, 42, 43, 32])

# Fill in the empty array with the values sorted
sorted_author_ages = np.sort(five_author_ages)

# Save the median value to median_value
median_age = np.median(sorted_author_ages)

# Print the sorted array and median value
print("The sorted array is: " + str(sorted_author_ages))
print("The median of the array is: " + str(median_age))


'''
Mode
The formal definition for the mode of a dataset is:

The most frequently occurring observation in the dataset. A dataset can have multiple modes if there is more than one value with the same maximum frequency.'''

from scipy import stats

example_array = np.array([24, 16, 12, 10, 12, 28, 38, 12, 28, 24])

example_mode = stats.mode(example_array)

'''
The code above calculates the mode of the values in example_array and saves it to example_mode.

The result of stats.mode() is an object with the mode value, and its count.

>>> example_mode
ModeResult(mode=array([12]), count=array([3]))


If there are multiple modes, the stats.mode() function will always return the smallest mode in the dataset.

Let’s look at an array with two modes, 12 and 24:
'''
from scipy import stats

example_array = np.array([24, 16, 12, 10, 12, 24, 38, 12, 28, 24])

example_mode = stats.mode(example_array)
'''
The result of stats.mode() is an object with the smallest mode value, and its count.

>>> example_mode
ModeResult(mode=array([12]), count=array([3]))'''


#review

# Import packages
import codecademylib
import numpy as np
import pandas as pd
from scipy import stats

# Import matplotlib pyplot
from matplotlib import pyplot as plt

# Read in transactions data
greatest_books = pd.read_csv("top-hundred-books.csv")

# Save transaction times to a separate numpy array
author_ages = greatest_books['Ages']

# Calculate the average and median value of the author_ages array
average_age = np.average(author_ages)
median_age = np.median(author_ages)
mode_age = 38

# Plot the figure
plt.hist(author_ages, range=(10, 80), bins=14,  edgecolor='black')
plt.title("Author Ages at Publication")
plt.xlabel("Publication Age")
plt.ylabel("Count")
plt.axvline(average_age, color='r', linestyle='solid', linewidth=3, label="Mean")
plt.axvline(median_age, color='y', linestyle='dotted', linewidth=3, label="Median")
plt.axvline(mode_age, color='orange', linestyle='dashed', linewidth=3, label="Mode")
plt.legend()

plt.show()

#--------------------------------------


# Import packages
import numpy as np
import pandas as pd
from scipy import stats

# Read in housing data
brooklyn_one_bed = pd.read_csv('brooklyn-one-bed.csv')
brooklyn_price = brooklyn_one_bed['rent']

manhattan_one_bed = pd.read_csv('manhattan-one-bed.csv')
manhattan_price = manhattan_one_bed['rent']

queens_one_bed = pd.read_csv('queens-one-bed.csv')
queens_price = queens_one_bed['rent']

# Add mean calculations below 
brooklyn_mean = np.mean(brooklyn_price)

manhattan_mean = np.mean(manhattan_price)

queens_mean = np.mean(queens_price)


# Add median calculations below
brooklyn_median = np.median(brooklyn_price)

manhattan_median = np.median(manhattan_price)

queens_median = np.median(queens_price)


# Add mode calculations below
brooklyn_mode = stats.mode(brooklyn_price)

manhattan_mode = stats.mode(manhattan_price)

queens_mode = stats.mode(queens_price)


# Don't look below here
# Mean
try:
    print("The mean price in Brooklyn is " + str(round(brooklyn_mean, 2)))
except NameError:
    print("The mean price in Brooklyn is not yet defined.")
try:
    print("The mean price in Manhattan is " + str(round(manhattan_mean, 2)))
except NameError:
    print("The mean in Manhattan is not yet defined.")
try:
    print("The mean price in Queens is " + str(round(queens_mean, 2)))
except NameError:
    print("The mean price in Queens is not yet defined.")
    
    
# Median
try:
    print("The median price in Brooklyn is " + str(brooklyn_median))
except NameError:
    print("The median price in Brooklyn is not yet defined.")
try:
    print("The median price in Manhattan is " + str(manhattan_median))
except NameError:
    print("The median price in Manhattan is not yet defined.")
try:
    print("The median price in Queens is " + str(queens_median))
except NameError:
    print("The median price in Queens is not yet defined.")
    
    
#Mode
try:
    print("The mode price in Brooklyn is " + str(brooklyn_mode[0][0]) + " and it appears " + str(brooklyn_mode[1][0]) + " times out of " + str(len(brooklyn_price)))
except NameError:
    print("The mode price in Brooklyn is not yet defined.")
try:
    print("The mode price in Manhattan is " + str(manhattan_mode[0][0]) + " and it appears " + str(manhattan_mode[1][0]) + " times out of " + str(len(manhattan_price)))
except NameError:
    print("The mode price in Manhattan is not yet defined.")
try:
    print("The mode price in Queens is " + str(queens_mode[0][0]) + " and it appears " + str(queens_mode[1][0]) + " times out of " + str(len(queens_price)))
except NameError:
    print("The mode price in Queens is not yet defined.")
    
    

'''VARIANCE
Variance
Finding the mean, median, and mode of a dataset is a good way to start getting an understanding of the general shape of your data

However, those three descriptive statistics only tell part of the story. Consider the two datasets below:

dataset_one = [-4, -2, 0, 2, 4]
dataset_two = [-400, -200, 0, 200, 400]
These two datasets have the same mean and median — both of those values happen to be 0. If we only reported these two statistics, we would not be communicating any meaninful difference between these two datasets.

This is where variance comes into play. Variance is a descriptive statistic that describes how spread out the points in a data set are.'''

'''
VARIANCE
Distance From Mean
Now that you have learned the importance of describing the spread of a dataset, let’s figure out how to mathematically compute this number.

How would you attempt to capture the spread of the data in a single number?

Let’s start with our intuition — we want the variance of a dataset to be a large number if the data is spread out, and a small number if the data is close together.

Two histograms. One with a large spread and one with a smaller spread.
A lot of people may initially consider using the range of the data. But that only considers two points in your entire dataset. Instead, we can include every point in our calculation by finding the difference between every data point and the mean.

The difference between the mean and four different points.
If the data is close together, then each data point will tend to be close to the mean, and the difference will be small. If the data is spread out, the difference between every data point and the mean will be larger.

Mathematically, we can write this comparison as

\text{difference} = X - \mudifference=X−μ
Where X is a single data point and the Greek letter mu is the mean.

VARIANCE
Average Distances
We now have five different values that describe how far away each point is from the mean. That seems to be a good start in describing the spread of the data. But the whole point of calculating variance was to get one number that describes the dataset. We don’t want to report five values — we want to combine those into one descriptive statistic.

To do this, we’ll take the average of those five numbers. By adding those numbers together and dividing by 5, we’ll end up with a single number that describes the average distance between our data points and the mean.

Note that we’re not quite done yet — our final answer is going to look a bit strange here. There’s a small problem that we’ll fix in the next exercise.'''

'''

VARIANCE
Square The Differences
We’re almost there! We have one small problem with our equation. Consider this very small dataset:

[-5, 5]
The mean of this dataset is 0, so when we find the difference between each point and the mean we get -5 - 0 = -5 and 5 - 0 = 5.

When we take the average of -5 and 5 to get the variance, we get 0!

Now think about what would happen if the dataset were [-200, 200]. We’d get the same result! That can’t possibly be right — the dataset with 200 is much more spread out than the dataset with 5, so the variance should be much larger!

The problem here is with negative numbers. Because one of our data points was 5 units below the mean and the other was 5 units above the mean, they canceled each other out!

When calculating variance, we don’t care if a data point was above or below the mean — all we care about is how far away it was. To get rid of those pesky negative numbers, we’ll square the difference between each data point and the mean.

Our equation for finding the difference between a data point and the mean now looks like this:

difference = (X - mu)^2

'''

import numpy as np

grades = [88, 82, 85, 84, 90]
mean = np.mean(grades)

#When calculating these variables, square the difference.
difference_one = (88 - mean)**2
difference_two = (82 - mean)**2
difference_three = (85 - mean)**2
difference_four = (84 - mean)**2
difference_five = (90 - mean)**2

difference_sum = difference_one + difference_two + difference_three + difference_four + difference_five

variance = difference_sum / 5

print("The sum of the squared differences is " + str(difference_sum))
print("The variance is " + str(variance))

'''

VARIANCE
Variance In NumPy
Well done! You’ve calculated the variance of a data set. The full equation for the variance is as follows:

 
​σ^2 = ∑ (N i=1) (Xi - mu )^2 / N

​	 
Let’s dissect this equation a bit.

Variance is usually represented by the symbol sigma squared.
We start by taking every point in the dataset — from point number 1 to point number N — and finding the difference between that point and the mean.
Next, we square each difference to make all differences positive.
Finally, we average those squared differences by adding them together and dividing by N, the total number of points in the dataset.
All of this work can be done quickly using Python’s NumPy library. The var() function takes a list of numbers as a parameter and returns the variance of that dataset.
'''
import numpy as np

dataset = [3, 5, -2, 49, 10]
variance = np.var(dataset)

'''
STANDARD DEVIATION
Variance Recap
When beginning to work with a dataset, one of the first pieces of information you might want to investigate is the spread — is the data close together or far apart? One of the tools in our statistics toolbelt to do this is the descriptive statistic variance:

        N
      ∑ i=1 (Xi - mu )^2
​σ^2 = --------------------
               N

​	 
By finding the variance of a dataset, we can get a numeric representation of the spread of the data. If you want to take a deeper dive into how to calculate variance, check out our variance course.

But what does that number really mean? How can we use this number to interpret the spread?

It turns out, using variance isn’t necessarily the best statistic to use to describe spread. Luckily, there is another statistic — standard deviation — that can be used instead.

In this lesson, we’ll be working with two datasets. The first dataset contains the heights (in inches) of a random selection of players from the NBA. The second dataset contains the heights (in inches) of a random selection of users on the dating platform OkCupid — let’s hope these users were telling the truth about their height!

Instructions
1.
Run the code to see a histogram of the datasets. Look at the console to see the mean and variance of each dataset. Try to answer the following questions:

What does it mean for the OkCupid dataset to have a larger variance than the NBA dataset?
What are the units of the mean? Is someone who is 80 inches tall taller than the average of either group? Which group(s)?
In this example, the units of variance are inches squared. Can you interpret what it means for the variance of the NBA dataset to be 13.32 inches squared?



STANDARD DEVIATION
Standard Deviation
Variance is a tricky statistic to use because its units are different from both the mean and the data itself. For example, the mean of our NBA dataset is 77.98 inches. Because of this, we can say someone who is 80 inches tall is about two inches taller than the average NBA player.

However, because the formula for variance includes squaring the difference between the data and the mean, the variance is measured in units squared. This means that the variance for our NBA dataset is 13.32 inches squared.

This result is hard to interpret in context with the mean or the data because their units are different. This is where the statistic standard deviation is useful.

Standard deviation is computed by taking the square root of the variance. sigma is the symbol commonly used for standard deviation. Conveniently, sigma squared is the symbol commonly used for variance:


σ  = sqrt(sigma ^2)  = sqrt(∑ (N i=1) (Xi - mu )^2 / N)

STANDARD DEVIATION
Standard Deviation in NumPy
There is a NumPy function dedicated to finding the standard deviation of a dataset — we can cut out the step of first finding the variance. The NumPy function std() takes a dataset as a parameter and returns the standard deviation of that dataset:
'''
import numpy as np

dataset = [4, 8, 15, 16, 23, 42]
standard_deviation = np.std()

'''

STANDARD DEVIATION
Using Standard Deviation
Now that we’re able to compute the standard deviation of a dataset, what can we do with it?

Now that our units match, our measure of spread is easier to interpret. By finding the number of standard deviations a data point is away from the mean, we can begin to investigate how unusual that datapoint truly is. In fact, you can usually expect around 68% of your data to fall within one standard deviation of the mean, 95% of your data to fall within two standard deviations of the mean, and 99.7% of your data to fall within three standard deviations of the mean.

A histogram showing where the standard deviations fallIf you have a data point that is over three standard deviations away from the mean, that's an incredibly unusual piece of data!


STANDARD DEVIATION
Review
In the last exercise you saw that Lebron James was 0.55 standard deviations above the mean of NBA player heights. He’s taller than average, but compared to the other NBA players, he’s not absurdly tall.

However, compared to the OkCupid dating pool, he is extremely rare! He’s almost three full standard deviations above the mean. You’d expect only about 0.15% of people on OkCupid to be more than 3 standard deviations away from the mean.

This is the power of standard deviation. By taking the square root of the variance, the standard deviation gives you a statistic about spread that can be easily interpreted and compared to the mean.

'''
import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from data import nba_data, okcupid_data

nba_mean = np.mean(nba_data)
okcupid_mean = np.mean(okcupid_data)

#Change this variable to your height (in inches)!
your_height = 0

nba_standard_deviation = np.std(nba_data)
okcupid_standard_deviation = np.std(okcupid_data)

plt.subplot(211)
plt.title("NBA Player Heights")
plt.xlabel("Height (inches)")

plt.hist(nba_data)

plt.axvline(nba_mean, color='#FD4E40', linestyle='solid', linewidth=2, label = "Mean")

plt.axvline(nba_mean + nba_standard_deviation, color='#FFB908', linestyle='solid', linewidth=2, label = "Standard Deviations")
plt.axvline(nba_mean - nba_standard_deviation, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(nba_mean + nba_standard_deviation * 2, color='#FFB908', linestyle='solid', linewidth=2)
plt.axvline(nba_mean - nba_standard_deviation * 2, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(nba_mean + nba_standard_deviation * 3, color='#FFB908', linestyle='solid', linewidth=2)
plt.axvline(nba_mean - nba_standard_deviation * 3, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(your_height, color='#62EDBF', linestyle='solid', linewidth=2, label = "You")

plt.xlim(55, 90)
plt.legend()


plt.subplot(212)
plt.title("OkCupid Profile Heights")
plt.xlabel("Height (inches)")

plt.hist(okcupid_data)

plt.axvline(okcupid_mean, color='#FD4E40', linestyle='solid', linewidth=2, label = "Mean")

plt.axvline(okcupid_mean + okcupid_standard_deviation, color='#FFB908', linestyle='solid', linewidth=2, label = "Standard Deviations")
plt.axvline(okcupid_mean - okcupid_standard_deviation, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(okcupid_mean + okcupid_standard_deviation * 2, color='#FFB908', linestyle='solid', linewidth=2)
plt.axvline(okcupid_mean - okcupid_standard_deviation * 2, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(okcupid_mean + okcupid_standard_deviation * 3, color='#FFB908', linestyle='solid', linewidth=2)
plt.axvline(okcupid_mean - okcupid_standard_deviation * 3, color='#FFB908', linestyle='solid', linewidth=2)

plt.axvline(your_height, color='#62EDBF', linestyle='solid', linewidth=2, label = "You")

plt.xlim(55, 90)
plt.legend()




plt.tight_layout()
plt.show()


'''
LEARN STATISTICS WITH PYTHON
Variance in Weather
You’re planning a trip to London and want to get a sense of the best time of the year to visit. Luckily, you got your hands on a dataset from 2015 that contains over 39,000 data points about weather conditions in London. Surely, with this much information, you can discover something useful about when to make your trip!

In this project, the data is stored in a Pandas DataFrame. If you’ve never used a DataFrame before, we’ll walk you through how to filter and manipulate this data. If you want to learn more about Pandas, check out our Data Science Path.

'''

import codecademylib3_seaborn
import pandas as pd
import numpy as np
from weather_data import london_data

#print(type(london_data))
#print(london_data.head())
#print(london_data.columns)
#print(london_data.loc[100:200]['station'])
#print(london_data.iloc[100:200]['station'])
#print(len(london_data))

temp = london_data["TemperatureC"]
average_temp = np.mean(temp)
temperature_var = np.var(temp)
temperature_standard_deviation = np.std(temp)

june = london_data.loc[london_data['month'] == 6]['TemperatureC']
july = london_data.loc[london_data['month'] == 7]['TemperatureC']

june_temp_mean = np.mean(june)
july_temp_mean = np.mean(july)

#print(june_temp_mean)
#print(july_temp_mean)

#print(np.std(june))
#print(np.std(july))

for i in range(1, 13):
  month = london_data.loc[london_data["month"] == i]["TemperatureC"]
  print("The mean temperature in month "+str(i) +" is "+ str(np.mean(month)))
  print("The standard deviation of temperature in month "+str(i) +" is "+ str(np.std(month)) +"\n")


'''

HISTOGRAMS
Histograms
While counting the number of values in a bin is straightforward, it is also time-consuming. How long do you think it would take you to count the number of values in each bin for:

an exercise class of 50 people?
a grocery store with 300 loaves of bread?
Most of the data you will analyze with histograms includes far more than ten values.

For these situations, we can use the numpy.histogram() function. In the example below, we use this function to find the counts for a twenty-person exercise class.
'''

exercise_ages = np.array([22, 27, 45, 62, 34, 52, 42, 22, 34, 26, 24, 65, 34, 25, 45, 23, 45, 33, 52, 55])

np.histogram(exercise_ages, range = (20, 70), bins = 5)

'''
Below, we explain each of the function’s inputs:

exercise_ages is the input array
range = (20, 70) — is the range of values we expect in our array. Range includes everything from 20, up until but not including 70.
bins = 5 is the number of bins. Python will automatically calculate equally-sized bins based on the range and number of bins.
Below, you can see the output of the numpy.histogram() function:

(array([7, 4, 4, 3, 2]), array([20., 30., 40., 50., 60., 70.]))
The first array, array([7, 4, 4, 3, 2]), is the counts for each bin. The second array, array([20., 30., 40., 50., 60., 70.]), includes the minimum and maximum values for each bin:

Bin 1: 20 to <30
Bin 2: 30 to <40
Bin 3: 40 to <50
Bin 4: 50 to <60
Bin 5: 60 to <70'''

'''
HISTOGRAMS
Plotting a Histogram
At this point, you’ve learned how to find the numerical inputs to a histogram. Thus far the size of our datasets and bins have produced results that we can interpret. This becomes increasingly difficult as the number of bins in a histogram increases.

Because of this, histograms are typically viewed graphically, with bin ranges on the x-axis and counts on the y-axis. The figure below shows the graphical representation of the histogram for our exercise class example from last exercise. Notice, there are five equally-spaced bars, with each displaying a count for an age range. Compare the graph to the table, just below it.

Histogram
20-29	30-39	40-49	50-59	60-69
7	4	4	3	2
Histograms are an easy way to visualize trends in your data. When I look at the above graph, I think, “More people in the exercise class are in their twenties than any other decade. Additionally, the histogram is skewed, indicating the class is made of more younger people than older people.”

We created the plot above using the matplotlib.pyplot package. We imported the package using the following code:
'''
from matplotlib import pyplot as plt

'''
We plotted the histogram with the following code. Notice, the range and bins arguments are the same as we used in the last exercise:
'''
plt.hist(exercise_ages, range = (20, 70), bins = 5, edgecolor='black')

plt.title("Decade Frequency")
plt.xlabel("Ages")
plt.ylabel("Count")

plt.show()

'''
In the code above, we used the plt.hist() function to create the plot, then added a title, x-label, and y-label before showing the graph with plt.show().'''

#-----------------------------------------

# Import packages
import numpy as np
import pandas as pd

# Read in transactions data
transactions = pd.read_csv("transactions.csv")

# Save transaction data to numpy arrays
times = transactions["Transaction Time"]
cost = transactions["Cost"]

# Find the minimum time, maximum time, and range
min_time = np.amin(times)
max_time = np.amax(times)
range_time = max_time - min_time

# Printing the values
print("Earliest Time: " + str(min_time))
print("Latest Time: " + str(max_time))
print("Time Range: " + str(range_time))

#----------------------------

# Import packages
import codecademylib
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# Read in transactions data
transactions = pd.read_csv("transactions.csv")

# Save transaction times to a separate numpy array
times = transactions["Transaction Time"].values

# Use plt.hist() below
plt.hist(times, range=(0, 24), bins=24,  edgecolor="black")
plt.title("Weekday Frequency of Customers")
plt.xlabel("Hours (1 hour increments)")
plt.ylabel("Count")

plt.show()


#----------------------------------------------------


'''
In this lesson, you will learn how to interpret a distribution using the following five features of a dataset:

Center
Spread
Skew
Modality
Outliers
If you’re one for mnemonics, maybe this will help:

Cream Shoes are Stylish, Modern, and Outstanding.
'''

import pandas as pd 
import numpy as np
from matplotlib import pyplot as plt
import codecademylib3_seaborn

cp_data = pd.read_csv("cp.csv") 

cp_average = cp_data[' Average Covered Charges '].mean()

cp_median = cp_data[' Average Covered Charges '].median()

plt.hist(cp_data[' Average Covered Charges '], bins=20, edgecolor='black')

plt.title("Distribution of Chest Pain Treatment Cost by Hospital", fontsize = 16)
plt.xlabel("Cost ($)", fontsize = 16)
plt.ylabel("Count", fontsize = 16)
plt.axvline(cp_average, color='r', linestyle='solid', linewidth=2, label="Mean")
plt.axvline(cp_median, color='y', linestyle='solid', linewidth=2, label="Median")
plt.legend()

plt.show()

'''
Skew
Once you have the center and range of your data, you can begin to describe its shape. The skew of a dataset is a description of the data’s symmetry.

A dataset with one prominent peak, and similar tails to the left and right is called symmetric. The median and mean of a symmetric dataset are similar.

A histogram with a tail that extends to the right is called a right-skewed dataset. The median of this dataset is less than the mean.

histogram

A histogram with one prominent peak to the right, and a tail that extends to the left is called a left-skewed dataset. The median of this dataset is greater than the mean.


Modality
The modality describes the number of peaks in a dataset. Thus far, we have only looked at datasets with one distinct peak, known as unimodal. This is the most common.

A bimodal dataset has two distinct peaks.

histogram

A multimodal dataset has more than two peaks. The histogram below displays three peaks.

histogram

You may also see datasets with no obvious clustering. Datasets such as these are called uniform distributions.

Outliers
An outlier is a data point that is far away from the rest of the dataset. Ouliers do not have a formal definition, but are easy to determine by looking at histogram. The histogram below shows an example of an oulier. There is one datapoint that is much larger than the rest.

title

If you see an outlier in your dataset, it’s worth reporting and investigating. This data can often indicate an error in your data or an interesting insight.'''



#-------------summary.txt ---------------

'''
This histogram displays the distribution of chest pain cost for over 2,000 hospitals across the United States. The average and median costs are $16,948 and $14,659.6, respectively. Given that the data is unimodal, with one local maximum and a right skew, the fact that the average is greater than the median, matches our expectation.

The range of costs is very large, $78,623, with the smallest cost equal to $2,459 and the largest cost equal to $81,083. There is one hospital, Bayonne Hospital Center that charges far more than the rest at $81,083.'''

#---------------script.py---------------

import pandas as pd 
import numpy as np
from matplotlib import pyplot as plt
import codecademylib3_seaborn


np.random.seed(1)

fig, axes = plt.subplots(nrows=4, ncols=1)

plt.subplot(4,1,1)
mu, sigma = 80,5
exam_1 = np.random.normal(mu, sigma, 120)
exam_1[50] = 55
exam_1[51] = 55
count, bins, ignored = plt.hist(exam_1, 25, range=[50, 100])
plt.ylabel("Count", fontsize=12)
plt.title("Exam 1", fontsize=14)




plt.subplot(4,1,2)
mu, sigma = 85,5
exam_2_norm = np.random.normal(mu, sigma, 85)
exam_2_u = np.random.uniform(60, 80, 35)
exam_2 = np.concatenate((exam_2_norm, exam_2_u))

count, bins, ignored = plt.hist(exam_2, 25, range=[50, 100])
plt.ylabel("Count", fontsize=12)
plt.title("Exam 2", fontsize=14)



plt.subplot(4,1,3)
mu, sigma = 85,5
exam_2_norm = np.random.normal(mu, sigma, 70)
exam_2_u = np.random.normal(65, 3.5, 50)
exam_2 = np.concatenate((exam_2_norm, exam_2_u))

count, bins, ignored = plt.hist(exam_2, 25, range=[50, 100])
plt.ylabel("Count", fontsize=12)
plt.title("Exam 3", fontsize=14)



plt.subplot(4,1,4)
mu, sigma = 80,6
exam_2_norm = np.random.normal(mu, sigma, 120)
exam_2 = np.concatenate((exam_2_norm, np.array([96,96])))
print(np.average(exam_2))
print(np.median(exam_2))


count, bins, ignored = plt.hist(exam_2, 25, range=[50, 100])
plt.xlabel("Score (%)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.title("Final Exam", fontsize=14)

fig.tight_layout()

plt.show()

#---------------summary.txt ---------------------
'''
On exam 1, the average and median scores were 80 and 80, respectively. The distribution is symmetric, with a similar distribution of scores to the left and right of the center.

The range is close to 55, with the lowest grade close to 35 and the largest grade close to 90. There is one student, who scored close to 55, who is considered an outlier.

#####################
#####################

On exam 2, the average and median scores were 82 and 84 respectively. The distribution has a left skew, which agrees with our finding that the average of our dataset is smaller than the median.

The range is close to 38 with the lowest grade close to 60 and the largest grade close to 98.

#####################
#####################

On exam 3, the average and median scores were 77 and 80, respectively. The distribution is bimodal, and both modes have a similar tail on both sides of their peak, indicating that each is symmetric.

The range is close to 42, with the lowest grade close to 56 and the largest grade close to 98.


#####################
#####################


On the final exam, the average and median scores were 80 and 80, respectively. The distribution is symmetric, with a similar distribution of scores to the left and right of the center.

The range is close to 30, with the lowest grade close to 68 and the largest grade close to 98. There is one student, who scored close to 98, that is considered an outlier.'''

'''

QUARTILES
Quartiles
A common way to communicate a high-level overview of a dataset is to find the values that split the data into four groups of equal size.

By doing this, we can then say whether a new datapoint falls in the first, second, third, or fourth quarter of the data.

20 data points, with three lines splitting the data into 4 groups of 5.
The values that split the data into fourths are the quartiles.

Those values are called the first quartile (Q1), the second quartile (Q2), and the third quartile (Q3)

In the image above, Q1 is 10, Q2 is 13, and Q3 is 22. Those three values split the data into four groups that each contain five datapoints.'''

'''
QUARTILES
The Second Quartile

Let’s begin by finding the second quartile (Q2). Q2 happens to be exactly the median. Half of the data falls below Q2 and half of the data falls above Q2.

The first step in finding the quartiles of a dataset is to sort the data from smallest to largest. For example, below is an unsorted dataset:

[8, 15, 4, -108, 16, 23, 42][8,15,4,−108,16,23,42]
After sorting the dataset, it looks like this:

[-108, 4, 8, 15, 16, 23, 42][−108,4,8,15,16,23,42]
Now that the list is sorted, we can find Q2. In the example dataset above, Q2 (and the median) is 15 — there are three points below 15 and three points above 15.

Even Number of Datapoints
You might be wondering what happens if there is an even number of points in the dataset. For example, if we remove the -108 from our dataset, it will now look like this:

[4, 8, 15, 16, 23, 42][4,8,15,16,23,42]
Q2 now falls somewhere between 15 and 16. There are a couple of different strategies that you can use to calculate Q2 in this situation. One of the more common ways is to take the average of those two numbers. In this case, that would be 15.5.

Recall that you can find the average of two numbers by adding them together and dividing by two.'''


'''
QUARTILES
Q1 and Q3
Now that we’ve found Q2, we can use that value to help us find Q1 and Q3. Recall our demo dataset:

[-108, 4, 8, 15, 16, 23, 42][−108,4,8,15,16,23,42]
In this example, Q2 is 15. To find Q1, we take all of the data points smaller than Q2 and find the median of those points. In this case, the points smaller than Q2 are:

[-108, 4, 8][−108,4,8]
The median of that smaller dataset is 4. That’s Q1!

To find Q3, do the same process using the points that are larger than Q2. We have the following points:

[16, 23, 42][16,23,42]
The median of those points is 23. That’s Q3! We now have three points that split the original dataset into groups of four equal sizes.
'''

'''
QUARTILES
Method Two: Including Q2
You just learned a commonly used method to calculate the quartiles of a dataset. However, there is another method that is equally accepted that results in different values!

Note that there is no universally agreed upon method of calculating quartiles, and as a result, two different tools might report different results.

The second method includes Q2 when trying to calculate Q1 and Q3. Let’s take a look at an example:

[-108, 4, 8, 15, 16, 23, 42][−108,4,8,15,16,23,42]
Using the first method, we found Q1 to be 4. When looking at all of the points below Q2, we excluded Q2. Using this second method, we include Q2 in each half.

For example, when calculating Q1 using this new method, we would now find the median of this dataset:

[-108, 4, 8, 15][−108,4,8,15]
Using this method, Q1 is 6.'''

'''
QUARTILES
Quartiles in NumPy
We were able to find quartiles manually by looking at the dataset and finding the correct division points. But that gets much harder when the dataset starts to get bigger. Luckily, there is a function in the NumPy library that will find the quartiles for you.

The NumPy function that we’ll be using is named quantile(). You can learn more about quantiles in our lesson, but for right now all you need to know is that a quartile is a specific kind of quantile.

The code below calculates the third quartile of the given dataset:

import numpy as np

dataset = [50, 10, 4, -3, 4, -20, 2]
third_quartile = np.quantile(dataset, 0.75)
The quantile() function takes two parameters. The first is the dataset you’re interested in. The second is a number between 0 and 1. Since we calculated the third quartile, we used 0.75 — we want the point that splits the first 75% of the data from the rest.

For the second quartile, we’d use 0.5. This will give you the point that 50% of the data is below and 50% is above.

Notice that the dataset doesn’t need to be sorted for NumPy’s function to work!'''


from song_data import songs
import numpy as np

#Create the variables songs_q1, songs_q2, and songs_q3 here:
songs_q1 = np.quantile(songs, 0.25)
songs_q2 = np.quantile(songs, 0.5)
songs_q3 = np.quantile(songs, 0.75)

favorite_song = 300
quarter = 4
#Ignore the code below here:
try:
  print("The first quartile of dataset one is " + str(songs_q1) + " seconds")
except NameError:
  print("You haven't defined songs_q1")
try:
  print("The second quartile of dataset one is " + str(songs_q2)+ " seconds")
except NameError:
  print("You haven't defined songs_q2")
try:
  print("The third quartile of dataset one is " + str(songs_q3)+ " seconds")
except NameError:
  print("You haven't defined songs_q3\n")
  
'''
QUANTILES
Quantiles
Quantiles are points that split a dataset into groups of equal size. For example, let’s say you just took a test and wanted to know whether you’re in the top 10% of the class. One way to determine this would be to split the data into ten groups with an equal number of datapoints in each group and see which group you fall into.

Thirty students grades split into ten groups of three.
There are nine values that split the dataset into ten groups of equal size — each group has 3 different test scores in it.

Those nine values that split the data are quantiles! Specifically, they are the 10-quantiles, or deciles.

You can find any number of quantiles. For example, if you split the dataset into 100 groups of equal size, the 99 values that split the data are the 100-quantiles, or percentiles.

The quartiles are some of the most commonly used quantiles. The quartiles split the data into four groups of equal size.

In this lesson, we’ll show you how to calculate quantiles using NumPy and discuss some of the most commonly used quantiles.'''



import codecademylib3_seaborn
from song_data import songs
import matplotlib.pyplot as plt
import numpy as np

q1 = np.quantile(songs, 0.25)
q2 = np.quantile(songs, 0.5)
q3 = np.quantile(songs, 0.75)

plt.subplot(3,1,1)
plt.hist(songs, bins = 200)
plt.axvline(x=q1, c = 'r')
plt.axvline(x=q2, c = 'r')
plt.axvline(x=q3, c = 'r')
plt.xlabel("Song Length (Seconds)")
plt.ylabel("Count")
plt.title("4-Quantiles")

plt.subplot(3,1,2)
plt.hist(songs, bins = 200)
plt.axvline(x=np.quantile(songs, 0.2), c = 'r')
plt.axvline(x=np.quantile(songs, 0.4), c = 'r')
plt.axvline(x=np.quantile(songs, 0.6), c = 'r')
plt.axvline(x=np.quantile(songs, 0.8), c = 'r')
plt.xlabel("Song Length (Seconds)")
plt.ylabel("Count")
plt.title("5-Quantiles")

plt.subplot(3,1,3)
plt.hist(songs, bins = 200)
for i in range(1, 10):
  plt.axvline(x=np.quantile(songs, i/10), c = 'r')
plt.xlabel("Song Length (Seconds)")
plt.ylabel("Count")
plt.title("10-Quantiles")

plt.tight_layout()
plt.show()


'''


QUANTILES
Many Quantiles
In the last exercise, we found a single “quantile” — we split the first 23% of the data away from the remaining 77%.

However, quantiles are usually a set of values that split the data into groups of equal size. For example, you wanted to get the 5-quantiles, or the four values that split the data into five groups of equal size, you could use this code:

import numpy as np

dataset = [5, 10, -20, 42, -9, 10]
ten_percent = np.quantile(dataset, [0.2, 0.4, 0.6, 0.8])
Note that we had to do a little math in our head to make sure that the values [0.2, 0.4, 0.6, 0.8] split the data into groups of equal size. Each group has 20% of the data.

The data is split into 5 groups where each group has 4 datapoints.
If we used the values [0.2, 0.4, 0.7, 0.8], the function would return the four values at those split points. However, those values wouldn’t split the data into five equally sized groups. One group would only have 10% of the data and another group would have 30% of the data!
'''

from song_data import songs
import numpy as np

# Define quartiles, deciles, and tenth here:
quartiles = np.quantile(songs, [0.25, 0.5, 0.75])
deciles = np.quantile(songs, [x* 0.1 for x in range(1,10)])
tenth = 3


#Ignore the code below here:
try:
  print("The quariles are " + str(quartiles) + "\n")
except NameError:
  print("You haven't defined quartiles.\n")
try:
  print("The deciles are " + str(deciles) + "\n")
except NameError:
  print("You haven't defined deciles.\n")

'''
QUANTILES
Common Quantiles
One of the most common quantiles is the 2-quantile. This value splits the data into two groups of equal size. Half the data will be above this value, and half the data will be below it. This is also known as the median!

Ten points are below the median and ten points are above the median.
The 4-quantiles, or the quartiles, split the data into four groups of equal size. We found the quartiles in the previous exercise. Options

Quartiles split a dataset of 20 points into 4 groups with 5 points each
Finally, the percentiles, or the values that split the data into 100 groups, are commonly used to compare new data points to the dataset. You might hear statements like “You are above the 80th percentile in height”. This means that your height is above whatever value splits the first 80% of the data from the remaining 20%.'''

import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np
from data import school_one, school_two, school_three

deciles_one = np.quantile(school_one, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
deciles_two = np.quantile(school_two, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
deciles_three = np.quantile(school_three, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

plt.subplot(311)
plt.hist(school_one)
for decile in deciles_one:
  plt.axvline(x=decile, c = 'r')
plt.title("School One")
plt.xlabel("SAT Score")
  
plt.subplot(312)
plt.hist(school_two)
for decile in deciles_two:
  plt.axvline(x=decile, c = 'r')
plt.title("School Two")
plt.xlabel("SAT Score")
  
plt.subplot(313)
plt.hist(school_three)
for decile in deciles_three:
  plt.axvline(x=decile, c = 'r')
plt.title("School Three")
plt.xlabel("SAT Score")
plt.tight_layout()
plt.show()

'''
INTERQUARTILE RANGE
Range Review
One of the most common statistics to describe a dataset is the range. The range of a dataset is the difference between the maximum and minimum values. While this descriptive statistic is a good start, it is important to consider the impact outliers have on the results:

A dataset with some outliers.
In this image, most of the data is between 0 and 15. However, there is one large negative outlier (-20) and one large positive outlier (40). This makes the range of the dataset 60 (The difference between 40 and -20). That’s not very representative of the spread of the majority of the data!

The interquartile range (IQR) is a descriptive statistic that tries to solve this problem. The IQR ignores the tails of the dataset, so you know the range around-which your data is centered.

In this lesson, we’ll teach you how to calculate the interquartile range and interpret it.'''

'''
INTERQUARTILE RANGE
Quartiles
The interquartile range is the difference between the third quartile (Q3) and the first quartile (Q1). If you need a refresher on quartiles, you can take a look at our lesson.

For now, all you need to know is that the first quartile is the value that separates the first 25% of the data from the remaining 75%.

The third quartile is the opposite — it separates the first 75% of the data from the remaining 25%.

The interquartile range of the dataset is shown to be between Q3 and Q1.
The interquartile range is the difference between these two values.'''

from song_data import songs
import numpy as np

q1, q3 = np.quantile(songs, [0.25, 0.75])
#Create the variables q3 and interquartile_range here:

interquartile_range = q3 - q1


#-----------------------------

import codecademylib3_seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("country_data.csv")
#print(data.head())

life_expectancy = data['Life Expectancy']
life_expectancy_quartiles = np.quantile(life_expectancy, [0.25, 0.5, 0.75])
print(life_expectancy_quartiles)

gdp = data['GDP']
#print(gdp.head())
median_gdp = gdp.median()
low_gdp = data[data['GDP'] <= median_gdp]
high_gdp = data[data['GDP'] > median_gdp]
#print(low_gdp.head())

low_gdp_quartiles = np.quantile(low_gdp['Life Expectancy'], [0.25, 0.5, 0.75])
high_gdp_quartiles = np.quantile(high_gdp['Life Expectancy'], [0.25, 0.5, 0.75])
print(low_gdp_quartiles)
print(high_gdp_quartiles)

plt.hist(high_gdp["Life Expectancy"], alpha = 0.5, label = "High GDP")
plt.hist(low_gdp["Life Expectancy"], alpha = 0.5, label = "Low GDP")
plt.legend()
plt.show()


'''

BOXPLOTS
Boxplots in Matplotlib
We’ve spent this lesson building a boxplot by hand. Let’s now look at how Python’s Matplotlib library does it!

The matplotlib.pyplot module has a function named boxplot(). boxplot() takes a dataset as a parameter. This dataset could be something like a list of numbers, or a Pandas DataFrame.'''

import matplotlib.pyplot as plt

data = [1, 2, 3, 4, 5]
plt.boxplot(data)
plt.show()
'''
One of the strengths of Matplotlib is the ease of plotting two boxplots side by side. If you pass boxplot() a list of datasets, Matplotlib will make a boxplot for each, allowing you to compare their spread and central tendencies,'''

import matplotlib.pyplot as plt

dataset_one = [1, 2, 3, 4, 5]
dataset_two = [3, 4, 5, 6, 7]
plt.boxplot([dataset_one, dataset_two], labels = ['a' , 'b'])
plt.show()


#----------------------------

import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt

healthcare = pd.read_csv("healthcare.csv")
#print(healthcare.head())
#print(healthcare['DRG Definition'].unique())

chest_pain = healthcare[healthcare['DRG Definition'] == '313 - CHEST PAIN']
#print(chest_pain.head())

alabama_chest_pain  = chest_pain[chest_pain['Provider State'] == 'AL']

average_cost = alabama_chest_pain[' Average Covered Charges ']
costs = alabama_chest_pain[' Average Covered Charges '].values
#print(type(average_cost))
#print(type(cost))

states = chest_pain['Provider State'].unique()

dataset = []

for state in states:
  dataset.append(chest_pain[chest_pain['Provider State'] == state][' Average Covered Charges '].values)
  
plt.figure(figsize=(20,6))

plt.boxplot(dataset, labels= states)
plt.show()

#-------------------------------------------------------------------------

'''
STATISTICAL CONCEPTS
Sample Mean and Population Mean
Suppose you want to know the average height of an oak tree in your local park. On Monday, you measure 10 trees and get an average height of 32 ft. On Tuesday, you measure 12 different trees and reach an average height of 35 ft. On Wednesday, you measure the remaining 11 trees in the park, whose average height is 31 ft. Overall, the average height for all trees in your local park is 32.8 ft.

The individual measurements on Monday, Tuesday, and Wednesday are called samples. A sample is a subset of the entire population. The mean of each sample is the sample mean and it is an estimate of the population mean.

Note that the sample means (32 ft., 35 ft., and 31 ft.) were all close to the population mean (32.8 ft.), but were all slightly different from the population mean and from each other.

For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole. There are many reasons we might use sampling, such as:

We don’t have data for the whole population.
We have the whole population data, but it is so large that it is infeasible to analyze.
We can provide meaningful answers to questions faster with sampling.
When we have a numerical dataset and want to know the average value, we calculate the mean. For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole.'''

import numpy as np

population = np.random.normal(loc=65, scale=3.5, size=300)
population_mean = np.mean(population)

print "Population Mean: {}".format(population_mean)

sample_1 = np.random.choice(population, size=30, replace=False)
sample_2 = np.random.choice(population, size=30, replace=False)
sample_3 = np.random.choice(population, size=30, replace=False)
sample_4 = np.random.choice(population, size=30, replace=False)
sample_5 = np.random.choice(population, size=30, replace=False)

sample_1_mean = np.mean(sample_1)
print "Sample 1 Mean: {}".format(sample_1_mean)

sample_2_mean = np.mean(sample_1)
sample_3_mean = np.mean(sample_2)
sample_4_mean = np.mean(sample_3)
sample_5_mean = np.mean(sample_4)

print "Sample 2 Mean: {}".format(sample_2_mean)
print "Sample 3 Mean: {}".format(sample_3_mean)
print "Sample 4 Mean: {}".format(sample_4_mean)
print "Sample 5 Mean: {}".format(sample_5_mean)


'''
STATISTICAL CONCEPTS
Central Limit Theorem
Perhaps, this time, you’re a tailor of school uniforms at a middle school. You need to know the average height of people from 10-13 years old in order to know which sizes to make the uniforms. Knowing the best decisions are based on data, you set out to do some research at your local middle school.

Organizing with the school, you measure the heights of some students. Their average height is 57.5 inches. You know a little about sampling and decide that measuring 30 out of the 300 students gives enough data to assume 57.5 inches is roughly the average height of everyone at the middle school. You set to work with this dimension and make uniforms that fit people of this height, some smaller and some larger.

Unfortunately, when you go about making your uniforms many reports come back saying that they are too small. Something must have gone wrong with your decision-making process! You go back to collect the rest of the data: you measure the sixth graders one day (56.7, not so bad), the seventh graders after that (59 inches tall on average), and the eighth graders the next day (61.7 inches!). Your sample mean was so far off from your population mean. How did this happen?

Well, your sample selection was skewed to one direction of the total population. It looks like you must have measured more sixth graders than is representative of the whole middle school. How do you get an average sample height that looks more like the average population height?

In the previous exercise, we looked at different sets of samples taken from a population and how the mean of each set could be different from the population mean. This is a natural consequence of the fact that a set of samples has less data than the population to which it belongs. If our sample selection is poor then we will have a sample mean seriously skewed from our population mean.

There is one surefire way to mitigate the risk of having a skewed sample mean — take a larger set of samples. The sample mean of a larger sample set will more closely approximate the population mean. This phenomenon, known as the Central Limit Theorem, states that if we have a large enough sample size, all of our sample means will be sufficiently close to the population mean.

Later, we’ll learn how to put numeric values on “large enough” and “sufficiently close”.'''

import numpy as np

# Create population and find population mean
population = np.random.normal(loc=65, scale=100, size=3000)
population_mean = np.mean(population)

# Select increasingly larger samples
extra_small_sample = population[:10]
small_sample = population[:50]
medium_sample = population[:100]
large_sample = population[:500]
extra_large_sample = population[:1000]

# Calculate the mean of those samples
extra_small_sample_mean = np.mean(extra_small_sample)
small_sample_mean = np.mean(small_sample)
medium_sample_mean = np.mean(medium_sample)
large_sample_mean = np.mean(large_sample)
extra_large_sample_mean = np.mean(extra_large_sample)

# Print them all out!
print "Extra Small Sample Mean: {}".format(extra_small_sample_mean)
print "Small Sample Mean: {}".format(small_sample_mean)
print "Medium Sample Mean: {}".format(medium_sample_mean)
print "Large Sample Mean: {}".format(large_sample_mean)
print "Extra Large Sample Mean: {}".format(extra_large_sample_mean)

print "\nPopulation Mean: {}".format(population_mean)

'''Question
How does taking a larger number of samples solve the issue of a skewed sample mean? How does the Central Limit Theorem help here?

Answer
The Central Limit Theorem (CLT) is, roughly, the following statement

Regardless of the distribution of our data, if we take a large number of samples of a fixed size and plot the sample statistic which we care about (e.g. mean or standard deviation) the distribution of the resulting plot will be roughly normal, i.e. a bell curve.

Note: The distribution that we get from plotting our sample statistics is called a sampling distribution.

We can see the truth of this claim, experimentally, by playing with the applet in this lesson 77.

Okay. So how does this help us solve the issue of a skewed sample? The CLT helps because it turns out that the mean of our sampling distribution will become arbitrarily close to the mean of our original distribution as we take more and more samples. This is great because we often never know the mean of the original distribution. The CLT gives us a mathematical assurance that we can calculate it from taking samples (which we can directly calculate the mean for).

In conclusion, if we have a skewed sample mean, by

taking a larger number of samples,
plotting the mean of each sample, and
taking the mean, call it M, of the resulting distribution
M is likely to be close to the population mean by the Central Limit Theorem.'''


'''
STATISTICAL CONCEPTS
Hypothesis Tests
When observing differences in data, a data analyst understands the possibility that these differences could be the result of random chance.

Suppose we want to know if men are more likely to sign up for a given programming class than women. We invite 100 men and 100 women to this class. After one week, 34 women sign up, and 39 men sign up. More men than women signed up, but is this a “real” difference?

We have taken sample means from two different populations, men and women. We want to know if the difference that we observe in these sample means reflects a difference in the population means. To formally answer this question, we need to re-frame it in terms of probability:

“What is the probability that men and women have the same level of interest in this class and that the difference we observed is just chance?”

In other words, “If we gave the same invitation to every person in the world, would more men still sign up?”

A more formal version is: “What is the probability that the two population means are the same and that the difference we observed in the sample means is just chance?”

These statements are all ways of expressing a null hypothesis. A null hypothesis is a statement that the observed difference is the result of chance.

Hypothesis testing is a mathematical way of determining whether we can be confident that the null hypothesis is false. Different situations will require different types of hypothesis testing, which we will learn about in the next lesson.'''


import numpy as np

def intersect(list1, list2):
  return [sample for sample in list1 if sample in list2]

# the true positives and negatives:
actual_positive = [2, 5, 6, 7, 8, 10, 18, 21, 24, 25, 29, 30, 32, 33, 38, 39, 42, 44, 45, 47]
actual_negative = [1, 3, 4, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 26, 27, 28, 31, 34, 35, 36, 37, 40, 41, 43, 46, 48, 49]

# the positives and negatives we determine by running the experiment:
experimental_positive = [2, 4, 5, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 32, 35, 36, 38, 39, 40, 45, 46, 49]
experimental_negative = [1, 3, 6, 12, 14, 23, 25, 29, 30, 31, 33, 34, 37, 41, 42, 43, 44, 47, 48]

#define type_i_errors and type_ii_errors here
type_i_errors = intersect(actual_negative, experimental_positive)

type_ii_errors = intersect(actual_positive, experimental_negative)

'''
STATISTICAL CONCEPTS
P-Values
We have discussed how a hypothesis test is used to determine the validity of a null hypothesis. A hypothesis test provides a numerical answer, called a p-value, that helps us decide how confident we can be in the result. In this context, a p-value is the probability that we yield the observed statistics under the assumption that the null hypothesis is true.

A p-value of 0.05 would mean that if we assume the null hypothesis is true, there is a 5% chance that the data results in what was observed due only to random sampling error. This generally means there is a 5% chance that there is no difference between the two population means.

Before conducting a hypothesis test, we determine the necessary threshold we would need before concluding that the results are significant. A higher p-value is more likely to give a false positive so if we want to be very sure that the result is not due to just chance, we will select a very small p-value.

It is important that we choose the significance level before we perform our statistical hypothesis tests to yield a p-value. If we wait until after we see the results, we might pick our threshold such that we get the result we want to see. For instance, if we’re trying to publish our results, we might set a significance level that makes our results seem statistically significant. Choosing our significance level in advance helps keep us honest.

Generally, we want a p-value of less than 0.05, meaning that there is less than a 5% chance that our results are due to random chance.'''

'''
Question
What is the difference between the p-value and the significance level of an experiment?

Answer
The p-value is a statistic that we can compute as a conditional probability. It is the probability that we would observe the same sample statistics given that the null hypothesis, H_0, is true. As an equation:

p-value: P( observe the same sample statistics | H_0)
By way of an example, let’s say we’re performing a test and we want to determine if there is a significant increase in time spent on our website if we change the background color from white to yellow. Suppose that the current average time spent on our website is 15 minutes. After changing the background color to yellow, we take a sample of 100 users and we find that the average time spent on the site is 20 minutes. Does this result show a significant change or not? The significance level, produced before we perform the sample, is the threshold value from which we will determine to reject, or fail to reject, the null hypothesis; often chosen as 0.05. The p-value, however, is the following conditional probability,

P( sample mean >= 20 minutes | background of site is unchanged )
The sample mean is the average time spent on the site from a given sample and the background of the site being unchanged is the null hypothesis. To conclude, the significance level is a threshold value chosen beforehand and the p-value is a conditional probability to calculate the significance of an observation.'''


'''
HYPOTHESIS TESTING
Types of Hypothesis Test
When we are trying to compare datasets, we often need a way to be confident knowing if datasets are significantly different from each other.
Some situations involve correlating numerical data, such as:

a professor expects an exam average to be roughly 75%, and wants to know if the actual scores line up with this expectation. Was the test actually too easy or too hard?
a manager of a chain of stores wants to know if certain locations have different revenues on different days of the week. Are the revenue differences a result of natural fluctuations or a significant difference between the stores’ sales patterns?
a PM for a website wants to compare the time spent on different versions of a homepage. Does one version make users stay on the page significantly longer?
Others involve categorical data, such as:

a pollster wants to know if men and women have significantly different yogurt flavor preferences. Does a result where men more often answer “chocolate” as their favorite reflect a significant difference in the population?
do different age groups have significantly different emotional reactions to different ads?
In this lesson, you will learn how about how we can use hypothesis testing to answer these questions. There are several different types of hypothesis tests for the various scenarios you may encounter. Luckily, SciPy has built-in functions that perform all of these tests for us, normally using just one line of code.

For numerical data, we will cover:

One Sample T-Tests
Two Sample T-Tests
ANOVA
Tukey Tests
For categorical data, we will cover:

Binomial Tests
Chi Square
After this lesson, you will have a wide range of tools in your arsenal to find meaningful correlations in data.
'''

'''
Introduction Video about  Test
https://www.youtube.com/watch?v=0Pd3dc1GcHc

t test is a statistic that check if two means (averages) are reliably different from each other. 
if just look at the means, may show a difference, but we can't be sure if that is a reliable difference.
t test is a inferential statistics, it allow us to make inferences about the population beyond our data. 

Bigger t-value = Different Group
Small t-value = similar Group

t= variance between groups / variance within groups

the p-value tells us the likelihood that there is a real difference.
the p-value is the probablity that the pattern of data in the sample could be produced by random data.

type of t test

independent-samples

	test the means of two different groups.
	
	testing the average quality of two different batches of beer.
	
	other names : Between-samples, unpaired sample t test, 2 sample t test

paired-samples

	test the means of one group twice (e.g before and after)

	also called : within - subjects, repeated - measures , dependent sampels

one-sample

	tests the means of one group against a set mean
	
'''
'''

HYPOTHESIS TESTING
1 Sample T-Testing
Let’s imagine the fictional business BuyPie, which sends ingredients for pies to your household, so that you can make them from scratch. Suppose that a product manager wants the average age of visitors to BuyPie.com to be 30. In the past hour, the website had 100 visitors and the average age was 31. Are the visitors too old? Or is this just the result of chance and a small sample size?

We can test this using a univariate T-test. A univariate T-test compares a sample mean to a hypothetical population mean. It answers the question “What is the probability that the sample came from a distribution with the desired mean?”

When we conduct a hypothesis test, we want to first create a null hypothesis, which is a prediction that there is no significant difference. The null hypothesis that this test examines can be phrased as such: “The set of samples belongs to a population with the target mean”.

The result of the 1 Sample T Test is a p-value, which will tell us whether or not we can reject this null hypothesis. Generally, if we receive a p-value of less than 0.05, we can reject the null hypothesis and state that there is a significant difference.

SciPy has a function called ttest_1samp, which performs a 1 Sample T-Test for you.

ttest_1samp requires two inputs, a distribution of values and an expected mean:

tstat, pval = ttest_1samp(example_distribution, expected_mean)
print pval
It also returns two outputs: the t-statistic (which we won’t cover in this course), and the p-value — telling us how confident we can be that the sample of values came from a distribution with the mean specified.'''

from scipy.stats import ttest_1samp
import numpy as np

ages = np.genfromtxt("ages.csv")

print(ages)

ages_mean = np.mean(ages)

tstat, pval = ttest_1samp(ages, 25)

print(pval)

print(tstat, pval)

'''

HYPOTHESIS TESTING
One Sample T-Test II
In the last exercise, we got a p-value that was much higher than 0.05, so we cannot reject the null hypothesis. Does this mean that if we wait for more visitors to BuyPie, the average age would definitely be 30 and not 31? Not necessarily. In fact, in this case, we know that the mean of our sample was 31.

P-values give us an idea of how confident we can be in a result. Just because we don’t have enough data to detect a difference doesn’t mean that there isn’t one. Generally, the more samples we have, the smaller a difference we’ll be able to detect. You can learn more about the exact relationship between the number of samples and detectable differences in the Sample Size Determination course.

To gain some intuition on how our confidence levels can change, let’s explore some distributions with different means and how our p-values from the 1 Sample T-Tests change.'''
'''
1.
We have loaded a dataset daily_visitors into the editor that represents the ages of visitors to BuyPie.com in the last 1000 days. Each entry daily_visitors[i] is an array of entries representing the age per visitor to the website on day i.

We predicted that the average age would be 30, and we want to know if the actual data differs from that.

We have made a for loop that goes through the 1000 inner lists. Inside this loop, perform a 1 Sample T-Test with each day of data (daily_visitors[i]). For now, just print out the p-value from each test.


Hint
To perform the T-Test in each iteration, you would use:

tstatistic, pval = ttest_1samp(daily_visitors[i], 30)
2.
If we get a pval < 0.05, we can conclude that it is unlikely that our sample has a true mean of 30. Thus, the hypothesis test has correctly rejected the null hypothesis, and we call that a correct result.

Every time we get a correct result within the 1000 experiments, add 1 to correct_results.'''

from scipy.stats import ttest_1samp
import numpy as np

correct_results = 0 # Start the counter at 0

daily_visitors = np.genfromtxt("daily_visitors.csv", delimiter=",")

for i in range(1000): # 1000 experiments
   #your ttest here:
    tstat, pval = ttest_1samp(daily_visitors[i], 30)
    #print(daily_visitors[i])
    if pval < 0.05:
      correct_results += 1
   
   #print the pvalue here:
    print(pval)
  
print "We correctly recognized that the distribution was different in " + str(correct_results) + " out of 1000 experiments."
print "We correctly recognized that the distribution was different in " + str(correct_results) + " out of 1000 experiments."

'''
First, let’s note that the null hypothesis is usually the status quo. If we expect that the population mean is 30, this is the status quo and this is why our null hypothesis is

The set of samples belongs to a population with the target mean of 30

By performing test_1samp(ages, 30), we are testing the likelihood that the samples that we have in ages were taken/drawn from a distribution with mean 30. We could of course have just gotten somewhat unlucky with our sampling in this case, especially since the number of samples for ages is small. If the resulting p-value is less than 0.05, we will reject the null hypothesis, meaning that we’re saying it is unlikely that the sample was drawn from a distribution with mean 30. A p-value greater than or equal to 0.05 means that we fail to reject the null hypothesis, meaning that we cannot be confident that the samples were not drawn from a distribution with mean 30.'''


'''
HYPOTHESIS TESTING
2 Sample T-Test
Suppose that last week, the average amount of time spent per visitor to a website was 25 minutes. This week, the average amount of time spent per visitor to a website was 28 minutes. Did the average time spent per visitor change? Or is this part of natural fluctuations?

One way of testing whether this difference is significant is by using a 2 Sample T-Test. A 2 Sample T-Test compares two sets of data, which are both approximately normally distributed.

The null hypothesis, in this case, is that the two distributions have the same mean.

We can use SciPy’s ttest_ind function to perform a 2 Sample T-Test. It takes the two distributions as inputs and returns the t-statistic (which we don’t use), and a p-value. If you can’t remember what a p-value is, refer to the earlier exercise on univariate t-tests.'''


from scipy.stats import ttest_ind
import numpy as np

week1 = np.genfromtxt("week1.csv",  delimiter=",")
week2 = np.genfromtxt("week2.csv",  delimiter=",")

week1_mean = np.mean(week1)
week2_mean = np.mean(week2)

week1_std = np.std(week1)
week2_std = np.std(week2)

tstatstic, pval = ttest_ind(week1, week2)

print (tstatstic, pval)


'''HYPOTHESIS TESTING
Dangers of Multiple T-Tests
Suppose that we own a chain of stores that sell ants, called VeryAnts. There are three different locations: A, B, and C. We want to know if the average ant sales over the past year are significantly different between the three locations.

At first, it seems that we could perform T-tests between each pair of stores.

We know that the p-value is the probability that we incorrectly reject the null hypothesis on each t-test. The more t-tests we perform, the more likely that we are to get a false positive, a Type I error.

For a p-value of 0.05, if the null hypothesis is true then the probability of obtaining a significant result is 1 – 0.05 = 0.95. When we run another t-test, the probability of still getting a correct result is 0.95 * 0.95, or 0.9025. That means our probability of making an error is now close to 10%! This error probability only gets bigger with the more t-tests we do.'''


from scipy.stats import ttest_ind
import numpy as np

a = np.genfromtxt("store_a.csv",  delimiter=",")
b = np.genfromtxt("store_b.csv",  delimiter=",")
c = np.genfromtxt("store_c.csv",  delimiter=",")

a_mean = np.mean(a)
b_mean = np.mean(b)
c_mean = np.mean(c)

a_std = np.std(a)
b_std = np.std(b)
c_std = np.std(c)

a_b_pval = ttest_ind(a, b).pvalue
a_c_pval = ttest_ind(a, c).pvalue
b_c_pval = ttest_ind(b, c).pvalue

#error_prob = (1-(0.95**3))
error_prob = 1- (1-a_b_pval)*(1-a_c_pval)*(1-b_c_pval)

'''I think the main confusion here is due to these lessons using the term “p-value” interchangeably for both the significance value (the threshold at which we will determine the results are significant) and the actual p-value that is returned by running a T-test.

Here are the concepts to remember with T-tests:

We are comparing samples of different populations to see if the populations are significantly different

We determine a significance value (or p-value threshold) prior to conducting the T-tests that will act as a cut-off point for whether we will find significance

A T-test returns two values: a test statistic (tstat) and a p-value. The test statistic is basically a number that represents the difference between population means based on the variations in your sample. The larger it is, the less likely a null hypothesis is true. If it is closer to 0, it is more likely there isn’t a significant difference. The p-value is the likelihood of getting a test statistic of equal or higher value to the one returned, if the null-hypothesis is true.

The p-value itself is not the probability of a Type I error, but rather the probability of getting a test statistic (tstat) of equal or higher value if the null hypothesis is true (i.e., if the populations have the same mean and the observed differences were merely by chance). The smaller the p-value, the more likely there is significance.

Prior to running the T-tests, however, we decide that a p-value at .05 or less will indicate significance – thus we are accepting a risk of being wrong 5% of the time when we reject the null hypothesis. We would reject a null hypothesis equally if the p-value was .04 or .00004. Thus, we have a fixed risk of Type I error per T-test that is determined prior to running the experiment. This is the error the lesson is referring to.

This 5% accepted risk is compounded for each T-test we need to run during the experiment to compare each sample with each other sample, and that is why running multiple T-tests can be problematic.

Hope this helps!'''

'''
HYPOTHESIS TESTING
ANOVA
In the last exercise, we saw that the probability of making a Type I error got dangerously high as we performed more t-tests.

When comparing more than two numerical datasets, the best way to preserve a Type I error probability of 0.05 is to use ANOVA. ANOVA (Analysis of Variance) tests the null hypothesis that all of the datasets have the same mean. If we reject the null hypothesis with ANOVA, we’re saying that at least one of the sets has a different mean; however, it does not tell us which datasets are different.

We can use the SciPy function f_oneway to perform ANOVA on multiple datasets. It takes in each dataset as a different input and returns the t-statistic and the p-value. For example, if we were comparing scores on a videogame between math majors, writing majors, and psychology majors, we could run an ANOVA test with this line:

fstat, pval = f_oneway(scores_mathematicians, scores_writers, scores_psychologists)
The null hypothesis, in this case, is that all three populations have the same mean score on this videogame. If we reject this null hypothesis (if we get a p-value less than 0.05), we can say that we are reasonably confident that a pair of datasets is significantly different. After using only ANOVA, we can’t make any conclusions on which two populations have a significant difference.

Let’s look at an example of ANOVA in action.'''

from scipy.stats import ttest_ind
from scipy.stats import f_oneway
import numpy as np

a = np.genfromtxt("store_a.csv",  delimiter=",")
b = np.genfromtxt("store_b.csv",  delimiter=",")
c = np.genfromtxt("store_c.csv",  delimiter=",")

fstat, pval = f_oneway(a, b, c)

print(pval)

# 0.000153411660078

# after change store_b_new.csv

#8.49989098083e-215
 
'''

I don’t get it:
In the explanation: “The null hypothesis, in this case, is that all three populations have the same mean … If we reject this null hypothesis (if we get a p-value less than 0.05), we can say that we are reasonably confident that a pair of datasets is significantly different.”
But in the exercise:
With store_b the means are : 58.349636084 65.6262871356 62.3611731859
and p-value is 0.000153411660078 ie we can reject the null hypothesis (see above) and the samples are different.
With store_b_new the means are: 58.349636084 148.354940186 62.3611731859
and p-value is 8.49989098083e-215 ie we cannot reject the null hypothesis (see above) and the samples are basically the same.
Surely that is the wrong way round?

'''

'''No, it’s correct.

The null hypothesis in this case is “There is no significant difference in sales between the stores.”

Rejecting the null hypothesis (p-value < 0.05) would mean there IS a significant difference between the at least one store.

The new sales numbers for Store B easily pass the eye test and you’d expect to reject the null hypothesis. And that’s exactly what happened in the ANOVA test (p-value = 8.49989098083e-215). You would say that there is a 99.999999…% chance that a store is significant.'''

'''


HYPOTHESIS TESTING
Assumptions of Numerical Hypothesis Tests
Before we use numerical hypothesis tests, we need to be sure that the following things are true:

1. The samples should each be normally distributed…ish
Data analysts in the real world often still perform hypothesis on sets that aren’t exactly normally distributed. What is more important is to recognize if there is some reason to believe that a normal distribution is especially unlikely. If your dataset is definitively not normal, the numerical hypothesis tests won’t work as intended.

For example, imagine we have three datasets, each representing a day of traffic data in three different cities. Each dataset is independent, as traffic in one city should not impact traffic in another city. However, it is unlikely that each dataset is normally distributed. In fact, each dataset probably has two distinct peaks, one at the morning rush hour and one during the evening rush hour. The histogram of a day of traffic data might look something like this:

histogram

In this scenario, using a numerical hypothesis test would be inappropriate.

2. The population standard deviations of the groups should be equal
For ANOVA and 2-Sample T-Tests, using datasets with standard deviations that are significantly different from each other will often obscure the differences in group means.

To check for similarity between the standard deviations, it is normally sufficient to divide the two standard deviations and see if the ratio is “close enough” to 1. “Close enough” may differ in different contexts but generally staying within 10% should suffice.

3. The samples must be independent
When comparing two or more datasets, the values in one distribution should not affect the values in another distribution. In other words, knowing more about one distribution should not give you any information about any other distribution.

Here are some examples where it would seem the samples are not independent:

the number of goals scored per soccer player before, during, and after undergoing a rigorous training regimen
a group of patients’ blood pressure levels before, during, and after the administration of a drug
It is important to understand your datasets before you begin conducting hypothesis tests on it so that you know you are choosing the right test.

'''
'''HYPOTHESIS TESTING
Tukey's Range Test
Let’s say that we have performed ANOVA to compare three sets of data from the three VeryAnts stores. We received the result that there is some significant difference between datasets.

Now, we have to find out which datasets are different.

We can perform a Tukey’s Range Test to determine the difference between datasets.

If we feed in three datasets, such as the sales at the VeryAnts store locations A, B, and C, Tukey’s Test can tell us which pairs of locations are distinguishable from each other.

The function to perform Tukey’s Range Test is pairwise_tukeyhsd, which is found in statsmodel, not scipy. We have to provide the function with one list of all of the data and a list of labels that tell the function which elements of the list are from which set. We also provide the significance level we want, which is usually 0.05.

For example, if we were looking to compare mean scores of movies that are dramas, comedies, or documentaries, we would make a call to pairwise_tukeyhsd like this:

movie_scores = np.concatenate([drama_scores, comedy_scores, documentary_scores])
labels = ['drama'] * len(drama_scores) + ['comedy'] * len(comedy_scores) + ['documentary'] * len(documentary_scores)

tukey_results = pairwise_tukeyhsd(movie_scores, labels, 0.05)
It will return a table of information, telling you whether or not to reject the null hypothesis for each pair of datasets.'''

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import f_oneway
import numpy as np

a = np.genfromtxt("store_a.csv",  delimiter=",")
b = np.genfromtxt("store_b.csv",  delimiter=",")
c = np.genfromtxt("store_c.csv",  delimiter=",")

stat, pval = f_oneway(a, b, c)
print pval

# Using our data from ANOVA, we create v and l
v = np.concatenate([a, b, c])
labels = ['a'] * len(a) + ['b'] * len(b) + ['c'] * len(c)

tukey_results = pairwise_tukeyhsd(v, labels, 0.05)

print(tukey_results )

#0.000153411660078
#Multiple Comparison of Means - Tukey HSD,FWER=0.05
#=============================================
#group1 group2 meandiff  lower   upper  reject
#---------------------------------------------
#  a      b     7.2767   3.2266 11.3267  True 
#  a      c     4.0115  -0.0385  8.0616 False 
#  b      c    -3.2651  -7.3152  0.7849 False 
#---------------------------------------------

'''
HYPOTHESIS TESTING
Binomial Test
Let’s imagine that we are analyzing the percentage of customers who make a purchase after visiting a website. We have a set of 1000 customers from this month, 58 of whom made a purchase. Over the past year, the number of visitors per every 1000 who make a purchase hovers consistently at around 72. Thus, our marketing department has set our target number of purchases per 1000 visits to be 72. We would like to know if this month’s number, 58, is a significant difference from that target or a result of natural fluctuations.

How do we begin comparing this, if there’s no mean or standard deviation that we can use? The data is divided into two discrete categories, “made a purchase” and “did not make a purchase”.

So far, we have been working with numerical datasets. The tests we have looked at, the 1- and 2-Sample T-Tests, ANOVA, and Tukey’s Range test, will not work if we can’t find the means of our distributions and compare them.

If we have a dataset where the entries are not numbers, but categories instead, we have to use different methods.

To analyze a dataset like this, with two different possibilities for entries, we can use a Binomial Test. A Binomial Test compares a categorical dataset to some expectation.

Examples include:

Comparing the actual percent of emails that were opened to the quarterly goals
Comparing the actual percentage of respondents who gave a certain survey response to the expected survey response
Comparing the actual number of heads from 1000 coin flips of a weighted coin to the expected number of heads
The null hypothesis, in this case, would be that there is no difference between the observed behavior and the expected behavior. If we get a p-value of less than 0.05, we can reject that hypothesis and determine that there is a difference between the observation and expectation.

SciPy has a function called binom_test, which performs a Binomial Test for you.

binom_test requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success. For example, with 1000 coin flips of a fair coin, we would expect a “success rate” (the rate of getting heads), to be 0.5, and the number of trials to be 1000. Let’s imagine we get 525 heads. Is the coin weighted? This function call would look like:
'''
pval = binom_test(525, n=1000, p=0.5)'''
It returns a p-value, telling us how confident we can be that the sample of values was likely to occur with the specified probability. If we get a p-value less than 0.05, we can reject the null hypothesis and say that it is likely the coin is actually weighted, and that the probability of getting heads is statistically different than 0.5.'''

'''
1.
Suppose the goal of VeryAnts’s marketing team this quarter was to have 6% of customers click a link that was emailed to them. They sent out a link to 10,000 customers and 510 clicked the link, which comes out to 5.1% instead of 6%. Did they do significantly worse than the target? Let’s use a binomial test to answer this question.

Use SciPy’s binom_test function to calculate the p-value the experiment returns for this distribution, where we wanted the mean to be 6% of emails opened, or p=0.06, but only saw 5.1% of emails opened.

Store the p-value in a variable called pval and print it out.


Stuck? Get a hint
2.
For the next quarter, marketing has tried out a new email tactic, including puns in every line of every email. As a result, 590 people out of 10000 opened the link in the newest email.

If we still wanted the mean to be 6% of emails opened, but now have 5.9% of emails opened, what is the new p-value. Save your results to the variable pval2

Does this new p-value make sense?'''

from scipy.stats import binom_test

pval = binom_test(510, n=10000, p=0.06)

pval2 = binom_test(590, n=10000, p=0.06)

print(pval) #0.000115920327245

print(pval2) #
'''

HYPOTHESIS TESTING
Chi Square Test
In the last exercise, we looked at data where customers visited a website and either made a purchase or did not make a purchase. What if we also wanted to track if visitors added any items to their shopping cart? With three discrete categories of data per dataset, we can no longer use a Binomial Test. If we have two or more categorical datasets that we want to compare, we should use a Chi Square test. It is useful in situations like:

An A/B test where half of users were shown a green submit button and the other half were shown a purple submit button. Was one group more likely to click the submit button?
Men and women were both given a survey asking “Which of the following three products is your favorite?” Did the men and women have significantly different preferences?
In SciPy, you can use the function chi2_contingency to perform a Chi Square test.

The input to chi2_contingency is a contingency table where:

The columns are each a different condition, such as men vs. women or Interface A vs. Interface B
The rows represent different outcomes, like “Survey Response A” vs. “Survey Response B” or “Clicked a Link” vs. “Didn’t Click”
This table can have as many rows and columns as you need.

In this case, the null hypothesis is that there’s no significant difference between the datasets. We reject that hypothesis, and state that there is a significant difference between two of the datasets if we get a p-value less than 0.05.

Instructions
1.
The management at the VeryAnts ant store wants to know if their two most popular species of ants, the Leaf Cutter and the Harvester, vary in popularity between 1st, 2nd, and 3rd graders.

We have created a table representing the different ants bought by the children in grades 1, 2, and 3 after the last big field trip to VeryAnts. Run the code to see what happens when we enter this table into SciPy’s chi-square test.

Does the resulting p-value mean that we should reject or accept the null hypothesis?

2.
A class of 40 4th graders comes into VeryAnts in the next week and buys 20 sets of Leaf Cutter ants and 20 sets of Harvester ants.

Add this data to the contingency table, rerun the chi-square test, and see if there is now a low enough value to reject the null hypothesis.

'''

from scipy.stats import chi2_contingency

# Contingency table
#         harvester |  leaf cutter
# ----+------------------+------------
# 1st gr | 30       |  10
# 2nd gr | 35       |  5
# 3rd gr | 28       |  12

X = [[30, 10],
     [35, 5],
     [28, 12],
     [20,20]]

chi2, pval, dof, expected = chi2_contingency(X)
print pval


#0.155082308077
#0.00281283455955

'''
I don’t understand. How can p be so big (= 0.155082308077) for this table:

# Contingency table
#         harvester |  leaf cutter
# ----+------------------+------------
# 1st gr | 30       |  10
# 2nd gr | 35       |  5
# 3rd gr | 28       |  12
when there are visibly extreme differences between harvester and leaf cutter, while at the same time it’s so small (p = 0.00281283455955) for this table:

# Contingency table
#         harvester |  leaf cutter
# ----+------------------+------------
# 1st gr | 30       |  10
# 2nd gr | 35       |  5
# 3rd gr | 28       |  12
# 4th gr | 20       |  20
after we added an equal amount of ants to both columns, which should even things out and therefore increase the probability of there not being a major difference between these sets.'''
'''
answer:
This is a very good point. However, I think that the answer to your questions can be found in the way you read and interprete the tables; you look at the tables only in an “horizontal” way (harvester vs. leaf cutter) but you should look at them in a “vertical” way as well (harvester & leaf cutter in relation to each grade). Below is what I mean.

In the first place, I will tell you that it was easier for me to read the table and interprete the statistical results if I gave some discrete attributes to the grades of students. For example consider that the 1st grade come from USA, the 2nd from Canada and the 3rd from France. Now, recall what you are looking for: is species-ants preference related to the country of origin of the students? The Null Hypothesis is that there is no association between country and ants-preferences.

Before you make the statistical calculations, you can make a guess by looking at the first table: For each county individually, the majority of students show an obvious preference to harvester ; this makes you guess that the country of origin does not matter; whichever of the three countries students come from, most of them prefer harvester. After the calculation of the chi-square test and the result of the p-value, it seems that you were right. P-value is quite high , you can not reject the Ho, meaning that there is no association.
Now, a fourth group of students visit the VeryAnts ant store, they come from Spain. They are again 40 in total, but something different happens now. You notice a different attitude, the majority of them don’t show a preference towards harvester as the previous 3 groups did; their preferences are equally shared (20-20). You start to be suspicious that maybe the country does matter to the preferences of ants. The calculation of chi-square test and p-value proves that. Low p-value -> rejection of Ho -> there seems to be actually an association.

To feel more confident with the above results, I did the following: I added another group of “extreme” values to the contigency table. For instance, a 5th group of students, coming let’say from Japan, visit the store. They are again 40 in total. 39 buy harvester and only 1 leaf cutter! One may be very surprised by that overwhelming preference to harvester and may wonder why. Well, this might be a part of another survey. In the current survey, now we feel much more confident that country does matter. We may guess what the p-value will be , extemely low, the Ho is rejected .'''

'''
answer 2: 
Begin with the totals:

Grade	Harvester	Leaf	Total
1	       30	     10	     40
2	       35	      5	     40
3	       28	     12	     40
Total	   93	     27	    120

So, 120 students made 1 selection each, or 120 selections.

Of all 120 ant selections, harvester comprised 93, or 77.5%

Of all 120 students, 40 (33%) are in 1st grade

Therefore, we would expect from all of the 120 ant selections, 0.333 * 0.775 * 120 = 31 would be in the first grade:

This can be done in each block to create an “Expected” table:

Grade	Harvester	Leaf	Total
1	       31	     9	     40
2	       31	     9	     40
3	       31	     9	     40
Total	   93	     27	    120
Note that the totals remain the same, and that in this particular case, numbers are the same for each grade, since there are the same number of students in each grade, and our default (null) assumption is that there is no difference in the number of ants selected between grades 1, 2 and 3.

Now, chi-square is simply sum((Observed - Expected)**2/ Expected)

For instance for grade 1, Observed - Expected is -1; that squared is 1, and dividing by Expected gives 1/31, or 0.03226. The similarly calculated values for the other five blocks are (0.51613, 0.29032, 0.11111, 1.77778 and 1.0). The sum of those 6 is 3.7276, which, as you will note, compares nicely with the “chi2” value yielded in the exercise.

I do not know the details of getting there to the p-value, mainly because nearly every reference says either “look it up in a table” or “use an online p-value calculator.” You need to know the “degrees of freedom” which is one less than the number of categories, in our case 2. So you take chi-square of 3.7276 and 2 degrees of freedom to a p-value calculator 1 to get: 0.1551 Check!'''

'''

Quiz:

1.Let’s say that last month 7% of free users of a site converted to paid users, but this month only 5% of free users converted. What kind of test should we use to see if this difference is significant?

2.Let’s say we run a 1 Sample T-Test on means for an exam. We expect the mean to be 75%, but we want to see if the actual scores are significantly better or worse than what we expected. After running the T-Test, we get a p-value of 0.25. What does this result mean?

3.What kind of test would you use to see if men and women identify differently as “Republican”, “Democrat”, or “Independent”?

4.You regularly order delivery from two different Pho restaurants, “What the Pho” and “Pho Tonic”. You want to know if there’s a significant difference between these two restaurants’ average time to deliver to your house. What test could you use to determine this?

5.You own a juice bar and you theorize that 75% of your customers live in the surrounding 5 blocks. You survey a random sample of 12 customers and find that 7 of them live within those 5 blocks. What test do you run to determine if your results significantly differ from your expectation?

6.You’ve surveyed 10 people who work in finance, 10 people who work in education, and 10 people who work in the service industry on how many cups of coffee they drink per day. What test can you use to determine if there is a significant difference between the average coffee consumption of these three groups?

7.Let’s say we are comparing the time that users spend on three different versions of a landing page for a website. What test do we use to determine if there is a significant difference between any two of the sets?

8. You just bought a new tea kettle that is supposed to heat water to boiling in 2 minutes. What kind of test can you run to determine if the time-to-boil is averaging significantly more than 2 minutes?


9.If we perform an ANOVA test on 3 datasets and reject the null hypothesis, what test should we perform to determine which pairs of datasets are different?

10.You’ve collected data on 1000 different sites that end with .com, .edu, and .org and have recorded the number of each that have Times New Roman, Helvetica, or another font as their main font. What test can you use to determine if there’s a relationship between top-level domain and font type?


Quiz Answer

1.Chi Square
2.we cannot confidently reject the null-hypothesis, so we do not have enough data to say that the mean on this exam is different from 75%
3.Chi Square
4.2 sample test
5.Binomial test
6.Anova
7.Anova
8.1 sample
9.tukey's range test
10.Chi saqare

'''
'''1.
We’re going to start by including a data interface that a previous software engineer wrote for you, it’s aptly titled familiar, so just import that.


Stuck? Get a hint
2.
Perfect, now the first thing we want to show is that our most basic package, the Vein Pack, actually has a significant impact on the subscribers. It would be a marketing goldmine if we can show that subscribers to the Vein Pack live longer than other people.

Lifespans of Vein Pack users are returned by the function lifespans(package='vein'), which is part of the familiar module. Call that function and save the data into a variable called vein_pack_lifespans.


Stuck? Get a hint
3.
We’d like to find out if the average lifespan of a Vein Pack subscriber is significantly different from the average life expectancy of 71 years.

Import the statistical test we would use to determine if a sample comes from a population that has a given mean from scipy.stats.


Stuck? Get a hint
4.
Now use the 1-Sample T-Test to compare vein_pack_lifespans to the average life expectancy 71. Save the result into a variable called vein_pack_test.


Stuck? Get a hint
5.
Let’s check if the results are significant! Check the pvalue of vein_pack_test. If it’s less than 0.05, we’ve got significance!

6.
We want to present this information to the CEO, Vlad, of this incredible finding. Let’s print some information out! If the test’s p-value is less than 0.05, print “The Vein Pack Is Proven To Make You Live Longer!”. Otherwise print “The Vein Pack Is Probably Good For You Somehow!”


Stuck? Get a hint
Upselling Familiar: Pumping Life Into The Company
7.
In order to differentiate Familiar’s different product lines, we’d like to compare this lifespan data between our different packages. Our next step up from the Vein Pack is the Artery Pack. Let’s get the lifespans of Artery Pack subscribers using the same method, called with package='artery' instead. Save the value into a variable called artery_pack_lifespans.


Stuck? Get a hint
8.
Now we want to show that the subscribers to the Artery Pack experience a significant improvement even beyond what a Vein Pack subscriber’s benefits. Import the 2-Sample T-Test and we’ll use that to see if there is a significant difference between the two subscriptions.


Stuck? Get a hint
9.
Okay let’s run the 2-Sample test! Save the results into a variable named package_comparison_results.


Stuck? Get a hint
10.
Let’s see the results! If the p-value from our experiment is less than 0.05, the results are significant and we should print out “the Artery Package guarantees even stronger results!”. Otherwise we should print out “the Artery Package is also a great product!”


Stuck? Get a hint
11.
Well, shame that it’s not significantly better, but maybe there’s a way to demonstrate the benefits of the Artery Package yet.


Stuck? Get a hint
Benefitting Everyone: A Familiar Problem
12.
If your lifespan isn’t significantly increased by signing up for the Artery Package, maybe we can make some other claim about the benefits of the package. To that end, we’ve sent out a survey collecting the iron counts for our subscribers, and filtered that data into “low”, “normal”, and “high”.

We received 200 responses from our Vein Package subscribers. 70% of them had low iron counts, 20% had normal, and 10% of them have high iron counts.

We were only able to get 145 responses from our Artery Package subscribers, but only 20% of them had low iron counts. 60% had normal, and 20% have high iron counts.

13.
The data from the survey has been collected and formatted into a contingency table. You can access that data from the function familiar.iron_counts_for_package(). Save the survey results into a variable called iron_contingency_table.


Stuck? Get a hint
14.
We want to be able to tell if what seems like a higher number of our Artery Package subscribers is a significant difference from what was reported by Vein Package subscribers. Import the Chi-Squared test so that we can find out.


Stuck? Get a hint
15.
Run the Chi-Squared test on the iron_contingency_table and save the p-value in a variable called iron_pvalue. Remember that this test returns four things: the test statistic, the p-value, the number of degrees of freedom, and the expected frequencies.


Hint
Run the Chi-Squared test on the contingency table and save the p-value like so:

_, iron_pvalue, _, _ = chi2_contingency(iron_contingency_table)
16.
Here’s the big moment: if the iron_pvalue is less than 0.05, print out “The Artery Package Is Proven To Make You Healthier!” otherwise we’ll have to use our other marketing copy: “While We Can’t Say The Artery Package Will Help You, I Bet It’s Nice!”

17.
Fantastic! With proven benefits to both of our product lines, we can definitely ramp up our marketing and sales. Look out for a Familiar face in drug stores everywhere.'''



#------------------------script.py--------------------------------------
from familiar import *
import numpy as np
from scipy.stats import ttest_1samp
from scipy.stats import ttest_ind
from scipy.stats import chi2_contingency

vein_pack_lifespans = lifespans('vein')
artery_pack_lifespans = lifespans('artery')

vein_pack_lifespans_mean = np.mean(vein_pack_lifespans)
artery_pack_lifespans_mean = np.mean(artery_pack_lifespans)
#print(vein_pack_lifespans_mean)

vein_pack_test = ttest_1samp(vein_pack_lifespans , 71)

print(vein_pack_test)

if vein_pack_test.pvalue > 0.05:
  print('The Vein Pack Is Probably Good For You Somehow!')
else:
  print('The Vein Pack Is Proven To Make You Live Longer!')

package_comparison_results = ttest_ind(vein_pack_lifespans, artery_pack_lifespans)

print(package_comparison_results)

if package_comparison_results.pvalue > 0.05:
  print('The Artery Package is also a great product!')
else:
  print('The Artery Package guarantees even stronger results!')

iron_contingency_table = iron_counts_for_package()

_, iron_pvalue, _, _ = chi2_contingency(iron_contingency_table)
print(iron_pvalue)

if iron_pvalue < 0.05:
  print('The Artery Package Is Proven To Make You Healthier!')
else:
  print('while we can\'t say the artery package will help you, i bet it''s nice')

#------------------familiar.py------------------------------------------

def lifespans(package):
  if package == 'vein':
    return [76.937674313716172, 75.993359130146814, 74.798150123540481, 74.502021471585508, 77.48888897587436, 72.142565731540429, 75.993031671911822, 76.341550480952279, 77.484755629998816, 76.532101480086695, 76.255089552764176, 77.58398316566651, 77.047370349622938, 72.874751745947108, 77.435045470028442, 77.492341410789194, 78.326720468799522, 73.343702468870674, 79.969157652363464, 74.838005833003251]
  elif package == 'artery':
    return [76.335370084268348, 76.923082315590619, 75.952441644877794, 74.544983480720305, 76.404504275447195, 73.079248886365761, 77.023544610529925, 74.117420420068797, 77.38650656208344, 73.044765837189928, 74.963118508661665, 73.319543019334859, 75.857401376968625, 76.152653513512547, 73.355102863226705, 73.902212564587884, 73.771211950924751, 68.314898302855781, 74.639757177753282, 78.385477308439789]
  else:
    print "Package not found. Possible values 'vein' or 'artery'"
    return None

def iron_counts_for_package():
  """
            vein     |  artery
    ----+------------+------------
     low|200 * 0.7   |145 * 0.2
  normal|200 * 0.2   |145 * 0.2
    high|200 * 0.1   |145 * 0.6
  """
  return [[140, 29],
          [40, 87],
          [20, 29]]


#-----------------------Hypothesis testing project #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-

#----dog_data.csv -----


is_rescue,weight,tail_length,age,color,likes_children,is_hypoallergenic,name,breed
0,4,3.59,3,grey,0,0,Nonah,chihuahua
0,2,6.89,1,grey,1,0,Kai,chihuahua
0,5,1.54,6,grey,1,1,Janith,chihuahua
0,3,3.39,5,grey,0,0,Nicky,chihuahua
0,5,4.28,6,grey,0,1,Tobe,chihuahua
0,8,4.63,2,grey,0,1,Carroll,chihuahua
0,1,3.48,3,grey,0,0,Bjorn,chihuahua
0,8,0.97,3,grey,0,0,Janina,chihuahua
0,3,1.93,3,grey,1,1,Vick,chihuahua
0,3,4.18,2,grey,0,0,Gunilla,chihuahua
0,4,2.63,4,grey,0,1,Kahaleel,chihuahua
1,6,2.26,1,grey,1,0,Loleta,chihuahua
0,4,2.79,3,grey,1,1,Brook,chihuahua
0,6,6.76,3,grey,0,0,Janine,chihuahua
0,6,2.27,2,grey,0,0,Tilly,chihuahua
0,8,4,2,grey,1,0,Oren,chihuahua
1,7,1.47,4,grey,0,0,Hadleigh,chihuahua
0,5,5.95,2,grey,0,1,Zorina,chihuahua
0,7,0.62,2,grey,0,0,Gasparo,chihuahua
0,6,5.42,3,grey,1,0,Lowrance,chihuahua
0,8,2.63,3,grey,0,0,Keelia,chihuahua
0,1,1.45,1,grey,0,1,Jannel,chihuahua
0,3,2.88,4,grey,0,0,Perkin,chihuahua
0,7,3.23,3,grey,1,0,Chrissy,chihuahua
0,4,5.64,3,grey,0,0,Robena,chihuahua
0,5,1.84,2,grey,0,0,Glenden,chihuahua
0,3,4.74,1,grey,1,0,Inez,chihuahua
0,7,0.89,2,white,1,0,Zed,chihuahua
0,5,1.16,1,white,0,1,Silvio,chihuahua
0,3,10.08,1,white,1,0,Weber,chihuahua
0,6,4.88,2,white,0,0,Domenic,chihuahua
0,72,13.13,6,black,1,0,Talbert,greyhound
0,88,20.98,4,black,0,0,Brandtr,greyhound
0,90,17.6,7,black,1,0,Dorthea,greyhound
0,66,13.71,7,black,0,1,Kip,greyhound
0,57,20.3,6,black,1,0,Stephanus,greyhound
0,76,9.53,3,black,0,1,Stephanie,greyhound
0,70,10.18,7,black,0,1,Lorenzo,greyhound
0,100,17.92,1,black,0,1,Eadmund,greyhound
0,65,14.18,4,black,0,1,Tyne,greyhound
0,84,18.91,4,black,0,0,Evania,greyhound
0,62,21.14,5,grey,1,0,Derrik,greyhound
0,81,16.96,4,grey,0,0,Hervey,greyhound
0,84,17.45,8,grey,1,1,Tobie,greyhound
0,85,16.92,1,grey,0,0,Say,greyhound
0,87,27.34,2,grey,1,1,Winne,greyhound
0,72,16.6,5,grey,1,1,Lyn,greyhound
0,72,18.48,1,grey,0,0,Shelia,greyhound
0,70,14.91,4,grey,1,1,Shep,greyhound
0,88,18.95,6,grey,0,1,Chrystel,greyhound
0,61,15.23,1,grey,0,1,Sheff,greyhound
0,82,12.83,3,grey,1,0,Verla,greyhound
0,65,22.53,3,grey,0,1,Jarrett,greyhound
0,77,16.25,12,grey,1,0,Julius,greyhound
0,71,20.58,2,grey,0,0,Mack,greyhound
0,90,12.36,1,grey,0,1,Anallise,greyhound
0,54,17.98,7,grey,0,1,Jenifer,greyhound
0,68,26.59,1,grey,0,1,Ransell,greyhound
0,66,14.11,8,grey,1,0,Sher,greyhound
0,93,24.3,9,grey,1,0,Tiffi,greyhound
0,74,15.12,5,grey,1,1,Lianna,greyhound
1,65,23.64,6,grey,1,1,Steward,greyhound
0,86,14.62,3,grey,1,0,Farly,greyhound
0,35,14.67,1,white,1,0,Hamish,pitbull
0,52,12.8,7,white,0,0,Walsh,pitbull
0,43,7.88,4,white,0,0,Robert,pitbull
0,40,3.78,4,white,1,1,Raynor,pitbull
0,41,10.23,4,white,0,0,Rene,pitbull
0,39,9.13,4,white,0,0,Jolene,pitbull
0,44,9.53,4,white,0,1,Alicea,pitbull
0,58,8.05,1,black,1,0,Moise,poodle
0,56,9.44,4,black,1,0,Boote,poodle
1,59,4.04,4,black,1,0,Beatrix,poodle
0,70,12.37,1,black,1,0,Rabbi,poodle
0,52,11.42,2,black,0,0,Tallou,poodle
0,56,8.7,5,black,1,0,Evvie,poodle
0,57,9.47,5,black,1,1,Sayers,poodle
0,66,8.32,5,black,1,0,Hillie,poodle
1,58,11.89,1,black,0,1,Kath,poodle
0,61,7.57,5,black,0,0,Joelly,poodle
0,56,9.66,2,black,1,0,Kellen,poodle
0,65,8.65,4,black,0,0,Arch,poodle
0,60,11.95,8,grey,1,0,Sibella,poodle
1,63,10.78,6,grey,0,0,Turner,poodle
0,67,10,2,grey,1,0,Bibby,poodle
0,64,12.03,10,grey,1,0,Gregg,poodle
0,64,6.34,8,grey,1,0,Nevin,poodle
1,66,9.68,6,grey,1,0,Sonny,poodle
0,54,8,7,white,1,0,Ford,poodle
0,67,3.9,11,white,1,0,Haroun,poodle
1,60,5.66,3,white,0,0,Almeta,poodle
0,57,3.55,10,grey,0,0,Corbin,rottweiler
0,37,2.53,8,grey,1,0,Gaby,rottweiler
0,58,3.57,12,grey,0,0,Bea,rottweiler
0,58,5.36,5,grey,0,1,Fransisco,rottweiler
0,54,3.94,3,grey,1,0,Heindrick,rottweiler
0,58,1.8,7,grey,1,0,Lorrin,rottweiler
0,47,5.14,9,grey,0,0,Katinka,rottweiler
1,58,4.42,3,grey,0,0,Onida,rottweiler
0,58,2.02,1,grey,0,0,Jsandye,rottweiler
1,46,6.08,5,grey,0,0,Stirling,rottweiler
0,50,0.82,8,grey,0,0,Kata,rottweiler
0,75,5.67,4,grey,0,0,Bettine,rottweiler
0,51,6.45,3,grey,0,0,Conant,rottweiler
0,65,4.22,7,grey,1,0,Lucien,rottweiler
1,46,1.36,7,grey,1,0,Gladys,rottweiler
0,54,3.25,11,grey,1,0,Court,rottweiler
0,67,4.4,5,grey,0,1,Niccolo,rottweiler
0,64,5.16,9,grey,1,1,Gage,rottweiler
0,54,5.33,7,grey,1,0,Bryn,rottweiler
0,50,9.32,10,grey,0,0,Paulina,rottweiler
0,47,3.06,12,grey,0,0,Northrup,rottweiler
0,11,2.77,5,brown,0,0,Ursulina,shihtzu
0,13,2.02,2,brown,1,0,Lind,shihtzu
0,11,2.99,3,brown,0,1,Gabriela,shihtzu
0,9,2.38,4,brown,1,0,Loralie,shihtzu
0,16,3.38,4,brown,1,0,Townie,shihtzu
0,12,2.99,4,brown,1,0,Mariquilla,shihtzu
0,12,1.33,2,brown,1,0,Tobye,shihtzu
0,10,2.81,3,brown,1,1,Mercedes,shihtzu
0,16,1.31,2,brown,0,0,Birdie,shihtzu
0,14,2.13,2,brown,0,0,Adolpho,shihtzu
0,10,2.53,3,brown,1,1,Michaelina,shihtzu
0,12,2.79,4,brown,1,0,Stevena,shihtzu
0,6,2.83,5,grey,1,0,Celie,shihtzu
0,13,2.87,5,grey,0,0,Mitchael,shihtzu
0,11,2.97,4,grey,1,0,Anselma,shihtzu
0,14,1.13,4,grey,0,1,Lorita,shihtzu
0,10,1.4,3,grey,1,1,Lola,shihtzu
0,15,3.48,4,grey,1,0,Jonah,shihtzu
0,14,2.93,4,grey,1,0,Fanny,shihtzu
0,9,0.57,2,grey,1,0,Cammie,shihtzu
0,11,1.91,3,grey,0,0,Niven,shihtzu
0,9,1.26,3,grey,1,0,Daile,shihtzu
0,11,2.21,2,grey,0,0,Felix,shihtzu
0,29,5.33,1,grey,1,1,Charla,terrier
0,21,3.05,1,grey,1,0,Coral,terrier
0,30,6.45,4,grey,1,0,Sybille,terrier
0,30,5.15,3,grey,0,0,Neils,terrier
0,28,4.51,1,grey,1,0,Daisi,terrier
0,50,3.27,2,grey,0,1,Haleigh,terrier
0,36,1.65,2,grey,0,0,Porter,terrier
0,24,5.57,3,grey,1,1,Eva,terrier
0,40,5.67,2,grey,0,1,Kaitlin,terrier
0,35,0.48,1,grey,0,0,Anni,terrier
0,13,3.95,1,grey,1,0,Asher,terrier
1,42,2.96,1,grey,0,0,Leoine,terrier
0,30,4.23,3,grey,1,0,Maritsa,terrier
0,23,3.38,1,grey,0,0,Klarrisa,terrier
0,39,3.69,2,grey,0,1,Constancia,terrier
0,32,4.64,3,grey,0,1,Hedy,terrier
0,23,0.56,1,grey,1,0,Ardith,terrier
0,22,1,2,grey,1,0,Meridith,terrier
0,23,7.5,4,grey,1,0,Farrell,terrier
0,27,5.28,1,grey,0,0,Ira,terrier
0,29,5.25,3,grey,0,0,Eloisa,terrier
0,31,16.25,2,grey,0,0,Trudy,whippet
1,56,10.38,7,grey,1,0,Morgan,whippet
0,68,14.89,6,grey,0,1,Engelbert,whippet
0,37,9.95,8,grey,0,0,Theo,whippet
0,24,14.34,6,grey,0,0,Galvin,whippet
0,42,7.04,12,grey,0,1,Cecilla,whippet
0,58,10.59,8,grey,1,0,Regina,whippet
0,62,18.83,1,grey,1,0,Ellwood,whippet
0,27,15.18,3,grey,0,0,Gard,whippet
  
  
#-------------------fetchmaker.py---------------


import pandas as pd
import numpy as np

dogs = pd.read_csv("dog_data.csv")

def get_attribute(breed, attribute):
  if breed in dogs.breed.unique():
    if attribute in dogs.columns:
			return dogs[dogs["breed"] == breed][attribute]
    else:
      raise NameError('Attribute {} does not exist.'.format(attribute))
  else:
    raise NameError('Breed {} does not exist.'.format(breed))
  

def get_weight(breed):
  return get_attribute(breed, 'weight')
  
def get_tail_length(breed):
  return get_attribute(breed, 'tail_length')

def get_color(breed):
	return get_attribute(breed, 'color')

def get_age(breed):
	return get_attribute(breed, 'age')

def get_is_rescue(breed):
	return get_attribute(breed, 'is_rescue')

def get_likes_children(breed):
	return get_attribute(breed, 'likes_children')

def get_is_hypoallergenic(breed):
	return get_attribute(breed, "is_hypoallergenic")

def get_name(breed):
	return get_attribute(breed, "name")


#----------------script.py---------------

import numpy as np
import fetchmaker
from scipy.stats import binom_test, f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import chi2_contingency

rottweiler_tl = fetchmaker.get_tail_length("rottweiler")

#print (np.mean(rottweiler_tl))
#print(np.std(rottweiler_tl))

#-------------

whippet_rescue = fetchmaker.get_is_rescue('whippet')
num_whippet_rescues = np.count_nonzero(whippet_rescue)
num_whippets = np.size(whippet_rescue)

pval = binom_test(num_whippet_rescues, n=num_whippets, p=0.08)
print (pval)#0.58

whippets_wt = fetchmaker.get_weight('whippet')
terriers_wt = fetchmaker.get_weight('terrier')
pitbulls_wt = fetchmaker.get_weight('pitbull')

print(np.mean(whippets_wt))
print(np.mean(terriers_wt))
print(np.mean(pitbulls_wt))

_, pval2 = f_oneway(whippets_wt, terriers_wt, pitbulls_wt)
print(pval2)

v = np.concatenate([whippets_wt, terriers_wt, pitbulls_wt])
labels = ['whippet'] * len(whippets_wt) + ['terrier'] * len(terriers_wt) + ['pitbull'] * len(pitbulls_wt)

tukey_results = pairwise_tukeyhsd(v, labels, 0.05)
print(tukey_results)

#-------------

poodle_colors = fetchmaker.get_color('poodle')
shihtzu_colors = fetchmaker.get_color('shihtzu')

colors_list = ['brown', 'gold', 'Grey', 'White']

poodle_color_count = []
shihtzu_color_count = []
x = []

for color in colors_list:
  temp = []
  temp.append(np.count_nonzero(poodle_colors == color.lower() ))
  temp.append(np.count_nonzero(shihtzu_colors == color.lower() ))
  x.append(temp)

'''	
Poodle	Shih Tzu
Black	  x   	x
Brown	  x   	x
Gold	  x	    x
Grey	  x	    x
White	  x	    x'''

chi2, pval, dof, expected = chi2_contingency(x)
print pval






#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-HYPOTHESIS TESTING CHEATSHEET SCIPY / PYTHON SCIPY HYPOTHESIS TSSTING #-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''


Cheatsheets / Hypothesis Testing with SciPy

Hypothesis Testing
Print PDF icon
Print Cheatsheet


TOPICS

Hypothesis Testing
Sample Size Determination
Analyze FarmBurg's A/B Test
Hypothesis Test Errors
Type I errors, also known as false positives, is the error of rejecting a null hypothesis when it is actually true. This can be viewed as a miss being registered as a hit. The acceptable rate of this type of error is called significance level and is usually set to be 0.05 (5%) or 0.01 (1%).

Type II errors, also known as false negatives, is the error of not rejecting a null hypothesis when the alternative hypothesis is the true. This can be viewed as a hit being registered as a miss.

Depending on the purpose of testing, testers decide which type of error to be concerned. But, usually type I error is more important than type II.

Sample Vs. Population Mean
In statistics, we often use the mean of a sample to estimate or infer the mean of the broader population from which the sample was taken. In other words, the sample mean is an estimation of the population mean.

Central Limit Theorem
The central limit theorem states that as samples of larger size are collected from a population, the distribution of sample means approaches a normal distribution with the same mean as the population. No matter the distribution of the population (uniform, binomial, etc), the sampling distribution of the mean will approximate a normal distribution and its mean is the same as the population mean.

The central limit theorem allows us to perform tests, make inferences, and solve problems using the normal distribution, even when the population is not normally distributed.

Hypothesis Test P-value
Statistical hypothesis tests return a p-value, which indicates the probability that the null hypothesis of a test is true. If the p-value is less than or equal to the significance level, then the null hypothesis is rejected in favor of the alternative hypothesis. And, if the p-value is greater than the significance level, then the null hypothesis is not rejected.

Univariate T-test
A univariate T-test (or 1 Sample T-test) is a type of hypothesis test that compares a sample mean to a hypothetical population mean and determines the probability that the sample came from a distribution with the desired mean.

This can be performed in Python using the ttest_1samp() function of the SciPy library. The code block shows how to call ttest_1samp(). It requires two inputs, a sample distribution of values and an expected mean and returns two outputs, the t-statistic and the p-value.
'''
from scipy.stats import ttest_1samp

t_stat, p_val = ttest_1samp(example_distribution, expected_mean)'''
Tukey’s Range Hypothesis Tests
A Tukey’s Range hypothesis test can be used to check if the relationship between two datasets is statistically significant.

The Tukey’s Range test can be performed in Python using the StatsModels library function pairwise_tukeyhsd(). The example code block shows how to call pairwise_tukeyhsd(). It accepts a list of data, a list of labels, and the desired significance level.
'''
from statsmodels.stats.multicomp import pairwise_tukeyhsd

tukey_results = pairwise_tukeyhsd(data, labels, alpha=significance_level)



'''




'''











































#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON MATPLOTLIB PYTHON MATPLOTLIB PYTHON MATPLOTLIB PYTHON MATPLOTLIB PYTHON MATPLOTLIB#-#-#-#-#-#-#-#-#-
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
import codecademylib
from matplotlib import pyplot as plt

days = [0, 1, 2, 3, 4, 5, 6]
money_spend = [10, 12, 12, 10, 14, 22, 24]

plt.plot(days, money_spend)
plt.show()

#-----------------------

# Days of the week:
days = [0, 1, 2, 3, 4, 5, 6]
# Your Money:
money_spent = [10, 12, 12, 10, 14, 22, 24]
# Your Friend's Money:
money_spent_2 = [11, 14, 15, 15, 22, 21, 12]
# Plot your money:
plt.plot(days, money_spent)
# Plot your friend's money:
plt.plot(days, money_spent_2)
# Display the result:
plt.show()

#----------------------

plt.plot(days, money_spent, color='green')
plt.plot(days, money_spent_2, color='#AAAAAA')

# Dashed:
plt.plot(x_values, y_values, linestyle='--')
# Dotted:
plt.plot(x_values, y_values, linestyle=':')
# No line:
plt.plot(x_values, y_values, linestyle='')

# A circle:
plt.plot(x_values, y_values, marker='o')
# A square:
plt.plot(x_values, y_values, marker='s')
# A star:
plt.plot(x_values, y_values, marker='*')

plt.plot(days, money_spent, color='green', linestyle='--')
plt.plot(days, money_spent_2, color='#AAAAAA',  marker='o')

#-------------------------------

#For example, if we want to display a plot from x=0 to x=3 and from y=2 to y=5, we would call plt.axis([0, 3, 2, 5]).

x = [0, 1, 2, 3, 4]
y = [0, 1, 4, 9, 16]
plt.plot(x, y)
plt.axis([0, 3, 2, 5])
plt.show()

#-----------------------------------------------
#We can label the x- and y- axes by using plt.xlabel() and plt.ylabel(). The plot title can be set by using plt.title().

hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
happiness = [9.8, 9.9, 9.2, 8.6, 8.3, 9.0, 8.7, 9.1, 7.0, 6.4, 6.9, 7.5]
plt.plot(hours, happiness)
plt.xlabel('Time of day')
plt.ylabel('Happiness Rating (out of 10)')
plt.title('My Self-Reported Happiness While Awake')
plt.show()

#----------------------------------------

#We can create subplots using .subplot().

#The command plt.subplot() needs three arguments to be passed into it:

#The number of rows of subplots
#The number of columns of subplots
#The index of the subplot we want to create

#For instance, the command plt.subplot(2, 3, 4) would create “Subplot 4” from the figure above.


import codecademylib3_seaborn
import matplotlib.pyplot as plt
import numpy as np 

from os.path import join, dirname, abspath
from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()

x = iris.data
y = iris.target

fignum = 1

# Plot the ground truth

fig = plt.figure(fignum, figsize=(4, 3))

ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

for name, label in [('Robots', 0),
                    ('Cyborgs', 1),
                    ('Humans', 2)]:
    ax.text3D(x[y == label, 3].mean(),
              x[y == label, 0].mean(),
              x[y == label, 2].mean() + 2, name,
              horizontalalignment='center',
              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))

# Reorder the labels to have colors matching the cluster results

y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(x[:, 3], x[:, 0], x[:, 2], c=y, edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

ax.set_xlabel('Time to Heal')
ax.set_ylabel('Reading Speed')
ax.set_zlabel('EQ')

ax.set_title('')
ax.dist = 12

plt.show(head)

#Any plt.plot() that comes after plt.subplot() will create a line plot in the specified subplot. For instance:

# Data sets
x = [1, 2, 3, 4]
y = [1, 2, 3, 4]

# First Subplot
plt.subplot(1, 2, 1)
plt.plot(x, y, color='green')
plt.title('First Subplot')

# Second Subplot
plt.subplot(1, 2, 2)
plt.plot(x, y, color='steelblue')
plt.title('Second Subplot')

# Display both subplots
plt.show()

#----------------------------------------
#.subplots_adjust() has some keyword arguments that can move your plots within the figure:

#left — the left-side margin, with a default of 0.125. You can increase this number to make room for a y-axis label
#right — the right-side margin, with a default of 0.9. You can increase this to make more room for the figure, or decrease it to make room for a legend
#bottom — the bottom margin, with a default of 0.1. You can increase this to make room for tick mark labels or an x-axis label
#top — the top margin, with a default of 0.9
#wspace — the horizontal space between adjacent subplots, with a default of 0.2
#hspace — the vertical space between adjacent subplots, with a default of 0.2
#For example, if we were adding space to the bottom of a graph by changing the bottom margin to 0.2 (instead of the default of 0.1), we would use the command:

#plt.subplots_adjust(bottom=0.2)
#We can also use multiple keyword arguments, if we need to adjust multiple margins. For instance, we could adjust both the top and the hspace:

#plt.subplots_adjust(top=0.95, hspace=0.25)
#Let’s use wspace to fix the figure above:

# Left Plot
plt.subplot(1, 2, 1)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Right Plot
plt.subplot(1, 2, 2)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Subplot Adjust
plt.subplots_adjust(wspace=0.35)

plt.show()

#-------------------------------------------


plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.legend(['parabola', 'cubic'])
plt.show()

#-----

Number Code	String
0	best
1	upper right
2	upper left
3	lower left
4	lower right
5	right
6	center left
7	center right
8	lower center
9	upper center
10	center
Note: If you decide not to set a value for loc, it will default to choosing the “best” location.	

#------

plt.legend(['parabola', 'cubic'], loc=6)
plt.show()

#------

plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],
         label="parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64],
         label="cubic")
plt.legend() # Still need this command!
plt.show()

#--------------------------------------------------
#for multiple subplots
ax = plt.subplot(1, 1, 1)
#for only one subplots
ax = plt.subplot()

#ax arguments

ax.set_xticks([1, 2, 4])
ax.set_yticks([0.1, 0.6, 0.8])
ax.set_yticklabels(['10%', '60%', '80%'])

#plot mark rather than line
ax = plt.subplot()
plt.plot([1, 3, 3.5], [0.1, 0.6, 0.8], 'o')
ax.set_yticks([0.1, 0.6, 0.8])
ax.set_yticklabels(['10%', '60%', '80%'])

#--------------------------------------------------
plt.close('all') to clear all existing plots

# Figure 2
plt.figure(figsize=(4, 10)) 
plt.plot(x, parabola)
plt.savefig('tall_and_narrow.png')

#---------------------------------------------------

import codecademylib
from matplotlib import pyplot as plt

months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]

visits_per_month = [9695, 7909, 10831, 12942, 12495, 16794, 14161, 12762, 12777, 12439, 10309, 8724]

# numbers of limes of different species sold each month
key_limes_per_month = [92.0, 109.0, 124.0, 70.0, 101.0, 79.0, 106.0, 101.0, 103.0, 90.0, 102.0, 106.0]
persian_limes_per_month = [67.0, 51.0, 57.0, 54.0, 83.0, 90.0, 52.0, 63.0, 51.0, 44.0, 64.0, 78.0]
blood_limes_per_month = [75.0, 75.0, 76.0, 71.0, 74.0, 77.0, 69.0, 80.0, 63.0, 69.0, 73.0, 82.0]

x_values = range(len(months))


# create your figure here
plt.figure(figsize = (12, 8))

ax1 = plt.subplot(1, 2, 1)
plt.plot(x_values, visits_per_month, marker = "o")
plt.xlabel ('Months')
plt.ylabel ('Visits')
plt.title ('Total Visits by Months')
ax1.set_xticks(x_values)
ax1.set_xticklabels(months)

ax2 = plt.subplot(1, 2, 2)
plt.plot(x_values, key_limes_per_month, color = 'green')
plt.plot(x_values, persian_limes_per_month, color = 'yellow')
plt.plot(x_values, blood_limes_per_month, color = 'red')
plt.title ('Limes Sales by Months')
plt.xlabel ('Months')
plt.ylabel ('Limes')
plt.legend(['Key', 'persian', 'blood'])
ax2.set_xticks(x_values)
ax2.set_xticklabels(months)

plt.subplots_adjust(wspace = 0.3)
plt.show()
plt.savefig('lime_sales.png')


#
# DIFFERENT PLOT TYPES
# Simple Bar Chart
# The plt.bar function allows you to create simple bar charts to compare multiple categories of data.

# Some possible data that would be displayed with a bar chart:

# x-axis — famous buildings, y-axis — heights
# x-axis — different planets, y-axis — number of days in the year
# x-axis — programming languages, y-axis — lines of code written by you
# You call plt.bar with two arguments:

# the x-values — a list of x-positions for each bar
# the y-values — a list of heights for each bar
# In most cases, we will want our x-values to be a list that looks like [0, 1, 2, 3 ...] and has the same number of elements as our y-values list. We can create that list manually, but we can also use the following code:

heights = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
x_values = range(len(heights))
# The range function creates a list of consecutive integers (i.e., [0, 1, 2, 3, ...]). It needs an argument to tell it how many numbers should be in the list. For instance, range(5) would make a list with 5 numbers. We want our list to be as long as our bar heights (heights in this example). len(heights) tell us how many elements are in the list heights.

# Here is an example of how to make a bar chart using plt.bar to compare the number of days in a year on the different planets:

days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
plt.bar(range(len(days_in_year)),
        # days_in_year)
plt.show()
# The result of this is:

# planet_bar_chart

# At this point, it’s hard to tell what this represents, because it’s unclearly labeled. We’ll fix that in later sections!

# In the instructions below, we’ll use plt.bar to create a chart for a fake cafe called MatplotSip. We will be comparing the sales of different beverages on a given day.

#----------------------------------------------------------------

# DIFFERENT PLOT TYPES
# Simple Bar Chart II
# When we create a bar chart, we want each bar to be meaningful and correspond to a category of data. In the drinks chart from the last exercise, we could see that sales were different for different drink items, but this wasn’t very helpful to us, since we didn’t know which bar corresponded to which drink.

# In the previous lesson, we learned how to customize the tick marks on the x-axis in three steps:

# Create an axes object
ax = plt.subplot()
# Set the x-tick positions using a list of numbers
ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])
# Set the x-tick labels using a list of strings
ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'])
# If your labels are particularly long, you can use the rotation keyword to rotate your labels by a specified number of degrees:
ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'],rotation=30)
# Note: We have to set the x-ticks before we set the x-labels because the default ticks won’t necessarily be one tick per bar, especially if we’re plotting a lot of bars. If we skip setting the x-ticks before the x-labels, we might end up with labels in the wrong place.

# Remember from Lesson I that we can label the x-axis (plt.xlabel) and y-axis (plt.ylabel) as well. Now, our graph is much easier to understand:labeled_planet_chart

# Let’s add the appropriate labels for the chart you made in the last exercise for the coffee shop, MatplotSip.

#------------------------------------------

import codecademylib
from matplotlib import pyplot as plt

drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales =  [91, 76, 56, 66, 52, 27]

plt.bar(range(len(drinks)), sales)

#create your ax object here

ax = plt.subplot()
ax.set_xticks(range(len(drinks)))
ax.set_xticklabels(drinks, rotation = 30)

plt.show()

#------------------------------------------

# DIFFERENT PLOT TYPES
# Side-By-Side Bars
# We can use a bar chart to compare two sets of data with the same types of axis values. To do this, we plot two sets of bars next to each other, so that the values of each category can be compared. For example, here is a chart with side-by-side bars for the populations of the United States and China over the age of 65 (in percentages):population_bars

# (data taken from World Bank)

# Some examples of data that side-by-side bars could be useful for include:

# the populations of two countries over time
# prices for different foods at two different restaurants
# enrollments in different classes for males and females
# In the graph above, there are 7 sets of bars, with 2 bars in each set. Each bar has a width of 0.8 (the default width for all bars in Matplotlib).

# If our first blue bar is at x=0, then we want the next blue bar to be at x=2, and the next to be at x=4, etc.
# Our first orange bar should be at x=0.8 (so that it is touching the blue bar), and the next orange bar should be at x=2.8, etc.
# This is a lot of math, but we can make Python do it for us by copying and pasting this code:

# China Data (blue bars)
n = 1  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values1 = [t*element + w*n for element
             in range(d)]
# That just generated the first set of x-values. To generate the second set, paste the code again, but change n to 2, because this is the second dataset:

# US Data (orange bars)
n = 2  # This is our second dataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values2 = [t*element + w*n for element
             in range(d)]
# Let’s examine our special code:

[t*element + w*n for element in range(d)]
# This is called a list comprehension. It’s a special way of generating a list from a formula. You can learn more about it in this article. For making side-by-side bar graphs, you’ll never need to change this line; just paste it into your code and make sure to define n, t, d, and w correctly.

# In the instructions below, we’ll experiment with side-by-side bars to compare different locations of the MatplotSip coffee empire.

#------------------------------------------------

import codecademylib
from matplotlib import pyplot as plt

drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales1 =  [91, 76, 56, 66, 52, 27]
sales2 = [65, 82, 36, 68, 38, 40]

#Paste the x_values code here

n = 1  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = len(sales1) # Number of sets of bars
w = 0.8 # Width of each bar
store1_x = [t*element + w*n for element
             in range(d)]
plt.bar(store1_x, sales1)

n = 2  # This is our second dataset (out of 2)
t = 2 # Number of datasets
d = len(sales2) # Number of sets of bars
w = 0.8 # Width of each bar
store2_x = [t*element + w*n for element
             in range(d)]
plt.bar(store2_x, sales2)


plt.show()

#--------------------------------------------------------------

# DIFFERENT PLOT TYPES
# Stacked Bars
# If we want to compare two sets of data while preserving knowledge of the total between them, we can also stack the bars instead of putting them side by side. For instance, if someone was plotting the hours they’ve spent on entertaining themselves with video games and books in the past week, and wanted to also get a feel for total hours spent on entertainment, they could create a stacked bar chart:

# entertainment

# We do this by using the keyword bottom. The top set of bars will have bottom set to the heights of the other set of bars. So the first set of bars is plotted normally:

video_game_hours = [1, 2, 2, 1, 2]

plt.bar(range(len(video_game_hours)),
  video_game_hours) 
# and the second set of bars has bottom specified:

book_hours = [2, 3, 4, 2, 1]

plt.bar(range(len(book_hours)),
  book_hours,
  bottom=video_game_hours)
# This starts the book_hours bars at the heights of the video_game_hours bars. So, for example, on Monday the orange bar representing hours spent reading will start at a value of 1 instead of 0, because 1 hour was spent playing video games.

# Let’s try this out with the MatplotSip data from the last exercise.

#-------------------------------------------------

import codecademylib
from matplotlib import pyplot as plt

drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales1 =  [91, 76, 56, 66, 52, 27]
sales2 = [65, 82, 36, 68, 38, 40]

plt.bar(range(len(sales1)), sales1)
plt.bar(range(len(sales2)), sales2, bottom = sales1)

plt.legend(['Location 1','Location 2'])

plt.show()

#-----------------------------------
values = [10, 13, 11, 15, 20]
yerr = 2
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show()

values = [10, 13, 11, 15, 20]
yerr = [1, 3, 0.5, 2, 4]
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show()
#--------------------------------------

x_values = range(10)
y_values = [10, 12, 13, 13, 15, 19, 20, 22, 23, 29]
y_lower = [8, 10, 11, 11, 13, 17, 18, 20, 21, 27]
y_upper = [12, 14, 15, 15, 17, 21, 22, 24, 25, 31]

plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) #this is the shaded error
plt.plot(x_values, y_values) #this is the line itself
plt.show()


# Having to calculate y_lower and y_upper by hand is time-consuming. If we try to just subtract 2 from y_values, we will get an error.

# TypeError: unsupported operand type(s) for -: 'list' and 'int'
# In order to correctly add or subtract from a list, we need to use list comprehension:

y_lower = [i - 2 for i in y_values]


#--------------------------------------------------

import codecademylib
from matplotlib import pyplot as plt

months = range(12)
month_names = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
revenue = [16000, 14000, 17500, 19500, 21500, 21500, 22000, 23000, 20000, 19500, 18000, 16500]

y_lower = [i*0.9 for i in revenue]
y_upper = [i*1.1 for i in revenue]


#your work here
plt.plot(months, revenue)
plt.fill_between(months, y_lower, y_upper, alpha = 0.2)

ax = plt.subplot()
ax.set_xticks(months)
ax.set_xticklabels(month_names)

plt.show()

#----------------------------------------------------------------------------
# Pie chart
budget_data = [500, 1000, 750, 300, 100]

plt.pie(budget_data)
plt.show()




import codecademylib
from matplotlib import pyplot as plt
import numpy as np

payment_method_names = ["Card Swipe", "Cash", "Apple Pay", "Other"]
payment_method_freqs = [270, 77, 32, 11]

#make your pie chart here
plt.pie(payment_method_freqs)
plt.axis('equal')

plt.show()

#-----------------------------------------------
# Pie Chart Labeling Method 1

budget_data = [500, 1000, 750, 300, 100]
budget_categories = ['marketing', 'payroll', 'engineering', 'design', 'misc']

plt.pie(budget_data)
plt.legend(budget_categories)

# Pie Chart Labeling Method 2

#option 2
plt.pie(budget_data, labels=budget_categories)

# Pie Chart percentages

'%0.2f' — 2 decimal places, like 4.08
'%0.2f%%' — 2 decimal places, but with a percent sign at the end, like 4.08%. You need two consecutive percent signs because the first one acts as an escape character, so that the second one gets displayed on the chart.
'%d%%' — rounded to the nearest int and with a percent sign at the end, like 4%.
So, a full call to plt.pie might look like:

plt.pie(budget_data,
        labels=budget_categories,
        autopct='%0.1f%%')

		
#--------------------------

DIFFERENT PLOT TYPES
Histogram
Sometimes we want to get a feel for a large dataset with many samples beyond knowing just the basic metrics of mean, median, or standard deviation. To get more of an intuitive sense for a dataset, we can use a histogram to display all the values.

A histogram tells us how many values in a dataset fall between different sets of numbers (i.e., how many numbers fall between 0 and 10? Between 10 and 20? Between 20 and 30?). Each of these questions represents a bin, for instance, our first bin might be between 0 and 10.

All bins in a histogram are always the same size. The width of each bin is the distance between the minimum and maximum values of each bin. In our example, the width of each bin would be 10.

Each bin is represented by a different rectangle whose height is the number of elements from the dataset that fall within that bin.

Here is an example:

histogram

To make a histogram in Matplotlib, we use the command plt.hist. plt.hist finds the minimum and the maximum values in your dataset and creates 10 equally-spaced bins between those values.

The histogram above, for example, was created with the following code:

plt.hist(dataset) 
plt.show()
If we want more than 10 bins, we can use the keyword bins to set how many bins we want to divide the data into. The keyword range selects the minimum and maximum values to plot. For example, if we wanted to take our data from the last example and make a new histogram that just displayed the values from 66 to 69, divided into 40 bins (instead of 10), we could use this function call:

plt.hist(dataset, range=(66,69), bins=40)
which would result in a histogram that looks like this:

histogram_range

Histograms are best for showing the shape of a dataset. For example, you might see that values are close together, or skewed to one side. With this added intuition, we often discover other types of analysis we want to perform.

#-------------------------------------------------

Multiple Histogram
######
use the keyword alpha, which can be a value between 0 and 1. This sets the transparency of the histogram. A value of 0 would make the bars entirely transparent. A value of 1 would make the bars completely opaque.

plt.hist(a, range=(55, 75), bins=20, alpha=0.5)
plt.hist(b, range=(55, 75), bins=20, alpha=0.5)
#######
use the keyword histtype with the argument 'step' to draw just the outline of a histogram:

plt.hist(a, range=(55, 75), bins=20, histtype='step')
plt.hist(b, range=(55, 75), bins=20, histtype='step')
#######
Another problem we face is that our histograms might have different numbers of samples, making one much bigger than the other. We can see how this makes it difficult to compare qualitatively, by adding a dataset b with a much bigger size value:

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20)
plt.hist(b, range=(55, 75), bins=20)
plt.show()

To solve this, we can normalize our histograms using normed=True. This command divides the height of each column by a constant such that the total shaded area of the histogram sums to 1.

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.hist(b, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.show()

#--------------------------------
import codecademylib
from matplotlib import pyplot as plt

past_years_averages = [82, 84, 83, 86, 74, 84, 90]
years = [2000, 2001, 2002, 2003, 2004, 2005, 2006]
error = [1.5, 2.1, 1.2, 3.2, 2.3, 1.7, 2.4]

# Make your chart here
plt.figure(figsize = (10,8))
plt.bar(range(len(past_years_averages)), past_years_averages, yerr = error, capsize= 10)

plt.axis([-0.5, 6.5, 70, 95])
plt.xlabel('Year')
plt.ylabel('Test average')
plt.title('Final Exam Averages')

ax = plt.subplot()
ax.set_xticks(range(len(years)))
ax.set_xticklabels(years)


plt.show()
plt.savefig('my_bar_chart.png')

#----------------------------------------------------
import codecademylib
from matplotlib import pyplot as plt

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
middle_school_a = [80, 85, 84, 83, 86]
middle_school_b = [73, 78, 77, 82, 86]

def create_x(t, w, n, d):
    return [t*x + w*n for x in range(d)]
#school_a_x = [0.8, 2.8, 4.8, 6.8, 8.8]
#school_b_x = [1.6, 3.6, 5.6, 7.6, 9.6]
# Make your chart here

school_a_x = create_x(2, 0.8, 1, len(middle_school_a))
school_b_x = create_x(2, 0.8, 2, len(middle_school_b))
middle_x = [(a+b)/2 for a,b in zip(school_a_x, school_b_x)] 
#middle_x for label position

plt.figure(figsize = (10, 8))

plt.xlabel('Unit')
plt.ylabel('Test Average')
plt.title('Test Averages on Different Units')
plt.axis([0, 10.6, 70, 90])

ax = plt.subplot()
ax.set_xticks(middle_x)
ax.set_xticklabels(unit_topics)

plt.bar(school_a_x, middle_school_a)
plt.bar(school_b_x, middle_school_b)
plt.legend(['Middle School A', 'Middle School B'])


plt.show()
plt.savefig('my_side_by_side.png')

------------------------------------------

import codecademylib
from matplotlib import pyplot as plt
import numpy as np

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
As = [6, 3, 4, 3, 5]
Bs = [8, 12, 8, 9, 10]
Cs = [13, 12, 15, 13, 14]
Ds = [2, 3, 3, 2, 1]
Fs = [1, 0, 0, 3, 0]

x = range(5)

c_bottom = np.add(As, Bs)
#create d_bottom and f_bottom here
d_bottom = np.add(c_bottom, Cs)
f_bottom = np.add(d_bottom, Ds)

#create your plot here
plt.figure(figsize = (10, 8))

plt.bar(range(len(unit_topics)), As)
plt.bar(range(len(unit_topics)), Bs, bottom = As)
plt.bar(range(len(unit_topics)), Cs, bottom = c_bottom)
plt.bar(range(len(unit_topics)), Ds, bottom = d_bottom)
plt.bar(range(len(unit_topics)), Fs, bottom = f_bottom)

ax = plt.subplot()
ax.set_xticks(range(len(unit_topics)))
ax.set_xticklabels(unit_topics)
plt.xlabel('Unit')
plt.ylabel('Number of Students')
plt.title('Grade distribution')

plt.savefig('my_stacked_bar.png')    

plt.show()

#-----------------------------------------
import codecademylib
from matplotlib import pyplot as plt

exam_scores1 = [62.58, 67.63, 81.37, 52.53, 62.98, 72.15, 59.05, 73.85, 97.24, 76.81, 89.34, 74.44, 68.52, 85.13, 90.75, 70.29, 75.62, 85.38, 77.82, 98.31, 79.08, 61.72, 71.33, 80.77, 80.31, 78.16, 61.15, 64.99, 72.67, 78.94]
exam_scores2 = [72.38, 71.28, 79.24, 83.86, 84.42, 79.38, 75.51, 76.63, 81.48,78.81,79.23,74.38,79.27,81.07,75.42,90.35,82.93,86.74,81.33,95.1,86.57,83.66,85.58,81.87,92.14,72.15,91.64,74.21,89.04,76.54,81.9,96.5,80.05,74.77,72.26,73.23,92.6,66.22,70.09,77.2]

# Make your plot here
plt.figure(figsize = (10,8))

plt.hist(exam_scores1, bins = 12, normed = True, histtype = 'step', linewidth = 2)
plt.hist(exam_scores2, bins = 12, normed = True, histtype = 'step', linewidth = 2)

plt.legend(['1st Yr Teaching', '2nd Yr Teaching'])
plt.xlabel('Percentage')
plt.ylabel('Frequency')
plt.title('Final Exam Score Distribution')

plt.show()

plt.savefig('my_histogram.png')
#----------------------------------

import codecademylib
from matplotlib import pyplot as plt

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
num_hardest_reported = [1, 3, 10, 15, 1]

#Make your plot here
plt.figure(figsize = (10, 8))

plt.pie(num_hardest_reported, labels = unit_topics, autopct = '%d%%')
plt.axis('equal')
plt.title('Hardest Topics')

plt.show()
plt.savefig('my_pie_chart.png')

#-----------------------------------------
import codecademylib
from matplotlib import pyplot as plt

hours_reported =[3, 2.5, 2.75, 2.5, 2.75, 3.0, 3.5, 3.25, 3.25,  3.5, 3.5, 3.75, 3.75,4, 4.0, 3.75,  4.0, 4.25, 4.25, 4.5, 4.5, 5.0, 5.25, 5, 5.25, 5.5, 5.5, 5.75, 5.25, 4.75]
exam_scores = [52.53, 59.05, 61.15, 61.72, 62.58, 62.98, 64.99, 67.63, 68.52, 70.29, 71.33, 72.15, 72.67, 73.85, 74.44, 75.62, 76.81, 77.82, 78.16, 78.94, 79.08, 80.31, 80.77, 81.37, 85.13, 85.38, 89.34, 90.75, 97.24, 98.31]

plt.figure(figsize=(10,8))

# Create your hours_lower_bound and hours_upper_bound lists here 

hours_lower_bound = [x*0.8 for x in hours_reported]
hours_upper_bound = [x*1.2 for x in hours_reported]


# Make your graph here
plt.plot(exam_scores, hours_reported, linewidth = 2)
plt.fill_between(exam_scores, hours_lower_bound, hours_upper_bound, alpha = 0.2 )

plt.xlabel('Score')
plt.ylabel('Hours studying (self-reported)')
plt.title('Time spent studying vs final exam scores')

plt.show()
plt.savefig('my_line_graph.png')

'''
How to Select a Meaningful Visualization
This article will guide you through the process of selecting a graph for a visualization.

Brainstorming your visualization
The three steps in the data visualization process are preparing, visualizing, and styling data. When faced with a blank canvas, the second step of the process, visualizing the data, can be overwhelming. To help, we’ve created a diagram to guide the selection of a chart based on what you want to explore in your data.

When planning out a visualization, you’ll usually have an idea of what questions you’ll want to explore. However, you may initially wonder exactly which chart to use. This moment is one of the most exciting parts of the process!

During your brainstorming phase, you should consider two things:

The focusing question you want to answer with your chart
The type of data that you want to visualize
Depending on the focusing questions you’re trying to answer, the type of chart you select should be different and intentional in its difference. In the diagram below, we have assigned Matplotlib visualizations to different categories. These categories explore common focusing questions and types of data you may want to display in a visualization.

A Diagram of Diagrams!
SVG

Chart categories
Composition charts
Focusing Question: What are the parts of some whole? What is the data made of?

Datasets that work well: Data pertaining to probabilities, proportions, and percentages can be visualized as with the graphs in this composition category. Charts in this category illustrate the different data components and their percentages as part of a whole.

Distribution Charts
Datasets that work well: Data in large quantities and/or with an array of attributes works well for these types of charts. Visualizations in this category will allow you to see patterns, re-occurrences, and a clustering of data points.

Note: In statistics, a commonly seen distribution is a bell curve, also known as a normal distribution. A bell curve is a bell-shaped distribution where most of the values in the dataset crowd around the average (also known as the mean), therefore causing the curve to form. If you want to see how values in the data are “distributed” across variables, the best way to do that would be with the visualizations in this category.

Relationship Charts
Focusing Question: How do variables relate to each other?

Datasets that work well: Data with two or more variables can be displayed in these charts. These charts typically illustrate a correlation between two or more variables. You can communicate this relationship by mapping multiple variables in the same chart. Correlation measures the strength of a relationship between variables.

Comparison Charts
Focusing Question: How do variables compare to each other?

Datasets that work well: Data must have multiple variables, and the visualizations in this category allow readers to compare those items against the others. For example, a line graph that has multiple lines, each belonging to a different variable. Multi-colored bar charts are also a great way to compare items in data.

Summary
When brainstorming a visualization, use the diagram above to guide the selection of your chart. Remember to be intentional in your selection by thinking about what type of data you’re dealing with and what focusing question you wish to answer.'''

#sample from https://matplotlib.org/gallery/mplot3d/subplot3d.html?highlight=add_subplot

import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

from mpl_toolkits.mplot3d.axes3d import get_test_data


# set up a figure twice as wide as it is tall
fig = plt.figure(figsize=plt.figaspect(0.5))

#===============
#  First subplot
#===============
# set up the axes for the first plot
ax = fig.add_subplot(1, 2, 1, projection='3d')

# plot a 3D surface like in the example mplot3d/surface3d_demo
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)
surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)
ax.set_zlim(-1.01, 1.01)
fig.colorbar(surf, shrink=0.5, aspect=10)

#===============
# Second subplot
#===============
# set up the axes for the second plot
ax = fig.add_subplot(1, 2, 2, projection='3d')

# plot a 3D wireframe like in the example mplot3d/wire3d_demo
X, Y, Z = get_test_data(0.05)
ax.plot_wireframe(X, Y, Z, rstride=10, cstride=10)

plt.show()

'''
Project: Visualizing the Orion Constellation

In this project you are Dr. Jillian Bellovary, a real-life astronomer for the Hayden Planetarium at the American Museum of Natural History. As an astronomer, part of your job is to study the stars. You've recently become interested in the constellation Orion, a collection of stars that appear in our night sky and form the shape of Orion, a warrior God from ancient Greek mythology. 

As a researcher on the Hayden Planetarium team, you are in charge of visualizing the Orion constellation in 3D using the Matplotlib function .scatter(). To learn more about the .scatter() you can see the Matplotlib documentation here. 

You will create a rotate-able visualization of the position of the Orion's stars and get a better sense of their actual positions. To achieve this, you will be mapping real data from outer space that maps the position of the stars in the sky

The goal of the project is to understand spatial perspective. Once you visualize Orion in both 2D and 3D, you will be able to see the difference in the constellation shape humans see from earth versus the actual position of the stars that make up this constellation. 

'''

'''
1. Set-Up

The following set-up is new and specific to the project. It is very similar to the way you have imported Matplotlib in previous lessons.

•Add %matplotlib notebook in the cell below. This is a new statement that you may not have seen before. It will allow you to be able to rotate your visualization in this jupyter notebook.


•We will be using a subset of Matplotlib: matplotlib.pyplot. Import the subset as you have been importing it in previous lessons: from matplotlib import pyplot as plt

•In order to see our 3D visualization, we also need to add this new line after we import Matplotlib: from mpl_toolkits.mplot3d import Axes3D
'''

#allow you to be able to rotate your visualization in this jupyter notebook
%matplotlib notebook
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


'''
2. Get familiar with real data¶

Astronomers describe a star's position in the sky by using a pair of angles: declination and right ascension. Declination is similar to longitude, but it is projected on the celestian fear. Right ascension is known as the "hour angle" because it accounts for time of day and earth's rotaiton. Both angles are relative to the celestial equator. You can learn more about star position here.

The x, y, and z lists below are composed of the x, y, z coordinates for each star in the collection of stars that make up the Orion constellation as documented in a paper by Nottingham Trent Univesity on "The Orion constellation as an installation" found here.

Spend some time looking at x, y, and z, does each fall within a range?
'''

# Orion
x = [-0.41, 0.57, 0.07, 0.00, -0.29, -0.32,-0.50,-0.23, -0.23]
y = [4.12, 7.71, 2.36, 9.10, 13.35, 8.13, 7.19, 13.25,13.43]
z = [2.06, 0.84, 1.56, 2.07, 2.36, 1.72, 0.66, 1.25,1.38]

'''
3. Create a 2D Visualization

Before we visualize the stars in 3D, let's get a sense of what they look like in 2D. 

Create a figure for the 2d plot and save it to a variable name fig. (hint: plt.figure())

Add your subplot .add_subplot() as the single subplot, with 1,1,1.(hint: add_subplot(1,1,1))

Use the scatter function to visualize your x and y coordinates. (hint: .scatter(x,y))

Render your visualization. (hint: plt.show())

Does the 2D visualization look like the Orion constellation we see in the night sky? Do you recognize its shape in 2D? There is a curve to the sky, and this is a flat visualization, but we will visualize it in 3D in the next step to get a better sense of the actual star positions. 
'''
fig = plt.figure()
subplot = fig.add_subplot(1,1,1)
plt.scatter(x,y)
plt.show()


'''
4. Create a 3D Visualization

Create a figure for the 3D plot and save it to a variable name fig_3d. (hint: plt.figure())

Since this will be a 3D projection, we want to make to tell Matplotlib this will be a 3D plot. 

To add a 3D projection, you must include a the projection argument. It would look like this:
projection="3d"

Add your subplot with .add_subplot() as the single subplot 1,1,1 and specify your projection as 3d:

fig_3d.add_subplot(1,1,1,projection="3d"))

Since this visualization will be in 3D, we will need our third dimension. In this case, our z coordinate. 

Create a new variable constellation3d and call the scatter function with your x, y and z coordinates. 

Include z just as you have been including the other two axes. (hint: .scatter(x,y,z))

Render your visualization. (hint plt.show().)
'''

fig_3d = plt.figure()
fig_3d.add_subplot(1,1,1, projection="3d")
constellation3d = plt.scatter(x,y,z)
plt.show()

'''

5. Rotate and explore

Use your mouse to click and drag the 3D visualization in the previous step. This will rotate the scatter plot. As you rotate, can you see Orion from different angles? 

Note: The on and off button that appears above the 3D scatter plot allows you to toggle rotation of your 3D visualization in your notebook.

Take your time, rotate around! Remember, this will never look exactly like the Orion we see from Earth. The visualization does not curve as the night sky does. There is beauty in the new understanding of Earthly perspective! We see the shape of the warrior Orion because of Earth's location in the universe and the location of the stars in that constellation.

Feel free to map more stars by looking up other celestial x, y, z coordinates here.'''


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- SAMPLES #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-

import codecademylib3_seaborn
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd

games = ["LoL", "Dota 2", "CS:GO", "DayZ", "HOS", "Isaac", "Shows", "Hearth", "WoT", "Agar.io"]

viewers =  [1070, 472, 302, 239, 210, 171, 170, 90, 86, 71]

plt.bar(range(len(games)), viewers, color='slateblue')

plt.legend(["Twitch"])

plt.xlabel('Games')
plt.ylabel('Viewers')

ax = plt.subplot()

ax.set_xticks(range(0, 10))

ax.set_xticklabels(games, rotation=30)

plt.show()

# We can add plt.clf() to clear the current figure, our bar graph, before creating our next figure, the pie chart

plt.clf()



labels = ["US", "DE", "CA", "N/A", "GB", "TR", "BR", "DK", "PL", "BE", "NL", "Others"]

countries = [447, 66, 64, 49, 45, 28, 25, 20, 19, 17, 17, 279]

colors = ['lightskyblue', 'gold', 'lightcoral', 'gainsboro', 'royalblue', 'lightpink', 'darkseagreen', 'sienna', 'khaki', 'gold', 'violet', 'yellowgreen']

# Make your pie chart here

explode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

plt.pie(countries, explode=explode, colors=colors, shadow=True, startangle=345, autopct='%1.0f%%', pctdistance=1.15, labels = lavels)

plt.title("League of Legends Viewers' Whereabouts")

plt.legend(labels, loc="right")

plt.show()

# We can add plt.clf() to clear the current figure, our pie chart,before creating our next figure, the line graph

plt.clf()

hour = range(24)

viewers_hour = [30, 17, 34, 29, 19, 14, 3, 2, 4, 9, 5, 48, 62, 58, 40, 51, 69, 55, 76, 81, 102, 120, 71, 63]

plt.title("Time Series")

plt.xlabel("Hour")
plt.ylabel("Viewers")

plt.plot(hour, viewers_hour)

plt.legend(['2015-01-01'])

ax = plt.subplot()

ax.set_xticks(hour)
ax.set_yticks([0, 20, 40, 60, 80, 100, 120])

y_upper = [i + (i*0.15) for i in viewers_hour]
y_lower = [i - (i*0.15) for i in viewers_hour]

plt.fill_between(hour, y_lower, y_upper, alpha=0.2)

plt.show()



#-#-#-#-#-#-#-#-#-#-#-#-  matplotlib Tutorials #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-

https://matplotlib.org/tutorials/index.html



#-#-#-#-#-#-#-#-#-#-#-#-#-#-SEABORN #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

'''LEARN SEABORN INTRODUCTION
Plotting Bars with Seaborn
Take a look at the file called results.csv. You’ll plot that data soon, but before you plot it, take a minute to understand the context behind that data, which is based on a hypothetical situation we have created:

Suppose we are analyzing data from a survey: we asked 1,000 patients at a hospital how satisfied they were with their experience. Their response was measured on a scale of 1 - 10, with 1 being extremely unsatisfied, and 10 being extremely satisfied. We have summarized that data in a CSV file called results.csv.

To plot this data using Matplotlib, you would write the following:
'''

df = pd.read_csv("results.csv")
ax = plt.subplot()
plt.bar(range(len(df)),
        df["Mean Satisfaction"])
ax.set_xticks(range(len(df)))
ax.set_xticklabels(df.Gender)
plt.xlabel("Gender")
plt.ylabel("Mean Satisfaction")
'''

That's a lot of work for a simple bar chart! Seaborn gives us a much simpler option. With Seaborn, you can use the `sns.barplot()` command to do the same thing.
The Seaborn function sns.barplot(), takes at least three keyword arguments:

data: a Pandas DataFrame that contains the data (in this example, data=df)
x: a string that tells Seaborn which column in the DataFrame contains otheur x-labels (in this case, x="Gender")
y: a string that tells Seaborn which column in the DataFrame contains the heights we want to plot for each bar (in this case y="Mean Satisfaction")
By default, Seaborn will aggregate and plot the mean of each category. In the next exercise you will learn more about aggregation and how Seaborn handles it.'''



import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

# Load results.csv here:
df = pd.read_csv('results.csv')

print(df)

sns.barplot(
  data= df,
  x= 'Gender',
	y= 'Mean Satisfaction'
)

plt.show()





'''
LEARN SEABORN INTRODUCTION
Understanding Aggregates
Seaborn can also calculate aggregate statistics for large datasets. To understand why this is helpful, we must first understand what an aggregate is.

An aggregate statistic, or aggregate, is a single number used to describe a set of data. One example of an aggregate is the average, or mean of a data set. There are many other aggregate statistics as well.

Suppose we have a grade book with columns student, assignment_name, and grade, as shown below.

student	assignment_name	grade
Amy	Assignment 1	75
Amy	Assignment 2	82
Bob	Assignment 1	99
Bob	Assignment 2	90
Chris	Assignment 1	72
Chris	Assignment 2	66
…	…	…
To calculate a student’s current grade in the class, we need to aggregate the grade data by student. To do this, we’ll calculate the average of each student’s grades, resulting in the following data set:

student	grade
Amy	78.5
Bob	94.5
Chris	69
…	…
On the other hand, we may be interested in understanding the relative difficulty of each assignment. In this case, we would aggregate by assignment, taking the average of all student’s scores on each assignment:

assignment_name	grade
Assignment 1	82
Assignment 2	79.3
…	…
In both of these cases, the function we used to aggregate our data was the average or mean, but there are many types of aggregate statistics including:

Median
Mode
Standard Deviation
In Python, you can compute aggregates fairly quickly and easily using Numpy, a popular Python library for computing. You’ll use Numpy in this exercise to compute aggregates for a DataFrame.'''

#plot mean grade against assignment
import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

gradebook = pd.read_csv("gradebook.csv")
#print(gradebook)

assignment1 = gradebook[gradebook['assignment_name'] == 'Assignment 1']
print(assignment1)

asn1_median = np.median(assignment1['grade'])
print(asn1_median)

#plot mean grade against assignment, done with seaborn

import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

gradebook = pd.read_csv("gradebook.csv")

sns.barplot(
  data = gradebook,
  x = 'assignment_name',
  y = 'grade'
)

plt.show()

'''
LEARN SEABORN INTRODUCTION
Modifying Error Bars
By default, Seaborn will place error bars on each bar when you use the barplot() function.

Error bars are the small lines that extend above and below the top of each bar. Errors bars visually indicate the range of values that might be expected for that bar.


For example, in our assignment average example, an error bar might indicate what grade we expect an average student to receive on this assignment.


There are several different calculations that are commonly used to determine error bars.

By default, Seaborn uses something called a bootstrapped confidence interval. Roughly speaking, this interval means that “based on this data, 95% of similar situations would have an outcome within this range”.

In our gradebook example, the confidence interval for the assignments means “if we gave this assignment to many, many students, we’re confident that the mean score on the assignment would be within the range represented by the error bar”.

The confidence interval is a nice error bar measurement because it is defined for different types of aggregate functions, such as medians and mode, in addition to means.

If you’re calculating a mean and would prefer to use standard deviation for your error bars, you can pass in the keyword argument ci="sd" to sns.barplot() which will represent one standard deviation. It would look like this:

sns.barplot(data=gradebook, x="name", y="grade", ci="sd")'''


'''
LEARN SEABORN INTRODUCTION
Calculating Different Aggregates
In most cases, we’ll want to plot the mean of our data, but sometimes, we’ll want something different:

If our data has many outliers, we may want to plot the median.
If our data is categorical, we might want to count how many times each category appears (such as in the case of survey responses).
Seaborn is flexible and can calculate any aggregate you want. To do so, you’ll need to use the keyword argument estimator, which accepts any function that works on a list.

For example, to calculate the median, you can pass in np.median to the estimator keyword:
'''
sns.barplot(data=df,
  x="x-values",
  y="y-values",
  estimator=np.median)
  
 '''
Consider the data in results.csv. To calculate the number of times a particular value appears in the Response column , we pass in len:'''

sns.barplot(data=df,
  x="Patient ID",
  y="Response",
  estimator=len)

#--------------------------- script.py---------------

import codecademylib3_seaborn
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

df = pd.read_csv("survey.csv")

print(df)

#sns.barplot(data = df,
#  x = 'Gender',
#  y = 'Response',
#  estimator = len
#  )

sns.barplot(data = df,
  x = 'Gender',
  y = 'Response',
  estimator = np.median
  )

plt.show()

'''
LEARN SEABORN INTRODUCTION
Aggregating by Multiple Columns
Sometimes we’ll want to aggregate our data by multiple columns to visualize nested categorical variables.

For example, consider our hospital survey data. The mean satisfaction seems to depend on Gender, but it might also depend on another column: Age Range.

We can compare both the Gender and Age Range factors at once by using the keyword hue.
'''
sns.barplot(data=df,
            x="Gender",
            y="Response",
            hue="Age Range")
            
            '''
The hue parameter adds a nested categorical variable to the plot.

*Visualizing survey results by gender with age range nested*.
Notice that we keep the same x-labels, but we now have different color bars representing each Age Range. We can compare two bars of the same color to see how patients with the same Age Range, but different Gender rated the survey.'''

'''
LEARN SEABORN INTRODUCTION
Review
In this lesson you learned how to extend Matplotlib with Seaborn to create meaningful visualizations from data in DataFrames.

You’ve also learned how Seaborn creates aggregated charts and how to change the way aggregates and error bars are calculated.

Finally, you learned how to aggregate by multiple columns, and how the hue parameter adds a nested categorical variable to a visualization.

To review the seaborn workflow:
1. Ingest data from a CSV file to Pandas DataFrame.'''
df = pd.read_csv('file_name.csv')'''
2. Set sns.barplot() with desired values for x, y, and set data equal to your DataFrame.'''
sns.barplot(data=df, x='X-Values', y='Y-Values')'''
3. Set desired values for estimator and hue parameters.'''
sns.barplot(data=df, x='X-Values', y='Y-Values', estimator=len, hue='Value')'''
4. Render the plot using plt.show().'''
plt.show()

'''

LEARN SEABORN: DISTRIBUTIONS
Introduction
In this lesson, we will explore how to use Seaborn to graph multiple statistical distributions, including box plots and violin plots.

Seaborn is optimized to work with large datasets — from its ability to natively interact with Pandas DataFrames, to automatically calculating and plotting aggregates. One of the most powerful aspects of Seaborn is its ability to visualize and compare distributions. Distributions provide us with more information about our data — how spread out it is, its range, etc.

Calculating and graphing distributions is integral to analyzing massive amounts of data. We’ll look at how Seaborn allows us to move beyond the traditional distribution graphs to plots that enable us to communicate important statistical information.'''

'''
Plotting Distributions with Seaborn
Seaborn's strength is in visualizing statistical calculations. Seaborn includes several plots that allow you to graph univariate distribution, including KDE plots, box plots, and violin plots. Explore the Jupyter notebook below to get an understanding of how each plot works.'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

'''
First, we'll read in three datasets. In order to plot them in Seaborn, we'll combine them using NumPy's .concatenate() function into a Pandas DataFrame.'''

n = 500
dataset1 = np.genfromtxt("dataset1.csv", delimiter=",")
dataset2 = np.genfromtxt("dataset2.csv", delimiter=",")
dataset3 = np.genfromtxt("dataset3.csv", delimiter=",")


df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n,
    "value": np.concatenate([dataset1, dataset2, dataset3])
})

sns.set()

'''
First, let's plot each dataset as bar charts.'''

sns.barplot(data=df, x='label', y='value')
plt.show()

'''

We can use barplots to find out information about the mean - but it doesn't give us a sense of how spread out the data is in each set. To find out more about the distribution, we can use a KDE plot.'''

sns.kdeplot(dataset1, shade=True, label="dataset1")
sns.kdeplot(dataset2, shade=True, label="dataset2")
sns.kdeplot(dataset3, shade=True, label="dataset3")

plt.legend()
plt.show()

'''
A KDE plot will give us more information, but it's pretty difficult to read this plot.'''

sns.boxplot(data=df, x='label', y='value')
plt.show()

'''
A box plot, on the other hand, makes it easier for us to compare distributions. It also gives us other information, like the interquartile range and any outliers. However, we lose the ability to determine the shape of the data.'''

sns.violinplot(data=df, x="label", y="value")
plt.show()

'''
A violin plot brings together shape of the KDE plot with additional information that a box plot provides. It's understandable why many people like this plot!'''



'''
LEARN SEABORN: DISTRIBUTIONS
Bar Charts Hide Information
Before we dive into these new charts, we need to understand why we’d want to use them. To best illustrate this idea, we need to revisit bar charts.

We previously learned that Seaborn can quickly aggregate data to plot bar charts using the mean.

Here is a bar chart that uses three different randomly generated sets of data:

sns.barplot(data=df, x="label", y="value")
plt.show()
alt

These three datasets look identical! As far as we can tell, they each have the same mean and similar confidence intervals.

We can get a lot of information from these bar charts, but we can’t get everything. For example, what are the minimum and maximum values of these datasets? How spread out is this data?

While we may not see this information in our bar chart, these differences might be significant and worth understanding better.'''

import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

# Take in the data from the CSVs as NumPy arrays:
set_one = np.genfromtxt("dataset1.csv", delimiter=",")
set_two = np.genfromtxt("dataset2.csv", delimiter=",")
set_three = np.genfromtxt("dataset3.csv", delimiter=",")
set_four = np.genfromtxt("dataset4.csv", delimiter=",")

# Creating a Pandas DataFrame:
n=500
df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n + ["set_four"] * n,
    "value": np.concatenate([set_one, set_two, set_three, set_four])
})

# Setting styles:
sns.set_style("darkgrid")
sns.set_palette("pastel")

# Add your code below:
sns.barplot(data = df, 
           x = 'label',
           y = 'value')

plt.show()

'''
LEARN SEABORN: DISTRIBUTIONS
KDE Plots, Part I
Bar plots can tell us what the mean of our dataset is, but they don’t give us any hints as to the distribution of the dataset values. For all we know, the data could be clustered around the mean or spread out evenly across the entire range.

To find out more about each of these datasets, we’ll need to examine their distributions. A common way of doing so is by plotting the data as a histogram, but histograms have their drawback as well.

Seaborn offers another option for graphing distributions: KDE Plots.

KDE stands for Kernel Density Estimator. A KDE plot gives us the sense of a univariate as a curve. A univariate dataset only has one variable and is also referred to as being one-dimensional, as opposed to bivariate or two-dimensional datasets which have two variables.

KDE plots are preferable to histograms because depending on how you group the data into bins and the width of the bins, you can draw wildly different conclusions about the shape of the data. Using a KDE plot can mitigate these issues, because they smooth the datasets, allow us to generalize over the shape of our data, and aren’t beholden to specific data points.'''

#good youtube video explained what is KDE  https://www.youtube.com/watch?v=x5zLaWT5KPs

'''
To plot a KDE in Seaborn, we use the method sns.kdeplot().

A KDE plot takes the following arguments:

data - the univariate dataset being visualized, like a Pandas DataFrame, Python list, or NumPy array
shade - a boolean that determines whether or not the space underneath the curve is shaded
Let’s examine the KDE plots of our three datasets:
'''
sns.kdeplot(dataset1, shade=True)
sns.kdeplot(dataset2, shade=True)
sns.kdeplot(dataset3, shade=True)
plt.legend()
plt.show()

'''
For this lesson, the KDE plots we work will be using univariate data. So, only one of the axes will represent actual values in the data.

The horizontal or x-axis of a KDE plot is the range of values in the data set. This is similar to the x axis for histograms.

The vertical or y-axis of a KDE plot represents the Kernel Density Estimate of the Probability Density Function of a random variable, which is interpreted as a probability differential. The probability of a value being between the points x1 and x2 is the total shaded area of the curve under the two points.
'''

'''
LEARN SEABORN: DISTRIBUTIONS
Box Plots, Part I
While a KDE plot can tell us about the shape of the data, it’s cumbersome to compare multiple KDE plots at once. They also can’t tell us other statistical information, like the values of outliers.

The box plot (also known as a box-and-whisker plot) can’t tell us about how our dataset is distributed, like a KDE plot. But it shows us the range of our dataset, gives us an idea about where a significant portion of our data lies, and whether or not any outliers are present.

Let’s examine how we interpret a box plot:

The box represents the interquartile range
The line in the middle of the box is the median
The end lines are the first and third quartiles
The diamonds show outliers'''

'''
To plot a box plot in Seaborn, we use the method sns.boxplot().

A box plot takes the following arguments:

data - the dataset we’re plotting, like a DataFrame, list, or an array
x - a one-dimensional set of values, like a Series, list, or array
y - a second set of one-dimensional data
If you use a Pandas Series for the x and y values, the Series will also generate the axis labels. For example, if you use the value Series as your y value data, Seaborn will automatically apply that name as the y-axis label.'''


'''
LEARN SEABORN: DISTRIBUTIONS
Violin Plots, Part I
As we saw in the previous exercises, while it’s possible to plot multiple histograms, it is not a great option for comparing distributions. Seaborn gives us another option for comparing distributions - a violin plot. Violin plots provide more information than box plots because instead of mapping each individual data point, we get an estimation of the dataset thanks to the KDE.

Violin plots are less familiar and trickier to read, so let’s break down the different parts:

There are two KDE plots that are symmetrical along the center line.
A white dot represents the median.
The thick black line in the center of each violin represents the interquartile range.
The lines that extend from the center are the confidence intervals - just as we saw on the bar plots, a violin plot also displays the 95% confidence interval.'''

'''LEARN SEABORN: DISTRIBUTIONS
Violin Plots, Part II
Violin Plots are a powerful graphing tool that allows you to compare multiple distributions at once.

Let’s look at how our original three data sets look like as violin plots:

sns.violinplot(data=df, x="label", y="value")
plt.show()
alt

As we can see, violin plots allow us to graph and compare multiple distributions. It also retains the shape of the distributions, so we can easily tell that Dataset 1 is skewed left and that Dataset 3 is bimodal.

To plot a violin plot in Seaborn, use the method sns.violinplot().

There are several options for passing in relevant data to the x and y parameters:

data - the dataset that we’re plotting, such as a list, DataFrame, or array
x, y, and hue - a one-dimensional set of data, such as a Series, list, or array
any of the parameters to the function sns.boxplot()'''




'''
Seaborn Styling, Part 1: Figure Style and Scale
Learn how to customize your figures and scale plots for different presentation settings.

Introduction
When creating a data visualization, your goal is to communicate the insights found in the data. While visualizing communicates important information, styling will influence how your audience understands what you’re trying to convey.

After you have formatted and visualized your data, the third and last step of data visualization is styling. Styling is the process of customizing the overall look of your visualization, or figure. Making intentional decisions about the details of the visualization will increase their impact and set your work apart.

In this article, we’ll look at how to do the following techniques in Seaborn:

customize the overall look of your figure, using background colors, grids, spines, and ticks
scale plots for different contexts, such as presentations and reports
Customizing the Overall Look of Your Figure
Seaborn enables you to change the presentation of your figures by changing the style of elements like the background color, grids, and spines. When deciding how to style your figures, you should take into consideration your audience and the context. Is your visualization part of a report and needs to convey specific information? Or is it part of a presentation? Or is your visualization meant as its own stand-alone, with no narrator in front of it, and no other visualizations to compare it to?

In this section, we’ll explore three main aspects of customizing figures in Seaborn - background color, grids, and spines - and how these elements can change the look and meaning of your visualizations.

Built-in Themes
Seaborn has five built-in themes to style its plots: darkgrid, whitegrid, dark, white, and ticks. Seaborn defaults to using the darkgrid theme for its plots, but you can change this styling to better suit your presentation needs.

To use any of the preset themes pass the name of it to sns.set_style().
'''
sns.set_style("darkgrid")
sns.stripplot(x="day", y="total_bill", data=tips)
'''
image1

We’ll explore the rest of the themes in the examples below.

Background Color
When thinking about the look of your visualization, one thing to consider is the background color of your plot. The higher the contrast between the color palette of your plot and your figure background, the more legible your data visualization will be. Fun fact: dark blue on white is actually more legible than black on white!

The dark background themes provide a nice change from the Matplotlib styling norms, but doesn’t have as much contrast:
'''
sns.set_style("dark")
sns.stripplot(x="day", y="total_bill", data=tips)'''
image2

The white and tick themes will allow the colors of your dataset to show more visibly and provides higher contrast so your plots are more legible:
'''
sns.set_style("ticks")
sns.stripplot(x="day", y="total_bill", data=tips)'''
image3

Grids
In addition to being able to define the background color of your figure, you can also choose whether or not to include a grid. Remember that the default theme includes a grid.

It’s a good choice to use a grid when you want your audience to be able to draw their own conclusions about data. A grid allows the audience to read your chart and get specific information about certain values. Research papers and reports are a good example of when you would want to include a grid.
'''
sns.set_style("whitegrid")
sns.stripplot(x="day", y="total_bill", data=tips)'''
image4

There are also instances where it would make more sense to not use a grid. If you’re delivering a presentation, simplifying your charts in order to draw attention to the important visual details may mean taking out the grid. If you’re interested in making more specific design choices, then leaving out the grids might be part of that aesthetic decision.
'''
sns.set_style("white")
sns.stripplot(x="day", y="total_bill", data=tips)'''
image5

In this case, a blank background would allow your plot to shine.

Despine
In addition to changing the color background, you can also define the usage of spines. Spines are the borders of the figure that contain the visualization. By default, an image has four spines.

You may want to remove some or all of the spines for various reasons. A figure with the left and bottom spines resembles traditional graphs. You can automatically take away the top and right spines using the sns.despine()function. Note: this function must be called after you have called your plot.
'''
sns.set_style("white")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.despine()'''
image6

Not including any spines at all may be an aesthetic decision. You can also specify how many spines you want to include by calling despine() and passing in the spines you want to get rid of, such as: left, bottom, top, right.
'''
sns.set_style("whitegrid")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.despine(left=True, bottom=True)'''
image7

Scaling Figure Styles for Different Mediums
Matplotlib allows you to generate powerful plots, but styling those plots for different presentation purposes is difficult. Seaborn makes it easy to produce the same plots in a variety of different visual formats so you can customize the presentation of your data for the appropriate context, whether it be a research paper or a conference poster.

You can set the visual format, or context, using sns.set_context()

Within the usage of sns.set_context(), there are three levels of complexity:

Pass in one parameter that adjusts the scale of the plot
Pass in two parameters - one for the scale and the other for the font size
Pass in three parameters - including the previous two, as well as the rc with the style parameter that you want to override
Scaling Plots
Seaborn has four presets which set the size of the plot and allow you to customize your figure depending on how it will be presented.

In order of relative size they are: paper, notebook, talk, and poster. The notebook style is the default.
'''
sns.set_style("ticks")

# Smallest context:
sns.set_context("paper")
sns.stripplot(x="day", y="total_bill", data=tips)


sns.set_style("ticks")

# Largest Context:
sns.set_context("poster")
sns.stripplot(x="day", y="total_bill", data=tips)'''


Scaling Fonts and Line Widths
You are also able to change the size of the text using the font_scale parameter for sns.set_context()

You may want to also change the line width so it matches. We do this with the rc parameter, which we’ll explain in detail below.
'''
# Set font scale and reduce grid line width to match
sns.set_context("poster", font_scale = .5, rc={"grid.linewidth": 0.6})
sns.stripplot(x="day", y="total_bill", data=tips)'''
image10

While you’re able to change these parameters, you should keep in mind that it’s not always useful to make certain changes. Notice in this example that we’ve changed the line width, but because of it’s relative size to the plot, it distracts from the actual plotted data.'''

# Set font scale and increase grid line width to match
sns.set_context("poster", font_scale = 1, rc={"grid.linewidth": 5})
sns.stripplot(x="day", y="total_bill", data=tips)'''
image11

The RC Parameter
As we mentioned above, if you want to override any of these standards, you can use sns.set_context and pass in the parameter rc to target and reset the value of an individual parameter in a dictionary. rc stands for the phrase ‘run command’ - essentially, configurations which will execute when you run your code.
'''
sns.set_style("ticks")
sns.set_context("poster")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.plotting_context()
Returns:

{'axes.labelsize': 17.6,
 'axes.titlesize': 19.200000000000003,
 'font.size': 19.200000000000003,
 'grid.linewidth': 1.6,
 'legend.fontsize': 16.0,
 'lines.linewidth': 2.8000000000000003,
 'lines.markeredgewidth': 0.0,
 'lines.markersize': 11.200000000000001,
 'patch.linewidth': 0.48,
 'xtick.labelsize': 16.0,
 'xtick.major.pad': 11.200000000000001,
 'xtick.major.width': 1.6,
 'xtick.minor.width': 0.8,
 'ytick.labelsize': 16.0,
 'ytick.major.pad': 11.200000000000001,
 'ytick.major.width': 1.6,
 'ytick.minor.width': 0.8}'''
 
Conclusion
As you can see, Seaborn offers a lot of opportunities to customize your plots and have them show a distinct style. The color of your background, background style such as lines and ticks, and the size of your font all play a role in improving legibility and aesthetics.'''

'''
Seaborn Styling, Part 2: Color
Learn how to work with color in Seaborn and choose appropriate color palettes for your datasets.

Introduction
When creating a data visualization, your goal is to communicate the insights found in the data. While visualizing communicates important information, styling will influence how your audience understands what you’re trying to convey.

After you have formatted and visualized your data, the third and last step of data visualization is styling. Styling is the process of customizing the overall look of your visualization, or figure. Making intentional decisions about the details of the visualization will increase their impact and set your work apart.

In this article, we’ll look at how we can effectively use color to convey meaning. We’ll cover:

How to set a palette
Seaborn default and built-in color palettes
Color Brewer Palettes
Selecting palettes for your dataset
Commands for Working with Palettes
You can build color palettes using the function sns.color_palette(). This function can take any of the Seaborn built-in palettes (see below). You can also build your own palettes by passing in a list of colors in any valid Matplotlib format, including RGB tuples, hex color codes, or HTML color names.

If you want to quickly see what a palette looks like, use the function sns.palplot() to plot a palette as an array of colors:

# Save a palette to a variable:
palette = sns.color_palette("bright")

# Use palplot and pass in the variable:
sns.palplot(palette)
image1

To select and set a palette in Seaborn, use the command sns.set_palette() and pass in the name of the palette that you would like to use:

# Set the palette using the name of a palette:
sns.set_palette("Paired")

# Plot a chart:
sns.stripplot(x="day", y="total_bill", data=tips)
image2

Seaborn Default Color Palette
If you do not pass in a color palette to sns.color_palette() or sns.set_palette(), Seaborn will use a default set of colors. These defaults improve upon the Matplotlib default color palettes and are one significant reason why people choose to use Seaborn for their data visualizations. Here’s a comparison of the two default palettes:

image3

image4

Seaborn also allows you to style Matplotlib plots. So even if you’re using a plot that only exists in Matplotlib, such as a histogram, you can do so using Seaborn defaults.

To do so, call the sns.set() function before your plot:

# Call the sns.set() function 
sns.set()
for col in 'xy':
  plt.hist(data[col], normed=True, alpha=0.5)
image5

Not only does this function allow you the ability to use Seaborn default colors, but also any of Seaborn’s other styling techniques.

Seaborn has six variations of its default color palette: deep, muted, pastel, bright, dark, and colorblind.

image6

To use one of these palettes, pass the name into sns.set_palette():

# Set the palette to the "pastel" default palette:
sns.set_palette("pastel")

# plot using the "pastel" palette
sns.stripplot(x="day", y="total_bill", data=tips)
image7

Using Color Brewer Palettes
In addition to the default palette and its variations, Seaborn also allows the use of Color Brewer palettes. Color Brewer is the name of a set of color palettes inspired by the research of cartographer Cindy Brewer. The color palettes are specifically chosen to be easy to interpret when used to represent ordered categories. They are also colorblind accessible, as each color differs from its neighbors in lightness or tint.

To use, pass the name of any Color Brewer palette directly to any of the color functions:

custom_palette = sns.color_palette("Paired", 9)
sns.palplot(custom_palette)
image8

Here is a list of the the Color Brewer palettes, with their names for easy reference:

image9

Check out http://colorbrewer2.org (http://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3)for more information about color palette configuration options.

Selecting Color Palettes for Your Dataset
Qualitative Palettes for Categorical Datasets
When using a dataset that uses distinct but non-ordered categories, it’s good to use qualitative palettes. Qualitative palettes are sets of distinct colors which make it easy to distinguish the categories when plotted but don’t imply any particular ordering or meaning.

An example of categorical data is breed of dog. Each of these values, such as Border Collie or Poodle, are distinct from each other but there isn’t any inherent order to these categories.

Here’s an example of a qualitative Color Brewer palette:

qualitative_colors = sns.color_palette("Set3", 10)
sns.palplot(qualitative_colors)
image10

Sequential Palettes
Just as the name implies, sequential palettes are a set of colors that move sequentially from a lighter to a darker color. Sequential color palettes are appropriate when a variable exists as ordered categories, such as grade in school, or as continuous values that can be put into groups, such as yearly income. Because the darkest colors will attract the most visual attention, sequential palettes are most useful when only high values need to be emphasized.

Here’s an example of a sequential Color Brewer palette:

sequential_colors = sns.color_palette("RdPu", 10)
sns.palplot(sequential_colors)
image11

Diverging Palettes
Diverging palettes are best suited for datasets where both the low and high values might be of equal interest, such as hot and cold temperatures.

In the example below, both ends of the spectrum — fire red and deep blue — are likely to attract attention.

diverging_colors = sns.color_palette("RdBu", 10)
sns.palplot(diverging_colors)
image12

Here is a quick diagram that depicts each of the palette types:

image13

Credit: Michael Waskom
Summary
The ability to use easily choose different color palettes is one of the important affordances of styling your plots with Seaborn. Seaborn gives you a range of built-in plots to choose from: whether it’s variations on the defaults or access to all of the Color Brewer palettes. It’s easy to choose a palette that is well suited to your dataset, thanks to Color Brewer, as it supports palettes for qualitative, sequential, and diverging datasets.

For more on using color in Seaborn, check out their documentation.
'''

https://seaborn.pydata.org/tutorial/color_palettes.html?highlight=color

#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-  SNS PROJECT #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-


import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import numpy as np

df = pd.read_csv('all_data.csv')
'''
#general inquiry of df 
print(df.head())
df.info()
print(df.describe())
print(df.Year.value_counts)
print(df.Year.unique())
print(df.Country.unique())'''

#option to display all rows for printing
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

#print(df)

#sory by country first , then by year.
df = df.sort_values(by=['Country', 'Year'], ascending= True)
#print(df)


df = df.rename(columns = {'Life expectancy at birth (years)' : 'LEABY'})

#plot GDP per Country

labels = df.Year.unique()

chile_gdp = df[df.Country == 'Chile'].GDP.values
china_gdp = df[df.Country == 'China'].GDP.values
germany_gdp = df[df.Country == 'Germany'].GDP.values
mexico_gdp = df[df.Country == 'Mexico'].GDP.values
mexico_gdp = df[df.Country == 'Mexico'].GDP.values
us_gdp = df[df.Country == 'United States of America'].GDP.values
zimbabwe_gdp = df[df.Country == 'Zimbabwe'].GDP.values

x = np.arange(len(labels)) # the label locations
width = 1/(7)  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - 2.5*width, chile_gdp, width, label='Chile')
rects2 = ax.bar(x - 1.5*width, china_gdp, width, label='China')
rects3 = ax.bar(x - width/2, germany_gdp, width, label='Germany')
rects4 = ax.bar(x + width/2, mexico_gdp, width, label='Mexico')
rects5 = ax.bar(x + 1.5*width, us_gdp, width, label='United States of America')
rects6 = ax.bar(x + 2.5*width, zimbabwe_gdp, width, label='Zimbabwe')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('GDP')
ax.set_title('GDP per Year Group By Country')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


#autolabel(rects1)
#autolabel(rects2)
#autolabel(rects3)
#autolabel(rects4)
#autolabel(rects5)
#autolabel(rects6)

fig.tight_layout()

#plt.show()

#plot LEABY per Country

print(df.head())

labels = df.Year.unique()
countries = df.Country.unique()

x = np.arange(len(labels)) # the label locations
width = 1/(7)  # the width of the bars

fig1, ax1 = plt.subplots()

i = -2.5

for country in countries:
    country_leaby = df[df.Country == country].LEABY.values
    rec = ax1.bar(x - i * width, country_leaby, width, label=country)
    i += 1

# Add some text for labels, title and custom x-axis tick labels, etc.
ax1.set_ylabel('Life Expectation by year of Born')
ax1.set_title('Life Expectation by Countries')
ax1.set_xticks(x)
ax1.set_xticklabels(labels)
ax1.set_yticks(np.arange(40, 100, step=10))
ax1.set_ylim([40,100])
ax1.legend()

#autolabel(rects1)
#autolabel(rects2)
#autolabel(rects3)
#autolabel(rects4)
#autolabel(rects5)
#autolabel(rects6)

fig.tight_layout()

#use seabron to plot

fig2, ax2 = plt.subplots()

sns.set_palette("Blues")
ax2 = sns.barplot(x="Year", y="GDP", hue="Country", data=df)

plt.xticks(rotation=90)

fig3, ax3 = plt.subplots()

# WORDBANK:
# "Year"
# "Country"
# "GDP"
# "LEABY"
# plt.scatter


# Uncomment the code below and fill in the blanks
ax3 = sns.FacetGrid(df, col='Year', hue='Country', col_wrap=4, size=2)
ax3 = (ax3.map(plt.scatter, "GDP", "LEABY", edgecolor="w").add_legend())

fig4, ax4 = plt.subplots()

# WORDBANK:
# plt.plot
# "LEABY"
# "Year"
# "Country"


# Uncomment the code below and fill in the blanks
g3 = sns.FacetGrid(df, col="Country", col_wrap=3, size=4)
g3 = (g3.map(plt.plot, "Year", "LEABY").add_legend())



plt.show()







  






#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-SEABORN CHEATSHEET SEABORN CHEATSHEET SEABORN CHEATSHEET  #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Seaborn
Seaborn is a Python data visualization library that builds off the functionalities of Matplotlib and integrates nicely with Pandas DataFrames. It provides a high-level interface to draw statistical graphs, and makes it easier to create complex visualizations.

Estimator argument in barplot
The estimator argument of the barplot() method in Seaborn can alter how the data is aggregated. By default, each bin of a barplot displays the mean value of a variable. Using the estimator argument this behaviour would be different.

The estimator argument can receive a function such as np.sum, len, np.median or other statistical function. This function can be used in combination with raw data such as a list of numbers and display in a barplot the desired statistic of this list.

Seaborn barplot
In Seaborn, drawing a barplot is simple using the function sns.barplot(). This function takes in the paramaters data, x, and y. It then plots a barplot using data as the dataframe, or dataset for the plot. x is the column of the dataframe that contains the labels for the x axis, and y is the column of the dataframe that contains the data to graph (aka what will end up on the y axis).

Using the Seaborn sample data “tips”, we can draw a barplot having the days of the week be the x axis labels, and the total_bill be the y axis values:

sns.barplot(data = tips, x = "day", y = "total_bill")

Barplot error bars
By default, Seaborn’s barplot() function places error bars on the bar plot. Seaborn uses a bootstrapped confidence interval to calculate these error bars.

The confidence interval can be changed to standard deviation by setting the parameter ci = "sd".

Seaborn hue
For the Seaborn function sns.barplot(), the hue parameter can be used to create a bar plot with more than one dimension, or, in other words, such that the data can be divided into more than one set of columns.

Using the Seaborn sample data “tips”, we can draw a barplot with the days of the week as the labels of the columns on the x axis, and the total_bill as the y axis values as follows:
'''
sns.barplot(data = tips, x = "day", y = "total_bill", hue = "sex")
'''
As you can see, hue divides the data into two columns based on the “sex” - male and female.

Seaborn function plots means by default
By default, the seaborn function sns.barplot() plots the means of each category on the x axis.

In the example code block, the barplot will show the mean satisfaction for every gender in the dataframe df.

sns.barplot(data = df, x = "Gender", y = "Satisfaction")
Box and Whisker Plots in Seaborn
A box and whisker plot shows a dataset’s median value, quartiles, and outliers. The box’s central line is the dataset’s median, the upper and lower lines marks the 1st and 3rd quartiles, and the “diamonds” shows the dataset’s outliers. With Seaborn, multiple data sets can be plotted as adjacent box and whisker plots for easier comparison.

Seaborn Package
Seaborn is a suitable package to plot variables and compare their distributions. With this package users can plot univariate and bivariate distributions among variables. It has superior capabilities than the popular methods of charts such as the barchart. Seaborn can show information about outliers, spread, lowest and highest points that otherwise would not be shown on a traditional barchart.

'''



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-PYTHON BEAUTIFUL PYTHON SOUP BEAUTIFULSOUP PYTHON BEAUTIFUL PYTHON SOUP BEAUTIFULSOUPG#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

WEB SCRAPING WITH BEAUTIFUL SOUP
Requests
In order to get the HTML of the website, we need to make a request to get the content of the webpage. To learn more about requests in a general sense, you can check out this article.

Python has a requests library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to GET:

import requests

webpage = requests.get('https://www.codecademy.com/articles/http-requests')
print(webpage.text)
This code will print out the HTML of the page.

We don’t want to unleash a bunch of requests on any one website in this lesson, so for the rest of this lesson we will be scraping a local HTML file and pretending it’s an HTML file hosted online.

-------------------------------------------------------------------------------

WEB SCRAPING WITH BEAUTIFUL SOUP
The BeautifulSoup Object
When we printed out all of that HTML from our request, it seemed pretty long and messy. How could we pull out the relevant information from that long string?

BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in. We can import it by using the line:

from bs4 import BeautifulSoup
Then, all we have to do is convert the HTML document to a BeautifulSoup object!

If this is our HTML file, rainbow.html:

<body>
  <div>red</div>
  <div>orange</div>
  <div>yellow</div>
  <div>green</div>
  <div>blue</div>
  <div>indigo</div>
  <div>violet</div>
</body>
soup = BeautifulSoup("rainbow.html", "html.parser")
"html.parser" is one option for parsers we could use. There are other options, like "lxml" and "html5lib" that have different advantages and disadvantages, but for our purposes we will be using "html.parser" throughout.

With the requests skills we just learned, we can use a website hosted online as that HTML:

webpage = requests.get("http://rainbow.com/rainbow.html", "html.parser")
soup = BeautifulSoup(webpage.content)
When we use BeautifulSoup in combination with pandas, we can turn websites into DataFrames that are easy to manipulate and gain insights from.

-----------------------------------------------------------------------------------------

WEB SCRAPING WITH BEAUTIFUL SOUP
Object Types
BeautifulSoup breaks the HTML page into several types of objects.

Tags
A Tag corresponds to an HTML Tag in the original document. These lines of code:

soup = BeautifulSoup('<div id="example">An example div</div><p>An example p tag</p>')
print(soup.div)
Would produce output that looks like:

<div id="example">An example div</div>
Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page.

You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs:

print(soup.div.name)
print(soup.div.attrs)
div
{'id': 'example'}
NavigableStrings
NavigableStrings are the pieces of text that are in the HTML tags on the page. You can get the string inside of the tag by calling .string:

print(soup.div.string)
An example div

---------------------------------------------------------------------------------------

WEB SCRAPING WITH BEAUTIFUL SOUP
Navigating by Tags
To navigate through a tree, we can call the tag names themselves. Imagine we have an HTML page that looks like this:

<h1>World's Best Chocolate Chip Cookies</h1>
<div class="banner">
  <h1>Ingredients</h1>
</div>
<ul>
  <li> 1 cup flour </li>
  <li> 1/2 cup sugar </li>
  <li> 2 tbsp oil </li>
  <li> 1/2 tsp baking soda </li>
  <li> ½ cup chocolate chips </li> 
  <li> 1/2 tsp vanilla <li>
  <li> 2 tbsp milk </li>
</ul>
If we made a soup object out of this HTML page, we have seen that we can get the first h1 element by calling:

print(soup.h1)
<h1>World's Best Chocolate Chip Cookies</h1>
We can get the children of a tag by accessing the .children attribute:

for child in soup.ul.children:
    print(child)
<li> 1 cup flour </li>
<li> 1/2 cup sugar </li>
<li> 2 tbsp oil </li>
<li> 1/2 tsp baking soda </li>
<li> ½ cup chocolate chips </li> 
<li> 1/2 tsp vanilla <li>
<li> 2 tbsp milk </li>
We can also navigate up the tree of a tag by accessing the .parents attribute:

for parent in soup.li.parents:
    print(parent)
This loop will first print:

<ul>
<li> 1 cup flour </li>
<li> 1/2 cup sugar </li>
<li> 2 tbsp oil </li>
<li> 1/2 tsp baking soda </li>
<li> ½ cup chocolate chips </li>
<li> 1/2 tsp vanilla </li>
<li> 2 tbsp milk </li>
</ul>
Then, it will print the tag that contains the ul (so, the body tag of the document). Then, it will print the tag that contains the body tag (so, the html tag of the document).

------------------------------------------------------------------------------------------------------
WEB SCRAPING WITH BEAUTIFUL SOUP
Find All
If we want to find all of the occurrences of a tag, instead of just the first one, we can use .find_all().

This function can take in just the name of a tag and returns a list of all occurrences of that tag.

print(soup.find_all("h1"))
['<h1>World's Best Chocolate Chip Cookies</h1>', '<h1>Ingredients</h1>']
.find_all() is far more flexible than just accessing elements directly through the soup object. With .find_all(), we can use regexes, attributes, or even functions to select HTML elements more intelligently.

Using Regex
What if we want every <ol> and every <ul> that the page contains? We can select both of these types of elements with a regex in our .find_all():

import re
soup.find_all(re.compile("[ou]l"))
What if we want all of the h1 - h9 tags that the page contains? Regex to the rescue again!

import re
soup.find_all(re.compile("h[1-9]"))
Using Lists
We can also just specify all of the elements we want to find by supplying the function with a list of the tag names we are looking for:

soup.find_all(['h1', 'a', 'p'])
Using Attributes
We can also try to match the elements with relevant attributes. We can pass a dictionary to the attrs parameter of find_all with the desired attributes of the elements we’re looking for. If we want to find all of the elements with the "banner" class, for example, we could use the command:

soup.find_all(attrs={'class':'banner'})
Or, we can specify multiple different attributes! What if we wanted a tag with a "banner" class and the id "jumbotron"?

soup.find_all(attrs={'class':'banner', 'id':'jumbotron'})
Using A Function
If our selection starts to get really complicated, we can separate out all of the logic that we’re using to choose a tag into its own function. Then, we can pass that function into .find_all()!

def has_banner_class_and_hello_world(tag):
    return tag.attr('class') == "banner" and tag.string == "Hello world"

soup.find_all(has_banner_class_and_hello_world)
This command would find an element that looks like this:

<div class="banner">Hello world</div>
but not an element that looks like this:

<div>Hello world</div>
Or this:

<div class="banner">What's up, world!</div>

#-----------------------------------------------------------

WEB SCRAPING WITH BEAUTIFUL SOUP
Select for CSS Selectors
Another way to capture your desired elements with the soup object is to use CSS selectors. The .select() method will take in all of the CSS selectors you normally use in a .css file!

<h1 class='results'>Search Results for: <span class='searchTerm'>Funfetti</span></h1>
<div class='recipeLink'><a href="spaghetti.html">Funfetti Spaghetti</a></div>
<div class='recipeLink' id="selected"><a href="lasagna.html">Lasagna de Funfetti</a></div>
<div class='recipeLink'><a href="cupcakes.html">Funfetti Cupcakes</a></div>
<div class='recipeLink'><a href="pie.html">Pecan Funfetti Pie</a></div>
If we wanted to select all of the elements that have the class 'recipeLink', we could use the command:

soup.select(".recipeLink")
If we wanted to select the element that has the id 'selected', we could use the command:

soup.select("#selected")
Let’s say we wanted to loop through all of the links to these funfetti recipes that we found from our search.

for link in soup.select(".recipeLink > a"):
  webpage = requests.get(link)
  new_soup = BeautifulSoup(webpage)
This loop will go through each link in each .recipeLink div and create a soup object out of the webpage it links to. So, it would first make soup out of <a href="spaghetti.html">Funfetti Spaghetti</a>, then <a href="lasagna.html">Lasagna de Funfetti</a>, and so on.
#-----------------------------------------------------

import requests
from bs4 import BeautifulSoup

prefix = "https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/"
webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')

webpage = webpage_response.content
soup = BeautifulSoup(webpage, "html.parser")

turtle_links = soup.find_all("a")
links = []
#go through all of the a tags and get the links associated with them:
for a in turtle_links:
    links.append(prefix+a["href"])
    
#Define turtle_data:
turtle_data = {}

#follow each link:
for link in links:
  webpage = requests.get(link)
  turtle = BeautifulSoup(webpage.content, "html.parser")
  #Add your code here:
  turtle_name = turtle.select(".name")[0]
  print (turtle_name)
  turtle_data[turtle_name] = []
  
print(turtle_data)

#---------------------------

WEB SCRAPING WITH BEAUTIFUL SOUP
Reading Text
When we use BeautifulSoup to select HTML elements, we often want to grab the text inside of the element, so that we can analyze it. We can use .get_text() to retrieve the text inside of whatever tag we want to call it on.

<h1 class="results">Search Results for: <span class='searchTerm'>Funfetti</span></h1>
If this is the HTML that has been used to create the soup object, we can make the call:

soup.get_text()
Which will return:

'Search Results for: Funfetti'
Notice that this combined the text inside of the outer h1 tag with the text contained in the span tag inside of it! Using get_text(), it looks like both of these strings are part of just one longer string. If we wanted to separate out the texts from different tags, we could specify a separator character. This command would use a . character to separate:

soup.get_text('|')
Now, the command returns:

'Search Results for: |Funfetti'
```

#-----------------------------------------------------

import requests
from bs4 import BeautifulSoup

prefix = "https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/"
webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/shellter.html')

webpage = webpage_response.content
soup = BeautifulSoup(webpage, "html.parser")

turtle_links = soup.find_all("a")
links = []
#go through all of the a tags and get the links associated with them"
for a in turtle_links:
    links.append(prefix+a["href"])
    
#Define turtle_data:
turtle_data = {}

#follow each link:
for link in links:
  webpage = requests.get(link)
  turtle = BeautifulSoup(webpage.content, "html.parser")
  turtle_name = turtle.select(".name")[0].get_text()
  
  stats = turtle.find("ul")
  stats_text = stats.get_text("|")
  turtle_data[turtle_name] = stats_text.split("|")
  

#--------------------------------------------------------------------

LEARN WEB SCRAPING WITH BEAUTIFUL SOUP
Chocolate Scraping with Beautiful Soup
After eating chocolate bars your whole life, you’ve decided to go on a quest to find the greatest chocolate bar in the world.

You’ve found a website that has over 1700 reviews of chocolate bars from all around the world. It’s displayed in the web browser on this page.

The data is displayed in a table, instead of in a csv or json. Thankfully, we have the power of BeautifulSoup that will help us transform this webpage into a DataFrame that we can manipulate and analyze.

The rating scale is from 1-5, as described in this review guide. A 1 is “unpleasant” chocolate, while a 5 is a bar that transcends “beyond the ordinary limits”.

Some questions we thought about when we found this dataset were: Where are the best cocoa beans grown? Which countries produce the highest-rated bars? What’s the relationship between cocoa solids percentage and rating?

Can we find a way to answer these questions, or uncover more questions, using BeautifulSoup and Pandas?

Tasks
18/19Complete
Mark the tasks as complete by checking them off
Make Some Chocolate Soup
1.
Explore the webpage displayed in the browser. What elements could be useful to scrape here? Which elements do we not want to scrape?

If you want to use your browser to inspect the website, you may need a refresher on DevTools.

2.
Let’s make a request to this site to get the raw HTML, which we can later turn into a BeautifulSoup object.

The URL is:

https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/cacao/index.html
You can pass this into the .get() method of the requests module to get the HTML.

To make a request to a webpage, you can use the syntax:

webpage = requests.get("http://websitetoscrape.com")
3.
Create a BeautifulSoup object called soup to traverse this HTML.

Use "html.parser" as the parser, and the content of the response you got from your request as the document.

To make the object, you can use syntax like:

soup = BeautifulSoup(your_document, "your_parser")
your_document should be the .content of the webpage response you got from the last step.

4.
If you want, print out the soup object to explore the HTML.

So many table rows! You’re probably very relieved that we don’t have to scrape this information by hand.

How are ratings distributed?
5.
How many terrible chocolate bars are out there? And how many earned a perfect 5? Let’s make a histogram of this data.

The first thing to do is to put all of the ratings into a list.

Use a command on the soup object to get all of the tags that contain the ratings.

It looks like all of the rating tds have a class "Rating".

Remember that we can use .find_all() to get all elements of a class "ClassName" with this syntax:

soup.find_all(attrs={"class": "ClassName"})
6.
Create an empty list called ratings to store all the ratings in.

7.
Loop through the ratings tags and get the text contained in each one. Add it to the ratings list.

As you do this, convert the rating to a float, so that the ratings list will be numerical. This should help with calculations later.

The first element of your tags list probably contains the header string "Ratings", so we probably should leave this off the list. Start your loop at element 1 of the list instead.

You can cast a string x to a float with this syntax:

float(x)
You can get the text of a BeautifulSoup tag using .get_text()

8.
Using Matplotlib, create a histogram of the ratings values:

plt.hist(ratings)
Remember to show the plot using plt.show()!

Your plot will show up at localhost in the web browser. You will have to navigate away from the cacao ratings webpage to see it.

Which chocolatier makes the best chocolate?
9.
We want to now find the 10 most highly rated chocolatiers. One way to do this is to make a DataFrame that has the chocolate companies in one column, and the ratings in another. Then, we can do a groupby to find the ones with the highest average rating.

First, let’s find all the tags on the webpage that contain the company names.

It seems like all of the company tds have the class name "Company".

We can use .select() to grab BeautifulSoup elements by CSS selector:

soup.select(".ClassName")
Do this with the class name "Company" to get all of the right tags.

10.
Just like we did with ratings, we now want to make an empty list to hold company names.

11.
Loop through the tags containing company names, and add the text from each tag to the list you just created.

We might want to use syntax like

for td in company_tags[1:]:
  companies.append(td.get_text())
12.
Create a DataFrame with a column “Company” corresponding to your companies list, and a column “Ratings” corresponding to your ratings list.

You can make a DataFrame with defined columns using this syntax:

d = {"Column 1 Name": column_1_list, "Column 2 Name": column_2_list}
your_df = pd.DataFrame.from_dict(d)
13.
Use .groupby to group your DataFrame by Company and take the average of the grouped ratings.

Then, use the .nlargest command to get the 10 highest rated chocolate companies. Print them out.

Look at the hint if you get stuck on this step!

Your Pandas commands should look something like:

mean_vals = cacao_df.groupby("Company").Rating.mean()
ten_best = mean_ratings.nlargest(10)
print(ten_best)
Is more cacao better?
14.
We want to see if the chocolate experts tend to rate chocolate bars with higher levels of cacao to be better than those with lower levels of cacao.

It looks like the cocoa percentages are in the table under the Cocoa Percent column.

Using the same methods you used in the last couple of tasks, create a list that contains all of the cocoa percentages. Store each percent as an integer, after stripping off the % character.

You’ll have to access the tags with class "CocoaPercent" and loop through them to get the text.

cocoa_percents = []
cocoa_percent_tags = soup.select(".CocoaPercent")

for td in cocoa_percent_tags[1:]:
  percent = int(td.get_text().strip('%'))
  cocoa_percents.append(percent)
15.
Add the cocoa percentages as a column called "CocoaPercentage" in the DataFrame that has companies and ratings in it.

You can add the pairing "CocoaPercentage":cocoa_percents to the dictionary you used to create the DataFrame.

16.
Make a scatterplot of ratings (your_df.Rating) vs percentage of cocoa (your_df.CocoaPercentage).

You can do this in Matplotlib with these commands:

plt.scatter(df.CocoaPercentage, df.Rating)
plt.show()
Call plt.clf() to clear the figure between showing your histogram and this scatterplot.

Remember that your plots will show up at the address localhost in the web browser.

17.
Is there any correlation here? We can use some numpy commands to draw a line of best-fit over the scatterplot.

Copy this code and paste it after you create the scatterplot, but before you call .show():

z = np.polyfit(df.CocoaPercentage, df.Rating, 1)
line_function = np.poly1d(z)
plt.plot(df.CocoaPercentage, line_function(df.CocoaPercentage), "r--")
18.
Is there any correlation here? We can use some numpy commands to draw a line of best-fit over the scatterplot.

Copy this code and paste it after you create the scatterplot, but before you call .show():

z = np.polyfit(df.CocoaPercentage, df.Rating, 1)
line_function = np.poly1d(z)
plt.plot(df.CocoaPercentage, line_function(df.CocoaPercentage), "r--")
Explore!
19.
We have explored a couple of the questions about chocolate that inspired us when we looked at this chocolate table.

What other kinds of questions can you answer here? Try to use a combination of BeautifulSoup and Pandas to explore some more.

For inspiration: Where are the best cocoa beans grown? Which countries produce the highest-rated bars?

#------------------------------------------------------------------------------------------------------------------

import codecademylib3_seaborn
from bs4 import BeautifulSoup
import requests
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

webpage_response = requests.get('https://s3.amazonaws.com/codecademy-content/courses/beautifulsoup/cacao/index.html')
webpage = webpage_response.content
soup = BeautifulSoup(webpage, 'html.parser')

ratings =[]
companys = []
cacao_percents = []
cacao_pp = []

for rating in soup.find_all("td", attrs={"class":"Rating"}):
		ratings.append(rating.get_text())

ratings.remove("Rating")

ratings2 =[]
for rating in ratings:
  ratings2.append(float(rating))

#plt.hist(ratings2)
#plt.show()

for company in soup.find_all("td", attrs={"class":"Company"}):
		companys.append(company.get_text())

companys.pop(0)


df1 = pd.DataFrame({
  "Company":companys,
  "Ratings":ratings2
})

top_company = df1.groupby("Company").Ratings.mean()
ten_best = top_company.nlargest(10)

#print (ten_best)

for cacao_p in soup.find_all("td",attrs = {'class' : 'CocoaPercent'}):
  cacao_percents.append(cacao_p.get_text())
cacao_percents.pop(0)

for cacao_p in cacao_percents:
  cacao = cacao_p.split("%")[0]
  cacao_pp.append(float(cacao))

#print (cacao_pp)

df1['CocoaPercentage'] = cacao_pp

plt.scatter(df1.CocoaPercentage, df1.Ratings)

z = np.polyfit(df1.CocoaPercentage, df1.Ratings, 1)
line_function = np.poly1d(z)
plt.plot(df1.CocoaPercentage, line_function(df1.CocoaPercentage), "r--")

plt.show()

plt.clf()

#------------note from corey schafer--------------------------------------------------------------

pip install beautifulsoup4
			lxml
			html5lib
			requests

from bs4 import BeautifulSoup
improt requests
with open ('simple.html') as html_file:
	soup = BeautifulSoup(html.file, 'lxml')

soup.prettify()

match = soup.title # get first title
print(match.text)

match = soup.div
match = soup.find('div', class = "footer")

article = soup.find('div', class = 'article')

#<div class = "article">
#	<h2> <a herf = "article_l.html"> .....
#	<p> ..... </p>
#	</div>

headline = article.h2.a.text
summary = article.p.text

for article in soup.find_all('div', class = 'article'):
	headline = article.h2.a.text
	summary = article.p.text
	
	
	
	

from bs4 import BeautifulSoup
import requests

source = requests.get('https://xxx.xxx.com').text
soup = BeautifulSoup(source, 'lxml')
soup.prettify()

article = soup.find('article')
headline = article.h2.a.text
summary = articl.find('div', class = 'entry-content').p.text
vid_src = article.find('iframe', class = 'youtubeplay')['src']
#to access the results as a dictionary, use ['src'] at the end

#http://www.youtube.com/embedkdkjfdorij....

vid_id = vid.split(',')[4]

yt-link = f'https://youtube.com/watch?v={vid_id}'

article = soup.find('article')
for article in soup.find_all('article'):
	head = ....
	summary = ....
	vid_src = .....
	
	try:
		vid_scr = .....
		vid_id = ......
		yt-link = f'https://youtube.com/watch?v={vid_id}'
	except Exception as e:
		yt-link = None
		
		
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-REGULAR EXPRESSIONS REGEX NAME NAME NAME NAME NAME NAME NAME NAME NAME NAME NAME NAME NAME#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

OR

cat|dog

Character sets

[chr]at # will match cat , hat, rat

[cat] # will only match c or a or t but not cat

[^cat] # will filter out any c, a ,t  ^means negates

wildcards

. # will match any single character (letter, number, symbol or whitespace) 

\. # We can use the escape character to match an actual period, .

Ranges

[abc] = [a-c]
[A-Z]
[a-z]
[0-9]
[A-Za-z] # To match any single capital or lowercase alphabetical character
[] within any character set [] only match one character.

Shorthand Character Classes

\w: the “word character” class represents the regex range [A-Za-z0-9_], and it matches a single uppercase character, lowercase character, digit or underscore

\d: the “digit character” class represents the regex range [0-9], and it matches a single digit character

\s: the “whitespace character” class represents the regex range [ \t\r\n\f\v], matching a single space, tab, carriage return, line break, form feed, or vertical tab

\W: the “non-word character” class represents the regex range [^A-Za-z0-9_], matching any character that is not included in the range represented by \w

\D: the “non-digit character” class represents the regex range [^0-9], matching any character that is not included in the range represented by \d

\S: the “non-whitespace character” class represents the regex range [^ \t\r\n\f\v], matching any character that is not included in the range represented by \s


Grouping

I love baboons|gorillas # will match "I love baboons" and "gorillas"

I love (baboons|gorillas) # will match "I love baboons" and "I love gorillas"


Quantifiers - Fixed

\w{3} will match exactly 3 word characters

\w{4,7} will match at minimum 4 word characters and at maximum 7 word characters

roa{3}r = roaaar

mo{2,4} is greedy, #it will match moooo (if it's in the text) but not mo and moo

Quantifiers - Optional

? #can appear either 0 times or 1 time

humou?r #= humor / humour

a (rotten)? banana #= a rotten banana / a banana

beautiful\? #= beautiful?

Quantifiers - 0 or More, 1 or More

* #= 0 or more chracter
meo*w #= mew, meow, meoow, meoooow .... (0 or more)
meo+w #= meow, meoow, meoooow....(1 or more)
My cat is a \* #= My cat is a *

Anchors

^Monkeys enemy$ # will completely match the text Monkeys enemy but not Spider Monkeys: enemy in the wild

^ indicate the start
$ indicate the end

\^ #= ^
\$ #= $



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-REGULAR EXPRESSIONS REGEX CHEATSHEET REGULAR EXPRESSIONS REGEX CHEATSHEET REGULAR EXPRESSI#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
Cheatsheets / Practical Data Cleaning

Data Cleaning with Pandas
Print Cheatsheet


TOPICS

Data Cleaning with Pandas
Regular Expressions
Regular expressions are sequence of characters defining a pattern of text that needs to be found. They can be used for parsing the text files for specific pattern, verifying test results, and finding keywords in emails or webpages.

Literals in Regular Expressions
In Regular expression, the literals are the simplest characters that will match the exact text of the literals. For example, the regex monkey will completely match the text monkey but will also match monkey in text The monkeys like to eat bananas.

Alternation in Regular Expressions
Alternation indicated by the pipe symbol |, allows for the matching of either of two subexpressions. For example, the regex baboons|gorillas will match the text baboons as well as the text gorillas.

Character Sets in Regular Expressions
Regular expression character sets denoted by a pair of brackets [] will match any of the characters included within the brackets. For example, the regular expression con[sc]en[sc]us will match any of the spellings consensus, concensus, consencus, and concencus.

Wildcards in Regular expressions
In Regular expression, wildcards are denoted with the period . and it can match any single character (letter, number, symbol or whitespace) in a piece of text. For example, the regular expression ......... will match the text orangutan, marsupial, or any other 9-character text.

Regular Expression Ranges
Regular expression ranges are used to specify a range of characters that can be matched. Common regular expression ranges include: [A-Z]. : match any uppercase letter [a-z]. : match any lowercase letter [0-9]. : match any digit [A-Za-z] : match any uppercase or lowercase letter.

Shorthand Character Classes in Regular Expressions
Shorthand character classes simplify writing regular expressions. For example, \w represents the regex range [A-Za-z0-9_], \d represents [0-9], \W represents [^A-Za-z0-9_] matching any character not included by \w, \D represents [^0-9] matching any character not included by \d.

Grouping in Regular Expressions
In Regular expressions, grouping is accomplished by open ( and close parenthesis ). Thus the regular expression I love (baboons|gorillas) will match the text I love baboons as well as I love gorillas, as the grouping limits the reach of the | to the text within the parentheses.

Fixed Quantifiers in Regular Expressions
In Regular expressions, fixed quantifiers are denoted by curly braces {}. It contains either the exact quantity or the quantity range of characters to be matched. For example, the regular expression roa{3}r will match the text roaaar, while the regular expression roa{3,6}r will match roaaar, roaaaar, roaaaaar, or roaaaaaar.

Optional Quantifiers in Regular Expressions
In Regular expressions, optional quantifiers are denoted by a question mark ?. It indicates that a character can appear either 0 or 1 time. For example, the regular expression humou?r will match the text humour as well as the text humor.

Kleene Star & Kleene Plus in Regular Expressions
In Regular expressions, the Kleene star(*) indicates that the preceding character can occur 0 or more times. For example, meo*w will match mew, meow, meooow, and meoooooooooooow. The Kleene plus(+) indicates that the preceding character can occur 1 or more times. For example, meo+w will match meow, meooow, and meoooooooooooow, but not match mew.

Anchors in Regular Expressions
Anchors (hat ^ and dollar sign $) are used in regular expressions to match text at the start and end of a string, respectively. For example, the regex ^Monkeys: my mortal enemy$ will completely match the text Monkeys: my mortal enemy but not match Spider Monkeys: my mortal enemy or Monkeys: my mortal enemy in the wild. The ^ ensures that the matched text begins with Monkeys, and the $ ensures the matched text ends with enemy.
'''


#-#-#-#-#-#-#-#-#-#-DATA CLEANING WITH PANDAS #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
'''
DATA CLEANING WITH PANDAS
Introduction
A huge part of data science involves acquiring raw data and getting it into a form ready for analysis. Some have estimated that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it or building models from it.

When we receive raw data, we have to do a number of things before we’re ready to analyze it, possibly including:

diagnosing the “tidiness” of the data — how much data cleaning we will have to do
reshaping the data — getting right rows and columns for effective analysis
combining multiple files
changing the types of values — how we fix a column where numerical values are stored as strings, for example
dropping or filling missing values - how we deal with data that is incomplete or missing
manipulating strings to represent the data better
We will go through the techniques data scientists use to accomplish these goals by looking at some “unclean” datasets and trying to get them into a good, clean state.'''

'''
DATA CLEANING WITH PANDAS
Diagnose the Data
We often describe data that is easy to analyze and visualize as “tidy data”. What does it mean to have tidy data?

For data to be tidy, it must have:

Each variable as a separate column
Each row as a separate observation
For example, we would want to reshape a table like:

Account	Checkings	Savings
“12456543”	8500	8900
“12283942”	6410	8020
“12839485”	78000	92000

Into a table that looks more like:

Account	Account Type	Amount
“12456543”	“Checking”	8500
“12456543”	“Savings”	8900
“12283942”	“Checking”	6410
“12283942”	“Savings”	8020
“12839485”	“Checking”	78000
“12839485”	“Savings”	920000

The first step of diagnosing whether or not a dataset is tidy is using pandas functions to explore and probe the dataset.

You’ve seen most of the functions we often use to diagnose a dataset for cleaning. Some of the most useful ones are:

.head() — display the first 5 rows of the table
.info() — display a summary of the table
.describe() — display the summary statistics of the table
.columns — display the column names of the table
.value_counts() — display the distinct values for a column'''

import codecademylib3_seaborn
import pandas as pd

df1 = pd.read_csv("df1.csv")
df2 = pd.read_csv("df2.csv")

print(df1)

print(df1.head())
#print(df2.head())
print('\n')

print(df1.info())

print('\n')

print(df1.describe())

clean = 2

'''
DATA CLEANING WITH PANDAS
Dealing with Multiple Files
Often, you have the same data separated out into multiple files.

Let’s say that we have a ton of files following the filename structure: 'file1.csv', 'file2.csv', 'file3.csv', and so on. The power of pandas is mainly in being able to manipulate large amounts of structured data, so we want to be able to get all of the relevant information into one table so that we can analyze the aggregate data.

We can combine the use of glob, a Python library for working with files, with pandas to organize this data better. glob can open multiple files by using regex matching to get the filenames:
'''
import glob

files = glob.glob("file*.csv")

df_list = []
for filename in files:
  data = pd.read_csv(filename)
  df_list.append(data)

df = pd.concat(df_list)

print(files)'''
This code goes through any file that starts with 'file' and has an extension of .csv. It opens each file, reads the data into a DataFrame, and then concatenates all of those DataFrames together.'''

import codecademylib3_seaborn
import pandas as pd
import glob

student_files  =  glob.glob("exams*.csv")

df_list = []
for filename in student_files:
  data = pd.read_csv(filename)
  df_list.append(data)

students = pd.concat(df_list)

print(students)
print(len(students))

'''

DATA CLEANING WITH PANDAS
Reshaping your Data
Since we want

Each variable as a separate column
Each row as a separate observation
We would want to reshape a table like:

Account	Checking	Savings
“12456543”	8500	8900
“12283942”	6410	8020
“12839485”	78000	92000

Into a table that looks more like:
Account	Account Type	Amount
“12456543”	“Checking”	8500
“12456543”	“Savings”	8900
“12283942”	“Checking”	6410
“12283942”	“Savings”	8020
“12839485”	“Checking”	78000
“12839485”	“Savings”	920000

We can use pd.melt() to do this transformation. .melt() takes in a DataFrame, and the columns to unpack:
'''
pd.melt(frame=df, id_vars='name', value_vars=['Checking','Savings'], value_name="Amount", var_name="Account Type")
'''
The parameters you provide are:

frame: the DataFrame you want to melt
id_vars: the column(s) of the old DataFrame to preserve
value_vars: the column(s) of the old DataFrame that you want to turn into variables
value_name: what to call the column of the new DataFrame that stores the values
var_name: what to call the column of the new DataFrame that stores the variables
The default names may work in certain situations, but it’s best to always have data that is self-explanatory. Thus, we often use .columns() to rename the columns after melting:
'''
df.columns(["Account", "Account Type", "Amount"])
'''
'''


import codecademylib3_seaborn
import pandas as pd
from students import students

print(students.columns)
students = pd.melt(frame=students, id_vars=['full_name','gender_age','grade'], value_vars=['fractions', 'probability'], value_name='score', var_name='exam')

print(students.head())
print(students.columns)
print(students.exam.value_counts())


'''
DATA CLEANING WITH PANDAS
Dealing with Duplicates
Often we see duplicated rows of data in the DataFrames we are working with. This could happen due to errors in data collection or in saving and loading the data.

To check for duplicates, we can use the pandas function .duplicated(), which will return a Series telling us which rows are duplicate rows.

Let’s say we have a DataFrame fruits that represents this table:

item	price	calories
“banana”	“$1”	105
“apple”	“$0.75”	95
“apple”	“$0.75”	95
“peach”	“$3”	55
“peach”	“$4”	55
“clementine”	“$2.5”	35

If we call '''fruits.duplicated()''', we would get the following table:

id	value
0	False
1	False
2	True
3	False
4	False
5	False

We can see that row 2, which represents an "apple" with price "$0.75" and 95 calories, is a duplicate row. Every value in this row is the same as in another row.

We can use the pandas .drop_duplicates() function to remove all rows that are duplicates of another row.

If we call '''fruits.drop_duplicates()''', we would get the table:

item	price	calories
“banana”	“$1”	105
“apple”	“$0.75”	95
“peach”	“$3”	55
“peach”	“$4”	55
“clementine”	“$2.5”	35

The "apple" row was deleted because it was exactly the same as another row. But the two "peach" rows remain because there is a difference in the price column.

If we wanted to remove every row with a duplicate value in the item column, we could specify a subset:
'''
fruits = fruits.drop_duplicates(subset=['item'])'''
By default, this keeps the first occurrence of the duplicate:

item	price	calories
“banana”	“$1”	105
“apple”	“$0.75”	95
“peach”	“$3”	55
“clementine”	“$2.5”	35

Make sure that the columns you drop duplicates from are specifically the ones where duplicates don’t belong. You wouldn’t want to drop duplicates with the price column as a subset, for example, because it’s okay if multiple items cost the same amount!'''


import codecademylib3_seaborn
import pandas as pd
from students import students

#print(students)

duplicates = students.duplicated()
print(duplicates)
print(duplicates.value_counts)
print(duplicates.unique())

students = students.drop_duplicates()
print(students)

duplicates = students.duplicated()

'''
DATA CLEANING WITH PANDAS
Splitting by Index
In trying to get clean data, we want to make sure each column represents one type of measurement. Often, multiple measurements are recorded in the same column, and we want to separate these out so that we can do individual analysis on each variable.

Let’s say we have a column “birthday” with data formatted in MMDDYYYY format. In other words, “11011993” represents a birthday of November 1, 1993. We want to split this data into day, month, and year so that we can use these columns as separate features.

In this case, we know the exact structure of these strings. The first two characters will always correspond to the month, the second two to the day, and the rest of the string will always correspond to year. We can easily break the data into three separate columns by splitting the strings using .str:'''

# Create the 'month' column
df['month'] = df.birthday.str[0:2]

# Create the 'day' column
df['day'] = df.birthday.str[2:4]

# Create the 'year' column
df['year'] = df.birthday.str[4:]'''
The first command takes the first two characters of each value in the birthday column and puts it into a month column. The second command takes the second two characters of each value in the birthday column and puts it into a day column. The third command takes the rest of each value in the birthday column and puts it into a year column.

This would transform a table like:

id	birthday
1011	“12241989”
1112	“10311966”
1113	“01052011”

into a table like:

id	birthday	month	day	year
1011	“12241989”	“12”	“24”	“1989”
1112	“10311966”	“10”	“31”	“1966”
1113	“01052011”	“01”	“05”	“2011”

We will practice changing string columns into numerical columns (like converting "10" to 10) in a future exercise.'''

'''
Instructions
1.
Print out the columns of the students DataFrame.

2.
The column gender_age sounds like it contains both gender and age!

Print out the .head() of the column to see what kind of data it contains.


Hint
To print out the head of a column, we can use the pandas syntax:

print(df.column_name.head())
or:

print(df['column_name'].head())
3.
It looks like the first character of the values in gender_age contains the gender, while the rest of the string contains the age. Let’s separate out the gender data into a new column called gender.


Hint
To create a new column in pandas, you can use the syntax:

df.new_column_name = new_values
or:

df['new_column_name'] = new_values
4.
Now, separate out the age data into a new column called age.


Hint
The age is the rest of the gender_age string!

students.gender_age.str[1:]
This line of code takes everything after the first character of each string in gender_age.

5.
Good job! Let’s print the .head() of students to see how the DataFrame looks after our creation of new columns.

6.
Now, we don’t need that gender_age column anymore.

Let’s set the students DataFrame to be the students DataFrame with all columns except gender_age.


Hint
To select a subset of columns in pandas, you can use the notation:
'''
df[['column1', 'column2', 'column3']]'''
This will return a DataFrame containing just column1, column2 and column3 of the original DataFrame.'''


import codecademylib3_seaborn
import pandas as pd
from students import students

print(students.columns)

print(students.head())

students['gender'] = students.gender_age.str[0]
students['age'] = students.gender_age.str[1:]

print(students.head())
print(students.columns)

students = students [['full_name',  'grade', 'exam', 'score', 'gender', 'age']]

'''
DATA CLEANING WITH PANDAS
Splitting by Character
Let’s say we have a column called “type” with data entries in the format "admin_US" or "user_Kenya". Just like we saw before, this column actually contains two types of data. One seems to be the user type (with values like “admin” or “user”) and one seems to be the country this user is in (with values like “US” or “Kenya”).

We can no longer just split along the first 4 characters because admin and user are of different lengths. Instead, we know that we want to split along the "_". Using that, we can split this column into two separate, cleaner columns:
'''

# Create the 'str_split' column
df['str_split'] = df.type.str.split('_')

# Create the 'usertype' column
df['usertype'] = df.str_split.str.get(0)

# Create the 'country' column
df['country'] = df.str_split.str.get(1)'''
This would transform a table like:

id	type
1011	“user_Kenya”
1112	“admin_US”
1113	“moderator_UK”

into a table like:

id	type	country	usertype
1011	“user_Kenya”	“Kenya”	“user”
1112	“admin_US”	“US”	“admin”
1113	“moderator_UK”	“UK”	“moderator”'''

'''
Instructions
1.
The students’ names are stored in a column called full_name.

We want to separate this data out into two new columns, first_name and last_name.

First, let’s create a Series object called name_split that splits the full_name by the " " character.

2.
Now, let’s create a column called first_name that takes the first item in name_split.

3.
Finally, let’s create a column called last_name that takes the second item in name_split.

4.
Print out the .head() of students to see how the DataFrame has changed.'''

import codecademylib3_seaborn
import pandas as pd
from students import students

print(students)

name_split = students.full_name.str.split()

students['first_name'] = name_split.str[0]
students['last_name'] = name_split.str[1]

print(students.head())

'''
DATA CLEANING WITH PANDAS
Looking at Types
Each column of a DataFrame can hold items of the same data type or dtype. The dtypes that pandas uses are: float, int, bool, datetime, timedelta, category and object. Often, we want to convert between types so that we can do better analysis. If a numerical category like "num_users" is stored as a Series of objects instead of ints, for example, it makes it more difficult to do something like make a line graph of users over time.

To see the types of each column of a DataFrame, we can use:

print(df.dtypes)
For a DataFrame like this:

item	price	calories
“banana”	“$1”	105
“apple”	“$0.75”	95
“peach”	“$3”	55
“clementine”	“$2.5”	35

the .dtypes attribute would be:

item        object
price       object
calories     int64
dtype: object
We can see that the dtype of the dtypes attribute itself is an object! It is a Series object, which you have already worked with. Series objects compose all DataFrames.

We can see that the price column is made up of objects, which will probably make our analysis of price more difficult. We’ll look at how to convert columns into numeric values in the next few exercises.'''

'''DATA CLEANING WITH PANDAS
String Parsing
Sometimes we need to modify strings in our DataFrames to help us transform them into more meaningful metrics. For example, in our fruits table from before:

item	price	calories
“banana”	“$1”	105
“apple”	“$0.75”	95
“peach”	“$3”	55
“peach”	“$4”	55
“clementine”	“$2.5”	35

We can see that the 'price' column is actually composed of strings representing dollar amounts. This column could be much better represented in floats, so that we could take the mean, calculate other aggregate statistics, or compare different fruits to one another in terms of price.

First, we can use what we know of regex to get rid of all of the dollar signs:

fruit.price = fruit['price'].replace('[\$,]', '', regex=True)
Then, we can use the pandas function .to_numeric() to convert strings containing numerical values to integers or floats:

fruit.price = pd.to_numeric(fruit.price)
Now, we have a DataFrame that looks like:

item	price	calories
“banana”	1	105
“apple”	0.75	95
“peach”	3	55
“peach”	4	55
“clementine”	2.5	35

'''

import codecademylib3_seaborn
import pandas as pd
from students import students

print(students.head())

students['score'] = students['score'].replace('[\%]', '', regex = True)

students['score'] = pd.to_numeric(students['score'])

'''
DATA CLEANING WITH PANDAS
More String Parsing
Sometimes we want to do analysis on numbers that are hidden within string values. We can use regex to extract this numerical data from the strings they are trapped in. Suppose we had this DataFrame df representing a workout regimen:

date	exerciseDescription
10/18/2018	“lunges - 30 reps”
10/18/2018	“squats - 20 reps”
10/18/2018	“deadlifts - 25 reps”
10/18/2018	“jumping jacks - 30 reps”
10/19/2018	“lunges - 40 reps”
10/19/2018	“chest flyes - 15 reps”
…	…

It would be helpful to separate out data like "30 lunges" into 2 columns with the number of reps, "30", and the type of exercise, "lunges". Then, we could compare the increase in the number of lunges done over time, for example.

To extract the numbers from the string we can use pandas’ .str.split() function:
'''
split_df = df['exerciseDescription'].str.split('(\d+)', expand=True)
'''
which would result in this DataFrame split_df:

* *	0	1	2
0	“lunges - “	“30”	“reps”
1	“squats - “	“20”	“reps”
2	“deadlifts - “	“25”	“reps”
3	“jumping jacks - “	“30”	“reps”
4	“lunges - “	“40”	“reps”
5	“chest flyes - “	“15”	“reps”
…	…	…	…

Then, we can assign columns from this DataFrame to the original df:
'''
df.reps = pd.to_numeric(split_df[1])
df.exercise = split_df[2].replace('[\- ]', '', regex=True)'''
Now, our df looks like this:

date	exerciseDescription	reps	exercise
10/18/2018	“lunges - 30 reps”	30	“lunges”
10/18/2018	“squats - 20 reps”	20	“squats”
10/18/2018	“deadlifts - 25 reps”	25	“deadlifts”
10/18/2018	“jumping jacks - 30 reps”	30	“jumping jacks”
10/19/2018	“lunges - 40 reps”	40	“lunges”
10/19/2018	“chest flyes - 15 reps”	15	“chest flyes”
…	…	…	…
'''

import codecademylib3_seaborn
import pandas as pd
from students import students

#print(students)
print(students.grade.head())

grade_split = students['grade'].str.split('(\d+)', expand = True)
students['grade'] = grade_split[1]

print(students.dtypes)

students.grade = pd.to_numeric(students.grade)
avg_grade =students.grade.mean()

'''
DATA CLEANING WITH PANDAS
Missing Values
We often have data with missing elements, as a result of a problem with the data collection process or errors in the way the data was stored. The missing elements normally show up as NaN (or Not a Number) values:

day	bill	tip	num_guests
“Mon”	10.1	1	1
“Mon”	20.75	5.5	2
“Tue”	19.95	5.5	NaN
“Wed”	44.10	15	3
“Wed”	NaN	1	1

The num_guests value for the 3rd row is missing, and the bill value for the 5th row is missing. Some calculations we do will just skip the NaN values, but some calculations or visualizations we try to perform will break when a NaN is encountered.

Most of the time, we use one of two methods to deal with missing values.

Method 1: drop all of the rows with a missing value
We can use .dropna() to do this:
'''
bill_df = bill_df.dropna()'''
This command will result in the DataFrame without the incomplete rows:

day	bill	tip	num_guests
“Mon”	10.1	1	1
“Mon”	20.75	5.5	2
“Wed”	44.10	15	3

If we wanted to remove every row with a NaN value in the num_guests column only, we could specify a subset:
'''
bill_df = bill_df.dropna(subset=['num_guests'])'''
Method 2: fill the missing values with the mean of the column, or with some other aggregate value.
We can use .fillna() to do this:
'''
bill_df = bill_df.fillna(value={"bill":bill_df.bill.mean(), "num_guests":bill_df.num_guests.mean()})'''
This command will result in the DataFrame with the respective mean of the column in the place of the original NaNs:

bill_df = bill_df.fillna(100)
this command will fill all NaN to 100

day	bill	tip	num_guests
“Mon”	10.1	1	1
“Mon”	20.75	5.5	2
“Tue”	19.95	5.5	1.75
“Wed”	44.10	15	3
“Wed”	23.725	1	1
'''
import codecademylib3_seaborn
import pandas as pd
from students import students

print(students)

score_mean = students.score.mean()

students = students.fillna(value = {'score':0})

score_mean_2 = students.score.mean()

print(score_mean, score_mean_2)


#-------------- project ------------------ 

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import codecademylib3_seaborn
import glob

files = glob.glob('states*.csv')

df_list = []
for filename in files:
  data = pd.read_csv(filename)
  df_list.append(data)

df = pd.concat(df_list)

#print(df.head(20))
#print(df.info())
#print(df.describe())
#print(df.columns)

df['Income'] = df['Income'].replace('[\$]', '', regex = True)
df['Income'] = pd.to_numeric(df['Income'])



split_df = df['GenderPop'].str.split('_', expand = True)
split_df[0] = split_df[0].replace('[M]', '', regex = True)
split_df[1] = split_df[1].replace('[F]', '', regex = True)
split_df[0] = pd.to_numeric(split_df[0])
split_df[1] = pd.to_numeric(split_df[1])

df['Male'] = split_df[0]
df['Female'] = split_df[1]



df = df.reset_index()

df = df.drop(columns = [ 'Unnamed: 0', 'GenderPop'])


values = {'Female': (df.TotalPop - df.Male)}
df = df.fillna(value=values)


print(df['State'].duplicated())
df = df.drop_duplicates(['State'])


print(df)

plt.scatter(df.TotalPop,df.Income)
plt.show()
#print (df.Female, df.Income)






#open datasets for test # Python Practice Project

https://www.kaggle.com/datasets


#----------------PROJECT PORJECT PROJECT DATA ANALYSIS CAPSTONE PROJECTS ---------------------

#!/usr/bin/env python
# coding: utf-8

# # Capstone 2: Biodiversity Project

# # Introduction
# You are a biodiversity analyst working for the National Parks Service.  You're going to help them analyze some data about species at various national parks.
# 
# Note: The data that you'll be working with for this project is *inspired* by real data, but is mostly fictional.

# # Step 1
# Import the modules that you'll be using in this assignment:
# - `from matplotlib import pyplot as plt`
# - `import pandas as pd`

# In[1]:


from matplotlib import pyplot as plt
import pandas as pd


# # Step 2
# You have been given two CSV files. `species_info.csv` with data about different species in our National Parks, including:
# - The scientific name of each species
# - The common names of each species
# - The species conservation status
# 
# Load the dataset and inspect it:
# - Load `species_info.csv` into a DataFrame called `species`

# In[2]:


species = pd.read_csv('species_info.csv')


# Inspect each DataFrame using `.head()`.

# In[3]:


species.head()


# # Step 3
# Let's start by learning a bit more about our data.  Answer each of the following questions.

# How many different species are in the `species` DataFrame?

# In[4]:


species.scientific_name.nunique()


# What are the different values of `category` in `species`?

# In[5]:


species.category.unique()


# What are the different values of `conservation_status`?

# In[6]:


species.conservation_status.unique()


# # Step 4
# Let's start doing some analysis!
# 
# The column `conservation_status` has several possible values:
# - `Species of Concern`: declining or appear to be in need of conservation
# - `Threatened`: vulnerable to endangerment in the near future
# - `Endangered`: seriously at risk of extinction
# - `In Recovery`: formerly `Endangered`, but currnetly neither in danger of extinction throughout all or a significant portion of its range
# 
# We'd like to count up how many species meet each of these criteria.  Use `groupby` to count how many `scientific_name` meet each of these criteria.

# In[7]:


species.groupby('conservation_status').scientific_name.nunique().reset_index()


# As we saw before, there are far more than 200 species in the `species` table.  Clearly, only a small number of them are categorized as needing some sort of protection.  The rest have `conservation_status` equal to `None`.  Because `groupby` does not include `None`, we will need to fill in the null values.  We can do this using `.fillna`.  We pass in however we want to fill in our `None` values as an argument.
# 
# Paste the following code and run it to see replace `None` with `No Intervention`:
# ```python
# species.fillna('No Intervention', inplace=True)
# ```

# In[8]:


species.fillna('No Intervention', inplace=True)


# Great! Now run the same `groupby` as before to see how many species require `No Protection`.

# In[9]:


species.groupby('conservation_status').scientific_name.nunique().reset_index()


# Let's use `plt.bar` to create a bar chart.  First, let's sort the columns by how many species are in each categories.  We can do this using `.sort_values`.  We use the the keyword `by` to indicate which column we want to sort by.
# 
# Paste the following code and run it to create a new DataFrame called `protection_counts`, which is sorted by `scientific_name`:
# ```python
# protection_counts = species.groupby('conservation_status')\
#     .scientific_name.nunique().reset_index()\
#     .sort_values(by='scientific_name')
# ```

# In[11]:


protection_counts = species.groupby('conservation_status')    .scientific_name.nunique().reset_index()    .sort_values(by='scientific_name')


# Now let's create a bar chart!
# 1. Start by creating a wide figure with `figsize=(10, 4)`
# 1. Start by creating an axes object called `ax` using `plt.subplot`.
# 2. Create a bar chart whose heights are equal to `scientific_name` column of `protection_counts`.
# 3. Create an x-tick for each of the bars.
# 4. Label each x-tick with the label from `conservation_status` in `protection_counts`
# 5. Label the y-axis `Number of Species`
# 6. Title the graph `Conservation Status by Species`
# 7. Plot the grap using `plt.show()`

# In[12]:


plt.figure(figsize=(10, 4))
ax = plt.subplot()
plt.bar(range(len(protection_counts)),
        protection_counts.scientific_name.values)
ax.set_xticks(range(len(protection_counts)))
ax.set_xticklabels(protection_counts.conservation_status.values)
plt.ylabel('Number of Species')
plt.title('Conservation Status by Species')
plt.show()


# # Step 4
# Are certain types of species more likely to be endangered?

# Let's create a new column in `species` called `is_protected`, which is `True` if `conservation_status` is not equal to `No Intervention`, and `False` otherwise.

# In[13]:


species['is_protected'] = species.conservation_status != 'No Intervention'


# Let's group the `species` data frame by the `category` and `is_protected` columns and count the unique `scientific_name`s in each grouping.
# 
# Save your results to `category_counts`.

# In[14]:


category_counts = species.groupby(['category', 'is_protected'])                         .scientific_name.nunique().reset_index()


# Examine `category_counts` using `head()`.

# In[15]:


category_counts.head()


# It's going to be easier to view this data if we pivot it.  Using `pivot`, rearange `category_counts` so that:
# - `columns` is `is_protected`
# - `index` is `category`
# - `values` is `scientific_name`
# 
# Save your pivoted data to `category_pivot`. Remember to `reset_index()` at the end.

# In[16]:


category_pivot = category_counts.pivot(columns='is_protected',
                                      index='category',
                                      values='scientific_name')\
                                .reset_index()


# Examine `category_pivot`.

# In[17]:


category_pivot


# Use the `.columns` property to  rename the categories `True` and `False` to something more description:
# - Leave `category` as `category`
# - Rename `False` to `not_protected`
# - Rename `True` to `protected`

# In[18]:


category_pivot.columns = ['category', 'not_protected', 'protected']


# Let's create a new column of `category_pivot` called `percent_protected`, which is equal to `protected` (the number of species that are protected) divided by `protected` plus `not_protected` (the total number of species).

# In[19]:


category_pivot['percent_protected'] = category_pivot.protected /                                       (category_pivot.protected + category_pivot.not_protected)


# Examine `category_pivot`.

# In[20]:


category_pivot


# It looks like species in category `Mammal` are more likely to be endangered than species in `Bird`.  We're going to do a significance test to see if this statement is true.  Before you do the significance test, consider the following questions:
# - Is the data numerical or categorical?
# - How many pieces of data are you comparing?

# Based on those answers, you should choose to do a *chi squared test*.  In order to run a chi squared test, we'll need to create a contingency table.  Our contingency table should look like this:
# 
# ||protected|not protected|
# |-|-|-|
# |Mammal|?|?|
# |Bird|?|?|
# 
# Create a table called `contingency` and fill it in with the correct numbers

# In[21]:


contingency = [[30, 146],
              [75, 413]]


# In order to perform our chi square test, we'll need to import the correct function from scipy.  Past the following code and run it:
# ```py
# from scipy.stats import chi2_contingency
# ```

# In[22]:


from scipy.stats import chi2_contingency


# Now run `chi2_contingency` with `contingency`.

# In[23]:


chi2_contingency(contingency)


# It looks like this difference isn't significant!
# 
# Let's test another.  Is the difference between `Reptile` and `Mammal` significant?

# In[24]:


contingency = [[30, 146],
               [5, 73]]
chi2_contingency(contingency)


# Yes! It looks like there is a significant difference between `Reptile` and `Mammal`!

# # Step 5

# Conservationists have been recording sightings of different species at several national parks for the past 7 days.  They've saved sent you their observations in a file called `observations.csv`.  Load `observations.csv` into a variable called `observations`, then use `head` to view the data.

# In[25]:


observations = pd.read_csv('observations.csv')
observations.head()


# Some scientists are studying the number of sheep sightings at different national parks.  There are several different scientific names for different types of sheep.  We'd like to know which rows of `species` are referring to sheep.  Notice that the following code will tell us whether or not a word occurs in a string:

# In[26]:


# Does "Sheep" occur in this string?
str1 = 'This string contains Sheep'
'Sheep' in str1


# In[27]:


# Does "Sheep" occur in this string?
str2 = 'This string contains Cows'
'Sheep' in str2


# Use `apply` and a `lambda` function to create a new column in `species` called `is_sheep` which is `True` if the `common_names` contains `'Sheep'`, and `False` otherwise.

# In[28]:


species['is_sheep'] = species.common_names.apply(lambda x: 'Sheep' in x)
species.head()


# Select the rows of `species` where `is_sheep` is `True` and examine the results.

# In[29]:


species[species.is_sheep]


# Many of the results are actually plants.  Select the rows of `species` where `is_sheep` is `True` and `category` is `Mammal`.  Save the results to the variable `sheep_species`.

# In[30]:


sheep_species = species[(species.is_sheep) & (species.category == 'Mammal')]
sheep_species


# Now merge `sheep_species` with `observations` to get a DataFrame with observations of sheep.  Save this DataFrame as `sheep_observations`.

# In[31]:


sheep_observations = observations.merge(sheep_species)
sheep_observations


# How many total sheep observations (across all three species) were made at each national park?  Use `groupby` to get the `sum` of `observations` for each `park_name`.  Save your answer to `obs_by_park`.
# 
# This is the total number of sheep observed in each park over the past 7 days.

# In[32]:


obs_by_park = sheep_observations.groupby('park_name').observations.sum().reset_index()
obs_by_park


# Create a bar chart showing the different number of observations per week at each park.
# 
# 1. Start by creating a wide figure with `figsize=(16, 4)`
# 1. Start by creating an axes object called `ax` using `plt.subplot`.
# 2. Create a bar chart whose heights are equal to `observations` column of `obs_by_park`.
# 3. Create an x-tick for each of the bars.
# 4. Label each x-tick with the label from `park_name` in `obs_by_park`
# 5. Label the y-axis `Number of Observations`
# 6. Title the graph `Observations of Sheep per Week`
# 7. Plot the grap using `plt.show()`

# In[33]:


plt.figure(figsize=(16, 4))
ax = plt.subplot()
plt.bar(range(len(obs_by_park)),
        obs_by_park.observations.values)
ax.set_xticks(range(len(obs_by_park)))
ax.set_xticklabels(obs_by_park.park_name.values)
plt.ylabel('Number of Observations')
plt.title('Observations of Sheep per Week')
plt.show()


# Our scientists know that 15% of sheep at Bryce National Park have foot and mouth disease.  Park rangers at Yellowstone National Park have been running a program to reduce the rate of foot and mouth disease at that park.  The scientists want to test whether or not this program is working.  They want to be able to detect reductions of at least 5 percentage point.  For instance, if 10% of sheep in Yellowstone have foot and mouth disease, they'd like to be able to know this, with confidence.
# 
# Use the <a href="https://s3.amazonaws.com/codecademy-content/courses/learn-hypothesis-testing/a_b_sample_size/index.html">Codecademy sample size calculator</a> to calculate the number of sheep that they would need to observe from each park.  Use the default level of significance (90%).
# 
# Remember that "Minimum Detectable Effect" is a percent of the baseline.

# In[1]:


minimum_detectable_effect = 100 * 0.05 / 0.15
minimum_detectable_effect


# In[2]:


baseline = 15


# In[4]:


sample_size_per_variant = 870
# Note: This could be 890 if you used 33% for the "Minimum Detectable Effect" instead of 33.33%.  That's fine.


# How many weeks would you need to observe sheep at Bryce National Park in order to observe enough sheep?  How many weeks would you need to observe at Yellowstone National Park to observe enough sheep?

# In[7]:


bryce = 870 / 250.
yellowstone = 810 / 507.

# Approximately 3.5 weeks at Bryce and 1.5 weeks at Yellowstone.


# In[ ]:



#----------------PROJECT PORJECT PROJECT DATA ANALYSIS CAPSTONE PROJECTS ---------------------

#!/usr/bin/env python
# coding: utf-8

# # Capstone Project 1: MuscleHub AB Test

# ## Step 1: Get started with SQL

# Like most businesses, Janet keeps her data in a SQL database.  Normally, you'd download the data from her database to a csv file, and then load it into a Jupyter Notebook using Pandas.
# 
# For this project, you'll have to access SQL in a slightly different way.  You'll be using a special Codecademy library that lets you type SQL queries directly into this Jupyter notebook.  You'll have pass each SQL query as an argument to a function called `sql_query`.  Each query will return a Pandas DataFrame.  Here's an example:

# In[1]:


# This import only needs to happen once, at the beginning of the notebook
from codecademySQL import sql_query


# In[2]:


# Here's an example of a query that just displays some data
sql_query('''
SELECT *
FROM visits
LIMIT 5
''')


# In[3]:


# Here's an example where we save the data to a DataFrame
df = sql_query('''
SELECT *
FROM applications
LIMIT 5
''')


# ## Step 2: Get your dataset

# Let's get started!
# 
# Janet of MuscleHub has a SQLite database, which contains several tables that will be helpful to you in this investigation:
# - `visits` contains information about potential gym customers who have visited MuscleHub
# - `fitness_tests` contains information about potential customers in "Group A", who were given a fitness test
# - `applications` contains information about any potential customers (both "Group A" and "Group B") who filled out an application.  Not everyone in `visits` will have filled out an application.
# - `purchases` contains information about customers who purchased a membership to MuscleHub.
# 
# Use the space below to examine each table.

# In[4]:


# Examine visits here
sql_query('''
SELECT *
FROM visits
LIMIT 5
''')


# In[5]:


# Examine fitness_tests here
sql_query('''
SELECT *
FROM fitness_tests
LIMIT 5
''')


# In[6]:


# Examine applications here
sql_query('''
SELECT *
FROM applications
LIMIT 5
''')


# In[7]:


# Examine purchases here
sql_query('''
SELECT *
FROM purchases
LIMIT 5
''')


# We'd like to download a giant DataFrame containing all of this data.  You'll need to write a query that does the following things:
# 
# 1. Not all visits in  `visits` occurred during the A/B test.  You'll only want to pull data where `visit_date` is on or after `7-1-17`.
# 
# 2. You'll want to perform a series of `LEFT JOIN` commands to combine the four tables that we care about.  You'll need to perform the joins on `first_name`, `last_name`, and `email`.  Pull the following columns:
# 
# 
# - `visits.first_name`
# - `visits.last_name`
# - `visits.gender`
# - `visits.email`
# - `visits.visit_date`
# - `fitness_tests.fitness_test_date`
# - `applications.application_date`
# - `purchases.purchase_date`
# 
# Save the result of this query to a variable called `df`.
# 
# Hint: your result should have 5004 rows.  Does it?

# In[8]:


df = sql_query('''
SELECT visits.first_name,
       visits.last_name,
       visits.visit_date,
       fitness_tests.fitness_test_date,
       applications.application_date,
       purchases.purchase_date
FROM visits
LEFT JOIN fitness_tests
    ON fitness_tests.first_name = visits.first_name
    AND fitness_tests.last_name = visits.last_name
    AND fitness_tests.email = visits.email
LEFT JOIN applications
    ON applications.first_name = visits.first_name
    AND applications.last_name = visits.last_name
    AND applications.email = visits.email
LEFT JOIN purchases
    ON purchases.first_name = visits.first_name
    AND purchases.last_name = visits.last_name
    AND purchases.email = visits.email
WHERE visits.visit_date >= '7-1-17'
''')


# ## Step 3: Investigate the A and B groups

# We have some data to work with! Import the following modules so that we can start doing analysis:
# - `import pandas as pd`
# - `from matplotlib import pyplot as plt`

# In[9]:


import pandas as pd
from matplotlib import pyplot as plt


# We're going to add some columns to `df` to help us with our analysis.
# 
# Start by adding a column called `ab_test_group`.  It should be `A` if `fitness_test_date` is not `None`, and `B` if `fitness_test_date` is `None`.

# In[10]:


df['ab_test_group'] = df.fitness_test_date.apply(lambda x:
                                                'A' if pd.notnull(x) else 'B')


# Let's do a quick sanity check that Janet split her visitors such that about half are in A and half are in B.
# 
# Start by using `groupby` to count how many users are in each `ab_test_group`.  Save the results to `ab_counts`.

# In[11]:


ab_counts = df.groupby('ab_test_group').first_name.count().reset_index()
ab_counts


# We'll want to include this information in our presentation.  Let's create a pie cart using `plt.pie`.  Make sure to include:
# - Use `plt.axis('equal')` so that your pie chart looks nice
# - Add a legend labeling `A` and `B`
# - Use `autopct` to label the percentage of each group
# - Save your figure as `ab_test_pie_chart.png`

# In[12]:


plt.pie(ab_counts.first_name.values, labels=['A', 'B'], autopct='%0.2f%%')
plt.axis('equal')
plt.show()
plt.savefig('ab_test_pie_chart.png')


# ## Step 4: Who picks up an application?

# Recall that the sign-up process for MuscleHub has several steps:
# 1. Take a fitness test with a personal trainer (only Group A)
# 2. Fill out an application for the gym
# 3. Send in their payment for their first month's membership
# 
# Let's examine how many people make it to Step 2, filling out an application.
# 
# Start by creating a new column in `df` called `is_application` which is `Application` if `application_date` is not `None` and `No Application`, otherwise.

# In[13]:


df['is_application'] = df.application_date.apply(lambda x: 'Application'
                                                  if pd.notnull(x) else 'No Application')


# Now, using `groupby`, count how many people from Group A and Group B either do or don't pick up an application.  You'll want to group by `ab_test_group` and `is_application`.  Save this new DataFrame as `app_counts`

# In[14]:


app_counts = df.groupby(['ab_test_group', 'is_application'])               .first_name.count().reset_index()


# We're going to want to calculate the percent of people in each group who complete an application.  It's going to be much easier to do this if we pivot `app_counts` such that:
# - The `index` is `ab_test_group`
# - The `columns` are `is_application`
# Perform this pivot and save it to the variable `app_pivot`.  Remember to call `reset_index()` at the end of the pivot!

# In[15]:


app_pivot = app_counts.pivot(columns='is_application',
                            index='ab_test_group',
                            values='first_name')\
            .reset_index()
app_pivot


# Define a new column called `Total`, which is the sum of `Application` and `No Application`.

# In[16]:


app_pivot['Total'] = app_pivot.Application + app_pivot['No Application']


# Calculate another column called `Percent with Application`, which is equal to `Application` divided by `Total`.

# In[17]:


app_pivot['Percent with Application'] = app_pivot.Application / app_pivot.Total
app_pivot


# It looks like more people from Group B turned in an application.  Why might that be?
# 
# We need to know if this difference is statistically significant.
# 
# Choose a hypothesis tests, import it from `scipy` and perform it.  Be sure to note the p-value.
# Is this result significant?

# In[18]:


from scipy.stats import chi2_contingency

contingency = [[250, 2254], [325, 2175]]
chi2_contingency(contingency)


# ## Step 4: Who purchases a membership?

# Of those who picked up an application, how many purchased a membership?
# 
# Let's begin by adding a column to `df` called `is_member` which is `Member` if `purchase_date` is not `None`, and `Not Member` otherwise.

# In[19]:


df['is_member'] = df.purchase_date.apply(lambda x: 'Member' if pd.notnull(x) else 'Not Member')


# Now, let's create a DataFrame called `just_apps` the contains only people who picked up an application.

# In[20]:


just_apps = df[df.is_application == 'Application']


# Great! Now, let's do a `groupby` to find out how many people in `just_apps` are and aren't members from each group.  Follow the same process that we did in Step 4, including pivoting the data.  You should end up with a DataFrame that looks like this:
# 
# |is_member|ab_test_group|Member|Not Member|Total|Percent Purchase|
# |-|-|-|-|-|-|
# |0|A|?|?|?|?|
# |1|B|?|?|?|?|
# 
# Save your final DataFrame as `member_pivot`.

# In[21]:


member_count = just_apps.groupby(['ab_test_group', 'is_member'])                 .first_name.count().reset_index()
member_pivot = member_count.pivot(columns='is_member',
                                  index='ab_test_group',
                                  values='first_name')\
                           .reset_index()

member_pivot['Total'] = member_pivot.Member + member_pivot['Not Member']
member_pivot['Percent Purchase'] = member_pivot.Member / member_pivot.Total
member_pivot


# It looks like people who took the fitness test were more likely to purchase a membership **if** they picked up an application.  Why might that be?
# 
# Just like before, we need to know if this difference is statistically significant.  Choose a hypothesis tests, import it from `scipy` and perform it.  Be sure to note the p-value.
# Is this result significant?

# In[22]:


contingency = [[200, 50], [250, 75]]
chi2_contingency(contingency)


# Previously, we looked at what percent of people **who picked up applications** purchased memberships.  What we really care about is what percentage of **all visitors** purchased memberships.  Return to `df` and do a `groupby` to find out how many people in `df` are and aren't members from each group.  Follow the same process that we did in Step 4, including pivoting the data.  You should end up with a DataFrame that looks like this:
# 
# |is_member|ab_test_group|Member|Not Member|Total|Percent Purchase|
# |-|-|-|-|-|-|
# |0|A|?|?|?|?|
# |1|B|?|?|?|?|
# 
# Save your final DataFrame as `final_member_pivot`.

# In[23]:


final_member_count = df.groupby(['ab_test_group', 'is_member'])                 .first_name.count().reset_index()
final_member_pivot = final_member_count.pivot(columns='is_member',
                                  index='ab_test_group',
                                  values='first_name')\
                           .reset_index()

final_member_pivot['Total'] = final_member_pivot.Member + final_member_pivot['Not Member']
final_member_pivot['Percent Purchase'] = final_member_pivot.Member / final_member_pivot.Total
final_member_pivot


# Previously, when we only considered people who had **already picked up an application**, we saw that there was no significant difference in membership between Group A and Group B.
# 
# Now, when we consider all people who **visit MuscleHub**, we see that there might be a significant different in memberships between Group A and Group B.  Perform a significance test and check.

# In[24]:


contingency = [[200, 2304], [250, 2250]]
chi2_contingency(contingency)


# ## Step 5: Summarize the acquisition funel with a chart

# We'd like to make a bar chart for Janet that shows the difference between Group A (people who were given the fitness test) and Group B (people who were not given the fitness test) at each state of the process:
# - Percent of visitors who apply
# - Percent of applicants who purchase a membership
# - Percent of visitors who purchase a membership
# 
# Create one plot for **each** of the three sets of percentages that you calculated in `app_pivot`, `member_pivot` and `final_member_pivot`.  Each plot should:
# - Label the two bars as `Fitness Test` and `No Fitness Test`
# - Make sure that the y-axis ticks are expressed as percents (i.e., `5%`)
# - Have a title

# In[25]:


# Percent of Visitors who Apply
ax = plt.subplot()
plt.bar(range(len(app_pivot)),
       app_pivot['Percent with Application'].values)
ax.set_xticks(range(len(app_pivot)))
ax.set_xticklabels(['Fitness Test', 'No Fitness Test'])
ax.set_yticks([0, 0.05, 0.10, 0.15, 0.20])
ax.set_yticklabels(['0%', '5%', '10%', '15%', '20%'])
plt.show()
plt.savefig('percent_visitors_apply.png')


# In[26]:


# Percent of Applicants who Purchase
ax = plt.subplot()
plt.bar(range(len(member_pivot)),
       member_pivot['Percent Purchase'].values)
ax.set_xticks(range(len(app_pivot)))
ax.set_xticklabels(['Fitness Test', 'No Fitness Test'])
ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])
ax.set_yticklabels(['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])
plt.show()
plt.savefig('percent_apply_purchase.png')


# In[27]:


# Percent of Visitors who Purchase
ax = plt.subplot()
plt.bar(range(len(final_member_pivot)),
       final_member_pivot['Percent Purchase'].values)
ax.set_xticks(range(len(app_pivot)))
ax.set_xticklabels(['Fitness Test', 'No Fitness Test'])
ax.set_yticks([0, 0.05, 0.10, 0.15, 0.20])
ax.set_yticklabels(['0%', '5%', '10%', '15%', '20%'])
plt.show()
plt.savefig('percent_visitors_purchase.png')



























    



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT GIT #-#-#-#-#-#
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
git init
#initialize git

git status

git add filename [can be multiple filename s]
git add . # add every file in the staging area

#(use "git restore --staged <file>..." to unstage)


git diff filename
#compare working directory and staging area
#can also use "git diff" to check last changes

git commit -m "Complete first line of dialogue"

git log

#----------------

git init creates a new Git repository
git status inspects the contents of the working directory and staging area
git add adds files from the working directory to the staging area
git diff shows the difference between the working directory and the staging area
git commit permanently stores file changes from the staging area in the repository
git log shows a list of all previous commits

#----------------------

git remote add origin https://github.com/guanxv/projectname.git
git push -u origin master
git pull # bring the change from original

git show HEAD # show the latested changes
git checkout HEAD filename#The command will restore the file in your working directory to look exactly as it did when you last made a commit.
git checkout -- filename # exactly same function as above

git reset HEAD filename
#This command resets the file in the staging area to be the same as the HEAD commit. It does not discard file changes from the working directory, it just removes them from the staging area.

git reset commit_SHA
#This command works by using the first 7 characters of the SHA of a previous commit. For example, if the SHA of the previous commit is 5d692065cf51a2f50ea8e7b19b5a7ae512f633ba , use 5d69206

#HEAD is most recent changes


git checkout HEAD filename: Discards changes in the working directory.
git reset HEAD filename: Unstages file changes in the staging area.
git reset commit_SHA: Resets to a previous commit in your commit history.

git branch
#show current branch

git branch new_branch 
#to create a new branch,  branch names can’t contain whitespaces

git checkout branch_name
#switch to branch

git merge branchF_name
#merging the branch into master

#Your goal is to update master with changes you made to fencing.
#fencing is the giver branch, since it provides the changes.
#master is the receiver branch, since it accepts those changes.

git branch -d branch_name
#delete branch

git branch#: Lists all a Git project’s branches.
git branch branch_name#: Creabites a new branch.
git checkout branch_name#: Used to switch from one branch to another.
git merge branch_name#: Used to join file changes from one branch to another.
git branch -d branch_name#: Deletes the branch specified.

git clone remote_location clone_name
#remote_location tells Git where to go to find the remote. This could be a web address, or a filepath, such as:
#/Users/teachers/Documents/some-remote

#clone_name is the name you give to the directory in which Git will clone the repository.
 
git remote -v
#Git lists the name of the remote, origin, as well as its location.
#Git automatically names this remote origin, because it refers to the remote repository of origin. However, it is possible to safely change its name.
#The remote is listed twice: once for (fetch) and once for (push). We’ll learn about these later in the lesson.

git fetch
#This command will not merge changes from the remote into your local repository. It brings those changes onto what’s called a remote branch.


#Now we’ll use the git merge command to integrate origin/master into your local master branch. The command:

git merge origin/master
#Even though Sally’s new commits have been fetched to your local copy of the Git project, those commits are on the origin/master branch. Your local master branch has not been updated yet, so you can’t view or make changes to any of the work she has added.

#The workflow for Git collaborations typically follows this order:

#Fetch and merge changes from the remote
#Create a branch to work on a new project feature
#Develop the feature on your branch and commit your work
#Fetch and merge from the remote again (in case new commits were made while you were working)
#Push your branch up to the remote for review

git push origin your_branch_name
#will push your branch up to the remote, origin.

git clone#: Creates a local copy of a remote.
git remote -v#: Lists a Git project’s remotes.
git fetch#: Fetches work from the remote into the local copy.
git merge origin/master#: Merges origin/master into your local branch.
git push origin <branch_name>#: Pushes a local branch to the origin remote.


#bash command
cd ../veggie-clone


#git config

git config --global user.name "xxx"
git config --global user.email "xxx@gmail.com"

git config --list


#add a text file called .gitignore at the root of the porject can make git not monitoring certain files
.gitignore

#if the .gitignore added later , you have to run

git rm -rf --cached .  # remove everything from the repositroy
git add . # add everyting back, but under .gitignore control

#to make it work 





#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#-Venv python Virtual enviroment Venv #-#-#-#-#-#-#-#-#-
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

# in windows enviroment

'''
Why use Virtual Enviroment ?

in virtual enviroment, the package is installed only for that specific job. 
that prevent things happend like when you update your certain package, and old script not working. 

if you use a single globe enviroment, when you update some package, it may borke some old project. 

How to create:

it come with python , any version higher than 3.3 is good. 

'''

pip list # check what package is installed

python -m venv new_venv # create a new enviroment , -m means run a moudle

new_venv\Scripts\activate.bat #activate enviroment

where python # check if it is activated

pip list 

pip freeze # same as pip list but give in a correct form for txt file

#export this to a txt file. (new_venv.txt)

pip install packagename #anything install from now will be only for this venv

new_venv\Scripts\deactivate.bat #deactivate current enviroment

rmdir new_venv /s #delete the proejct and virtual enviroment

mkdir Another_Project

python -m venv Another_Project\venv #make a new venv for another proejct

Another_Project\venv\Scripts\activate.bat # activate this enviroment

pip install -r new_venv.txt # this will install all the listed package in the txt file

pip list # we should see all the package is installed.  

#now you can start to coding, all the code and resoures stay at root of Another_Project\ . but should not stay under venv.
#the venv is a folder you can totally throw away and rebuild

python -m venv third_venv --system-site-packages # this command will create a new v enviroment that have access to the globe packages 

third_venv\scripts\activate.bat

pip list # now you can see the globe package are included also here

# after this env is activated, any package installed will only be for this project, globle package is not affected

pip list --local # check local list

pip freeze --local 


# to activate an virtual enviroment  in Git Bash, you can use

source /venv/Scripts/activate


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#- python Sys. #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

import sys

print(sys.version)
print(sys.executable)

# to find out where is python in windows

# try 

where python

# or try

python

>>>import sys
>>>sys.executable

# this should find the address of python.



#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
#-#-#-#-#-#-#- VS code  VScode visual studio code. #-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#

#short cut key

#command line ctrl + `

#setting ctrl + ,

{
    "files.maxMemoryForLargeFilesMB": 16384,
    "editor.codeLens": false,
    "workbench.iconTheme": "ayu",
    // Determines which settings editor to use by default.
    //  - ui: Use the settings UI editor.
    //  - json: Use the JSON file editor.
    "workbench.settings.editor": "json",
    "workbench.settings.openDefaultSettings": true,
    "workbench.startupEditor": "newUntitledFile",
    "python.formatting.provider": "black",
    "editor.formatOnSave": false,
    "code-runner.clearPreviousOutput": true,
    "code-runner.showExecutionMessage": false,
    "code-runner.executorMap": {
        "python": "$pythonPath -u $fullFileName"
    }
}

#search anything
#ctrl + shift + p 

#code formating
#alt + shift + f




#-#-#-#-#-#-#-#-#-#-#-#-#- PROEJCT IDEAS PROJECT IDEA PROJECT_IDEA #-#-#-#-#-#-#-#-#-#-#-#-#-

# improve price tracker
# text based advanter
# bill checking , reminding
# opencv tutorial
# sudoku solver, using recersion , OPEN CV + HAND WRITING REGNIGTION
# Odoo connection and data extraction (set up an home odoo)
# 














	

	

   